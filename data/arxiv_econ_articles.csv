id,title,summary,published_date,authors,primary_category
http://arxiv.org/abs/2510.27636v1,Delegate Pricing Decisions to an Algorithm? Experimental Evidence,"We analyze the delegation of pricing by participants, representing firms, to
a collusive, self-learning algorithm in a repeated Bertrand experiment. In the
baseline treatment, participants set prices themselves. In the other
treatments, participants can either delegate pricing to the algorithm at the
beginning of each supergame or receive algorithmic recommendations that they
can override. Participants delegate more when they can override the algorithm's
decisions. In both algorithmic treatments, prices are lower than in the
baseline. Our results indicate that while self-learning pricing algorithms can
be collusive, they can foster competition rather than collusion with
humans-in-the-loop.",2025-10-31 17:07:29+00:00,"['Hans-Theo Normann', 'Nina Rulié', 'Olaf Stypa', 'Tobias Werner']",econ.GN
http://arxiv.org/abs/2510.27625v1,Hiring Intrinsically Motivated Agents: A Principal's Dilemma,"Employers are concerned not only with a prospective worker's ability, but
also their propensity to avoid shirking. This paper proposes a new experimental
framework to study how Principals trade-off measures of ability and prosocial
behavior when ranking Agents for independent jobs. Subjects participate in a
simulated, incentivized job market. In an initial session, subjects are Workers
and generate a database of signals and job results. Managers in subsequent
sessions observe the signals of Worker behavior and ability and job details
before a rank-and-value task, ranking and reporting a value for each Worker for
two distinct jobs. Results highlight Managers' preference for ability over
prosocial behavior on average, especially for Managers in STEM fields. There is
evidence of homophily: the relative value of prosocial behavior is higher for
highly prosocial Managers, compensating for ability or even surpassing it in
value.",2025-10-31 16:53:42+00:00,['Andrew Leal'],econ.GN
http://arxiv.org/abs/2510.27112v1,Targeted Advertising Platforms: Data Sharing and Customer Poaching,"E-commerce platforms are rolling out ambitious targeted advertising
initiatives that rely on merchants sharing customer data with each other via
the platform. Yet current platform designs fail to address participating
merchants' concerns about customer poaching. This paper proposes a model of
designing targeted advertising platforms that incentivizes merchants to
voluntarily share customer data despite poaching concerns. I characterize the
optimal mechanism that maximizes a weighted sum of platform's revenues,
customer engagement and merchants' surplus. In sufficiently large platforms,
the optimal mechanism can be implemented through the design of three markets:
$i)$ selling market, where merchants can sell all their data at a posted price
$p$, $ii)$ exchange market, where merchants share all their data in exchange
for high click-through rate (CTR) ads, and $iii)$ buying market, where
high-value merchants buy high CTR ads at the full price. The model is broad in
scope with applications in other market design settings like the greenhouse gas
credit markets and reallocating public resources, and points toward new
directions in combinatorial market exchange designs.",2025-10-31 02:25:36+00:00,['Klajdi Hoxha'],econ.TH
http://arxiv.org/abs/2510.27008v1,Algorithmic Predation: Equilibrium Analysis in Dynamic Oligopolies with Smooth Market Sharing,"Predatory pricing -- where a firm strategically lowers prices to undermine
competitors -- is a contentious topic in dynamic oligopoly theory, with
scholars debating practical relevance and the existence of predatory
equilibria. Although finite-horizon dynamic models have long been proposed to
capture the strategic intertemporal incentives of oligopolists, the existence
and form of equilibrium strategies in settings that allow for firm exit
(drop-outs following loss-making periods) have remained an open question. We
focus on the seminal dynamic oligopoly model by Selten (1965) that introduces
the subgame perfect equilibrium and analyzes smooth market sharing. Equilibrium
can be derived analytically in models that do not allow for dropouts, but not
in models that can lead to predatory pricing. In this paper, we leverage recent
advances in deep reinforcement learning to compute and verify equilibria in
finite-horizon dynamic oligopoly games. Our experiments reveal two key
findings: first, state-of-the-art deep reinforcement learning algorithms
reliably converge to equilibrium in both perfect- and imperfect-information
oligopoly models; second, when firms face asymmetric cost structures, the
resulting equilibria exhibit predatory pricing behavior. These results
demonstrate that predatory pricing can emerge as a rational equilibrium
strategy across a broad variety of model settings. By providing equilibrium
analysis of finite-horizon dynamic oligopoly models with drop-outs, our study
answers a decade-old question and offers new insights for competition
authorities and regulators.",2025-10-30 21:14:24+00:00,"['Fabian Raoul Pieroth', 'Ole Petersen', 'Martin Bichler']",cs.GT
http://arxiv.org/abs/2510.26957v1,Predicting Household Water Consumption Using Satellite and Street View Images in Two Indian Cities,"Monitoring household water use in rapidly urbanizing regions is hampered by
costly, time-intensive enumeration methods and surveys. We investigate whether
publicly available imagery-satellite tiles, Google Street View (GSV)
segmentation-and simple geospatial covariates (nightlight intensity, population
density) can be utilized to predict household water consumption in
Hubballi-Dharwad, India. We compare four approaches: survey features
(benchmark), CNN embeddings (satellite, GSV, combined), and GSV semantic maps
with auxiliary data. Under an ordinal classification framework, GSV
segmentation plus remote-sensing covariates achieves 0.55 accuracy for water
use, approaching survey-based models (0.59 accuracy). Error analysis shows high
precision at extremes of the household water consumption distribution, but
confusion among middle classes is due to overlapping visual proxies. We also
compare and contrast our estimates for household water consumption to that of
household subjective income. Our findings demonstrate that open-access imagery,
coupled with minimal geospatial data, offers a promising alternative to
obtaining reliable household water consumption estimates using surveys in urban
analytics.",2025-10-30 19:32:34+00:00,"['Qiao Wang', 'Joseph George']",cs.LG
http://arxiv.org/abs/2510.26783v1,A Unified Theory for Causal Inference: Direct Debiased Machine Learning via Bregman-Riesz Regression,"This note introduces a unified theory for causal inference that integrates
Riesz regression, covariate balancing, density-ratio estimation (DRE), targeted
maximum likelihood estimation (TMLE), and the matching estimator in average
treatment effect (ATE) estimation. In ATE estimation, the balancing weights and
the regression functions of the outcome play important roles, where the
balancing weights are referred to as the Riesz representer, bias-correction
term, and clever covariates, depending on the context. Riesz regression,
covariate balancing, DRE, and the matching estimator are methods for estimating
the balancing weights, where Riesz regression is essentially equivalent to DRE
in the ATE context, the matching estimator is a special case of DRE, and DRE is
in a dual relationship with covariate balancing. TMLE is a method for
constructing regression function estimators such that the leading bias term
becomes zero. Nearest Neighbor Matching is equivalent to Least Squares Density
Ratio Estimation and Riesz Regression.",2025-10-30 17:56:47+00:00,['Masahiro Kato'],stat.ML
http://arxiv.org/abs/2510.26727v1,Neither Consent nor Property: A Policy Lab for Data Law,"This paper makes the opaque data market in the AI economy empirically legible
for the first time, constructing a computational testbed to address a core
epistemic failure: regulators governing a market defined by structural opacity,
fragile price discovery, and brittle technical safeguards that have paralyzed
traditional empirics and fragmented policy. The pipeline begins with multi-year
fieldwork to extract the market's hidden logic, and then embeds these grounded
behaviors into a high-fidelity ABM, parameterized via a novel LLM-based
discrete-choice experiment that captures the preferences of unsurveyable
populations. The pipeline is validated against reality, reproducing observed
trade patterns. This policy laboratory delivers clear, counter-intuitive
results. First, property-style relief is a false promise: ''anonymous-data''
carve-outs expand trade but ignore risk, causing aggregate welfare to collapse
once external harms are priced in. Second, social welfare peaks when the
downstream buyer internalizes the full substantive risk. This least-cost
avoider approach induces efficient safeguards, simultaneously raising welfare
and sustaining trade, and provides a robust empirical foundation for the legal
drift toward two-sided reachability. The contribution is a reproducible
pipeline designed to end the reliance on intuition. It converts qualitative
insight into testable, comparative policy experiments, obsoleting armchair
conjecture by replacing it with controlled evidence on how legal rules actually
shift risk and surplus. This is the forward-looking engine that moves the field
from competing intuitions to direct, computational analysis.",2025-10-30 17:27:03+00:00,"['Haoyi Zhang', 'Tianyi Zhu']",econ.GN
http://arxiv.org/abs/2510.26723v1,Bridging the Gap between Empirical Welfare Maximization and Conditional Average Treatment Effect Estimation in Policy Learning,"The goal of policy learning is to train a policy function that recommends a
treatment given covariates to maximize population welfare. There are two major
approaches in policy learning: the empirical welfare maximization (EWM)
approach and the plug-in approach. The EWM approach is analogous to a
classification problem, where one first builds an estimator of the population
welfare, which is a functional of policy functions, and then trains a policy by
maximizing the estimated welfare. In contrast, the plug-in approach is based on
regression, where one first estimates the conditional average treatment effect
(CATE) and then recommends the treatment with the highest estimated outcome.
This study bridges the gap between the two approaches by showing that both are
based on essentially the same optimization problem. In particular, we prove an
exact equivalence between EWM and least squares over a reparameterization of
the policy class. As a consequence, the two approaches are interchangeable in
several respects and share the same theoretical guarantees under common
conditions. Leveraging this equivalence, we propose a novel regularization
method for policy learning. Our findings yield a convex and computationally
efficient training procedure that avoids the NP-hard combinatorial step
typically required in EWM.",2025-10-30 17:23:40+00:00,['Masahiro Kato'],stat.ML
http://arxiv.org/abs/2510.26636v1,Putting a Price on Immobility: Food Deliveries and Pricing Approaches,"Urban food delivery services have become an integral part of daily life, yet
their mobility and environmental externalities remain poorly addressed by
planners. Most studies neglect whether consumers pay enough to internalize the
broader social costs of these services. This study quantifies the value of
access to and use of food delivery services in Beijing, China, through two
discrete choice experiments. The first measures willingness to accept
compensation for giving up access, with a median value of CNY588 (approximately
USD80). The second captures willingness to pay for reduced waiting time and
improved reliability, showing valuations far exceeding typical delivery fees
(e.g., CNY96.6/hour and CNY4.83/min at work). These results suggest a
substantial consumer surplus and a clear underpricing problem. These findings
highlight the need for urban planning to integrate digital service economies
into pricing and mobility frameworks. We propose a quantity-based pricing model
that targets delivery speed rather than order volume, addressing the primary
source of externalities while maintaining net welfare gains. This approach
offers a pragmatic, equity-conscious strategy to curb delivery-related
congestion, emissions, and safety risks, especially in dense urban cores.",2025-10-30 16:05:21+00:00,"['Runyu Wang', 'Haotian Zhong']",econ.GN
http://arxiv.org/abs/2510.26857v1,Political Power and Mortality: Heterogeneous Effects of the U.S. Voting Rights Act,"We study the health consequences of redistributing political power through
the 1975 extension of the Voting Rights Act, which eliminated barriers to
voting for previously disenfranchised nonwhite populations. The intervention
led to broad declines in under-five mortality but sharply contrasting effects
in other age groups: mortality fell among non-white children, younger adults,
and older women, yet rose among whites and older non-white men. These
differences cannot be reconciled by changes in population composition or
material conditions. Instead, we present evidence suggesting psychosocial
stress and retaliatory responses arising from perceived status threat as key
mechanisms.",2025-10-30 16:00:02+00:00,"['Atheendar Venkataramani', ""Rourke O'Brien"", 'Elizabeth Bair', 'Christopher Lowenstein']",econ.GN
http://arxiv.org/abs/2510.26613v1,Tests of exogeneity in duration models with censored data,"Consider the setting in which a researcher is interested in the causal effect
of a treatment $Z$ on a duration time $T$, which is subject to right censoring.
We assume that $T=\varphi(X,Z,U)$, where $X$ is a vector of baseline
covariates, $\varphi(X,Z,U)$ is strictly increasing in the error term $U$ for
each $(X,Z)$ and $U\sim \mathcal{U}[0,1]$. Therefore, the model is
nonparametric and nonseparable. We propose nonparametric tests for the
hypothesis that $Z$ is exogenous, meaning that $Z$ is independent of $U$ given
$X$. The test statistics rely on an instrumental variable $W$ that is
independent of $U$ given $X$. We assume that $X,W$ and $Z$ are all categorical.
Test statistics are constructed for the hypothesis that the conditional rank
$V_T= F_{T \mid X,Z}(T \mid X,Z)$ is independent of $(X,W)$ jointly. Under an
identifiability condition on $\varphi$, this hypothesis is equivalent to $Z$
being exogenous. However, note that $V_T$ is censored by $V_C =F_{T \mid X,Z}(C
\mid X,Z)$, which complicates the construction of the test statistics
significantly. We derive the limiting distributions of the proposed tests and
prove that our estimator of the distribution of $V_T$ converges to the uniform
distribution at a rate faster than the usual parametric $n^{-1/2}$-rate. We
demonstrate that the test statistics and bootstrap approximations for the
critical values have a good finite sample performance in various Monte Carlo
settings. Finally, we illustrate the tests with an empirical application to the
National Job Training Partnership Act (JTPA) Study.",2025-10-30 15:39:36+00:00,"['Gilles Crommen', 'Ingrid Van Keilegom', 'Jean-Pierre Florens']",econ.EM
http://arxiv.org/abs/2510.26503v1,The sustainability of contribution norms with income dynamics,"The sustainability of cooperation is crucial for understanding the progress
of societies. We study a repeated game in which individuals decide the share of
their income to transfer to other group members. A central feature of our model
is that individuals may, with some probability, switch incomes across periods,
our measure of income mobility, while the overall income distribution remains
constant over time. We analyze how income mobility and income inequality affect
the sustainability of contribution norms, informal agreements about how much
each member should transfer to the group. We find that greater income mobility
facilitates cooperation. In contrast, the effect of inequality is ambiguous and
depends on the progressivity of the contribution norm and the degree of
mobility. We apply our framework to an optimal taxation problem to examine the
interaction between public and private redistribution.",2025-10-30 13:55:56+00:00,"['Pau Juan-Bartroli', 'Esteban Muñoz-Sobrado']",econ.GN
http://arxiv.org/abs/2510.26470v1,In Defense of the Pre-Test: Valid Inference when Testing Violations of Parallel Trends for Difference-in-Differences,"The difference-in-differences (DID) research design is a key identification
strategy which allows researchers to estimate causal effects under the parallel
trends assumption. While the parallel trends assumption is counterfactual and
cannot be tested directly, researchers often examine pre-treatment periods to
check whether the time trends are parallel before treatment is administered.
Recently, researchers have been cautioned against using preliminary tests which
aim to detect violations of parallel trends in the pre-treatment period. In
this paper, we argue that preliminary testing can -- and should -- play an
important role within the DID research design. We propose a new and more
substantively appropriate conditional extrapolation assumption, which requires
an analyst to conduct a preliminary test to determine whether the severity of
pre-treatment parallel trend violations falls below an acceptable level before
extrapolation to the post-treatment period is justified. This stands in
contrast to prior work which can be interpreted as either setting the
acceptable level to be exactly zero (in which case preliminary tests lack
power) or assuming that extrapolation is always justified (in which case
preliminary tests are not required). Under mild assumptions on how close the
actual violation is to the acceptable level, we provide a consistent
preliminary test as well confidence intervals which are valid when conditioned
on the result of the test. The conditional coverage of these intervals
overcomes a common critique made against the use of preliminary testing within
the DID research design. We use real data as well as numerical simulations to
illustrate the performance of the proposed methods.",2025-10-30 13:21:23+00:00,"['Jonas M. Mikhaeil', 'Christopher Harshaw']",stat.ME
http://arxiv.org/abs/2510.26387v1,Robust Welfare under Imperfect Competition,"We study welfare analysis for policy changes when supply behavior is only
partially known. We augment the robust-demand approach of Kang and Vasserman
(2025) with two supply primitives--intervals of feasible pass-through and
conduct (market-power) parameters--applied to two equilibrium snapshots. A
simple accounting identity distills the supply-side contribution to welfare to
a simple integral expression. From there, we deduce that the bounds are
produced by a single-threshold ""bang-bang"" inverse pass-through function. This,
plus a modification of Kang and Vasserman's (2025) demand-side
characterization, delivers simple bounds for consumer surplus, producer
surplus, tax revenue, total surplus, and deadweight loss. We also study an ad
valorem extension.",2025-10-30 11:30:30+00:00,"['Konstantin von Beringe', 'Mark Whitmeyer']",econ.TH
http://arxiv.org/abs/2510.26263v2,The Effect of Using Popular Mathematical Puzzles on The Mathematical Thinking of Syrian Schoolchildren,"In this paper we provide a good overview of the problems and the background
of mathematics education in Syrian schools. We aimed to study the effect of
using popular mathematical puzzles on the mathematical thinking of
schoolchildren, by conducting a paired experimental study (pre-test and
post-test control group design) of the data we obtained through a sample taken
from students of sixth-grade primary school students in Syria the Lady Mary
School in Syria, in order to evaluate the extent of the impact of popular
mathematical puzzles on students' ability to solve problems and mathematical
skills, and then the skills were measured and the results were analyzed using a
t-test as a tool for statistical analysis.",2025-10-30 08:44:12+00:00,"['Duaa Abdullah', 'Jasem Hamoud']",econ.TH
http://arxiv.org/abs/2510.26106v1,Causal Inference with Groupwise Matching,"This paper examines methods of causal inference based on groupwise matching
when we observe multiple large groups of individuals over several periods. We
formulate causal inference validity through a generalized matching condition,
generalizing the parallel trend assumption in difference-in-differences
designs. We show that difference-in-differences, synthetic control, and
synthetic difference-in-differences designs are distinguished by the specific
matching conditions that they invoke. Through regret analysis, we demonstrate
that difference-in-differences and synthetic control with differencing are
complementary; the former dominates the latter if and only if the latter's
extrapolation error exceeds the former's matching error up to a term vanishing
at the parametric rate. The analysis also reveals that synthetic control with
differencing is equivalent to difference-in-differences when the parallel trend
assumption holds for both the pre-treatment and post-treatment periods. We
develop a statistical inference procedure based on synthetic control with
differencing and present an empirical application demonstrating its usefulness.",2025-10-30 03:31:49+00:00,"['Ratzanyel Rincón', 'Kyungchul Song']",econ.EM
http://arxiv.org/abs/2510.26091v1,TEE-BFT: Pricing the Security of Data Center Execution Assurance,"Blockchains face inherent limitations when communicating outside their own
ecosystem, largely due to the Byzantine Fault Tolerant (BFT) 3f+1 security
model. Trusted Execution Environments (TEEs) are a promising mitigation because
they allow a single trusted broker to interface securely with external systems.
  This paper develops a cost-of-collusion principal-agent model for
compromising a TEE in a Data Center Execution Assurance design. The model
isolates the main drivers of attack profitability: a K-of-n coordination
threshold, independent detection risk q, heterogeneous per-member sanctions
F_i, and a short-window flow prize (omega) proportional to the value secured
(beta times V).
  We derive closed-form deterrence thresholds and a conservative design bound
(V_safe) that make collusion unprofitable under transparent parameter choices.
Calibrations based on time-advantaged arbitrage indicate that plausible TEE
parameters can protect on the order of one trillion dollars in value. The
analysis informs the design of TEE-BFT, a blockchain architecture that combines
BFT consensus with near-stateless TEEs, distributed key generation, and
on-chain attestation to maintain security when interacting with external
systems.",2025-10-30 03:02:39+00:00,"['Alex Shamis', 'Matt Stephenson', 'Linfeng Zhou']",econ.TH
http://arxiv.org/abs/2510.26065v1,Price Levels in Heterogeneous-Agent Models,"We study a model of the Fiscal Theory of the Price Level (FTPL) in a
Bewley-Huggett-Aiyagari framework with heterogeneous agents. The model is set
in continuous time, and ex post heterogeneity arises due to idiosyncratic,
uninsurable income shocks. Such models have a natural interpretation as
mean-field games, introduced by Huang, Caines, and Malham\'e and by Lasry and
Lions. We highlight this connection and discuss the existence and multiplicity
of stationary equilibria in models with and without capital. Our focus is on
the mathematical analysis, and we prove the existence of two equilibria in
which the government runs constant primary deficits, which in turn implies the
existence of multiple price levels.",2025-10-30 01:38:08+00:00,['Felix Höfer'],econ.TH
http://arxiv.org/abs/2510.26051v1,Estimation and Inference in Boundary Discontinuity Designs: Distance-Based Methods,"We study the statistical properties of nonparametric distance-based
(isotropic) local polynomial regression estimators of the boundary average
treatment effect curve, a key causal functional parameter capturing
heterogeneous treatment effects in boundary discontinuity designs. We present
necessary and/or sufficient conditions for identification, estimation, and
inference in large samples, both pointwise and uniformly along the boundary.
Our theoretical results highlight the crucial role played by the ``regularity''
of the boundary (a one-dimensional manifold) over which identification,
estimation, and inference are conducted. Our methods are illustrated with
simulated data. Companion general-purpose software is provided.",2025-10-30 01:03:57+00:00,"['Matias D. Cattaneo', 'Rocio Titiunik', 'Ruiqi', 'Yu']",econ.EM
http://arxiv.org/abs/2510.26035v1,Budget Forecasting and Integrated Strategic Planning for Leaders,"This study explored how advanced budgeting techniques and economic indicators
influence funding levels and strategic alignment in California Community
Colleges (CCCs). Despite widespread implementation of budgeting reforms, many
CCCs continue to face challenges aligning financial planning with institutional
missions, particularly in supporting diversity, equity, and inclusion (DEI)
initiatives. The study used a quantitative correlational design, analyzing 30
years of publicly available economic data, including unemployment rates, GDP
growth, and CPI, in relation to CCC funding trends. Results revealed a strong
positive correlation between GDP growth and CCC funding levels, as well as
between CPI and funding levels, underscoring the predictive value of
macroeconomic indicators in budget planning. These findings emphasize the need
for educational leaders to integrate economic forecasting into budget planning
processes to safeguard institutional effectiveness and sustain programs serving
underrepresented student populations.",2025-10-30 00:22:50+00:00,['Matt Salehi'],q-fin.GN
http://arxiv.org/abs/2510.26030v1,World personal income distribution evolution measured by purchasing power parity exchange rates,"The evolution of global income distribution from 1988 to 2018 is analyzed
using purchasing power parity exchange rates and well-established statistical
distributions. This research proposes the use of two separate distributions to
more accurately represent the overall data, rather than relying on a single
distribution. The global income distribution was fitted to log-normal and gamma
functions, which are standard tools in econophysics. Despite limitations in
data completeness during the early years, the available information covered the
vast majority of the world's population. Probability density function (PDF)
curves enabled the identification of key peaks in the distribution, while
complementary cumulative distribution function (CCDF) curves highlighted
general trends in inequality. Initially, the global income distribution
exhibited a bimodal pattern; however, the growth of middle classes in highly
populated countries such as China and India has driven the transition to a
unimodal distribution in recent years. While single-function fits with gamma or
log-normal distributions provided reasonable accuracy, the bimodal approach
constructed as a sum of log-normal distributions yielded near-perfect fits.",2025-10-30 00:10:35+00:00,"['J. D. A. Islas-García', 'M. del Castillo-Mussot', 'Marcelo B. Ribeiro']",econ.GN
http://arxiv.org/abs/2510.25743v1,Agentic Economic Modeling,"We introduce Agentic Economic Modeling (AEM), a framework that aligns
synthetic LLM choices with small-sample human evidence for reliable econometric
inference. AEM first generates task-conditioned synthetic choices via LLMs,
then learns a bias-correction mapping from task features and raw LLM choices to
human-aligned choices, upon which standard econometric estimators perform
inference to recover demand elasticities and treatment effects.We validate AEM
in two experiments. In a large scale conjoint study with millions of
observations, using only 10% of the original data to fit the correction model
lowers the error of the demand-parameter estimates, while uncorrected LLM
choices even increase the errors. In a regional field experiment, a mixture
model calibrated on 10% of geographic regions estimates an out-of-domain
treatment effect of -65\pm10 bps, closely matching the full human experiment
(-60\pm8 bps).Under time-wise extrapolation, training with only day-one human
data yields -24 bps (95% CI: [-26, -22], p<1e-5),improving over the human-only
day-one baseline (-17 bps, 95% CI: [-43, +9], p=0.2049).These results
demonstrate AEM's potential to improve RCT efficiency and establish a
foundation method for LLM-based counterfactual generation.",2025-10-29 17:46:07+00:00,"['Bohan Zhang', 'Jiaxuan Li', 'Ali Hortaçsu', 'Xiaoyang Ye', 'Victor Chernozhukov', 'Angelo Ni', 'Edward Huang']",econ.EM
http://arxiv.org/abs/2510.25738v1,Walrasian equilibria are almost always finite in number,"We show that in the context of exchange economies defined by aggregate excess
demand functions on the full open price simplex, the generic economy has a
finite number of equilibria. Genericicity is proved also for critical economies
and, in both cases, in the strong sense that it holds for an open dense subset
of economies in the Whitney topology. We use the concept of finite singularity
type from singularity theory. This concept ensures that the number of
equilibria of a map appear only in finite number. We then show that maps of
finite singularity type make up an open and dense subset of all smooth maps and
translate the result to the set of aggregate excess demand functions of an
exchange economy.
  Along the way, we extend the classical results of Sonnenschein-Mantel-Debreu
to aggregate excess demand functions defined on the full open price simplex,
rather than just compact subsets of the simplex.",2025-10-29 17:41:59+00:00,"['Sofia B. S. D. Castro', 'Peter B. Gothen']",econ.TH
http://arxiv.org/abs/2510.25607v1,Inference on Welfare and Value Functionals under Optimal Treatment Assignment,"We provide theoretical results for the estimation and inference of a class of
welfare and value functionals of the nonparametric conditional average
treatment effect (CATE) function under optimal treatment assignment, i.e.,
treatment is assigned to an observed type if and only if its CATE is
nonnegative. For the optimal welfare functional defined as the average value of
CATE on the subpopulation with nonnegative CATE, we establish the $\sqrt{n}$
asymptotic normality of the semiparametric plug-in estimators and provide an
analytical asymptotic variance formula. For more general value functionals, we
show that the plug-in estimators are typically asymptotically normal at the
1-dimensional nonparametric estimation rate, and we provide a consistent
variance estimator based on the sieve Riesz representer, as well as a proposed
computational procedure for numerical integration on submanifolds. The key
reason underlying the different convergence rates for the welfare functional
versus the general value functional lies in that, on the boundary subpopulation
for whom CATE is zero, the integrand vanishes for the welfare functional but
does not for general value functionals. We demonstrate in Monte Carlo
simulations the good finite-sample performance of our estimation and inference
procedures, and conduct an empirical application of our methods on the
effectiveness of job training programs on earnings using the JTPA data set.",2025-10-29 15:16:11+00:00,"['Xiaohong Chen', 'Zhenxiao Chen', 'Wayne Yuan Gao']",econ.EM
http://arxiv.org/abs/2510.25487v1,The Latin Monetary Union and Trade: A Closer Look,"This paper reexamines the effects of the Latin Monetary Union (LMU) - a 19th
century agreement among several European countries to standardize their
currencies through a bimetallic system based on fixed gold and silver content -
on trade. Unlike previous studies, this paper adopts the latest advances in
gravity modeling and a more rigorous approach to defining the control group by
accounting for the diversity of currency regimes during the early years of the
LMU. My findings suggest that the LMU had a positive effect on trade between
its members until the early 1870s, when bimetallism was still considered a
viable monetary system. These effects then faded, converging to zero. Results
are robust to the inclusion of additional potential confounders, the use of
various samples spanning different countries and trade data sources, and
alternative methodological choices.",2025-10-29 13:08:06+00:00,['Jacopo Timini'],econ.GN
http://arxiv.org/abs/2510.25275v1,New methods to compensate artists in music streaming platforms,"We study the problem of measuring the popularity of artists in music
streaming platforms and the ensuing methods to compensate them (from the
revenues platforms raise by charging users). We uncover the space of popularity
indices upon exploring the implications of several axioms capturing principles
with normative appeal. As a result, we characterize several families of
indices. Some of them are intimately connected to the Shapley value, the
central tool in cooperative game theory. Our characterizations might help to
address the rising concern in the music industry to explore new methods that
reward artists more appropriately. We actually connect our families to the new
royalties models, recently launched by Spotify and Deezer.",2025-10-29 08:36:25+00:00,"['Gustavo Bergantiños', 'Juan D. Moreno-Ternero']",econ.TH
http://arxiv.org/abs/2510.25066v1,Frequentist Persuasion,"A sender persuades a strategically naive decisionmaker (DM) by committing
privately to an experiment. Sender's choice of experiment is unknown to the DM,
who must form her posterior beliefs nonparametrically by applying some learning
rule to an IID sample of (state, message) realizations.
  We show that, given mild regularity conditions, the empirical payoff
functions hypo-converge to the full-information counterpart. This is sufficient
to ensure that payoffs and optimal signals converge to the Bayesian benchmark.
  For finite sample sizes, the force of this ""sampling friction"" is
nonmonotonic: it can induce more informative experiments than the Bayesian
benchmark in settings like the classic Prosecutor-Judge game, and less
revelation even in situations with perfectly aligned preferences. For many
problems with state-independent preferences, we show that there is an optimal
finite sample size for the DM. Although the DM would always prefer a larger
sample for a fixed experiment, this result holds because the sample size
affects sender's choice of experiment.
  Our results are robust to imperfectly informative feedback and the choice of
learning rule.",2025-10-29 01:09:43+00:00,"['Arnav Sood', 'James Best']",econ.TH
http://arxiv.org/abs/2510.24990v1,The Economics of AI Training Data: A Research Agenda,"Despite data's central role in AI production, it remains the least understood
input. As AI labs exhaust public data and turn to proprietary sources, with
deals reaching hundreds of millions of dollars, research across computer
science, economics, law, and policy has fragmented. We establish data economics
as a coherent field through three contributions. First, we characterize data's
distinctive properties -- nonrivalry, context dependence, and emergent rivalry
through contamination -- and trace historical precedents for market formation
in commodities such as oil and grain. Second, we present systematic
documentation of AI training data deals from 2020 to 2025, revealing persistent
market fragmentation, five distinct pricing mechanisms (from per-unit licensing
to commissioning), and that most deals exclude original creators from
compensation. Third, we propose a formal hierarchy of exchangeable data units
(token, record, dataset, corpus, stream) and argue for data's explicit
representation in production functions. Building on these foundations, we
outline four open research problems foundational to data economics: measuring
context-dependent value, balancing governance with privacy, estimating data's
contribution to production, and designing mechanisms for heterogeneous,
compositional goods.",2025-10-28 21:37:35+00:00,"['Hamidah Oderinwale', 'Anna Kazlauskas']",cs.CY
http://arxiv.org/abs/2510.24923v1,Automation Experiments and Inequality,"An increasingly large number of experiments study the labor productivity
effects of automation technologies such as generative algorithms. A popular
question in these experiments relates to inequality: does the technology
increase output more for high- or low-skill workers? The answer is often used
to anticipate the distributional effects of the technology as it continues to
improve. In this paper, we formalize the theoretical content of this empirical
test, focusing on automation experiments as commonly designed. Worker-level
output depends on a task-level production function, and workers are
heterogeneous in their task-level skills. Workers perform a task themselves, or
they delegate it to the automation technology. The inequality effect of
improved automation depends on the interaction of two factors: ($i$) the
correlation in task-level skills across workers, and ($ii$) workers' skills
relative to the technology's capability. Importantly, the sign of the
inequality effect is often non-monotonic -- as technologies improve, inequality
may decrease then increase, or vice versa. Finally, we use data and theory to
highlight cases when skills are likely to be positively or negatively
correlated. The model generally suggests that the diversity of automation
technologies will play an important role in the evolution of inequality.",2025-10-28 19:52:42+00:00,"['Seth Benzell', 'Kyle Myers']",econ.GN
http://arxiv.org/abs/2510.24916v1,Productivity Beliefs and Efficiency in Science,"We develop a method to estimate producers' productivity beliefs when output
quantities and input prices are unobservable, and we use it to evaluate the
market for science. Our model of researchers' labor supply shows how their
willingness to pay for inputs reveals their productivity beliefs. We estimate
the model's parameters using data from a nationally representative survey of
researchers and find the distribution of productivity to be very skewed. Our
counterfactuals indicate that a more efficient allocation of the current budget
could be worth billions of dollars. There are substantial gains from developing
new ways of identifying talented scientists.",2025-10-28 19:36:59+00:00,"['Fabio Bertolotti', 'Kyle Myers', 'Wei Yang Tham']",econ.GN
http://arxiv.org/abs/2510.24899v1,Estimating Nationwide High-Dosage Tutoring Expenditures: A Predictive Model Approach,"This study applies an optimized XGBoost regression model to estimate
district-level expenditures on high-dosage tutoring from incomplete
administrative data. The COVID-19 pandemic caused unprecedented learning loss,
with K-12 students losing up to half a grade level in certain subjects. To
address this, the federal government allocated \$190 billion in relief. We know
from previous research that small-group tutoring, summer and after school
programs, and increased support staff were all common expenditures for
districts. We don't know how much was spent in each category. Using a custom
scraped dataset of over 7,000 ESSER (Elementary and Secondary School Emergency
Relief) plans, we model tutoring allocations as a function of district
characteristics such as enrollment, total ESSER funding, urbanicity, and school
count. Extending the trained model to districts that mention tutoring but omit
cost information yields an estimated aggregate allocation of approximately
\$2.2 billion. The model achieved an out-of-sample $R^2$=0.358, demonstrating
moderate predictive accuracy given substantial reporting heterogeneity.
Methodologically, this work illustrates how gradient-boosted decision trees can
reconstruct large-scale fiscal patterns where structured data are sparse or
missing. The framework generalizes to other domains where policy evaluation
depends on recovering latent financial or behavioral variables from
semi-structured text and sparse administrative sources.",2025-10-28 19:02:07+00:00,"['Jason Godfrey', 'Trisha Banerjee']",econ.GN
http://arxiv.org/abs/2510.24714v1,Machine-Learning-Assisted Comparison of Regression Functions,"We revisit the classical problem of comparing regression functions, a
fundamental question in statistical inference with broad relevance to modern
applications such as data integration, transfer learning, and causal inference.
Existing approaches typically rely on smoothing techniques and are thus
hindered by the curse of dimensionality. We propose a generalized notion of
kernel-based conditional mean dependence that provides a new characterization
of the null hypothesis of equal regression functions. Building on this
reformulation, we develop two novel tests that leverage modern machine learning
methods for flexible estimation. We establish the asymptotic properties of the
test statistics, which hold under both fixed- and high-dimensional regimes.
Unlike existing methods that often require restrictive distributional
assumptions, our framework only imposes mild moment conditions. The efficacy of
the proposed tests is demonstrated through extensive numerical studies.",2025-10-28 17:59:15+00:00,"['Jian Yan', 'Zhuoxi Li', 'Yang Ning', 'Yong Chen']",stat.ME
http://arxiv.org/abs/2510.24496v1,Panel data models with randomly generated groups,"We develop a structural framework for modeling and inferring unobserved
heterogeneity in dynamic panel-data models. Unlike methods treating clustering
as a descriptive device, we model heterogeneity as arising from a latent
clustering mechanism, where the number of clusters is unknown and estimated.
Building on the mixture of finite mixtures (MFM) approach, our method avoids
the clustering inconsistency issues of Dirichlet process mixtures and provides
an interpretable representation of the population clustering structure. We
extend the Telescoping Sampler of Fruhwirth-Schnatter et al. (2021) to dynamic
panels with covariates, yielding an efficient MCMC algorithm that delivers full
Bayesian inference and credible sets. We show that asymptotically the posterior
distribution of the mixing measure contracts around the truth at parametric
rates in Wasserstein distance, ensuring recovery of clustering and structural
parameters. Simulations demonstrate strong finite-sample performance. Finally,
an application to the income-democracy relationship reveals latent
heterogeneity only when controlling for additional covariates.",2025-10-28 15:12:15+00:00,"['Jean-Pierre Florens', 'Anna Simoni']",econ.EM
http://arxiv.org/abs/2510.24433v1,Nearest Neighbor Matching as Least Squares Density Ratio Estimation and Riesz Regression,"This study proves that Nearest Neighbor (NN) matching can be interpreted as
an instance of Riesz regression for automatic debiased machine learning. Lin et
al. (2023) shows that NN matching is an instance of density-ratio estimation
with their new density-ratio estimator. Chernozhukov et al. (2024) develops
Riesz regression for automatic debiased machine learning, which directly
estimates the Riesz representer (or equivalently, the bias-correction term) by
minimizing the mean squared error. In this study, we first prove that the
density-ratio estimation method proposed in Lin et al. (2023) is essentially
equivalent to Least-Squares Importance Fitting (LSIF) proposed in Kanamori et
al. (2009) for direct density-ratio estimation. Furthermore, we derive Riesz
regression using the LSIF framework. Based on these results, we derive NN
matching from Riesz regression. This study is based on our work Kato (2025a)
and Kato (2025b).",2025-10-28 14:01:51+00:00,['Masahiro Kato'],econ.EM
http://arxiv.org/abs/2510.24388v1,A Characterization of Egalitarian and Proportional Sharing Principles: An Efficient Extension Operator Approach,"Some well-known solutions for cooperative games with transferable utility
(TU-games), such as the Banzhaf value, the Myerson value, and the Aumann-Dreze
value, fail to satisfy efficiency, although they possess other desirable
properties. This paper proposes a new approach to restore efficiency by
extending any underlying solution to an efficient one, through what we call an
efficient extension operator. We consider novel axioms for an efficient
extension operator and characterize the egalitarian surplus sharing method and
the proportional sharing method in a unified manner. These results can be
considered as new justifications for the f-ESS values and the f-PS values
introduced by Funaki and Koriyama (2025), which are generalizations of the
equal surplus sharing value and the proportional sharing value. Our results
offer an additional rationale for the values with an arbitrary underlying
solution. As applications, we develop an efficient-fair extension of the
solutions for the TU-games with communication networks and its variant for
TU-games with coalition structures.",2025-10-28 13:03:44+00:00,"['Yukihiko Funaki', 'Yukio Koriyama', 'Satoshi Nakada']",econ.TH
http://arxiv.org/abs/2510.24362v1,Implicit quantile preferences of the Fed and the Taylor rule,"We study optimal monetary policy when a central bank maximizes a quantile
utility objective rather than expected utility. In our framework, the central
bank's risk attitude is indexed by the quantile index level, providing a
transparent mapping between hawkish/dovish stances and attention to adverse
macroeconomic realizations. We formulate the infinite-horizon problem using a
Bellman equation with the quantile operator. Implementing an Euler-equation
approach, we derive Taylor-rule-type reaction functions. Using an indirect
inference approach, we derive a central bank risk aversion implicit quantile
index. An empirical implementation for the US is outlined based on reduced-form
laws of motion with conditional heteroskedasticity, enabling estimation of the
new monetary policy rule and its dependence on the Fed risk attitudes. The
results reveal that the Fed has mostly a dovish-type behavior but with some
periods of hawkish attitudes.",2025-10-28 12:35:33+00:00,"['Gabriel Montes-Rojas', 'Fernando Toledo', 'Nicolás Bertholet', 'Kevin Corfield']",econ.GN
http://arxiv.org/abs/2510.24344v1,Are They Willing to Participate? A Review on Behavioral Economics Approach to Voters Turnout,"This article investigates the fundamental factors influencing the rate and
manner of Electoral participation with an economic model-based approach. In
this study, the structural parameters affecting people's decision making are
divided into two categories. The first category includes general topics such as
economic and livelihood status, cultural factors and, also, psychological
variables. In this section, given that voters are analyzed within the context
of consumer behavior theory, inflation and unemployment are considered as the
most important economic factors. The second group of factors focuses more on
the type of voting, with emphasis on government performance. Since the
incumbent government and its supportive voters are in a game with two Nash
equilibrium, and also because the voters in most cases are retrospect, the
government seeks to keep its position by a deliberate change in economic
factors, especially inflation and unemployment rates. Finally, to better
understand the issue, a hypothetical example is presented and analyzed in a
developing country in the form of a state-owned populist employment plan.",2025-10-28 12:10:25+00:00,['Mostafa Raeisi Sarkandiz'],econ.GN
http://arxiv.org/abs/2510.24266v1,The Role of Mathematical Folk Puzzles in Developing mathematical Thinking and Problem-Solving Skills,"This paper covers a variety of mathematical folk puzzles, including geometric
(Tangrams, dissection puzzles), logic, algebraic, probability (Monty Hall
Problem, Birthday Paradox), and combinatorial challenges (Eight Queens Puzzle,
Tower of Hanoi). It also explores modern modifications, such as digital and
gamified approaches, to improve student involvement and comprehension.
Furthermore, a novel concept, the ""Minimal Dissection Path Problem for
Polyominoes,"" is introduced and proven, demonstrating that the minimum number
of straight-line cuts required to dissect a polyomino of N squares into its
constituent units is $\mathrm{N}-1$. This problem, along with other puzzles,
offers practical classroom applications that reinforce core mathematical
concepts like area, spatial reasoning, and optimization, making learning both
enjoyable and effective.",2025-10-28 10:25:16+00:00,"['Duaa Abdullah', 'Jasem Hamoud']",econ.TH
http://arxiv.org/abs/2510.24225v1,The Effects of Immigration on Places and People -- Identification and Interpretation,"Most studies on the labor market effects of immigration use repeated
cross-sectional data to estimate the effects of immigration on regions. This
paper shows that such regional effects are composites of effects that address
fundamental questions in the immigration debate but remain unidentified with
repeated cross-sectional data. We provide a unifying empirical framework that
decomposes the regional effects of immigration into their underlying components
and show how these are identifiable from data that track workers over time. Our
empirical application illustrates that such analysis yields a far more
informative picture of immigration's effects on wages, employment, and
occupational upgrading.",2025-10-28 09:36:47+00:00,"['Christian Dustmann', 'Sebastian Otten', 'Uta Schönberg', 'Jan Stuhler']",econ.GN
http://arxiv.org/abs/2510.24174v1,Moment connectedness and driving factors in the energy-food nexus: A time-frequency perspective,"With escalating macroeconomic uncertainty, the risk interlinkages between
energy and food markets have become increasingly complex, posing serious
challenges to global energy and food security. This paper proposes an
integrated framework combining the GJRSK model, the time-frequency
connectedness analysis, and the random forest method to systematically
investigate the moment connectedness within the energy-food nexus and explore
the key drivers of various spillover effects. The results reveal significant
multidimensional risk spillovers with pronounced time variation, heterogeneity,
and crisis sensitivity. Return and skewness connectedness are primarily driven
by short-term spillovers, kurtosis connectedness is more prominent over the
medium term, while volatility connectedness is dominated by long-term dynamics.
Notably, crude oil consistently serves as a central transmitter in diverse
connectedness networks. Furthermore, the spillover effects are influenced by
multiple factors, including macro-financial conditions, oil supply-demand
fundamentals, policy uncertainties, and climate-related shocks, with the core
drivers of connectedness varying considerably across different moments and
timescales. These findings provide valuable insights for the coordinated
governance of energy and food markets, the improvement of multilayered risk
early-warning systems, and the optimization of investment strategies.",2025-10-28 08:29:11+00:00,"['Yun-Shi Dai', 'Peng-Fei Dai', 'Stéphane Goutte', 'Duc Khuong Nguyen', 'Wei-Xing Zhou']",econ.GN
http://arxiv.org/abs/2510.25782v1,Short-Run Multi-Outcome Effects of Nightlife Regulation in San Juan,"I evaluate San Juan, Puerto Rico's late-night alcohol sales ordinance using a
multi-outcome synthetic control that pools economic and public-safety series. I
show that a common-weight estimator clarifies mechanisms under low-rank outcome
structure. I find economically meaningful reallocations in targeted sectors --
restaurants and bars, gasoline and convenience, and hospitality employment --
while late-night public disorder arrests and violent crime show no clear
departures from pre-policy trends. The short post-policy window and small donor
pool limit statistical power; joint conformal and permutation tests do not
reject the null at conventional thresholds. I therefore emphasize effect
magnitudes, temporal persistence, and pre-trend fit over formal significance.
Code and diagnostics are available for replication.",2025-10-28 04:57:47+00:00,['Jorge A. Arroyo'],econ.GN
http://arxiv.org/abs/2510.24002v1,How Does Environmental Information Disclosure Affect Corporate Environmental Performance? Evidence from Chinese A-Share Listed Companies,"Global climate warming and air pollution pose severe threats to economic
development and public safety, presenting significant challenges to sustainable
development worldwide. Corporations, as key players in resource utilization and
emissions, have drawn increasing attention from policymakers, researchers, and
the public regarding their environmental strategies and practices. This study
employs a two-way fixed effects panel model to examine the impact of
environmental information disclosure on corporate environmental performance,
its regional heterogeneity, and the underlying mechanisms. The results
demonstrate that environmental information disclosure significantly improves
corporate environmental performance, with the effect being more pronounced in
areas of high population density and limited green space. These findings
provide empirical evidence supporting the role of environmental information
disclosure as a critical tool for improving corporate environmental practices.
The study highlights the importance of targeted, region-specific policies to
maximize the effectiveness of disclosure, offering valuable insights for
promoting sustainable development through enhanced corporate transparency.",2025-10-28 02:16:13+00:00,['Zehao Lin'],econ.GN
http://arxiv.org/abs/2510.23951v1,Strategic Learning with Asymmetric Rationality,"This paper analyzes the dynamic interaction between a fully rational,
privately informed sender and a boundedly rational, uninformed receiver with
memory constraints. The sender controls the flow of information, while the
receiver designs a decision-making protocol, modeled as a finite-state machine,
that governs how information is interpreted, how internal memory states evolve,
and when and what decisions are made. The receiver must use the limited set of
states optimally, both to learn and to create incentives for the sender to
provide information. We show that behavior patterns such as information
avoidance, opinion polarization, and indecision arise as equilibrium responses
to asymmetric rationality. The model offers an expressive framework for
strategic learning and decision-making in environments with cognitive and
informational asymmetries, with applications to regulatory review and media
distrust.",2025-10-28 00:09:51+00:00,"['Qingmin Liu', 'Yuyang Miao']",econ.TH
http://arxiv.org/abs/2510.23762v1,Control VAR: a counterfactual based approach to inference in macroeconomics,"This paper addresses the challenges of giving a causal interpretation to
vector autoregressions (VARs). I show that under independence assumptions VARs
can identify average treatment effects, average causal responses, or a mix of
the two, depending on the distribution of the policy. But what about situations
in which the economist cannot rely on independence assumptions? I propose an
alternative method, defined as control-VAR, which uses control variables to
estimate causal effects. Control-VAR can estimate average treatment effects on
the treated for dummy policies or average causal responses over time for
continuous policies.
  The advantages of control-based approaches are demonstrated by examining the
impact of natural disasters on the US economy, using Germany as a control.
Contrary to previous literature, the results indicate that natural disasters
have a negative economic impact without any cyclical positive effect. These
findings suggest that control-VARs provide a viable alternative to strict
independence assumptions, offering more credible causal estimates and
significant implications for policy design in response to natural disasters.",2025-10-27 18:45:40+00:00,['Raimondo Pala'],econ.EM
http://arxiv.org/abs/2510.23540v1,The causal interpretation of panel vector autoregressions,"This paper discusses the different contemporaneous causal interpretations of
Panel Vector Autoregressions (PVAR). I show that the interpretation of PVARs
depends on the distribution of the causing variable, and can range from average
treatment effects, to average causal responses, to a combination of the two. If
the researcher is willing to postulate a no residual autocorrelation
assumption, and some units can be thought of as controls, PVAR can identify
average treatment effects on the treated. This method complements the toolkits
already present in the literature, such as staggered-DiD, or LP-DiD, as it
formulates assumptions in the residuals, and not in the outcome variables. Such
a method features a notable advantage: it allows units to be ``sparsely''
treated, capturing the impact of interventions on the innovation component of
the outcome variables. I provide an example related to the evaluation of the
effects of natural disasters economic activity at the weekly frequency in the
US.I conclude by discussing solutions to potential violations of the SUTVA
assumption arising from interference.",2025-10-27 17:14:20+00:00,['Raimondo Pala'],econ.EM
http://arxiv.org/abs/2510.23534v2,Direct Debiased Machine Learning via Bregman Divergence Minimization,"We develop a direct debiased machine learning framework comprising Neyman
targeted estimation and generalized Riesz regression. Our framework unifies
Riesz regression for automatic debiased machine learning, covariate balancing,
targeted maximum likelihood estimation (TMLE), and density-ratio estimation. In
many problems involving causal effects or structural models, the parameters of
interest depend on regression functions. Plugging regression functions
estimated by machine learning methods into the identifying equations can yield
poor performance because of first-stage bias. To reduce such bias, debiased
machine learning employs Neyman orthogonal estimating equations. Debiased
machine learning typically requires estimation of the Riesz representer and the
regression function. For this problem, we develop a direct debiased machine
learning framework with an end-to-end algorithm. We formulate estimation of the
nuisance parameters, the regression function and the Riesz representer, as
minimizing the discrepancy between Neyman orthogonal scores computed with known
and unknown nuisance parameters, which we refer to as Neyman targeted
estimation. Neyman targeted estimation includes Riesz representer estimation,
and we measure discrepancies using the Bregman divergence. The Bregman
divergence encompasses various loss functions as special cases, where the
squared loss yields Riesz regression and the Kullback-Leibler divergence yields
entropy balancing. We refer to this Riesz representer estimation as generalized
Riesz regression. Neyman targeted estimation also yields TMLE as a special case
for regression function estimation. Furthermore, for specific pairs of models
and Riesz representer estimation methods, we can automatically obtain the
covariate balancing property without explicitly solving the covariate balancing
objective.",2025-10-27 17:10:43+00:00,['Masahiro Kato'],econ.EM
http://arxiv.org/abs/2510.23434v1,Choosing What to Learn: Experimental Design when Combining Experimental with Observational Evidence,"Experiments deliver credible but often localized effects, tied to specific
sites, populations, or mechanisms. When such estimates are insufficient to
extrapolate effects for broader policy questions, such as external validity and
general-equilibrium (GE) effects, researchers combine trials with external
evidence from reduced-form or structural observational estimates, or prior
experiments. We develop a unified framework for designing experiments in this
setting: the researcher selects which parameters to identify experimentally
from a feasible set (which treatment arms and/or individuals to include in the
experiment), allocates sample size, and specifies how to weight experimental
and observational estimators. Because observational inputs may be biased in
ways unknown ex ante, we develop a minimax proportional regret objective that
evaluates any candidate design relative to an oracle that knows the bias and
jointly chooses the design and estimator. This yields a transparent
bias-variance trade-off that requires no prespecified bias bound and depends
only on information about the precision of the estimators and the estimand's
sensitivity to the underlying parameters. We illustrate the framework by (i)
designing small-scale cash transfer experiments aimed at estimating GE effects
and (ii) optimizing site selection for microfinance interventions.",2025-10-27 15:37:23+00:00,"['Aristotelis Epanomeritakis', 'Davide Viviano']",econ.EM
http://arxiv.org/abs/2510.23421v1,Exploring Vulnerability in AI Industry,"The rapid ascent of Foundation Models (FMs), enabled by the Transformer
architecture, drives the current AI ecosystem. Characterized by large-scale
training and downstream adaptability, FMs (as GPT family) have achieved massive
public adoption, fueling a turbulent market shaped by platform economics and
intense investment. Assessing the vulnerability of this fast-evolving industry
is critical yet challenging due to data limitations. This paper proposes a
synthetic AI Vulnerability Index (AIVI) focusing on the upstream value chain
for FM production, prioritizing publicly available data. We model FM output as
a function of five inputs: Compute, Data, Talent, Capital, and Energy,
hypothesizing that supply vulnerability in any input threatens the industry.
Key vulnerabilities include compute concentration, data scarcity and legal
risks, talent bottlenecks, capital intensity and strategic dependencies, as
well as escalating energy demands. Acknowledging imperfect input
substitutability, we propose a weighted geometrical average of aggregate
subindexes, normalized using theoretical or empirical benchmarks. Despite
limitations and room for improvement, this preliminary index aims to quantify
systemic risks in AI's core production engine, and implicitly shed a light on
the risks for downstream value chain.",2025-10-27 15:26:40+00:00,"['Claudio Pirrone', 'Stefano Fricano', 'Gioacchino Fazio']",econ.GN
http://arxiv.org/abs/2510.23347v1,Macroeconomic Forecasting for the G7 countries under Uncertainty Shocks,"Accurate macroeconomic forecasting has become harder amid geopolitical
disruptions, policy reversals, and volatile financial markets. Conventional
vector autoregressions (VARs) overfit in high dimensional settings, while
threshold VARs struggle with time varying interdependencies and complex
parameter structures. We address these limitations by extending the Sims Zha
Bayesian VAR with exogenous variables (SZBVARx) to incorporate domain-informed
shrinkage and four newspaper based uncertainty shocks such as economic policy
uncertainty, geopolitical risk, US equity market volatility, and US monetary
policy uncertainty. The framework improves structural interpretability,
mitigates dimensionality, and imposes empirically guided regularization. Using
G7 data, we study spillovers from uncertainty shocks to five core variables
(unemployment, real broad effective exchange rates, short term rates, oil
prices, and CPI inflation), combining wavelet coherence (time frequency
dynamics) with nonlinear local projections (state dependent impulse responses).
Out-of-sample results at 12 and 24 month horizons show that SZBVARx outperforms
14 benchmarks, including classical VARs and leading machine learning models, as
confirmed by Murphy difference diagrams, multivariate Diebold Mariano tests,
and Giacomini White predictability tests. Credible Bayesian prediction
intervals deliver robust uncertainty quantification for scenario analysis and
risk management. The proposed SZBVARx offers G7 policymakers a transparent,
well calibrated tool for modern macroeconomic forecasting under pervasive
uncertainty.",2025-10-27 14:01:41+00:00,"['Shovon Sengupta', 'Sunny Kumar Singh', 'Tanujit Chakraborty']",econ.EM
http://arxiv.org/abs/2510.23178v1,Feedback in Dynamic Contests: Theory and Experiment,"We study the effect of interim feedback policies in a dynamic all-pay auction
where two players bid over two stages to win a common-value prize. We show that
sequential equilibrium outcomes are characterized by Cheapest Signal
Equilibria, wherein stage 1 bids are such that one player bids zero while the
other chooses a cheapest bid consistent with some signal. Equilibrium payoffs
for both players are always zero, and the sum of expected total bids equals the
value of the prize. We conduct an experiment with four natural feedback policy
treatments -- full, rank, and two cutoff policies -- and while the bidding
behavior deviates from equilibrium, we fail to reject the hypothesis of no
treatment effect on total bids. Further, stage 1 bids induce sunk costs and
head starts, and we test for the resulting sunk cost and discouragement effects
in stage 2 bidding.",2025-10-27 10:14:33+00:00,"['Sumit Goel', 'Yiqing Yan', 'Jeffrey Zeidel']",econ.TH
http://arxiv.org/abs/2510.22908v1,Bridging Stratification and Regression Adjustment: Batch-Adaptive Stratification with Post-Design Adjustment in Randomized Experiments,"To increase statistical efficiency in a randomized experiment, researchers
often use stratification (i.e., blocking) in the design stage. However,
conventional practices of stratification fail to exploit valuable information
about the predictive relationship between covariates and potential outcomes. In
this paper, I introduce an adaptive stratification procedure for increasing
statistical efficiency when some information is available about the
relationship between covariates and potential outcomes. I show that, in a
paired design, researchers can rematch observations across different batches.
For inference, I propose a stratified estimator that allows for nonparametric
covariate adjustment. I then discuss the conditions under which researchers
should expect gains in efficiency from stratification. I show that
stratification complements rather than substitutes for regression adjustment,
insuring against adjustment error even when researchers plan to use covariate
adjustment. To evaluate the performance of the method relative to common
alternatives, I conduct simulations using both synthetic data and more
realistic data derived from a political science experiment. Results demonstrate
that the gains in precision and efficiency can be substantial.",2025-10-27 01:25:34+00:00,['Zikai Li'],stat.ME
http://arxiv.org/abs/2510.22884v1,"Identification, Estimation, and Inference in Two-Sided Interaction Models","This paper studies a class of models for two-sided interactions, where
outcomes depend on latent characteristics of two distinct agent types. Models
in this class have two core elements: the matching network, which records which
agent pairs interact, and the interaction function, which maps latent
characteristics of these agents to outcomes and determines the role of
complementarities. I introduce the Tukey model, which captures
complementarities with a single interaction parameter, along with two
extensions that allow richer complementarity patterns. First, I establish an
identification trade-off between the flexibility of the interaction function
and the density of the matching network: the Tukey model is identified under
mild conditions, whereas the more flexible extensions require dense networks
that are rarely observed in applications. Second, I propose a cycle-based
estimator for the Tukey interaction parameter and show that it is consistent
and asymptotically normal even when the network is sparse. Third, I use its
asymptotic distribution to construct a formal test of no complementarities.
Finally, an empirical illustration shows that the Tukey model recovers
economically meaningful complementarities.",2025-10-27 00:21:43+00:00,['Federico Crippa'],econ.EM
http://arxiv.org/abs/2510.22864v1,Unifying regression-based and design-based causal inference in time-series experiments,"Time-series experiments, also called switchback experiments or N-of-1 trials,
play increasingly important roles in modern applications in medical and
industrial areas. Under the potential outcomes framework, recent research has
studied time-series experiments from the design-based perspective, relying
solely on the randomness in the design to drive the statistical inference.
Focusing on simpler statistical methods, we examine the design-based properties
of regression-based methods for estimating treatment effects in time-series
experiments. We demonstrate that the treatment effects of interest can be
consistently estimated using ordinary least squares with an appropriately
specified working model and transformed regressors. Our analysis allows for
estimating a diverging number of treatment effects simultaneously, and
establishes the consistency and asymptotic normality of the regression-based
estimators. Additionally, we show that asymptotically, the heteroskedasticity
and autocorrelation consistent variance estimators provide conservative
estimates of the true, design-based variances. Importantly, although our
approach relies on regression, our design-based framework allows for
misspecification of the regression model.",2025-10-26 23:12:17+00:00,"['Zhexiao Lin', 'Peng Ding']",stat.ME
http://arxiv.org/abs/2510.22841v1,Testing for Grouped Patterns in Panel Data Models,"While the literature on grouped patterns in panel data analysis has received
significant attention, little to no results are available on testing for their
presence. We propose using existing tools for testing slope homogeneity in
panels for this purpose. We highlight the key advantages and limitations of the
available testing frameworks under a sequence of doubly local alternatives,
where slopes are divided into dominant and remainder groups, with the size of
the remainder groups and the slopes differences shrinking at a certain rate as
the sample size increases. A Monte Carlo study corroborate our theoretical
findings.",2025-10-26 21:20:02+00:00,"['Antonio Raiola', 'Nazarii Salish']",econ.EM
http://arxiv.org/abs/2510.22828v1,Efficiently Learning Synthetic Control Models for High-dimensional Disaggregated Data,"The Synthetic Control method (SC) has become a valuable tool for estimating
causal effects. Originally designed for single-treated unit scenarios, it has
recently found applications in high-dimensional disaggregated settings with
multiple treated units. However, challenges in practical implementation and
computational efficiency arise in such scenarios. To tackle these challenges,
we propose a novel approach that integrates the Multivariate Square-root Lasso
method into the synthetic control framework. We rigorously establish the
estimation error bounds for fitting the Synthetic Control weights using
Multivariate Square-root Lasso, accommodating high-dimensionality and time
series dependencies. Additionally, we quantify the estimation error for the
Average Treatment Effect on the Treated (ATT). Through simulation studies, we
demonstrate that our method offers superior computational efficiency without
compromising estimation accuracy. We apply our method to assess the causal
impact of COVID-19 Stay-at-Home Orders on the monthly unemployment rate in the
United States at the county level.",2025-10-26 20:43:52+00:00,"['Ye Shen', 'Rui Song', 'Alberto Abadie']",stat.ME
http://arxiv.org/abs/2510.22817v1,Wildfire and house prices: A synthetic control case study of Altadena (Jan 2025),"This study uses the Synthetic Control Method (SCM) to estimate the causal
impact of a January 2025 wildfire on housing prices in Altadena, California. We
construct a 'synthetic' Altadena from a weighted average of peer cities to
serve as a counterfactual; this approach assumes no spillover effects on the
donor pool. The results reveal a substantial negative price effect that
intensifies over time. Over the six months following the event, we estimate an
average monthly loss of $32,125. The statistical evidence for this effect is
nuanced. Based on the robust post-to-pre-treatment RMSPE ratio, the result is
statistically significant at the 10% level (p = 0.0508). In contrast, the
effect is not statistically significant when measured by the average
post-treatment gap (p = 0.3220). This analysis highlights the significant
financial risks faced by communities in fire-prone regions and demonstrates
SCM's effectiveness in evaluating disaster-related economic damages.",2025-10-26 20:16:28+00:00,['Yibo Sun'],econ.GN
http://arxiv.org/abs/2510.23669v2,What Work is AI Actually Doing? Uncovering the Drivers of Generative AI Adoption,"Purpose: The rapid integration of artificial intelligence (AI) systems like
ChatGPT, Claude AI, etc., has a deep impact on how work is done. Predicting how
AI will reshape work requires understanding not just its capabilities, but how
it is actually being adopted. This study investigates which intrinsic task
characteristics drive users' decisions to delegate work to AI systems.
Methodology: This study utilizes the Anthropic Economic Index dataset of four
million Claude AI interactions mapped to O*NET tasks. We systematically scored
each task across seven key dimensions: Routine, Cognitive, Social Intelligence,
Creativity, Domain Knowledge, Complexity, and Decision Making using 35
parameters. We then employed multivariate techniques to identify latent task
archetypes and analyzed their relationship with AI usage. Findings: Tasks
requiring high creativity, complexity, and cognitive demand, but low
routineness, attracted the most AI engagement. Furthermore, we identified three
task archetypes: Dynamic Problem Solving, Procedural & Analytical Work, and
Standardized Operational Tasks, demonstrating that AI applicability is best
predicted by a combination of task characteristics, over individual factors.
Our analysis revealed highly concentrated AI usage patterns, with just 5% of
tasks accounting for 59% of all interactions. Originality: This research
provides the first systematic evidence linking real-world generative AI usage
to a comprehensive, multi-dimensional framework of intrinsic task
characteristics. It introduces a data-driven classification of work archetypes
that offers a new framework for analyzing the emerging human-AI division of
labor.",2025-10-26 19:13:37+00:00,"['Peeyush Agarwal', 'Harsh Agarwal', 'Akshat Rana']",econ.GN
http://arxiv.org/abs/2510.22750v1,Information-Credible Stability in Matching with Incomplete Information,"In this paper, I develop a refinement of stability for matching markets with
incomplete information. I introduce Information-Credible Pairwise Stability
(ICPS), a solution concept in which deviating pairs can use credible, costly
tests to reveal match-relevant information before deciding whether to block. By
leveraging the option value of information, ICPS strictly refines Bayesian
stability, rules out fear-driven matchings, and connects belief-based and
information-based notions of stability. ICPS collapses to Bayesian stability
when testing is uninformative or infeasible and coincides with
complete-information stability when testing is perfect and free. I show that
any ICPS-blocking deviation strictly increases total expected surplus, ensuring
welfare improvement. I also prove that ICPS-stable allocations always exist,
promote positive assortative matching, and are unique when the test power is
sufficiently strong. The framework extends to settings with non-transferable
utility, correlated types, and endogenous or sequential testing.",2025-10-26 16:53:18+00:00,['Kaibalyapati Mishra'],econ.TH
http://arxiv.org/abs/2510.22714v1,Pairwise Difference Representations of Moments: Gini and Generalized Lagrange identities,"We provide pairwise-difference (Gini-type) representations of higher-order
central moments for both general random variables and empirical moments. Such
representations do not require a measure of location. For third and fourth
moments, this yields pairwise-difference representations of skewness and
kurtosis coefficients. We show that all central moments possess such
representations, so no reference to the mean is needed for moments of any
order. This is done by considering i.i.d. replications of the random variables
considered, by observing that central moments can be interpreted as covariances
between a random variable and powers of the same variable, and by giving
recursions which link the pairwise-difference representation of any moment to
lower order ones. Numerical summation identities are deduced. Through a similar
approach, we give analogues of the Lagrange and Binet-Cauchy identities for
general random variables, along with a simple derivation of the classic
Cauchy-Schwarz inequality for covariances. Finally, an application to unbiased
estimation of centered moments is discussed.",2025-10-26 15:16:54+00:00,"['Jean-Marie Dufour', 'Abderrahim Taamouti', 'Meilin Tong']",stat.ME
http://arxiv.org/abs/2510.22411v1,"Politics, Inequality, and the Robustness of Shared Infrastructure Systems","Our infrastructure systems enable our well-being by allowing us to move,
store, and transform materials and information given considerable social and
environmental variation. Critically, this ability is shaped by the degree to
which society invests in infrastructure, a fundamentally political question in
large public systems. There, infrastructure providers are distinguished from
users through political processes, such as elections, and there is considerable
heterogeneity among users. Previous political economic models have not taken
into account (i) dynamic infrastructures, (ii) dynamic user preferences, and
(iii) alternatives to rational actor theory. Meanwhile, engineering often
neglects politics. We address these gaps with a general dynamic model of shared
infrastructure systems that incorporates theories from political economy,
social-ecological systems, and political psychology. We use the model to
develop propositions on how multiple characteristics of the political process
impact the robustness of shared infrastructure systems to capacity shocks and
unequal opportunity for private infrastructure investment. Under user fees,
inequality decreases robustness, but taxing private infrastructure use can
increase robustness if non-elites have equal political influence. Election
cycle periods have a nonlinear effect where increasing them increases
robustness up to a point but decreases robustness beyond that point. Further,
there is a negative relationship between the ideological sensitivity of
candidates and robustness. Overall, the biases of voters and candidates
(whether they favor tax increases or decreases) mediate these
political-economic effects on robustness because biases may or may not match
the reality of system needs (whether system recovery requires tax increases).",2025-10-25 19:12:18+00:00,"['Adam Wiechman', 'John M. Anderies', 'Margaret Garcia']",econ.TH
http://arxiv.org/abs/2510.22399v1,Estimating unrestricted spatial interdependence in panel spatial autoregressive models with latent common factors,"We develop a new Bayesian approach to estimating panel spatial autoregressive
models with a known number of latent common factors, where N, the number of
cross-sectional units, is much larger than T, the number of time periods.
Without imposing any a priori structures on the spatial linkages between
variables, we let the data speak for themselves. Extensive Monte Carlo studies
show that our method is super-fast and our estimated spatial weights matrices
and common factors strongly resemble their true counterparts. As an
illustration, we examine the spatial interdependence of regional gross value
added (GVA) growth rates across the European Union (EU). In addition to
revealing the clear presence of predominant country-level clusters, our results
indicate that only a small portion of the variation in the data is explained by
the latent shocks that are uncorrelated with the explanatory variables.",2025-10-25 18:51:21+00:00,"['Deborah Gefang', 'Stephen G Hall', 'George S. Tavlas']",econ.EM
http://arxiv.org/abs/2510.24781v1,Dual-Channel Technology Diffusion: Spatial Decay and Network Contagion in Supply Chain Networks,"This paper develops a dual-channel framework for analyzing technology
diffusion that integrates spatial decay mechanisms from continuous functional
analysis with network contagion dynamics from spectral graph theory. Building
on our previous studies, which establish Navier-Stokes-based approaches to
spatial treatment effects and financial network fragility, we demonstrate that
technology adoption spreads simultaneously through both geographic proximity
and supply chain connections. Using comprehensive data on six technologies
adopted by 500 firms over 2010-2023, we document three key findings. First,
technology adoption exhibits strong exponential geographic decay with spatial
decay rate $\kappa \approx 0.043$ per kilometer, implying a spatial boundary of
$d^* \approx 69$ kilometers beyond which spillovers are negligible (R-squared =
0.99). Second, supply chain connections create technology-specific networks
whose algebraic connectivity ($\lambda_2$) increases 300-380 percent as
adoption spreads, with correlation between $\lambda_2$ and adoption exceeding
0.95 across all technologies. Third, traditional difference-in-differences
methods that ignore spatial and network structure exhibit 61 percent bias in
estimated treatment effects. An event study around COVID-19 reveals that
network fragility increased 24.5 percent post-shock, amplifying treatment
effects through supply chain spillovers in a manner analogous to financial
contagion documented in our recent study. Our framework provides
micro-foundations for technology policy: interventions have spatial reach of 69
kilometers and network amplification factor of 10.8, requiring coordinated
geographic and supply chain targeting for optimal effectiveness.",2025-10-25 16:46:34+00:00,['Tatsuru Kikuchi'],econ.EM
http://arxiv.org/abs/2510.22347v1,Distributionally Robust Dynamic Structural Estimation: Serial Dependence and Sensitivity Analysis,"Distributional assumptions that discipline serially correlated latent
variables play a central role in dynamic structural models. We propose a
framework to quantify the sensitivity of scalar parameters of interest (e.g.,
welfare, elasticity) to such distributional assumptions. We derive bounds on
the scalar parameter by perturbing a reference distribution, while imposing a
stationarity condition for time-homogeneous models or a Markovian condition for
time-inhomogeneous models. The bounds are the solutions to optimization
problems, for which we derive a computationally tractable dual formulation. We
establish consistency, convergence rate, and asymptotic distribution for the
estimator of the bounds. We demonstrate the approach with two applications: an
infinite-horizon dynamic demand model for new cars in the United Kingdom,
Germany, and France, and a finite-horizon dynamic labor supply model for taxi
drivers in New York City. In the car application, perturbed price elasticities
deviate by at most 15.24% from the reference elasticities, while perturbed
estimates of consumer surplus from an additional $3,000 electric vehicle
subsidy vary by up to 102.75%. In the labor supply application, the perturbed
Frisch labor supply elasticity deviates by at most 76.83% for weekday drivers
and 42.84% for weekend drivers.",2025-10-25 16:08:33+00:00,['Ertian Chen'],econ.EM
http://arxiv.org/abs/2510.22294v1,There's Nothing in the Air,"Why do wages grow faster in bigger cities? We use French administrative data
to decompose the urban wage growth premium and find that the answer has
surprisingly little to do with cities themselves. While we document
substantially faster wage growth in larger cities, 80% of the premium
disappears after controlling for the composition of firms and coworkers. We
also document significantly higher job-to-job transition rates in larger
cities, suggesting workers climb the job ladder faster. Most strikingly, when
we focus on workers who remain in the same job -- eliminating the job ladder
mechanism -- the urban wage growth premium falls by 94.1% after accounting for
firms and coworkers. The residual effect is statistically indistinguishable
from zero. These results challenge the view that cities generate human capital
spillovers ``in the air,'' suggesting instead that urban wage dynamics reflect
the sorting of firms and workers and the pace of job mobility.",2025-10-25 13:38:06+00:00,"['Jacob Adenbaum', 'Fil Babalievsky', 'William Jungerman']",econ.GN
http://arxiv.org/abs/2510.22232v1,Rational Adversaries and the Maintenance of Fragility: A Game-Theoretic Theory of Rational Stagnation,"Cooperative systems often remain in persistently suboptimal yet stable
states. This paper explains such ""rational stagnation"" as an equilibrium
sustained by a rational adversary whose utility follows the principle of
potential loss, $u_{D} = U_{ideal} - U_{actual}$. Starting from the Prisoner's
Dilemma, we show that the transformation $u_{i}' = a\,u_{i} + b\,u_{j}$ and the
ratio of mutual recognition $w = b/a$ generate a fragile cooperation band
$[w_{\min},\,w_{\max}]$ where both (C,C) and (D,D) are equilibria. Extending to
a dynamic model with stochastic cooperative payoffs $R_{t}$ and intervention
costs $(C_{c},\,C_{m})$, a Bellman-style analysis yields three strategic
regimes: immediate destruction, rational stagnation, and intervention
abandonment. The appendix further generalizes the utility to a
reference-dependent nonlinear form and proves its stability under reference
shifts, ensuring robustness of the framework. Applications to social-media
algorithms and political trust illustrate how adversarial rationality can
deliberately preserve fragility.",2025-10-25 09:28:15+00:00,['Daisuke Hirota'],cs.GT
http://arxiv.org/abs/2510.24775v1,Dynamic Spatial Treatment Effects and Network Fragility: Theory and Evidence from European Banking,"This paper develops and empirically implements a continuous functional
framework for analyzing systemic risk in financial networks, building on the
dynamic spatial treatment effect methodology established in our previous
studies. We extend the Navier-Stokes-based approach from our previous studies
to characterize contagion dynamics in the European banking system through the
spectral properties of network evolution operators. Using high-quality
bilateral exposure data from the European Banking Authority Transparency
Exercise (2014-2023), we estimate the causal impact of the COVID-19 pandemic on
network fragility using spatial difference-in-differences methods adapted from
our previous studies. Our empirical analysis reveals that COVID-19 elevated
network fragility, measured by the algebraic connectivity $\lambda_2$ of the
system Laplacian, by 26.9% above pre-pandemic levels (95% CI: [7.4%, 46.5%],
p<0.05), with effects persisting through 2023. Paradoxically, this occurred
despite a 46% reduction in the number of banks, demonstrating that
consolidation increased systemic vulnerability by intensifying
interconnectedness-consistent with theoretical predictions from continuous
spatial dynamics. Our findings validate the key predictions from
\citet{kikuchi2024dynamical}: treatment effects amplify over time through
spatial spillovers, consolidation increases fragility when coupling strength
rises, and systems exhibit structural hysteresis preventing automatic reversion
to pre-shock equilibria. The results demonstrate the empirical relevance of
continuous functional methods for financial stability analysis and provide new
insights for macroprudential policy design. We propose network-based capital
requirements targeting spectral centrality and stress testing frameworks
incorporating diffusion dynamics to address the coupling externalities
identified in our analysis.",2025-10-25 06:59:36+00:00,['Tatsuru Kikuchi'],econ.EM
http://arxiv.org/abs/2510.22086v1,Social preferences or moral concerns: What drives rejections in the Ultimatum game?,"Rejections of positive offers in the Ultimatum Game have been attributed to
different motivations. We show that a model combining social preferences and
moral concerns provides a unifying explanation for these rejections while
accounting for additional evidence. Under the preferences considered, a
positive degree of spite is a necessary and sufficient condition for rejecting
positive offers. This indicates that social preferences, rather than moral
concerns, drive rejection behavior. This does not imply that moral concerns do
not matter. We show that rejection thresholds increase with individuals' moral
concerns, suggesting that morality acts as an amplifier of social preferences.
Using data from van Leeuwen and Alger (2024), we estimate individuals' social
preferences and moral concerns using a finite mixture approach. Consistent with
previous evidence, we identify two types of individuals who reject positive
offers in the Ultimatum Game, but that differ in their Dictator Game behavior.",2025-10-24 23:54:06+00:00,"['Pau Juan-Bartroli', 'José Ignacio Rivero-Wildemauwe']",econ.TH
http://arxiv.org/abs/2510.21959v1,Beliefs about Bots: How Employers Plan for AI in White-Collar Work,"We provide experimental evidence on how employers adjust expectations to
automation risk in high-skill, white-collar work. Using a randomized
information intervention among tax advisors in Germany, we show that firms
systematically underestimate automatability. Information provision raises risk
perceptions, especially for routine-intensive roles. Yet, it leaves short-run
hiring plans unchanged. Instead, updated beliefs increase productivity and
financial expectations with minor wage adjustments, implying within-firm
inequality like limited rent-sharing. Employers also anticipate new tasks in
legal tech, compliance, and AI interaction, and report higher training and
adoption intentions.",2025-10-24 18:36:26+00:00,"['Eduard Brüll', 'Samuel Mäurer', 'Davud Rostam-Afschar']",econ.GN
http://arxiv.org/abs/2510.21943v1,MacroEnergy.jl: A large-scale multi-sector energy system framework,"MacroEnergy.jl (aka Macro) is an open-source framework for multi-sector
capacity expansion modeling and analysis of macro-energy systems. It is written
in Julia and uses the JuMP package to interface with a wide range of
mathematical solvers. It enables researchers and practitioners to design and
analyze energy and industrial systems that span electricity, fuels, bioenergy,
steel, chemicals, and other sectors. The framework is organized around a small
set of sector-agnostic components that can be combined into flexible graph
structures, making it straightforward to extend to new technologies, policies,
and commodities. Its companion packages support decomposition methods and other
advanced techniques, allowing users to scale models across fine temporal and
spatial resolutions. MacroEnergy.jl provides a versatile platform for studying
energy transitions at the detail and scale demanded by modern research and
policy.",2025-10-24 18:16:48+00:00,"['Ruaridh Macdonald', 'Filippo Pecci', 'Luca Bonaldo', 'Jun Wen Law', 'Yu Weng', 'Dharik Mallapragada', 'Jesse Jenkins']",physics.soc-ph
http://arxiv.org/abs/2510.21397v1,Optimal policies for environmental assets under spatial heterogeneity and global awareness,"The aim of this paper is to formulate and study a stochastic model for the
management of environmental assets in a geographical context where in each
place the local authorities take their policy decisions maximizing their own
welfare, hence not cooperating each other. A key feature of our model is that
the welfare depends not only on the local environmental asset, but also on the
global one, making the problem much more interesting but technically much more
complex to study, since strategic interaction among players arise.
  We study the problem first from the $N$-players game perspective and find
open and closed loop Nash equilibria in explicit form. We also study the
convergence of the $N$-players game (when $n\to +\infty$) to a suitable Mean
Field Game whose unique equilibrium is exactly the limit of both the open and
closed loop Nash equilibria found above, hence supporting their meaning for the
game. Then we solve explicitly the problem from the cooperative perspective of
the social planner and compare its solution to the equilibria of the
$N$-players game. Moreover we find the Pigouvian tax which aligns the
decentralized closed loop equilibrium to the social optimum.",2025-10-24 12:38:50+00:00,"['Emmanuelle Augeraud-Véron', 'Daria Ghilli', 'Fausto Gozzi', 'Marta Leocata']",math.OC
http://arxiv.org/abs/2510.21231v1,Scale-robust Auctions,"We study auctions that are robust at any scale, i.e., they can be applied to
sell both expensive and cheap items and achieve the best multiplicative
approximations of the optimal revenue in the worst case. We show that the
optimal mechanism is scale invariant, which randomizes between selling at the
second-price and a 2.45 multiple of the second-price.",2025-10-24 08:07:05+00:00,"['Jason Hartline', 'Aleck Johnsen', 'Yingkai Li']",cs.GT
http://arxiv.org/abs/2510.21178v1,Instance-Adaptive Hypothesis Tests with Heterogeneous Agents,"We study hypothesis testing over a heterogeneous population of strategic
agents with private information. Any single test applied uniformly across the
population yields statistical error that is sub-optimal relative to the
performance of an oracle given access to the private information. We show how
it is possible to design menus of statistical contracts that pair type-optimal
tests with payoff structures, inducing agents to self-select according to their
private information. This separating menu elicits agent types and enables the
principal to match the oracle performance even without a priori knowledge of
the agent type. Our main result fully characterizes the collection of all
separating menus that are instance-adaptive, matching oracle performance for an
arbitrary population of heterogeneous agents. We identify designs where
information elicitation is essentially costless, requiring negligible
additional expense relative to a single-test benchmark, while improving
statistical performance. Our work establishes a connection between proper
scoring rules and menu design, showing how the structure of the hypothesis test
constrains the elicitable information. Numerical examples illustrate the
geometry of separating menus and the improvements they deliver in error
trade-offs. Overall, our results connect statistical decision theory with
mechanism design, demonstrating how heterogeneity and strategic participation
can be harnessed to improve efficiency in hypothesis testing.",2025-10-24 06:00:44+00:00,"['Flora C. Shi', 'Martin J. Wainwright', 'Stephen Bates']",cs.GT
http://arxiv.org/abs/2510.21071v2,"Central Bank Digital Currency, Flight-to-Quality, and Bank-Runs in an Agent-Based Model","We analyse financial stability and welfare impacts associated with the
introduction of a Central Bank Digital Currency (CBDC) in a macroeconomic
agent-based model. The model considers firms, banks, and households interacting
on labour, goods, credit, and interbank markets. Households move their
liquidity from deposits to CBDC based on the perceived riskiness of their
banks. We find that the introduction of CBDC exacerbates bank-runs and may lead
to financial instability phenomena. The effect can be changed by introducing a
limit on CBDC holdings. The adoption of CBDC has little effect on macroeconomic
variables but the interest rate on loans to firms goes up and credit goes down
in a limited way. CBDC leads to a redistribution of wealth from firms and banks
to households with a higher bank default rate. CBDC may have negative welfare
effects, but a bound on holding enables a welfare improvement.",2025-10-24 01:00:52+00:00,"['Emilio Barucci', 'Andrea Gurgone', 'Giulia Iori', 'Michele Azzone']",econ.GN
http://arxiv.org/abs/2510.23628v1,Matchings Under Biased and Correlated Evaluations,"We study a two-institution stable matching model in which candidates from two
distinct groups are evaluated using partially correlated signals that are
group-biased. This extends prior work (which assumes institutions evaluate
candidates in an identical manner) to a more realistic setting in which
institutions rely on overlapping, but independently processed, criteria. These
evaluations could consist of a variety of informative tools such as
standardized tests, shared recommendation systems, or AI-based assessments with
local noise. Two key parameters govern evaluations: the bias parameter $\beta
\in (0,1]$, which models systematic disadvantage faced by one group, and the
correlation parameter $\gamma \in [0,1]$, which captures the alignment between
institutional rankings. We study the representation ratio, i.e., the ratio of
disadvantaged to advantaged candidates selected by the matching process in this
setting. Focusing on a regime in which all candidates prefer the same
institution, we characterize the large-market equilibrium and derive a
closed-form expression for the resulting representation ratio. Prior work shows
that when $\gamma = 1$, this ratio scales linearly with $\beta$. In contrast,
we show that the representation ratio increases nonlinearly with $\gamma$ and
even modest losses in correlation can cause sharp drops in the representation
ratio. Our analysis identifies critical $\gamma$-thresholds where institutional
selection behavior undergoes discrete transitions, and reveals structural
conditions under which evaluator alignment or bias mitigation are most
effective. Finally, we show how this framework and results enable interventions
for fairness-aware design in decentralized selection systems.",2025-10-24 00:33:52+00:00,"['Amit Kumar', 'Nisheeth K. Vishnoi']",physics.soc-ph
http://arxiv.org/abs/2510.20996v2,SLIM: Stochastic Learning and Inference in Overidentified Models,"We propose SLIM (Stochastic Learning and Inference in overidentified Models),
a scalable stochastic approximation framework for nonlinear GMM. SLIM forms
iterative updates from independent mini-batches of moments and their
derivatives, producing unbiased directions that ensure almost-sure convergence.
It requires neither a consistent initial estimator nor global convexity and
accommodates both fixed-sample and random-sampling asymptotics. We further
develop an optional second-order refinement achieving full-sample GMM
efficiency and inference procedures based on random scaling and plug-in
methods, including plug-in, debiased plug-in, and online versions of the
Sargan--Hansen $J$-test tailored to stochastic learning. In Monte Carlo
experiments based on a nonlinear demand system with 576 moment conditions, 380
parameters, and $n = 10^5$, SLIM solves the model in under 1.4 hours, whereas
full-sample GMM in Stata on a powerful laptop converges only after 18 hours.
The debiased plug-in $J$-test delivers satisfactory finite-sample inference,
and SLIM scales smoothly to $n = 10^6$.",2025-10-23 20:50:35+00:00,"['Xiaohong Chen', 'Min Seong Kim', 'Sokbae Lee', 'Myung Hwan Seo', 'Myunghyun Song']",econ.EM
http://arxiv.org/abs/2510.20992v1,Urban Planning in 3D with a Two-tier LUTI model,"The two-tier Lowry model brings dynamic simulations of population and
employment directly into the planning process. By linking regional modelling
with neighbourhood design, the framework enables planners to explore how
alternative planning scenarios may evolve over time. The upper tier captures
regional flows of people, jobs, and services, while the lower tier allocates
these to fine-grain zones such as neighbourhoods or parcels. Implemented in
CityEngine, the approach allows interactive visualisation and evaluation of
multi-scale scenarios. A case study in South Yorkshire (UK) illustrates how
regional forecasts can be translated into local design responses, connecting
quantitative modelling with 3D spatial planning.",2025-10-23 20:42:31+00:00,"['Flora Roumpani', 'Joel Dearden', 'Alan Wilson']",physics.soc-ph
http://arxiv.org/abs/2510.20986v1,Constrained Mediation: Bayesian Implementability of Joint Posteriors,"We examine information structures in settings with privately informed agents
and an informationally constrained mediator who supplies additional public
signals. Our focus is on characterizing the set of posteriors that the mediator
can induce. To this end, we employ a graph-theoretic framework: states are
represented as vertices, information sets correspond to edges, and a likelihood
ratio function on edges encodes the posterior beliefs. Within this framework,
we derive necessary and sufficient conditions, internal and external
consistency, for the rationalization of posteriors. Finally, we identify
conditions under which a single mediator can implement multiple posteriors,
effectively serving as a generator of Blackwell experiments.",2025-10-23 20:26:28+00:00,"['David Lagziel', 'Ehud Lehrer']",econ.TH
http://arxiv.org/abs/2510.20921v1,Discrete Screening,"We consider a principal who wishes to screen an agent with \emph{discrete}
types by offering a menu of \emph{discrete} quantities and \emph{discrete}
transfers. We assume that the principal's valuation is discrete strictly
concave and use a discrete first-order approach. We model the agent's cost
types as non-integer, with integer types as a limit case. Our modeling of cost
types allows us to replicate the typical constraint-simplification results and
thus to emulate the well-treaded steps of screening under a continuum of
contracts.
  We show that the solutions to the discrete F.O.C.s need not be unique
\textit{even under discrete strict concavity}, but we also show that there
cannot be more than two optimal contract quantities for each type, and that --
if there are two -- they must be adjacent. Moreover, we can only ensure weak
monotonicity of the quantities \textit{even if virtual costs are strictly
monotone}, unless we limit the ``degree of concavity'' of the principal's
utility. Our discrete screening approach facilitates the use of
rationalizability to solve the screening problem. We introduce a
rationalizability notion featuring robustness with respect to an open set of
beliefs over types called \textit{$\Delta$-O Rationalizability}, and show that
the set of $\Delta$-O rationalizable menus coincides with the set of usual
optimal contracts -- possibly augmented to include irrelevant contracts.",2025-10-23 18:19:46+00:00,"['Alejandro Francetich', 'Burkhard C. Schipper']",econ.TH
http://arxiv.org/abs/2510.20918v1,Rationalizable Screening and Disclosure under Unawareness,"We analyze a principal-agent procurement problem in which the principal (she)
is unaware some of the marginal cost types of the agent (he). Communication
arises naturally as some types of the agent may have an incentive to raise the
principal's awareness (totally or partially) before a contract menu is offered.
The resulting menu must not only reflect the principal's change in awareness,
but also her learning about types from the agent's decision to raise her
awareness in the first place. We capture this reasoning in a discrete concave
model via a rationalizability procedure in which marginal beliefs over types
are restricted to log-concavity, ``reverse'' Bayesianism, and mild assumptions
of caution.
  We show that if the principal is ex ante only unaware of high-cost types, all
of these types have an incentive raise her awareness of them -- otherwise, they
would not be served. With three types, the two lower-cost types that the
principal is initially aware of also want to raise her awareness of the
high-cost type: Their quantities suffer no additional distortions and they both
earn an extra information rent. Intuitively, the presence of an even higher
cost type makes the original two look better. With more than three types, we
show that this intuition may break down for types of whom the principal is
initially aware of so that raising the principal's awareness could cease to be
profitable for those types. When the principal is ex ante only unaware of more
efficient (low-cost) types, then \textit{no type} raises her awareness, leaving
her none the wiser.",2025-10-23 18:15:38+00:00,"['Alejandro Francetich', 'Burkhard C. Schipper']",econ.TH
http://arxiv.org/abs/2510.20907v1,The Economics of Convex Function Intervals,"We introduce convex function intervals (CFIs): families of convex functions
satisfying given level and slope constraints. CFIs naturally arise as
constraint sets in economic design, including problems with type-dependent
participation constraints and two-sided (weak) majorization constraints. Our
main results include: (i) a geometric characterization of the extreme points of
CFIs; (ii) sufficient optimality conditions for linear programs over CFIs; and
(iii) methods for nested optimization on their lower level boundary that can be
applied, e.g., to the optimal design of outside options. We apply these results
to four settings: screening and delegation problems with type-dependent outside
options, contest design with limited disposal, and mean-based persuasion with
informativeness constraints. We draw several novel economic implications using
our tools. For instance, we show that better outside options lead to larger
delegation sets, and that posted price mechanisms can be suboptimal in the
canonical monopolistic screening problem with nontrivial, type-dependent
participation constraints.",2025-10-23 18:02:21+00:00,"['Victor Augias', 'Lina Uhe']",econ.TH
http://arxiv.org/abs/2510.20748v1,Reinforcement Learning and Consumption-Savings Behavior,"This paper demonstrates how reinforcement learning can explain two puzzling
empirical patterns in household consumption behavior during economic downturns.
I develop a model where agents use Q-learning with neural network approximation
to make consumption-savings decisions under income uncertainty, departing from
standard rational expectations assumptions. The model replicates two key
findings from recent literature: (1) unemployed households with previously low
liquid assets exhibit substantially higher marginal propensities to consume
(MPCs) out of stimulus transfers compared to high-asset households (0.50 vs
0.34), even when neither group faces borrowing constraints, consistent with
Ganong et al. (2024); and (2) households with more past unemployment
experiences maintain persistently lower consumption levels after controlling
for current economic conditions, a ""scarring"" effect documented by Malmendier
and Shen (2024). Unlike existing explanations based on belief updating about
income risk or ex-ante heterogeneity, the reinforcement learning mechanism
generates both higher MPCs and lower consumption levels simultaneously through
value function approximation errors that evolve with experience. Simulation
results closely match the empirical estimates, suggesting that adaptive
learning through reinforcement learning provides a unifying framework for
understanding how past experiences shape current consumption behavior beyond
what current economic conditions would predict.",2025-10-23 17:14:49+00:00,['Brandon Kaplowitz'],econ.GN
http://arxiv.org/abs/2510.20631v1,Bilevel Programming Problems: A view through Set-valued Optimization,"Bilevel programming is one of the very active areas of research with many
real-life applications in economics and engineering. Bilevel problems are
hierarchical problems consisting of lower-level and upper-level problems,
respectively. The leader or the decision-maker for the upper-level problem
decides first, and then the follower or the lower-level decision-maker chooses
his/her strategy. In the case of multiple lower-level solutions, the bilevel
problems are not well defined, and there are many ways to handle such a
situation. One standard way is to put restrictions on the lower level problems
(like strict convexity) so that nonuniqueness does not arise. However, those
restrictions are not viable in many situations. Therefore, there are two
standard formulations, called pessimistic formulations and optimistic
formulations of the upper-level problem. A set-valued formulation has been
proposed and has been studied in the literature. However, the study is limited
to the continuous set-up with the assumption of value attainment, and the
general case has not been considered. In this paper, we focus on the general
case and study the connection among various notions of solution. Our main
findings suggest that the set-valued formulation may not hold any bigger
advantage than the existing optimistic and pessimistic formulation.",2025-10-23 15:04:19+00:00,"['Kuntal Som', 'Thirumulanathan D', 'Joydeep Dutta']",math.OC
http://arxiv.org/abs/2510.20612v1,Black Box Absorption: LLMs Undermining Innovative Ideas,"Large Language Models are increasingly adopted as critical tools for
accelerating innovation. This paper identifies and formalizes a systemic risk
inherent in this paradigm: \textbf{Black Box Absorption}. We define this as the
process by which the opaque internal architectures of LLM platforms, often
operated by large-scale service providers, can internalize, generalize, and
repurpose novel concepts contributed by users during interaction. This
mechanism threatens to undermine the foundational principles of innovation
economics by creating severe informational and structural asymmetries between
individual creators and platform operators, thereby jeopardizing the long-term
sustainability of the innovation ecosystem. To analyze this challenge, we
introduce two core concepts: the idea unit, representing the transportable
functional logic of an innovation, and idea safety, a multidimensional standard
for its protection. This paper analyzes the mechanisms of absorption and
proposes a concrete governance and engineering agenda to mitigate these risks,
ensuring that creator contributions remain traceable, controllable, and
equitable.",2025-10-23 14:43:09+00:00,['Wenjun Cao'],cs.CY
http://arxiv.org/abs/2510.20606v1,Strategic Costs of Perceived Bias in Fair Selection,"Meritocratic systems, from admissions to hiring, aim to impartially reward
skill and effort. Yet persistent disparities across race, gender, and class
challenge this ideal. Some attribute these gaps to structural inequality;
others to individual choice. We develop a game-theoretic model in which
candidates from different socioeconomic groups differ in their perceived
post-selection value--shaped by social context and, increasingly, by AI-powered
tools offering personalized career or salary guidance. Each candidate
strategically chooses effort, balancing its cost against expected reward;
effort translates into observable merit, and selection is based solely on
merit. We characterize the unique Nash equilibrium in the large-agent limit and
derive explicit formulas showing how valuation disparities and institutional
selectivity jointly determine effort, representation, social welfare, and
utility. We further propose a cost-sensitive optimization framework that
quantifies how modifying selectivity or perceived value can reduce disparities
without compromising institutional goals. Our analysis reveals a
perception-driven bias: when perceptions of post-selection value differ across
groups, these differences translate into rational differences in effort,
propagating disparities backward through otherwise ""fair"" selection processes.
While the model is static, it captures one stage of a broader feedback cycle
linking perceptions, incentives, and outcome--bridging rational-choice and
structural explanations of inequality by showing how techno-social environments
shape individual incentives in meritocratic systems.",2025-10-23 14:38:05+00:00,"['L. Elisa Celis', 'Lingxiao Huang', 'Milind Sohoni', 'Nisheeth K. Vishnoi']",cs.GT
http://arxiv.org/abs/2510.26810v1,Emergent Dynamical Spatial Boundaries in Emergency Medical Services: A Navier-Stokes Framework from First Principles,"Emergency medical services (EMS) response times are critical determinants of
patient survival, yet existing approaches to spatial coverage analysis rely on
discrete distance buffers or ad-hoc geographic information system (GIS)
isochrones without theoretical foundation. This paper derives continuous
spatial boundaries for emergency response from first principles using fluid
dynamics (Navier-Stokes equations), demonstrating that response effectiveness
decays exponentially with time: $\tau(t) = \tau_0 \exp(-\kappa t)$, where
$\tau_0$ is baseline effectiveness and $\kappa$ is the temporal decay rate.
Using 10,000 simulated emergency incidents from the National Emergency Medical
Services Information System (NEMSIS), I estimate decay parameters and calculate
critical boundaries $d^*$ where response effectiveness falls below
policy-relevant thresholds. The framework reveals substantial demographic
heterogeneity: elderly populations (85+) experience 8.40-minute average
response times versus 7.83 minutes for younger adults (18-44), with 33.6\% of
poor-access incidents affecting elderly populations despite representing 5.2\%
of the sample. Non-parametric kernel regression validation confirms exponential
decay is appropriate (mean squared error 8-12 times smaller than parametric),
while traditional difference-in-differences analysis validates treatment effect
existence (DiD coefficient = -1.35 minutes, $p < 0.001$). The analysis
identifies vulnerable populations--elderly, rural, and low-income
communities--facing systematically longer response times, informing optimal EMS
station placement and resource allocation to reduce health disparities.",2025-10-23 11:04:32+00:00,['Tatsuru Kikuchi'],stat.AP
http://arxiv.org/abs/2510.20404v1,Identification and Debiased Learning of Causal Effects with General Instrumental Variables,"Instrumental variable methods are fundamental to causal inference when
treatment assignment is confounded by unobserved variables. In this article, we
develop a general nonparametric framework for identification and learning with
multi-categorical or continuous instrumental variables. Specifically, we
propose an additive instrumental variable framework to identify mean potential
outcomes and the average treatment effect with a weighting function. Leveraging
semiparametric theory, we derive efficient influence functions and construct
consistent, asymptotically normal estimators via debiased machine learning.
Extensions to longitudinal data, dynamic treatment regimes, and multiplicative
instrumental variables are further developed. We demonstrate the proposed
method by employing simulation studies and analyzing real data from the Job
Training Partnership Act program.",2025-10-23 10:10:11+00:00,"['Shuyuan Chen', 'Peng Zhang', 'Yifan Cui']",stat.ME
http://arxiv.org/abs/2510.20372v2,Testing Most Influential Sets,"Small subsets of data with disproportionate influence on model outcomes can
have dramatic impacts on conclusions, with a few data points sometimes
overturning key findings. While recent work has developed methods to identify
these most influential sets, no formal theory exists to determine when their
influence reflects genuine problems rather than natural sampling variation. We
address this gap by developing a principled framework for assessing the
statistical significance of most influential sets. Our theoretical results
characterize the extreme value distributions of maximal influence and enable
rigorous hypothesis tests for excessive influence, replacing current ad-hoc
sensitivity checks. We demonstrate the practical value of our approach through
applications across economics, biology, and machine learning benchmarks.",2025-10-23 09:12:29+00:00,"['Lucas Darius Konrad', 'Nikolas Kuschnig']",stat.ML
http://arxiv.org/abs/2510.20863v1,"State capacity, innovation, and endogenous development in Chile","The study explores the evolution of Chile's industrial policy from 1990 to
2022 through the lens of state capacity, innovation and endogenous development.
In a global context where governments are reasserting their role as active
agents of innovation, Chile presents a paradox. It is a stable and open economy
that has expanded investment in science and technology but still struggles to
transform this effort into sustainable capabilities. Drawing on the works of
Mazzucato, Aghion, Howitt, Mokyr, Samuelson and Sampedro, the study integrates
evolutionary economics, public policy and humanist ethics. Using a longitudinal
case study approach and official data, it finds that Chile has improved its
innovation institutions but continues to experience weak coordination, regional
inequality and a fragile culture of knowledge. The research concludes that
achieving inclusive innovation requires adaptive governance and an ethical
vision of innovation as a public good.",2025-10-22 22:40:39+00:00,['Rodrigo Barra Novoa'],econ.GN
http://arxiv.org/abs/2510.20066v1,A Multi-Layer Machine Learning and Econometric Pipeline for Forecasting Market Risk: Evidence from Cryptoasset Liquidity Spillovers,"We study whether liquidity and volatility proxies of a core set of
cryptoassets generate spillovers that forecast market-wide risk. Our empirical
framework integrates three statistical layers: (A) interactions between core
liquidity and returns, (B) principal-component relations linking liquidity and
returns, and (C) volatility-factor projections that capture cross-sectional
volatility crowding. The analysis is complemented by vector autoregression
impulse responses and forecast error variance decompositions (see Granger 1969;
Sims 1980), heterogeneous autoregressive models with exogenous regressors
(HAR-X, Corsi 2009), and a leakage-safe machine learning protocol using
temporal splits, early stopping, validation-only thresholding, and SHAP-based
interpretation. Using daily data from 2021 to 2025 (1462 observations across 74
assets), we document statistically significant Granger-causal relationships
across layers and moderate out-of-sample predictive accuracy. We report the
most informative figures, including the pipeline overview, Layer A heatmap,
Layer C robustness analysis, vector autoregression variance decompositions, and
the test-set precision-recall curve. Full data and figure outputs are provided
in the artifact repository.",2025-10-22 22:36:34+00:00,"['Yimeng Qiu', 'Feihuang Fang']",cs.LG
http://arxiv.org/abs/2510.20032v1,Evaluating Local Policies in Centralized Markets,"We study a policy evaluation problem in centralized markets. We show that the
aggregate impact of any marginal reform, the Marginal Policy Effect (MPE), is
nonparametrically identified using data from a baseline equilibrium, without
additional variation in the policy rule. We achieve this by constructing the
equilibrium-adjusted outcome: a policy-invariant structural object that
augments an agent's outcome with the full equilibrium externality their
participation imposes on others. We show that these externalities can be
constructed using estimands that are already common in empirical work. The MPE
is identified as the covariance between our structural outcome and the reform's
direction, providing a flexible tool for optimal policy targeting and a novel
bridge to the Marginal Treatment Effects literature.",2025-10-22 21:22:04+00:00,"['Dmitry Arkhangelsky', 'Wisse Rutgers']",econ.GN
http://arxiv.org/abs/2510.19799v1,"Integrating Transparent Models, LLMs, and Practitioner-in-the-Loop: A Case of Nonprofit Program Evaluation","Public and nonprofit organizations often hesitate to adopt AI tools because
most models are opaque even though standard approaches typically analyze
aggregate patterns rather than offering actionable, case-level guidance. This
study tests a practitioner-in-the-loop workflow that pairs transparent
decision-tree models with large language models (LLMs) to improve predictive
accuracy, interpretability, and the generation of practical insights. Using
data from an ongoing college-success program, we build interpretable decision
trees to surface key predictors. We then provide each tree's structure to an
LLM, enabling it to reproduce case-level predictions grounded in the
transparent models. Practitioners participate throughout feature engineering,
model design, explanation review, and usability assessment, ensuring that field
expertise informs the analysis at every stage. Results show that integrating
transparent models, LLMs, and practitioner input yields accurate, trustworthy,
and actionable case-level evaluations, offering a viable pathway for
responsible AI adoption in the public and nonprofit sectors.",2025-10-22 17:35:13+00:00,"['Ji Ma', 'Albert Casella']",cs.CY
http://arxiv.org/abs/2510.19672v1,Policy Learning with Abstention,"Policy learning algorithms are widely used in areas such as personalized
medicine and advertising to develop individualized treatment regimes. However,
most methods force a decision even when predictions are uncertain, which is
risky in high-stakes settings. We study policy learning with abstention, where
a policy may defer to a safe default or an expert. When a policy abstains, it
receives a small additive reward on top of the value of a random guess. We
propose a two-stage learner that first identifies a set of near-optimal
policies and then constructs an abstention rule from their disagreements. We
establish fast O(1/n)-type regret guarantees when propensities are known, and
extend these guarantees to the unknown-propensity case via a doubly robust (DR)
objective. We further show that abstention is a versatile tool with direct
applications to other core problems in policy learning: it yields improved
guarantees under margin conditions without the common realizability assumption,
connects to distributionally robust policy learning by hedging against small
data shifts, and supports safe policy improvement by ensuring improvement over
a baseline policy with high probability.",2025-10-22 15:18:29+00:00,"['Ayush Sawarni', 'Jikai Jin', 'Justin Whitehouse', 'Vasilis Syrgkanis']",cs.LG
http://arxiv.org/abs/2510.21843v1,A quality of mercy is not trained: the imagined vs. the practiced in healthcare process-specialized AI development,"In high stakes organizational contexts like healthcare, artificial
intelligence (AI) systems are increasingly being designed to augment complex
coordination tasks. This paper investigates how the ethical stakes of such
systems are shaped by their epistemic framings: what aspects of work they
represent, and what they exclude. Drawing on an embedded study of AI
development for operating room (OR) scheduling at a Canadian hospital, we
compare scheduling-as-imagined in the AI design process: rule-bound,
predictable, and surgeon-centric, with scheduling-as-practiced as a fluid,
patient-facing coordination process involving ethical discretion. We show how
early representational decisions narrowed what the AI could support, resulting
in epistemic foreclosure: the premature exclusion of key ethical dimensions
from system design. Our findings surface the moral consequences of abstraction
and call for a more situated approach to designing healthcare
process-specialized artificial intelligence systems.",2025-10-22 14:48:35+00:00,"['Anand Bhardwaj', 'Samer Faraj']",cs.CY
http://arxiv.org/abs/2510.19630v2,Network Contagion Dynamics in European Banking: A Navier-Stokes Framework for Systemic Risk Assessment,"This paper develops a continuous functional framework for analyzing contagion
dynamics in financial networks, extending the Navier-Stokes-based approach to
network-structured spatial processes. We model financial distress propagation
as a diffusion process on weighted networks, deriving a network diffusion
equation from first principles that predicts contagion decay depends on the
network's algebraic connectivity through the relation $\kappa =
\sqrt{\lambda_2/D}$, where $\lambda_2$ is the second-smallest eigenvalue of the
graph Laplacian and $D$ is the diffusion coefficient. Applying this framework
to European banking data from the EBA stress tests (2018, 2021, 2023), we
estimate interbank exposure networks using maximum entropy methods and track
the evolution of systemic risk through the COVID-19 crisis. Our key finding is
that network connectivity declined by 45\% from 2018 to 2023, implying a 26\%
reduction in the contagion decay parameter. Difference-in-differences analysis
reveals this structural change was driven by regulatory-induced deleveraging of
systemically important banks, which experienced differential asset reductions
of 17\% relative to smaller institutions. The networks exhibit lognormal rather
than scale-free degree distributions, suggesting greater resilience than
previously assumed in the literature. Extensive robustness checks across
parametric and non-parametric estimation methods confirm declining systemic
risk, with cross-method correlations exceeding 0.95. These findings demonstrate
that post-COVID-19 regulatory reforms effectively reduced network
interconnectedness and systemic vulnerability in the European banking system.",2025-10-22 14:27:12+00:00,['Tatsuru Kikuchi'],econ.EM
http://arxiv.org/abs/2510.19511v2,Compensation-based risk-sharing,"This paper studies the mathematical problem of allocating payouts
(compensations) in an endowment contingency fund using a risk-sharing rule that
satisfies full allocation. Besides the participants, an administrator manages
the fund by collecting ex-ante contributions to establish the fund and
distributing ex-post payouts to members. Two types of administrators are
considered. An 'active' administrator both invests in the fund and receives the
payout of the fund when no participant receives a payout. A 'passive'
administrator performs only administrative tasks and neither invests in nor
receives a payout from the fund. We analyze the actuarial fairness of both
compensation-based risk-sharing schemes and provide general conditions under
which fairness is achieved. The results extend earlier work by Denuit and
Robert (2025) and Dhaene and Milevsky (2024), who focused on payouts based on
Bernoulli distributions, by allowing for general non-negative loss
distributions.",2025-10-22 12:05:05+00:00,"['Jan Dhaene', 'Atibhav Chaudhry', 'Ka Chun Cheung', 'Austin Riis-Due']",q-fin.RM
http://arxiv.org/abs/2510.19450v1,Towards a feminist understanding of digital platform work,"The rapid growth of the digital platform economy is transforming labor
markets, offering new employment opportunities with promises of flexibility and
accessibility. However, these benefits often come at the expense of increased
economic exploitation, occupational segregation, and deteriorating working
conditions. Research highlights that algorithmic management disproportionately
impacts marginalized groups, reinforcing gendered and racial inequalities while
deepening power imbalances within capitalist systems. This study seeks to
elucidate the complex nature of digital platform work by drawing on feminist
theories that have historically scrutinized and contested the structures of
power within society, especially in the workplace. It presents a framework
focused on four key dimensions to lay a foundation for future research: (i)
precarity and exploitation, (ii) surveillance and control, (iii) blurring
employment boundaries, and (iv) colonial legacies. It advocates for
participatory research, transparency in platform governance, and structural
changes to promote more equitable conditions for digital platform workers.",2025-10-22 10:27:45+00:00,['Clara Punzi'],cs.CY
http://arxiv.org/abs/2510.19426v1,"Using did_multiplegt_dyn to Estimate Event-Study Effects in Complex Designs: Overview, and Four Examples Based on Real Datasets","The command did_multiplegt_dyn can be used to estimate event-study effects in
complex designs with a potentially non-binary and/or non-absorbing treatment.
This paper starts by providing an overview of the estimators computed by the
command. Then, simulations based on three real datasets are used to demonstrate
the estimators' properties. Finally, the command is used on four real datasets
to estimate event-study effects in complex designs. The first example has a
binary treatment that can turn on an off. The second example has a continuous
absorbing treatment. The third example has a discrete multivalued treatment
that can increase or decrease multiple times over time. The fourth example has
two, binary and absorbing treatments, where the second treatment always happens
after the first.",2025-10-22 09:52:27+00:00,"['Clément de Chaisemartin', 'Diego Ciccia', 'Felix Knau', 'Mélitine Malézieux', 'Doulo Sow', 'David Arboleda', 'Romain Angotti', ""Xavier D'Haultfoeuille"", 'Bingxue Li', 'Henri Fabre', 'Anzony Quispe']",econ.EM
http://arxiv.org/abs/2510.19377v1,Government Transparency Affects Innovation: Evidence from Wireless Products,"Does government transparency affect innovation? I evaluate the launch of a
government database with detailed technical information on the universe of
wireless-enabled products on the U.S. market (N 347 thousand). The results show
the launch approximately doubled the use of new technologies in the following
ten years, an indicator of follow-on innovation. The increase affected both
products in the same and new product classes, suggesting novelty; waned over
several years, potentially due to an increase in secrecy and patenting; and
boosted foreign more than U.S. domestic competitors. These results highlight
the importance of information for private sector innovation.",2025-10-22 08:51:35+00:00,['Šimon Trlifaj'],econ.GN
http://arxiv.org/abs/2510.20854v1,Edgeworth's exact and naturally weighted evolutionary utilitarianism and the happiness of Mr. Pongo,"This article challenges the conventional reading of Francis Ysidro Edgeworth
by reconstructing his intellectual project of unifying the moral sciences
through mathematics. The contribution he made in the first phase of his
writing, culminating in \textit{Mathematical Psychics}, aimed to reconfigure
utilitarianism as an exact science, grounding it in psychophysics and
evolutionary biology. In order to solve the utilitarian problem of maximizing
pleasure for a given set of sentient beings, he modeled individuals as
``quasi-Fechnerian'' functions, which incorporated their capacity for pleasure
as determined by their place in the evolutionary order. The problem of
maximization is solved by distributing means according to the individuals'
capacity for pleasure. His radical anti-egalitarian conclusions did not stem
from an abstract principle of justice, but from the necessity to maximize
welfare among naturally unequal beings. This logic was applied not only to
sentients of different evolutionary orders, such as Mr. Pongo, a famous
gorilla, and humans, but also to human races, sexes, and classes. The system,
in essence, uses the apparent neutrality of science to naturalize and justify
pre-existing social hierarchies. This analysis reveals that the subsequent
surgical removal of his utilitarianism by economists, starting with Schumpeter,
while making his tools palatable, eviscerates his overarching philosophical
system.",2025-10-22 07:41:10+00:00,['Alberto Baccini'],econ.GN
http://arxiv.org/abs/2510.19306v1,Topology of Currencies: Persistent Homology for FX Co-movements: A Comparative Clustering Study,"This study investigates whether Topological Data Analysis (TDA) can provide
additional insights beyond traditional statistical methods in clustering
currency behaviours. We focus on the foreign exchange (FX) market, which is a
complex system often exhibiting non-linear and high-dimensional dynamics that
classical techniques may not fully capture. We compare clustering results based
on TDA-derived features versus classical statistical features using monthly
logarithmic returns of 13 major currency exchange rates (all against the euro).
Two widely-used clustering algorithms, \(k\)-means and Hierarchical clustering,
are applied on both types of features, and cluster quality is evaluated via the
Silhouette score and the Calinski-Harabasz index. Our findings show that
TDA-based feature clustering produces more compact and well-separated clusters
than clustering on traditional statistical features, particularly achieving
substantially higher Calinski-Harabasz scores. However, all clustering
approaches yield modest Silhouette scores, underscoring the inherent difficulty
of grouping FX time series. The differing cluster compositions under TDA vs.
classical features suggest that TDA captures structural patterns in currency
co-movements that conventional methods might overlook. These results highlight
TDA as a valuable complementary tool for analysing financial time series, with
potential applications in risk management where understanding structural
co-movements is crucial.",2025-10-22 07:10:54+00:00,"['Pattravadee de Favereau de Jeneret', 'Ioannis Diamantis']",stat.ML
