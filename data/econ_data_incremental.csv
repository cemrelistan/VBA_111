id,title,published_date,authors,primary_category,summary
http://arxiv.org/abs/2002.05016v1,Bifurcation<s in economic growth model with distributed time delay transformed to ODE,2020-02-12 14:25:28+00:00,"['Luca Guerrini', 'Adam Krawiec', 'Marek Szydlowski']",econ.TH,"We consider the model of economic growth with time delayed investment function. Assuming the investment is time distributed we can use the linear chain trick technique to transform delay differential equation system to equivalent system of ordinary differential system (ODE). The time delay parameter is a mean time delay of gamma distribution. We reduce the system with distribution delay to both three and four-dimensional ODEs. We study the Hopf bifurcation in these systems with respect to two parameters: the time delay parameter and the rate of growth parameter. We derive the results from the analytical as well as numerical investigations. From the former we obtain the sufficient criteria on the existence and stability of a limit cycle solution through the Hopf bifurcation. In numerical studies with the Dana and Malgrange investment function we found two Hopf bifurcations with respect to the rate growth parameter and detect the existence of stable long-period cycles in the economy. We find that depending on the time delay and adjustment speed parameters the range of admissible values of the rate of growth parameter breaks down into three intervals. First we have stable focus, then the limit cycle and again the stable solution with two Hopf bifurcations. Such behaviour appears for some middle interval of admissible range of values of the rate of growth parameter."
http://arxiv.org/abs/2001.06166v2,Comparing School Choice and College Admission Mechanisms By Their Immunity to Strategic Admissions,2020-01-17 06:38:51+00:00,"['Somouaoga Bonkoungou', 'Alexander S. Nesterov']",econ.TH,"Recently dozens of school districts and college admissions systems around the world have reformed their admission rules. As a main motivation for these reforms the policymakers cited strategic flaws of the rules: students had strong incentives to game the system, which caused dramatic consequences for non-strategic students. However, almost none of the new rules were strategy-proof. We explain this puzzle. We show that after the reforms the rules became more immune to strategic admissions: each student received a smaller set of schools that he can get in using a strategy, weakening incentives to manipulate. Simultaneously, the admission to each school became strategy-proof to a larger set of students, making the schools more available for non-strategic students. We also show that the existing explanation of the puzzle due to Pathak and Sönmez (2013) is incomplete."
http://arxiv.org/abs/2001.06052v1,Recovering Network Structure from Aggregated Relational Data using Penalized Regression,2020-01-16 19:43:52+00:00,"['Hossein Alidaee', 'Eric Auerbach', 'Michael P. Leung']",econ.EM,"Social network data can be expensive to collect. Breza et al. (2017) propose aggregated relational data (ARD) as a low-cost substitute that can be used to recover the structure of a latent social network when it is generated by a specific parametric random effects model. Our main observation is that many economic network formation models produce networks that are effectively low-rank. As a consequence, network recovery from ARD is generally possible without parametric assumptions using a nuclear-norm penalized regression. We demonstrate how to implement this method and provide finite-sample bounds on the mean squared error of the resulting estimator for the distribution of network links. Computation takes seconds for samples with hundreds of observations. Easy-to-use code in R and Python can be found at https://github.com/mpleung/ARD."
http://arxiv.org/abs/2002.11583v2,Econometric issues with Laubach and Williams' estimates of the natural rate of interest,2020-02-26 16:08:01+00:00,['Daniel Buncic'],econ.EM,"Holston, Laubach and Williams' (2017) estimates of the natural rate of interest are driven by the downward trending behaviour of 'other factor' $z_{t}$. I show that their implementation of Stock and Watson's (1998) Median Unbiased Estimation (MUE) to determine the size of the $λ_{z}$ parameter which drives this downward trend in $z_{t}$ is unsound. It cannot recover the ratio of interest $λ_{z}=a_{r}σ_{z}/σ_{\tilde{y}}$ from MUE required for the estimation of the full structural model. This failure is due to an 'unnecessary' misspecification in Holston et al.'s (2017) formulation of the Stage 2 model. More importantly, their implementation of MUE on this misspecified Stage 2 model spuriously amplifies the point estimate of $λ_{z}$. Using a simulation experiment, I show that their procedure generates excessively large estimates of $λ_{z}$ when applied to data generated from a model where the true $λ_{z}$ is equal to zero. Correcting the misspecification in their Stage 2 model and the implementation of MUE leads to a substantially smaller $λ_{z}$ estimate, and with this, a more subdued downward trending influence of 'other factor' $z_{t}$ on the natural rate. Moreover, the $λ_{z}$ point estimate is statistically highly insignificant, suggesting that there is no role for 'other factor' $z_{t}$ in this model. I also discuss various other estimation issues that arise in Holston et al.'s (2017) model of the natural rate that make it unsuitable for policy analysis."
http://arxiv.org/abs/2003.05725v1,Electoral systems and international trade policy,2020-03-12 12:12:17+00:00,"['Serkan Kucuksenel', 'Osman Gulseven']",econ.GN,"We develop a simple theoretic game a model to analyze the relationship between electoral sys tems and governments' choice in trade policies. We show that existence of international pressure or foreign lobby changes a government's final decision on trade policy, and trade policy in countries with proportional electoral system is more protectionist than in countries with majoritarian electoral system. Moreover, lobbies pay more to affect the trade policy outcomes in countries with proportional representation systems."
http://arxiv.org/abs/2002.11017v1,A Practical Approach to Social Learning,2020-02-25 16:41:23+00:00,"['Amir Ban', 'Moran Koren']",econ.TH,"Models of social learning feature either binary signals or abstract signal structures often deprived of micro-foundations. Both models are limited when analyzing interim results or performing empirical analysis. We present a method of generating signal structures which are richer than the binary model, yet are tractable enough to perform simulations and empirical analysis. We demonstrate the method's usability by revisiting two classical papers: (1) we discuss the economic significance of unbounded signals Smith and Sorensen (2000); (2) we use experimental data from Anderson and Holt (1997) to perform econometric analysis. Additionally, we provide a necessary and sufficient condition for the occurrence of action cascades."
http://arxiv.org/abs/2002.07147v1,Fair Prediction with Endogenous Behavior,2020-02-18 16:07:25+00:00,"['Christopher Jung', 'Sampath Kannan', 'Changhwa Lee', 'Mallesh M. Pai', 'Aaron Roth', 'Rakesh Vohra']",econ.TH,"There is increasing regulatory interest in whether machine learning algorithms deployed in consequential domains (e.g. in criminal justice) treat different demographic groups ""fairly."" However, there are several proposed notions of fairness, typically mutually incompatible. Using criminal justice as an example, we study a model in which society chooses an incarceration rule. Agents of different demographic groups differ in their outside options (e.g. opportunity for legal employment) and decide whether to commit crimes. We show that equalizing type I and type II errors across groups is consistent with the goal of minimizing the overall crime rate; other popular notions of fairness are not."
http://arxiv.org/abs/2002.00103v6,Estimating Welfare Effects in a Nonparametric Choice Model: The Case of School Vouchers,2020-01-31 23:46:48+00:00,"['Vishal Kamat', 'Samuel Norris']",econ.GN,"We develop new robust discrete choice tools to learn about the average willingness to pay for a price subsidy and its effects on demand given exogenous, discrete variation in prices. Our starting point is a nonparametric, nonseparable model of choice. We exploit the insight that our welfare parameters in this model can be expressed as functions of demand for the different alternatives. However, while the variation in the data reveals the value of demand at the observed prices, the parameters generally depend on its values beyond these prices. We show how to sharply characterize what we can learn when demand is specified to be entirely nonparametric or to be parameterized in a flexible manner, both of which imply that the parameters are not necessarily point identified. We use our tools to analyze the welfare effects of price subsidies provided by school vouchers in the DC Opportunity Scholarship Program. We find that the provision of the status quo voucher and a wide range of counterfactual vouchers of different amounts can have positive and potentially large benefits net of costs. The positive effect can be explained by the popularity of low-tuition schools in the program; removing them from the program can result in a negative net benefit. We also find that various standard logit specifications, in comparison, limit attention to demand functions with low demand for the voucher, which do not capture the large magnitudes of benefits credibly consistent with the data."
http://arxiv.org/abs/2002.03598v1,Markov Switching,2020-02-10 08:29:23+00:00,"['Yong Song', 'Tomasz Woźniak']",econ.EM,"Markov switching models are a popular family of models that introduces time-variation in the parameters in the form of their state- or regime-specific values. Importantly, this time-variation is governed by a discrete-valued latent stochastic process with limited memory. More specifically, the current value of the state indicator is determined only by the value of the state indicator from the previous period, thus the Markov property, and the transition matrix. The latter characterizes the properties of the Markov process by determining with what probability each of the states can be visited next period, given the state in the current period. This setup decides on the two main advantages of the Markov switching models. Namely, the estimation of the probability of state occurrences in each of the sample periods by using filtering and smoothing methods and the estimation of the state-specific parameters. These two features open the possibility for improved interpretations of the parameters associated with specific regimes combined with the corresponding regime probabilities, as well as for improved forecasting performance based on persistent regimes and parameters characterizing them."
http://arxiv.org/abs/2002.08156v5,Lattice structure of the random stable set in many-to-many matching market,2020-02-19 13:10:31+00:00,"['Noelia Juarez', 'Pablo A. Neme', 'Jorge Oviedo']",econ.TH,"For a many-to-many matching market, we study the lattice structure of the set of random stable matchings. We define a partial order on the random stable set and present two intuitive binary operations to compute the least upper bound and the greatest lower bound for each side of the matching market. Then, we prove that with these binary operations the set of random stable matchings forms two dual lattices."
http://arxiv.org/abs/2002.09814v1,Survey Bandits with Regret Guarantees,2020-02-23 03:24:03+00:00,"['Sanath Kumar Krishnamurthy', 'Susan Athey']",cs.LG,"We consider a variant of the contextual bandit problem. In standard contextual bandits, when a user arrives we get the user's complete feature vector and then assign a treatment (arm) to that user. In a number of applications (like healthcare), collecting features from users can be costly. To address this issue, we propose algorithms that avoid needless feature collection while maintaining strong regret guarantees."
http://arxiv.org/abs/2002.07285v5,Double/Debiased Machine Learning for Dynamic Treatment Effects via g-Estimation,2020-02-17 22:32:34+00:00,"['Greg Lewis', 'Vasilis Syrgkanis']",econ.EM,"We consider the estimation of treatment effects in settings when multiple treatments are assigned over time and treatments can have a causal effect on future outcomes or the state of the treated unit. We propose an extension of the double/debiased machine learning framework to estimate the dynamic effects of treatments, which can be viewed as a Neyman orthogonal (locally robust) cross-fitted version of $g$-estimation in the dynamic treatment regime. Our method applies to a general class of non-linear dynamic treatment models known as Structural Nested Mean Models and allows the use of machine learning methods to control for potentially high dimensional state variables, subject to a mean square error guarantee, while still allowing parametric estimation and construction of confidence intervals for the structural parameters of interest. These structural parameters can be used for off-policy evaluation of any target dynamic policy at parametric rates, subject to semi-parametric restrictions on the data generating process. Our work is based on a recursive peeling process, typical in $g$-estimation, and formulates a strongly convex objective at each stage, which allows us to extend the $g$-estimation framework in multiple directions: i) to provide finite sample guarantees, ii) to estimate non-linear effect heterogeneity with respect to fixed unit characteristics, within arbitrary function spaces, enabling a dynamic analogue of the RLearner algorithm for heterogeneous effects, iii) to allow for high-dimensional sparse parameterizations of the target structural functions, enabling automated model selection via a recursive lasso algorithm. We also provide guarantees for data stemming from a single treated unit over a long horizon and under stationarity conditions."
http://arxiv.org/abs/2002.05240v3,The Multiplayer Colonel Blotto Game,2020-02-12 21:22:33+00:00,"['Enric Boix-Adserà', 'Benjamin L. Edelman', 'Siddhartha Jayanti']",cs.GT,"We initiate the study of the natural multiplayer generalization of the classic continuous Colonel Blotto game. The two-player Blotto game, introduced by Borel as a model of resource competition across $n$ simultaneous fronts, has been studied extensively for a century and seen numerous applications throughout the social sciences. Our work defines the multiplayer Colonel Blotto game and derives Nash equilibria for various settings of $k$ (number of players) and $n$. We also introduce a ""Boolean"" version of Blotto that becomes interesting in the multiplayer setting. The main technical difficulty of our work, as in the two-player theoretical literature, is the challenge of coupling various marginal distributions into a joint distribution satisfying a strict sum constraint. In contrast to previous works in the continuous setting, we derive our couplings algorithmically in the form of efficient sampling algorithms."
http://arxiv.org/abs/2002.03205v2,Asymptotically Optimal Control of a Centralized Dynamic Matching Market with General Utilities,2020-02-08 17:34:14+00:00,"['Jose H. Blanchet', 'Martin I. Reiman', 'Viragh Shah', 'Lawrence M. Wein', 'Linjia Wu']",math.PR,"We consider a matching market where buyers and sellers arrive according to independent Poisson processes at the same rate and independently abandon the market if not matched after an exponential amount of time with the same mean. In this centralized market, the utility for the system manager from matching any buyer and any seller is a general random variable. We consider a sequence of systems indexed by $n$ where the arrivals in the $n^{\mathrm{th}}$ system are sped up by a factor of $n$. We analyze two families of one-parameter policies: the population threshold policy immediately matches an arriving agent to its best available mate only if the number of mates in the system is above a threshold, and the utility threshold policy matches an arriving agent to its best available mate only if the corresponding utility is above a threshold. Using a fluid analysis of the two-dimensional Markov process of buyers and sellers, we show that when the matching utility distribution is light-tailed, the population threshold policy with threshold $\frac{n}{\ln n}$ is asymptotically optimal among all policies that make matches only at agent arrival epochs. In the heavy-tailed case, we characterize the optimal threshold level for both policies. We also study the utility threshold policy in an unbalanced matching market with heavy-tailed matching utilities and find that the buyers and sellers have the same asymptotically optimal utility threshold. We derive optimal thresholds when the matching utility distribution is exponential, uniform, Pareto, and correlated Pareto. We find that as the right tail of the matching utility distribution gets heavier, the threshold level of each policy (and hence market thickness) increases, as does the magnitude by which the utility threshold policy outperforms the population threshold policy."
http://arxiv.org/abs/2003.11565v2,"The Millennial Boom, the Baby Bust, and the Housing Market",2020-03-25 18:03:45+00:00,"['Marijn A. Bolhuis', 'Judd N. L. Cramer']",econ.GN,"As baby boomers have begun to downsize and retire, their preferences now overlap with millennials' predilection for urban amenities and smaller living spaces. This confluence in tastes between the two largest age segments of the U.S. population has meaningfully changed the evolution of home prices in the United States. Utilizing a Bartik shift-share instrument for demography-driven demand shocks, we show that from 2000 to 2018 (i) the price growth of four- and five-bedroom houses has lagged the prices of one- and two-bedroom homes, (ii) within local labor markets, the relative home prices in baby boomer-rich zip codes have declined compared with millennial-rich neighborhoods, and (iii) the zip codes with the largest relative share of smaller homes have grown fastest. These patterns have become more pronounced during the latest economic cycle. We show that the effects are concentrated in areas where housing supply is most inelastic. If this pattern in the housing market persists or expands, the approximately 16.5 trillion in real estate wealth held by households headed by those aged 55 or older will be significantly affected. We find little evidence that these upcoming changes have been incorporated into current prices."
http://arxiv.org/abs/2001.07949v4,Oracle Efficient Estimation of Structural Breaks in Cointegrating Regressions,2020-01-22 10:32:01+00:00,['Karsten Schweikert'],econ.EM,"In this paper, we propose an adaptive group lasso procedure to efficiently estimate structural breaks in cointegrating regressions. It is well-known that the group lasso estimator is not simultaneously estimation consistent and model selection consistent in structural break settings. Hence, we use a first step group lasso estimation of a diverging number of breakpoint candidates to produce weights for a second adaptive group lasso estimation. We prove that parameter changes are estimated consistently by group lasso and show that the number of estimated breaks is greater than the true number but still sufficiently close to it. Then, we use these results and prove that the adaptive group lasso has oracle properties if weights are obtained from our first step estimation. Simulation results show that the proposed estimator delivers the expected results. An economic application to the long-run US money demand function demonstrates the practical importance of this methodology."
http://arxiv.org/abs/2004.00111v1,Historical Evolution of Global Inequality in Carbon Emissions and Footprints versus Redistributive Scenarios,2020-03-30 00:51:40+00:00,"['Gregor Semieniuk', 'Victor M. Yakovenko']",physics.soc-ph,"Ambitious scenarios of carbon emission redistribution for mitigating climate change in line with the Paris Agreement and reaching the sustainable development goal of eradicating poverty have been proposed recently. They imply a strong reduction in carbon footprint inequality by 2030 that effectively halves the Gini coefficient to about 0.25. This paper examines feasibility of these scenarios by analyzing the historical evolution of both weighted international inequality in CO2 emissions attributed territorially and global inequality in carbon footprints attributed to end consumers. For the latter, a new dataset is constructed that is more comprehensive than existing ones. In both cases, we find a decreasing trend in global inequality, partially attributed to the move of China from the lower to the middle part of the distribution, with footprints more unequal than territorial emissions. These results show that realization of the redistributive scenarios would require an unprecedented reduction in global inequality far below historical levels. Moreover, the territorial emissions data, available for more recent years up to 2017, show a saturation of the decreasing Gini coefficient at a level of 0.5. This observation confirms an earlier prediction based on maximal entropy reasoning that the Lorenz curve converges to the exponential distribution. This saturation further undermines feasibility of the redistributive scenarios, which are also hindered by structural tendencies that reinforce carbon footprint inequality under global capitalism. One way out of this conundrum is a fast decarbonization of the global energy supply in order to decrease global carbon emissions without relying crucially on carbon inequality reduction."
http://arxiv.org/abs/2003.02343v1,Bow-tie structure and community identification of global supply chain network,2020-03-04 21:56:12+00:00,"['Abhijit Chakraborty', 'Yuichi Ikeda']",physics.soc-ph,"We study on topological properties of global supply chain network in terms of degree distribution, hierarchical structure, and degree-degree correlation in the global supply chain network. The global supply chain data is constructed by collecting various company data from the web site of Standard & Poor's Capital IQ platform in 2018. The in- and out-degree distributions are characterized by a power law with in-degree exponent = 2.42 and out-degree exponent = 2.11. The clustering coefficient decays as power law with an exponent = 0.46. The nodal degree-degree correlation indicates the absence of assortativity. The Bow-tie structure of GWCC reveals that the OUT component is the largest and it consists 41.1% of total firms. The GSCC component comprises 16.4% of total firms. We observe that the firms in the upstream or downstream sides are mostly located a few steps away from the GSCC. Furthermore, we uncover the community structure of the network and characterize them according to their location and industry classification. We observe that the largest community consists of consumer discretionary sector mainly based in the US. These firms belong to the OUT component in the bow-tie structure of the global supply chain network. Finally, we confirm the validity for propositions S1 (short path length), S2 (power-law degree distribution), S3 (high clustering coefficient), S4 (""fit-gets-richer"" growth mechanism), S5 (truncation of power-law degree distribution), and S7 (community structure with overlapping boundaries) in the global supply chain network."
http://arxiv.org/abs/2003.04938v5,A Mean-Field Game Approach to Equilibrium Pricing in Solar Renewable Energy Certificate Markets,2020-03-10 19:23:22+00:00,"['Arvind Shrivats', 'Dena Firoozi', 'Sebastian Jaimungal']",q-fin.MF,"Solar Renewable Energy Certificate (SREC) markets are a market-based system that incentivizes solar energy generation. A regulatory body imposes a lower bound on the amount of energy each regulated firm must generate via solar means, providing them with a tradeable certificate for each MWh generated. Firms seek to navigate the market optimally by modulating their SREC generation and trading rates. As such, the SREC market can be viewed as a stochastic game, where agents interact through the SREC price. We study this stochastic game by solving the mean-field game (MFG) limit with sub-populations of heterogeneous agents. Market participants optimize costs accounting for trading frictions, cost of generation, non-linear non-compliance costs, and generation uncertainty. Moreover, we endogenize SREC price through market clearing. We characterize firms' optimal controls as the solution of McKean-Vlasov (MV) FBSDEs and determine the equilibrium SREC price. We establish the existence and uniqueness of a solution to this MV-FBSDE, and prove that the MFG strategies form an $ε$-Nash equilibrium for the finite player game. Finally, we develop a numerical scheme for solving the MV-FBSDEs and conduct a simulation study."
http://arxiv.org/abs/2003.02208v7,Estimating the Effect of Central Bank Independence on Inflation Using Longitudinal Targeted Maximum Likelihood Estimation,2020-03-04 17:26:51+00:00,"['Philipp F. M. Baumann', 'Michael Schomaker', 'Enzo Rossi']",econ.EM,"The notion that an independent central bank reduces a country's inflation is a controversial hypothesis. To date, it has not been possible to satisfactorily answer this question because the complex macroeconomic structure that gives rise to the data has not been adequately incorporated into statistical analyses. We develop a causal model that summarizes the economic process of inflation. Based on this causal model and recent data, we discuss and identify the assumptions under which the effect of central bank independence on inflation can be identified and estimated. Given these and alternative assumptions, we estimate this effect using modern doubly robust effect estimators, i.e., longitudinal targeted maximum likelihood estimators. The estimation procedure incorporates machine learning algorithms and is tailored to address the challenges associated with complex longitudinal macroeconomic data. We do not find strong support for the hypothesis that having an independent central bank for a long period of time necessarily lowers inflation. Simulation studies evaluate the sensitivity of the proposed methods in complex settings when certain assumptions are violated and highlight the importance of working with appropriate learning algorithms for estimation."
http://arxiv.org/abs/2001.04283v2,Electricity prices and tariffs to keep everyone happy: a framework for fixed and nodal prices coexistence in distribution grids with optimal tariffs for investment cost recovery,2020-01-08 21:17:22+00:00,"['Iacopo Savelli', 'Thomas Morstyn']",econ.GN,"Some consumers, particularly households, are unwilling to face volatile electricity prices, and they can perceive as unfair price differentiation in the same local area. For these reasons, nodal prices in distribution networks are rarely employed. However, the increasing availability of renewable resources and emerging price-elastic behaviours pave the way for the effective introduction of marginal nodal pricing schemes in distribution networks. The aim of the proposed framework is to show how traditional non-flexible consumers can coexist with flexible users in a local distribution area. Flexible users will pay nodal prices, whereas non-flexible consumers will be charged a fixed price derived from the underlying nodal prices. Moreover, the developed approach shows how a distribution system operator should manage the local grid by optimally determining the lines to be expanded, and the collected network tariff levied on grid users, while accounting for both congestion rent and investment costs. The proposed model is formulated as a non-linear integer bilevel program, which is then recast as an equivalent single optimization problem, by using integer algebra and complementarity relations. The power flows in the distribution area are modelled by resorting to a second-order cone relaxation, whose solution is exact for radial networks under mild assumptions. The final model results in a mixed-integer quadratically constrained program, which can be solved with off-the-shelf solvers. Numerical test cases based on both 5-bus and 33-bus networks are reported to show the effectiveness of the proposed method."
http://arxiv.org/abs/2003.00545v2,Simple Mechanisms for Agents with Non-linear Utilities,2020-03-01 18:32:21+00:00,"['Yiding Feng', 'Jason Hartline', 'Yingkai Li']",cs.GT,"We show that economic conclusions derived from Bulow and Roberts (1989) for linear utility models approximately extend to non-linear utility models. Specifically, we quantify the extent to which agents with non-linear utilities resemble agents with linear utilities, and we show that the approximation of mechanisms for agents with linear utilities approximately extend for agents with non-linear utilities.
  We illustrate the framework for the objectives of revenue and welfare on non-linear models that include agents with budget constraints, agents with risk aversion, and agents with endogenous valuations. We derive bounds on how much these models resemble the linear utility model and combine these bounds with well-studied approximation results for linear utility models. We conclude that simple mechanisms are approximately optimal for these non-linear agent models."
http://arxiv.org/abs/2003.09367v2,A Correlated Random Coefficient Panel Model with Time-Varying Endogeneity,2020-03-20 16:43:05+00:00,['Louise Laage'],econ.EM,"This paper studies a class of linear panel models with random coefficients. We do not restrict the joint distribution of the time-invariant unobserved heterogeneity and the covariates. We investigate identification of the average partial effect (APE) when fixed-effect techniques cannot be used to control for the correlation between the regressors and the time-varying disturbances. Relying on control variables, we develop a constructive two-step identification argument. The first step identifies nonparametrically the conditional expectation of the disturbances given the regressors and the control variables, and the second step uses ``between-group'' variations, correcting for endogeneity, to identify the APE. We propose a natural semiparametric estimator of the APE, show its $\sqrt{n}$ asymptotic normality and compute its asymptotic variance. The estimator is computationally easy to implement, and Monte Carlo simulations show favorable finite sample properties. Control variables arise in various economic and econometric models, and we propose applications of our argument in several models. As an empirical illustration, we estimate the average elasticity of intertemporal substitution in a labor supply model with random coefficients."
http://arxiv.org/abs/2003.07338v5,Keeping the Listener Engaged: a Dynamic Model of Bayesian Persuasion,2020-03-16 17:17:56+00:00,"['Yeon-Koo Che', 'Kyungmin Kim', 'Konrad Mierendorff']",econ.TH,"We consider a dynamic model of Bayesian persuasion in which information takes time and is costly for the sender to generate and for the receiver to process, and neither player can commit to their future actions. Persuasion may totally collapse in a Markov perfect equilibrium (MPE) of this game. However, for persuasion costs sufficiently small, a version of a folk theorem holds: outcomes that approximate Kamenica and Gentzkow (2011)'s sender-optimal persuasion as well as full revelation and everything in between are obtained in MPE, as the cost vanishes."
http://arxiv.org/abs/2001.01641v1,"Housing Investment, Stock Market Participation and Household Portfolio choice: Evidence from China's Urban Areas",2020-01-06 16:07:04+00:00,['Huirong Liu'],econ.GN,"This paper employs the survey data of CHFS (2013) to investigate the impact of housing investment on household stock market participation and portfolio choice. The results show that larger housing investment encourages the household participation in the stock market, but reduces the proportion of their stockholding. The above conclusion remains true even when the endogeneity problem is controlled with risk attitude classification, Heckman model test and subsample regression. This study shows that the growth in the housing market will not lead to stock market development because of lack of household financial literacy and the low expected yield on stock market."
http://arxiv.org/abs/2001.00122v1,Entropic Decision Making,2020-01-01 01:37:20+00:00,['Adnan Rebei'],q-bio.NC,"Using results from neurobiology on perceptual decision making and value-based decision making, the problem of decision making between lotteries is reformulated in an abstract space where uncertain prospects are mapped to corresponding active neuronal representations. This mapping allows us to maximize non-extensive entropy in the new space with some constraints instead of a utility function. To achieve good agreements with behavioral data, the constraints must include at least constraints on the weighted average of the stimulus and on its variance. Both constraints are supported by the adaptability of neuronal responses to an external stimulus. By analogy with thermodynamic and information engines, we discuss the dynamics of choice between two lotteries as they are being processed simultaneously in the brain by rate equations that describe the transfer of attention between lotteries and within the various prospects of each lottery. This model is able to give new insights on risk aversion and on behavioral anomalies not accounted for by Prospect Theory."
http://arxiv.org/abs/2001.03935v1,A multi-country dynamic factor model with stochastic volatility for euro area business cycle analysis,2020-01-12 14:07:53+00:00,"['Florian Huber', 'Michael Pfarrhofer', 'Philipp Piribauer']",econ.EM,"This paper develops a dynamic factor model that uses euro area (EA) country-specific information on output and inflation to estimate an area-wide measure of the output gap. Our model assumes that output and inflation can be decomposed into country-specific stochastic trends and a common cyclical component. Comovement in the trends is introduced by imposing a factor structure on the shocks to the latent states. We moreover introduce flexible stochastic volatility specifications to control for heteroscedasticity in the measurement errors and innovations to the latent states. Carefully specified shrinkage priors allow for pushing the model towards a homoscedastic specification, if supported by the data. Our measure of the output gap closely tracks other commonly adopted measures, with small differences in magnitudes and timing. To assess whether the model-based output gap helps in forecasting inflation, we perform an out-of-sample forecasting exercise. The findings indicate that our approach yields superior inflation forecasts, both in terms of point and density predictions."
http://arxiv.org/abs/2001.03488v1,Constructing a Social Accounting Matrix Framework to Analyse the Impact of Public Expenditure on Income Distribution in Malaysia,2020-01-07 08:37:27+00:00,"['Mukaramah Harun', 'A. R. Zakariah', 'M. Azali']",econ.GN,"The use of the social accounting matrix (SAM) in income distribution analysis is a method recommended by economists. However, until now, there have only been a few SAM developed in Malaysia. The last SAM produced for Malaysia was developed in 1984 based upon data from 1970 and has not been updated since this time despite the significance changes in the structure of the Malaysian economy. The paper proposes a new Malaysian SAM framework to analyse public expenditure impact on income distribution in Malaysia. The SAM developed in the present paper is based on more recent data, providing an up-to date and coherent picture of the complexity of the Malaysian economy. The paper describes the structure of the SAM framework with a detailed aggregation and disaggregation of accounts related to public expenditure and income distribution issues. In the SAM utilized in the present study, the detailed framework of the different components of public expenditure in the production sectors and household groups is essential in the analysis of the different effects of the various public expenditure programmes on the incomes of households among different groups."
http://arxiv.org/abs/2002.03922v2,The Effect of Weather Conditions on Fertilizer Applications: A Spatial Dynamic Panel Data Analysis,2020-02-10 16:31:15+00:00,"['Anna Gloria Billè', 'Marco Rogna']",econ.EM,"Given the extreme dependence of agriculture on weather conditions, this paper analyses the effect of climatic variations on this economic sector, by considering both a huge dataset and a flexible spatio-temporal model specification. In particular, we study the response of N-fertilizer application to abnormal weather conditions, while accounting for other relevant control variables. The dataset consists of gridded data spanning over 21 years (1993-2013), while the methodological strategy makes use of a spatial dynamic panel data (SDPD) model that accounts for both space and time fixed effects, besides dealing with both space and time dependences. Time-invariant short and long term effects, as well as time-varying marginal effects are also properly defined, revealing interesting results on the impact of both GDP and weather conditions on fertilizer utilizations. The analysis considers four macro-regions -- Europe, South America, South-East Asia and Africa -- to allow for comparisons among different socio-economic societies. In addition to finding both spatial (in the form of knowledge spillover effects) and temporal dependences as well as a good support for the existence of an environmental Kuznets curve for fertilizer application, the paper shows peculiar responses of N-fertilization to deviations from normal weather conditions of moisture for each selected region, calling for ad hoc policy interventions."
http://arxiv.org/abs/2003.07150v2,"Stochastic Frontier Analysis with Generalized Errors: inference, model comparison and averaging",2020-03-16 12:38:02+00:00,"['Kamil Makieła', 'Błażej Mazur']",econ.EM,"Contribution of this paper lies in the formulation and estimation of a generalized model for stochastic frontier analysis (SFA) that nests virtually all forms used and includes some that have not been considered so far. The model is based on the generalized t distribution for the observation error and the generalized beta distribution of the second kind for the inefficiency-related term. We use this general error structure framework for formal testing, to compare alternative specifications and to conduct model averaging. This allows us to deal with model specification uncertainty, which is one of the main unresolved issues in SFA, and to relax a number of potentially restrictive assumptions embedded within existing SF models. We also develop Bayesian inference methods that are less restrictive compared to the ones used so far and demonstrate feasible approximate alternatives based on maximum likelihood."
http://arxiv.org/abs/2003.00033v1,Dynamic Beveridge Curve Accounting,2020-02-28 19:20:41+00:00,"['Hie Joo Ahn', 'Leland D. Crane']",econ.GN,"We develop a dynamic decomposition of the empirical Beveridge curve, i.e., the level of vacancies conditional on unemployment. Using a standard model, we show that three factors can shift the Beveridge curve: reduced-form matching efficiency, changes in the job separation rate, and out-of-steady-state dynamics. We find that the shift in the Beveridge curve during and after the Great Recession was due to all three factors, and each factor taken separately had a large effect. Comparing the pre-2010 period to the post-2010 period, a fall in matching efficiency and out-of-steady-state dynamics both pushed the curve upward, while the changes in the separation rate pushed the curve downward. The net effect was the observed upward shift in vacancies given unemployment. In previous recessions changes in matching efficiency were relatively unimportant, while dynamics and the separation rate had more impact. Thus, the unusual feature of the Great Recession was the deterioration in matching efficiency, while separations and dynamics have played significant, partially offsetting roles in most downturns. The importance of these latter two margins contrasts with much of the literature, which abstracts from one or both of them. We show that these factors affect the slope of the empirical Beveridge curve, an important quantity in recent welfare analyses estimating the natural rate of unemployment."
http://arxiv.org/abs/2003.05221v3,A mixture autoregressive model based on Gaussian and Student's $t$-distributions,2020-03-11 11:16:36+00:00,['Savi Virolainen'],econ.EM,"We introduce a new mixture autoregressive model which combines Gaussian and Student's $t$ mixture components. The model has very attractive properties analogous to the Gaussian and Student's $t$ mixture autoregressive models, but it is more flexible as it enables to model series which consist of both conditionally homoscedastic Gaussian regimes and conditionally heteroscedastic Student's $t$ regimes. The usefulness of our model is demonstrated in an empirical application to the monthly U.S. interest rate spread between the 3-month Treasury bill rate and the effective federal funds rate."
http://arxiv.org/abs/2003.03173v2,Implementability of Honest Multi-Agent Sequential Decision-Making with Dynamic Population,2020-03-06 13:06:47+00:00,"['Tao Zhang', 'Quanyan Zhu']",eess.SY,We study the design of decision-making mechanism for resource allocations over a multi-agent system in a dynamic environment. Agents' privately observed preference over resources evolves over time and the population is dynamic due to the adoption of stopping rules. The proposed model designs the rules of encounter for agents participating in the dynamic mechanism by specifying an allocation rule and three payment rules to elicit agents' coupled decision makings of honest preference reporting and optimal stopping over multiple periods. The mechanism provides a special posted-price payment rule that depends only on each agent's realized stopping time to directly influence the population dynamics. This letter focuses on the theoretical implementability of the rules in perfect Bayesian Nash equilibrium and characterizes necessary and sufficient conditions to guarantee agents' honest equilibrium behaviors over periods. We provide the design principles to construct the payments in terms of the allocation rules and identify the restrictions of the designer's ability to influence the population dynamics. The established conditions make the designer's problem of finding multiple rules to determine an optimal allocation rule.
http://arxiv.org/abs/2002.09014v1,Heavy Tails Make Happy Buyers,2020-02-20 20:47:42+00:00,['Eric Bax'],cs.GT,"In a second-price auction with i.i.d. (independent identically distributed) bidder valuations, adding bidders increases expected buyer surplus if the distribution of valuations has a sufficiently heavy right tail. While this does not imply that a bidder in an auction should prefer for more bidders to join the auction, it does imply that a bidder should prefer it in exchange for the bidder being allowed to participate in more auctions. Also, for a heavy-tailed valuation distribution, marginal expected seller revenue per added bidder remains strong even when there are already many bidders."
http://arxiv.org/abs/2002.09036v2,Rational Choice Hypothesis as X-point of Utility Function and Norm Function,2020-02-19 10:14:31+00:00,"['Takeshi Kato', 'Yasuyuki Kudo', 'Junichi Miyakoshi', 'Jun Otsuka', 'Hayato Saigo', 'Kaori Karasawa', 'Hiroyuki Yamaguchi', 'Yasuo Deguchi']",econ.GN,"Towards the realization of a sustainable, fair and inclusive society, we proposed a novel decision-making model that incorporates social norms in a rational choice model from the standpoints of deontology and utilitarianism. We proposed a hypothesis that interprets choice of action as the X-point for individual utility function that increases with actions and social norm function that decreases with actions. This hypothesis is based on humans psychologically balancing the value of utility and norms in selecting actions. Using the hypothesis and approximation, we were able to isolate and infer utility function and norm function from real-world measurement data of actions on environmental conditions and elucidate the interaction between the both functions that led from current status to target actions. As examples of collective data that aggregate decision-making of individuals, we looked at the changes in power usage before and after the Great East Japan Earthquake and the correlation between national GDP and CO2 emission in different countries. The first example showed that the perceived benefits of power (i.e., utility of power usage) was stronger than the power usage restrictions imposed by norms after the earthquake, contrary to our expectation. The second example showed that a reduction of CO2 emission in each country was not related to utility derived from GDP but to norms related to CO2 emission. Going forward, we will apply this new X-point model to actual social practices involving normative problems, and design the approaches for the diagnosis, prognosis and intervention of social systems by IT systems."
http://arxiv.org/abs/2001.08003v2,Measuring the Input Rank in Global Supply Networks,2020-01-22 13:25:18+00:00,"['Armando Rungi', 'Loredana Fattorini', 'Kenan Huremovic']",econ.GN,"We introduce the Input Rank as a measure of relevance of direct and indirect suppliers in Global Value Chains. We conceive an intermediate input to be more relevant for a downstream buyer if a decrease in that input's productivity affects that buyer more. In particular, in our framework, the relevance of any input depends: i) on the network position of the supplier relative to the buyer, ii) the patterns of intermediate inputs vs labor intensities connecting the buyer and the supplier, iii) and the competitive pressures along supply chains. After we compute the Input Rank from both U.S. and world Input-Output tables, we provide useful insights on the crucial role of services inputs as well as on the relatively higher relevance of domestic suppliers and suppliers coming from regionally integrated partners. Finally, we test that the Input Rank is a good predictor of vertical integration choices made by 20,489 U.S. parent companies controlling 154,836 subsidiaries worldwide."
http://arxiv.org/abs/2002.05153v1,Efficient Policy Learning from Surrogate-Loss Classification Reductions,2020-02-12 18:54:41+00:00,"['Andrew Bennett', 'Nathan Kallus']",cs.LG,"Recent work on policy learning from observational data has highlighted the importance of efficient policy evaluation and has proposed reductions to weighted (cost-sensitive) classification. But, efficient policy evaluation need not yield efficient estimation of policy parameters. We consider the estimation problem given by a weighted surrogate-loss classification reduction of policy learning with any score function, either direct, inverse-propensity weighted, or doubly robust. We show that, under a correct specification assumption, the weighted classification formulation need not be efficient for policy parameters. We draw a contrast to actual (possibly weighted) binary classification, where correct specification implies a parametric model, while for policy learning it only implies a semiparametric model. In light of this, we instead propose an estimation approach based on generalized method of moments, which is efficient for the policy parameters. We propose a particular method based on recent developments on solving moment problems using neural networks and demonstrate the efficiency and regret benefits of this method empirically."
http://arxiv.org/abs/2002.05193v2,A Hierarchy of Limitations in Machine Learning,2020-02-12 19:39:29+00:00,['Momin M. Malik'],cs.CY,"""All models are wrong, but some are useful"", wrote George E. P. Box (1979). Machine learning has focused on the usefulness of probability models for prediction in social systems, but is only now coming to grips with the ways in which these models are wrong---and the consequences of those shortcomings. This paper attempts a comprehensive, structured overview of the specific conceptual, procedural, and statistical limitations of models in machine learning when applied to society. Machine learning modelers themselves can use the described hierarchy to identify possible failure points and think through how to address them, and consumers of machine learning models can know what to question when confronted with the decision about if, where, and how to apply machine learning. The limitations go from commitments inherent in quantification itself, through to showing how unmodeled dependencies can lead to cross-validation being overly optimistic as a way of assessing model performance."
http://arxiv.org/abs/2003.08064v1,Ethnic Groups' Access to State Power and Group Size,2020-03-18 06:35:35+00:00,['Hector Galindo-Silva'],econ.GN,"Many countries are ethnically diverse. However, despite the benefits of ethnic heterogeneity, ethnic-based political inequality and discrimination are pervasive. Why is this? This study suggests that part of the variation in ethnic-based political inequality depends on the relative size of ethnic groups within each country. Using group-level data for 569 ethnic groups in 175 countries from 1946 to 2017, I find evidence of an inverted-U-shaped relationship between an ethnic group's relative size and its access to power. This single-peaked relationship is robust to many alternative specifications, and a battery of robustness checks suggests that relative size influences access to power. Through a very simple model, I propose an explanation based on an initial high level of political inequality, and on the incentives that more powerful groups have to continue limiting other groups' access to power. This explanation incorporates essential elements of several existing theories on the relationship between group size and discrimination, and suggests a new empirical prediction: the single-peaked pattern should be weaker in countries where political institutions have historically been less open. This additional prediction is supported by the data."
http://arxiv.org/abs/2003.08091v1,Modeling of the Greek road transportation network using complex network analysis,2020-03-18 08:37:15+00:00,['Dimitrios Tsiotas'],physics.soc-ph,"This article studies the interregional Greek road network (GRN) by applying complex network analysis (CNA) and an empirical approach. The study aims to extract the socioeconomic information immanent to the GRN's topology and to interpret the way in which this road network serves and promotes the regional development. The analysis shows that the topology of the GRN is submitted to spatial constraints, having lattice-like characteristics. Also, the GRN's structure is described by a gravity pattern, where places of higher population enjoy greater functionality, and its interpretation in regional terms illustrates the elementary pattern expressed by regional development through road construction. The study also reveals some interesting contradictions between the metropolitan and non-metropolitan (excluding Attica and Thessaloniki) comparison. Overall, the article highlights the effectiveness of using complex network analysis in the modeling of spatial networks and in particular of transportation systems and promotes the use of the network paradigm in the spatial and regional research."
http://arxiv.org/abs/2003.13660v1,By Force of Habit: Self-Trapping in a Dynamical Utility Landscape,2020-03-30 17:45:51+00:00,"['José Moran', 'Antoine Fosset', 'Davide Luzzati', 'Jean-Philippe Bouchaud', 'Michael Benzaquen']",cond-mat.stat-mech,"Historically, rational choice theory has focused on the utility maximization principle to describe how individuals make choices. In reality, there is a computational cost related to exploring the universe of available choices and it is often not clear whether we are truly maximizing an underlying utility function. In particular, memory effects and habit formation may dominate over utility maximisation. We propose a stylized model with a history-dependent utility function where the utility associated to each choice is increased when that choice has been made in the past, with a certain decaying memory kernel. We show that self-reinforcing effects can cause the agent to get stuck with a choice by sheer force of habit. We discuss the special nature of the transition between free exploration of the space of choice and self-trapping. We find in particular that the trapping time distribution is precisely a Zipf law at the transition, and that the self-trapped phase exhibits super-aging behaviour."
http://arxiv.org/abs/2002.00202v1,Natural Experiments,2020-02-01 12:52:51+00:00,['Rocio Titiunik'],stat.ME,"The term natural experiment is used inconsistently. In one interpretation, it refers to an experiment where a treatment is randomly assigned by someone other than the researcher. In another interpretation, it refers to a study in which there is no controlled random assignment, but treatment is assigned by some external factor in a way that loosely resembles a randomized experiment---often described as an ""as if random"" assignment. In yet another interpretation, it refers to any non-randomized study that compares a treatment to a control group, without any specific requirements on how the treatment is assigned. I introduce an alternative definition that seeks to clarify the integral features of natural experiments and at the same time distinguish them from randomized controlled experiments. I define a natural experiment as a research study where the treatment assignment mechanism (i) is neither designed nor implemented by the researcher, (ii) is unknown to the researcher, and (iii) is probabilistic by virtue of depending on an external factor. The main message of this definition is that the difference between a randomized controlled experiment and a natural experiment is not a matter of degree, but of essence, and thus conceptualizing a natural experiment as a research design akin to a randomized experiment is neither rigorous nor a useful guide to empirical analysis. Using my alternative definition, I discuss how a natural experiment differs from a traditional observational study, and offer practical recommendations for researchers who wish to use natural experiments to study causal effects."
http://arxiv.org/abs/2001.06281v2,Entropy Balancing for Continuous Treatments,2020-01-17 12:56:17+00:00,['Stefan Tübbicke'],econ.EM,"This paper introduces entropy balancing for continuous treatments (EBCT) by extending the original entropy balancing methodology of Hainmüller (2012). In order to estimate balancing weights, the proposed approach solves a globally convex constrained optimization problem. EBCT weights reliably eradicate Pearson correlations between covariates and the continuous treatment variable. This is the case even when other methods based on the generalized propensity score tend to yield insufficient balance due to strong selection into different treatment intensities. Moreover, the optimization procedure is more successful in avoiding extreme weights attached to a single unit. Extensive Monte-Carlo simulations show that treatment effect estimates using EBCT display similar or lower bias and uniformly lower root mean squared error. These properties make EBCT an attractive method for the evaluation of continuous treatments."
http://arxiv.org/abs/2002.00949v1,Profit-oriented sales forecasting: a comparison of forecasting techniques from a business perspective,2020-02-03 14:50:24+00:00,"['Tine Van Calster', 'Filip Van den Bossche', 'Bart Baesens', 'Wilfried Lemahieu']",econ.EM,"Choosing the technique that is the best at forecasting your data, is a problem that arises in any forecasting application. Decades of research have resulted into an enormous amount of forecasting methods that stem from statistics, econometrics and machine learning (ML), which leads to a very difficult and elaborate choice to make in any forecasting exercise. This paper aims to facilitate this process for high-level tactical sales forecasts by comparing a large array of techniques for 35 times series that consist of both industry data from the Coca-Cola Company and publicly available datasets. However, instead of solely focusing on the accuracy of the resulting forecasts, this paper introduces a novel and completely automated profit-driven approach that takes into account the expected profit that a technique can create during both the model building and evaluation process. The expected profit function that is used for this purpose, is easy to understand and adaptable to any situation by combining forecasting accuracy with business expertise. Furthermore, we examine the added value of ML techniques, the inclusion of external factors and the use of seasonal models in order to ascertain which type of model works best in tactical sales forecasting. Our findings show that simple seasonal time series models consistently outperform other methodologies and that the profit-driven approach can lead to selecting a different forecasting model."
http://arxiv.org/abs/2002.00953v1,Quid Pro Quo allocations in Production-Inventory games,2020-02-03 17:11:33+00:00,"['Luis Guardiola', 'Ana Meca', 'Justo Puerto']",cs.GT,"The concept of Owen point, introduced in Guardiola et al. (2009), is an appealing solution concept that for Production-Inventory games (PI-games) always belongs to their core. The Owen point allows all the players in the game to operate at minimum cost but it does not take into account the cost reduction induced by essential players over their followers (fans). Thus, it may be seen as an altruistic allocation for essential players what can be criticized. The aim this paper is two-fold: to study the structure and complexity of the core of PI-games and to introduce new core allocations for PI-games improving the weaknesses of the Owen point. Regarding the first goal, we advance further on the analysis of PI-games and we analyze its core structure and algorithmic complexity. Specifically, we prove that the number of extreme points of the core of PI-games is exponential on the number of players. On the other hand, we propose and characterize a new core-allocation, the Omega point, which compensates the essential players for their role on reducing the costs of their fans. Moreover, we define another solution concept, the Quid Pro Quo set (QPQ-set) of allocations, which is based on the Owen and Omega points. Among all the allocations in this set, we emphasize what we call the Solomonic QPQ allocation and we provide some necessary conditions for the coincidence of that allocation with the Shapley value and the Nucleolus."
http://arxiv.org/abs/2002.01578v1,Rental Housing Spot Markets: How Online Information Exchanges Can Supplement Transacted-Rents Data,2020-02-04 23:23:35+00:00,"['Geoff Boeing', 'Jake Wegmann', 'Junfeng Jiao']",econ.GN,"Traditional US rental housing data sources such as the American Community Survey and the American Housing Survey report on the transacted market - what existing renters pay each month. They do not explicitly tell us about the spot market - i.e., the asking rents that current homeseekers must pay to acquire housing - though they are routinely used as a proxy. This study compares governmental data to millions of contemporaneous rental listings and finds that asking rents diverge substantially from these most recent estimates. Conventional housing data understate current market conditions and affordability challenges, especially in cities with tight and expensive rental markets."
http://arxiv.org/abs/2002.08092v2,Cointegration without Unit Roots,2020-02-19 10:02:50+00:00,"['James A. Duffy', 'Jerome R. Simons']",econ.EM,"It has been known since Elliott (1998) that standard methods of inference on cointegrating relationships break down entirely when autoregressive roots are near but not exactly equal to unity. We consider this problem within the framework of a structural VAR, arguing this it is as much a problem of identification failure as it is of inference. We develop a characterisation of cointegration based on the impulse response function, which allows long-run equilibrium relationships to remain identified even in the absence of exact unit roots. Our approach also provides a framework in which the structural shocks driving the common persistent components continue to be identified via long-run restrictions, just as in an SVAR with exact unit roots. We show that inference on the cointegrating relationships is affected by nuisance parameters, in a manner familiar from predictive regression; indeed the two problems are asymptotically equivalent. By adapting the approach of Elliott, Müller and Watson (2015) to our setting, we develop tests that robustly control size while sacrificing little power (relative to tests that are efficient in the presence of exact unit roots)."
http://arxiv.org/abs/2002.06554v1,Convex Combinatorial Auction of Pipeline Network Capacities,2020-02-16 11:19:41+00:00,['Dávid Csercsik'],econ.TH,"In this paper we propose a mechanism for the allocation of pipeline capacities, assuming that the participants bidding for capacities do have subjective evaluation of various network routes. The proposed mechanism is based on the concept of bidding for route-quantity pairs. Each participant defines a limited number of routes and places multiple bids, corresponding to various quantities, on each of these routes. The proposed mechanism assigns a convex combination of the submitted bids to each participant, thus its called convex combinatorial auction. The capacity payments in the proposed model are determined according to the Vickrey-Clarke-Groves principle. We compare the efficiency of the proposed algorithm with a simplified model of the method currently used for pipeline capacity allocation in the EU (simultaneous ascending clock auction of pipeline capacities) via simulation, according to various measures, such as resulting utility of players, utilization of network capacities, total income of the auctioneer and fairness."
http://arxiv.org/abs/2003.09167v1,Gender bias in the Erasmus students network,2020-03-20 09:57:24+00:00,"['Luca De Benedictis', 'Silvia Leoni']",physics.soc-ph,"The Erasmus Program (EuRopean community Action Scheme for the Mobility of University Students), the most important student exchange program in the world, financed by the European Union and started in 1987, is characterized by a strong gender bias. Girls participate to the program more than boys. This work quantifies the gender bias in the Erasmus program between 2008 and 2013, using novel data at the university level. It describes the structure of the program in great details, carrying out the analysis across fields of study, and identifies key universities as senders and receivers. In addition, it tests the difference in the degree distribution of the Erasmus network along time and between genders, giving evidence of a greater density in the female Erasmus network with respect to the one of the male Erasmus network."
http://arxiv.org/abs/2001.11165v10,Empirical Analysis of Fictitious Play for Nash Equilibrium Computation in Multiplayer Games,2020-01-30 03:47:09+00:00,['Sam Ganzfried'],cs.GT,"While fictitious play is guaranteed to converge to Nash equilibrium in certain game classes, such as two-player zero-sum games, it is not guaranteed to converge in non-zero-sum and multiplayer games. We show that fictitious play in fact leads to improved Nash equilibrium approximation over a variety of game classes and sizes than (counterfactual) regret minimization, which has recently produced superhuman play for multiplayer poker. We also show that when fictitious play is run several times using random initializations it is able to solve several known challenge problems in which the standard version is known to not converge, including Shapley's classic counterexample. These provide some of the first positive results for fictitious play in these settings, despite the fact that worst-case theoretical results are negative."
http://arxiv.org/abs/2002.10274v1,Bayesian Inference in High-Dimensional Time-varying Parameter Models using Integrated Rotated Gaussian Approximations,2020-02-24 14:07:50+00:00,"['Florian Huber', 'Gary Koop', 'Michael Pfarrhofer']",econ.EM,"Researchers increasingly wish to estimate time-varying parameter (TVP) regressions which involve a large number of explanatory variables. Including prior information to mitigate over-parameterization concerns has led to many using Bayesian methods. However, Bayesian Markov Chain Monte Carlo (MCMC) methods can be very computationally demanding. In this paper, we develop computationally efficient Bayesian methods for estimating TVP models using an integrated rotated Gaussian approximation (IRGA). This exploits the fact that whereas constant coefficients on regressors are often important, most of the TVPs are often unimportant. Since Gaussian distributions are invariant to rotations we can split the the posterior into two parts: one involving the constant coefficients, the other involving the TVPs. Approximate methods are used on the latter and, conditional on these, the former are estimated with precision using MCMC methods. In empirical exercises involving artificial data and a large macroeconomic data set, we show the accuracy and computational benefits of IRGA methods."
http://arxiv.org/abs/2002.09982v1,Estimation and Inference about Tail Features with Tail Censored Data,2020-02-23 20:43:24+00:00,"['Yulong Wang', 'Zhijie Xiao']",econ.EM,"This paper considers estimation and inference about tail features when the observations beyond some threshold are censored. We first show that ignoring such tail censoring could lead to substantial bias and size distortion, even if the censored probability is tiny. Second, we propose a new maximum likelihood estimator (MLE) based on the Pareto tail approximation and derive its asymptotic properties. Third, we provide a small sample modification to the MLE by resorting to Extreme Value theory. The MLE with this modification delivers excellent small sample performance, as shown by Monte Carlo simulations. We illustrate its empirical relevance by estimating (i) the tail index and the extreme quantiles of the US individual earnings with the Current Population Survey dataset and (ii) the tail index of the distribution of macroeconomic disasters and the coefficient of risk aversion using the dataset collected by Barro and Urs{ú}a (2008). Our new empirical findings are substantially different from the existing literature."
http://arxiv.org/abs/2003.12474v1,Challenge Theory: The Structure and Measurement of Risky Binary Choice Behavior,2020-03-24 20:45:11+00:00,"['Samuel Shye', 'Ido Haber']",econ.GN,"Challenge Theory (Shye & Haber 2015; 2020) has demonstrated that a newly devised challenge index (CI) attributable to every binary choice problem predicts the popularity of the bold option, the one of lower probability to gain a higher monetary outcome (in a gain problem); and the one of higher probability to lose a lower monetary outcome (in a loss problem). In this paper we show how Facet Theory structures the choice-behavior concept-space and yields rationalized measurements of gambling behavior. The data of this study consist of responses obtained from 126 student, specifying their preferences in 44 risky decision problems. A Faceted Smallest Space Analysis (SSA) of the 44 problems confirmed the hypothesis that the space of binary risky choice problems is partitionable by two binary axial facets: (a) Type of Problem (gain vs. loss); and (b) CI (Low vs. High). Four composite variables, representing the validated constructs: Gain, Loss, High-CI and Low-CI, were processed using Multiple Scaling by Partial Order Scalogram Analysis with base Coordinates (POSAC), leading to a meaningful and intuitively appealing interpretation of two necessary and sufficient gambling-behavior measurement scales."
http://arxiv.org/abs/2002.07880v1,The interconnectedness of the economic content in the speeches of the US Presidents,2020-02-18 21:10:55+00:00,"['Matteo Cinelli', 'Valerio Ficcadenti', 'Jessica Riccioni']",econ.GN,"The speeches stated by influential politicians can have a decisive impact on the future of a country. In particular, the economic content of such speeches affects the economy of countries and their financial markets. For this reason, we examine a novel dataset containing the economic content of 951 speeches stated by 45 US Presidents from George Washington (April 1789) to Donald Trump (February 2017). In doing so, we use an economic glossary carried out by means of text mining techniques. The goal of our study is to examine the structure of significant interconnections within a network obtained from the economic content of presidential speeches. In such a network, nodes are represented by talks and links by values of cosine similarity, the latter computed using the occurrences of the economic terms in the speeches. The resulting network displays a peculiar structure made up of a core (i.e. a set of highly central and densely connected nodes) and a periphery (i.e. a set of non-central and sparsely connected nodes). The presence of different economic dictionaries employed by the Presidents characterize the core-periphery structure. The Presidents' talks belonging to the network's core share the usage of generic (non-technical) economic locutions like ""interest"" or ""trade"". While the use of more technical and less frequent terms characterizes the periphery (e.g. ""yield"" ). Furthermore, the speeches close in time share a common economic dictionary. These results together with the economics glossary usages during the US periods of boom and crisis provide unique insights on the economic content relationships among Presidents' speeches."
http://arxiv.org/abs/2003.05913v2,Escaping Cannibalization? Correlation-Robust Pricing for a Unit-Demand Buyer,2020-03-12 17:24:56+00:00,"['Moshe Babaioff', 'Michal Feldman', 'Yannai A. Gonczarowski', 'Brendan Lucier', 'Inbal Talgam-Cohen']",cs.GT,"We consider a robust version of the revenue maximization problem, where a single seller wishes to sell $n$ items to a single unit-demand buyer. In this robust version, the seller knows the buyer's marginal value distribution for each item separately, but not the joint distribution, and prices the items to maximize revenue in the worst case over all compatible correlation structures. We devise a computationally efficient (polynomial in the support size of the marginals) algorithm that computes the worst-case joint distribution for any choice of item prices. And yet, in sharp contrast to the additive buyer case (Carroll, 2017), we show that it is NP-hard to approximate the optimal choice of prices to within any factor better than $n^{1/2-ε}$. For the special case of marginal distributions that satisfy the monotone hazard rate property, we show how to guarantee a constant fraction of the optimal worst-case revenue using item pricing; this pricing equates revenue across all possible correlations and can be computed efficiently."
http://arxiv.org/abs/2003.06844v1,A Model of Justification,2020-03-15 15:04:15+00:00,['Sarah Ridout'],econ.TH,"I consider decision-making constrained by considerations of morality, rationality, or other virtues. The decision maker (DM) has a true preference over outcomes, but feels compelled to choose among outcomes that are top-ranked by some preference that he considers ""justifiable."" This model unites a broad class of empirical work on distributional preferences, charitable donations, prejudice/discrimination, and corruption/bribery. I provide a behavioral characterization of the model. I also show that the set of justifications can be identified from choice behavior when the true preference is known, and that choice behavior substantially restricts both the true preference and justifications when neither is known. I argue that the justifiability model represents an advancement over existing models of rationalization because the structure it places on possible ""rationales"" improves tractability, interpretation and identification."
http://arxiv.org/abs/2003.09276v1,Kernel density decomposition with an application to the social cost of carbon,2020-03-20 13:44:53+00:00,['Richard S. J. Tol'],stat.ME,"A kernel density is an aggregate of kernel functions, which are itself densities and could be kernel densities. This is used to decompose a kernel into its constituent parts. Pearson's test for equality of proportions is applied to quantiles to test whether the component distributions differ from one another. The proposed methods are illustrated with a meta-analysis of the social cost of carbon. Different discount rates lead to significantly different Pigou taxes, but not different growth rates. Estimates have not varied over time. Different authors have contributed different estimates, but these differences are insignificant. Kernel decomposition can be applied in many other fields with discrete explanatory variables."
http://arxiv.org/abs/2002.09225v3,Kernel Conditional Moment Test via Maximum Moment Restriction,2020-02-21 10:58:57+00:00,"['Krikamol Muandet', 'Wittawat Jitkrittum', 'Jonas Kübler']",math.ST,"We propose a new family of specification tests called kernel conditional moment (KCM) tests. Our tests are built on a novel representation of conditional moment restrictions in a reproducing kernel Hilbert space (RKHS) called conditional moment embedding (CMME). After transforming the conditional moment restrictions into a continuum of unconditional counterparts, the test statistic is defined as the maximum moment restriction (MMR) within the unit ball of the RKHS. We show that the MMR not only fully characterizes the original conditional moment restrictions, leading to consistency in both hypothesis testing and parameter estimation, but also has an analytic expression that is easy to compute as well as closed-form asymptotic distributions. Our empirical studies show that the KCM test has a promising finite-sample performance compared to existing tests."
http://arxiv.org/abs/2002.08786v2,Cournot-Nash equilibrium and optimal transport in a dynamic setting,2020-02-20 15:09:05+00:00,"['Beatrice Acciaio', 'Julio Backhoff-Veraguas', 'Junchao Jia']",math.OC,"We consider a large population dynamic game in discrete time. The peculiarity of the game is that players are characterized by time-evolving types, and so reasonably their actions should not anticipate the future values of their types. When interactions between players are of mean-field kind, we relate Nash equilibria for such games to an asymptotic notion of dynamic Cournot-Nash equilibria. Inspired by the works of Blanchet and Carlier for the static situation, we interpret dynamic Cournot-Nash equilibria in the light of causal optimal transport theory. Further specializing to games of potential type, we establish existence, uniqueness and characterization of equilibria. Moreover we develop, for the first time, a numerical scheme for causal optimal transport, which is then leveraged in order to compute dynamic Cournot-Nash equilibria. This is illustrated in a detailed case study of a congestion game."
http://arxiv.org/abs/2003.13478v1,High-dimensional mixed-frequency IV regression,2020-03-30 13:41:02+00:00,['Andrii Babii'],econ.EM,This paper introduces a high-dimensional linear IV regression for the data sampled at mixed frequencies. We show that the high-dimensional slope parameter of a high-frequency covariate can be identified and accurately estimated leveraging on a low-frequency instrumental variable. The distinguishing feature of the model is that it allows handing high-dimensional datasets without imposing the approximate sparsity restrictions. We propose a Tikhonov-regularized estimator and derive the convergence rate of its mean-integrated squared error for time series data. The estimator has a closed-form expression that is easy to compute and demonstrates excellent performance in our Monte Carlo experiments. We estimate the real-time price elasticity of supply on the Australian electricity spot market. Our estimates suggest that the supply is relatively inelastic and that its elasticity is heterogeneous throughout the day.
http://arxiv.org/abs/2003.14002v1,The propagation of the economic impact through supply chains: The case of a mega-city lockdown against the spread of COVID-19,2020-03-31 07:48:46+00:00,"['Hiroyasu Inoue', 'Yasuyuki Todo']",cs.SI,"This study quantifies the economic effect of a possible lockdown of Tokyo to prevent spread of COVID-19. The negative effect of the lockdown may propagate to other regions through supply chains because of shortage of supply and demand. Applying an agent-based model to the actual supply chains of nearly 1.6 million firms in Japan, we simulate what would happen to production activities outside Tokyo when production activities that are not essential to citizens' survival in Tokyo were shut down for a certain period. We find that when Tokyo is locked down for a month, the indirect effect on other regions would be twice as large as the direct effect on Tokyo, leading to a total production loss of 27 trillion yen in Japan, or 5.3% of its annual GDP. Although the production shut down in Tokyo accounts for 21% of the total production in Japan, the lockdown would result in a reduction of the daily production in Japan by 86% in a month."
http://arxiv.org/abs/2003.10014v1,Reinforcement Learning in Economics and Finance,2020-03-22 22:31:35+00:00,"['Arthur Charpentier', 'Romuald Elie', 'Carl Remlinger']",econ.TH,"Reinforcement learning algorithms describe how an agent can learn an optimal action policy in a sequential decision process, through repeated experience. In a given environment, the agent policy provides him some running and terminal rewards. As in online learning, the agent learns sequentially. As in multi-armed bandit problems, when an agent picks an action, he can not infer ex-post the rewards induced by other action choices. In reinforcement learning, his actions have consequences: they influence not only rewards, but also future states of the world. The goal of reinforcement learning is to find an optimal policy -- a mapping from the states of the world to the set of actions, in order to maximize cumulative reward, which is a long term strategy. Exploring might be sub-optimal on a short-term horizon but could lead to optimal long-term ones. Many problems of optimal control, popular in economics for more than forty years, can be expressed in the reinforcement learning framework, and recent advances in computational science, provided in particular by deep learning algorithms, can be used by economists in order to solve complex behavioral problems. In this article, we propose a state-of-the-art of reinforcement learning techniques, and present applications in economics, game theory, operation research and finance."
http://arxiv.org/abs/2001.02200v3,Whos Ditching the Bus?,2020-01-07 17:56:43+00:00,"['Simon J. Berrebi', 'Kari E. Watkins']",physics.soc-ph,"This paper uses stop-level passenger count data in four cities to understand the nation-wide bus ridership decline between 2012 and 2018. The local characteristics associated with ridership change are evaluated in Portland, Miami, Minneapolis/St-Paul, and Atlanta. Poisson models explain ridership as a cross-section and the change thereof as a panel. While controlling for the change in frequency, jobs, and population, the correlation with local socio-demographic characteristics are investigated using data from the American Community Survey. The effect of changing neighborhood demographics on bus ridership are modeled using Longitudinal Employer-Household Dynamics data. At a point in time, neighborhoods with high proportions of non-white, carless, and most significantly, high-school-educated residents are the most likely to have high ridership. Over time, white neighborhoods are losing the most ridership across all four cities. In Miami and Atlanta, places with high concentrations of residents with college education and without access to a car also lose ridership at a faster rate. In Minneapolis/St-Paul, the proportion of college-educated residents is linked to ridership gain. The sign and significance of these results remain consistent even when controlling for intra-urban migration. Although bus ridership is declining across neighborhood characteristics, these results suggest that the underlying cause of bus ridership decline must be primarily affecting the travel behavior of white bus riders."
http://arxiv.org/abs/2001.11422v1,Spatial competition with unit-demand functions,2020-01-30 16:04:00+00:00,"['Gaëtan Fournier', 'Karine Van Der Straeten', 'Jörgen Weibull']",math.OC,"This paper studies a spatial competition game between two firms that sell a homogeneous good at some pre-determined fixed price. A population of consumers is spread out over the real line, and the two firms simultaneously choose location in this same space. When buying from one of the firms, consumers incur the fixed price plus some transportation costs, which are increasing with their distance to the firm. Under the assumption that each consumer is ready to buy one unit of the good whatever the locations of the firms, firms converge to the median location: there is ""minimal differentiation"". In this article, we relax this assumption and assume that there is an upper limit to the distance a consumer is ready to cover to buy the good. We show that the game always has at least one Nash equilibrium in pure strategy. Under this more general assumption, the ""minimal differentiation principle"" no longer holds in general. At equilibrium, firms choose ""minimal"", ""intermediate"" or ""full"" differentiation, depending on this critical distance a consumer is ready to cover and on the shape of the distribution of consumers' locations."
http://arxiv.org/abs/2001.11130v1,Blocked Clusterwise Regression,2020-01-29 23:29:31+00:00,['Max Cytrynbaum'],econ.EM,"A recent literature in econometrics models unobserved cross-sectional heterogeneity in panel data by assigning each cross-sectional unit a one-dimensional, discrete latent type. Such models have been shown to allow estimation and inference by regression clustering methods. This paper is motivated by the finding that the clustered heterogeneity models studied in this literature can be badly misspecified, even when the panel has significant discrete cross-sectional structure. To address this issue, we generalize previous approaches to discrete unobserved heterogeneity by allowing each unit to have multiple, imperfectly-correlated latent variables that describe its response-type to different covariates. We give inference results for a k-means style estimator of our model and develop information criteria to jointly select the number clusters for each latent variable. Monte Carlo simulations confirm our theoretical results and give intuition about the finite-sample performance of estimation and model selection. We also contribute to the theory of clustering with an over-specified number of clusters and derive new convergence rates for this setting. Our results suggest that over-fitting can be severe in k-means style estimators when the number of clusters is over-specified."
http://arxiv.org/abs/2002.00225v1,Insights on the Theory of Robust Games,2020-02-01 15:00:17+00:00,"['Giovanni Paolo Crespi', 'Davide Radi', 'Matteo Rocca']",econ.TH,"A robust game is a distribution-free model to handle ambiguity generated by a bounded set of possible realizations of the values of players' payoff functions. The players are worst-case optimizers and a solution, called robust-optimization equilibrium, is guaranteed by standard regularity conditions. The paper investigates the sensitivity to the level of uncertainty of this equilibrium. Specifically, we prove that it is an epsilon-Nash equilibrium of the nominal counterpart game, where the epsilon-approximation measures the extra profit that a player would obtain by reducing his level of uncertainty. Moreover, given an epsilon-Nash equilibrium of a nominal game, we prove that it is always possible to introduce uncertainty such that the epsilon-Nash equilibrium is a robust-optimization equilibrium. An example shows that a robust Cournot duopoly model can admit multiple and asymmetric robust-optimization equilibria despite only a symmetric Nash equilibrium exists for the nominal counterpart game."
http://arxiv.org/abs/2001.07042v1,Fundamental Limits of Testing the Independence of Irrelevant Alternatives in Discrete Choice,2020-01-20 10:15:28+00:00,"['Arjun Seshadri', 'Johan Ugander']",math.ST,"The Multinomial Logit (MNL) model and the axiom it satisfies, the Independence of Irrelevant Alternatives (IIA), are together the most widely used tools of discrete choice. The MNL model serves as the workhorse model for a variety of fields, but is also widely criticized, with a large body of experimental literature claiming to document real-world settings where IIA fails to hold. Statistical tests of IIA as a modelling assumption have been the subject of many practical tests focusing on specific deviations from IIA over the past several decades, but the formal size properties of hypothesis testing IIA are still not well understood. In this work we replace some of the ambiguity in this literature with rigorous pessimism, demonstrating that any general test for IIA with low worst-case error would require a number of samples exponential in the number of alternatives of the choice problem. A major benefit of our analysis over previous work is that it lies entirely in the finite-sample domain, a feature crucial to understanding the behavior of tests in the common data-poor settings of discrete choice. Our lower bounds are structure-dependent, and as a potential cause for optimism, we find that if one restricts the test of IIA to violations that can occur in a specific collection of choice sets (e.g., pairs), one obtains structure-dependent lower bounds that are much less pessimistic. Our analysis of this testing problem is unorthodox in being highly combinatorial, counting Eulerian orientations of cycle decompositions of a particular bipartite graph constructed from a data set of choices. By identifying fundamental relationships between the comparison structure of a given testing problem and its sample efficiency, we hope these relationships will help lay the groundwork for a rigorous rethinking of the IIA testing problem as well as other testing problems in discrete choice."
http://arxiv.org/abs/2001.06548v1,Who voted for a No Deal Brexit? A Composition Model of Great Britains 2019 European Parliamentary Elections,2020-01-16 11:38:44+00:00,['Stephen Clark'],physics.soc-ph,"The purpose of this paper is to use the votes cast at the 2019 European elections held in United Kingdom to re-visit the analysis conducted subsequent to its 2016 European Union referendum vote. This exercise provides a staging post on public opinion as the United Kingdom moves to leave the European Union during 2020. A composition data analysis in a seemingly unrelated regression framework is adopted that respects the compositional nature of the vote outcome; each outcome is a share that adds up to 100% and each outcome is related to the alternatives. Contemporary explanatory data for each counting area is sourced from the themes of socio-demographics, employment, life satisfaction and place. The study find that there are still strong and stark divisions in the United Kingdom, defined by age, qualifications, employment and place. The use of a compositional analysis approach produces challenges in regards to the interpretation of these models, but marginal plots are seen to aid the interpretation somewhat."
http://arxiv.org/abs/2001.06003v2,Examining the correlation of the level of wage inequality with labor market institutions,2020-01-16 10:49:13+00:00,['Virginia Tsoukatou'],econ.GN,"Technological change is responsible for major changes in the labor market. One of the offspring of technological change is the SBTC, which is for many economists the leading cause of the increasing wage inequality. However, despite that the technological change affected similarly the majority of the developed countries, nevertheless, the level of the increase of wage inequality wasn't similar. Following the predictions of the SBTC theory, the different levels of inequality could be due to varying degrees of skill inequality between economies, possibly caused by variations in the number of skilled workers available. However, recent research shows that the difference mentioned above can explain a small percentage of the difference between countries. Therefore, most of the resulting inequality could be due to the different ways in which the higher level of skills is valued in each labor market. The position advocated in this article is that technological change is largely given for all countries without much scope to reverse. Therefore, in order to illustrate the changes in the structure of wage distribution that cause wage inequality, we need to understand how technology affects labor market institutions.In this sense, the pay inequality caused by technological progress is not a phenomenon we passively accept. On the contrary, recognizing that the structure and the way labor market institutions function is largely influenced by the way institutions respond to technological change, we can understand and maybe reverse this underlying wage inequality."
http://arxiv.org/abs/2001.07790v1,Investor Experiences and International Capital Flows,2020-01-21 21:54:39+00:00,"['Ulrike Malmendier', 'Demian Pouzo', 'Victoria Vanasco']",econ.GN,"We propose a novel explanation for classic international macro puzzles regarding capital flows and portfolio investment, which builds on modern macro-finance models of experience-based belief formation. Individual experiences of past macroeconomic outcomes have been shown to exert a long-lasting influence on beliefs about future realizations, and to explain domestic stock-market investment. We argue that experience effects can explain the tendency of investors to hold an over proportional fraction of their equity wealth in domestic stocks (home bias), to invest in domestic equity markets in periods of domestic crises (retrenchment), and to withdraw capital from foreign equity markets in periods of foreign crises (fickleness). Experience-based learning generates additional implications regarding the strength of these puzzles in times of higher or lower economic activity and depending on the demographic composition of market participants. We test and confirm these predictions in the data."
http://arxiv.org/abs/2001.03045v1,Estimating the Impact of GST Implementation on Cost of Production and Cost of Living in Malaysia,2020-01-07 07:40:07+00:00,['Mukaramah Harun'],econ.GN,"The implementation of Goods and Services Tax(GST) is often attributed as the main cause of the rising prices of goods and services. The main objective of this study is to estimate the extent of GST implementation impact on the costs of production, which in turn have implication on households living costs."
http://arxiv.org/abs/2001.03122v1,Targeting in social networks with anonymized information,2020-01-09 17:33:58+00:00,"['Francis Bloch', 'Shaden Shabayek']",econ.TH,"This paper studies whether a planner who only has information about the network topology can discriminate among agents according to their network position. The planner proposes a simple menu of contracts, one for each location, in order to maximize total welfare, and agents choose among the menu. This mechanism is immune to deviations by single agents, and to deviations by groups of agents of sizes 2, 3 and 4 if side-payments are ruled out. However, if compensations are allowed, groups of agents may have an incentive to jointly deviate from the optimal contract in order to exploit other agents. We identify network topologies for which the optimal contract is group incentive compatible with transfers: undirected networks and regular oriented trees, and network topologies for which the planner must assign uniform quantities: single root and nested neighborhoods directed networks."
http://arxiv.org/abs/2001.03646v1,Commuting Service Platform: Concept and Analysis,2020-01-10 19:48:18+00:00,"['Rong Fan', 'Xuegang', 'Ban']",econ.GN,"We propose and investigate the concept of commuting service platforms (CSP) that leverage emerging mobility services to provide commuting services and connect directly commuters (employees) and their worksites (employers). By applying the two-sided market analysis framework, we show under what conditions a CSP may present the two-sidedness. Both the monopoly and duopoly CSPs are then analyzed. We showhowthe price allocation, i.e., the prices charged to commuters and worksites, can impact the participation and profit of the CSPs. We also add demand constraints to the duopoly model so that the participation rates ofworksites and employees are (almost) the same. With demand constraints, the competition between the two CSPs becomes less intense in general. Discussions are presented on how the results and findings in this paper may help build CSP in practice and how to develop new, CSP-based travel demand management strategies."
http://arxiv.org/abs/2001.02509v1,"Ways to Reduce Cost of Living: A Case Study among Low Income Household in Kubang Pasu, Kedah, Malaysia",2020-01-07 06:39:23+00:00,['Mukaramah Harun'],econ.GN,"This study was conducted to examine and understand the spending behavior of low income households (B40), namely households with income of RM3800 and below. The study focused on the area Kubang Pasu District, Kedah."
http://arxiv.org/abs/2001.03486v1,Macroeconomic Instability And Fiscal Decentralization: An Empirical Analysis,2020-01-07 08:18:49+00:00,"['Ahmad Zafarullah Abdul Jalil', 'Mukaramah Harun', 'Siti Hadijah Che Mat']",econ.GN,"The main objective of this paper is to fill a critical gap in the literature by analyzing the effects of decentralization on the macroeconomic stability. A survey of the voluminous literature on decentralization suggests that the question of the links between decentralization and macroeconomic stability has been relatively scantily analyzed. Even though there is still a lot of room for analysis as far as the effects of decentralization on other aspects of the economy are concerned, we believe that it is in this area that a more thorough analyses are mostly called for. Through this paper, we will try to shed more light on the issue notably by looking at other dimension of macroeconomic stability than the ones usually employed in previous studies as well as by examining other factors that might accentuate or diminish the effects of decentralization on macroeconomic stability. Our results found that decentralization appears to lead to a decrease in inflation rate. However, we do not find any correlation between decentralization with the level of fiscal deficit. Our results also show that the impact of decentralization on inflation is conditional on the level of perceived corruption and political institutions."
http://arxiv.org/abs/2001.02115v1,Understanding the Great Recession Using Machine Learning Algorithms,2020-01-02 15:30:52+00:00,"['Rickard Nyman', 'Paul Ormerod']",econ.GN,"Nyman and Ormerod (2017) show that the machine learning technique of random forests has the potential to give early warning of recessions. Applying the approach to a small set of financial variables and replicating as far as possible a genuine ex ante forecasting situation, over the period since 1990 the accuracy of the four-step ahead predictions is distinctly superior to those actually made by the professional forecasters. Here we extend the analysis by examining the contributions made to the Great Recession of the late 2000s by each of the explanatory variables. We disaggregate private sector debt into its household and non-financial corporate components. We find that both household and non-financial corporate debt were key determinants of the Great Recession. We find a considerable degree of non-linearity in the explanatory models. In contrast, the public sector debt to GDP ratio appears to have made very little contribution. It did rise sharply during the Great Recession, but this was as a consequence of the sharp fall in economic activity rather than it being a cause. We obtain similar results for both the United States and the United Kingdom."
http://arxiv.org/abs/2001.02804v1,Institutions and China's comparative development,2020-01-09 01:43:27+00:00,['Paul Minard'],econ.GN,"Robust assessment of the institutionalist account of comparative development is hampered by problems of omitted variable bias and reverse causation, since institutional quality is not randomly assigned with respect to geographic and human capital endowments. A recent series of papers has applied spatial regression discontinuity designs to estimate the impact of institutions on incomes at international borders, drawing inference from the abrupt discontinuity in governance at borders, whereas other determinants of income vary smoothly across borders. I extend this literature by assessing the importance of sub-national variation in institutional quality at provincial borders in China. Employing nighttime lights emissions as a proxy for income, across multiple specifications I find no evidence in favour of an institutionalist account of the comparative development of Chinese provinces."
http://arxiv.org/abs/2001.01116v2,Bayesian Median Autoregression for Robust Time Series Forecasting,2020-01-04 19:44:33+00:00,"['Zijian Zeng', 'Meng Li']",stat.AP,"We develop a Bayesian median autoregressive (BayesMAR) model for time series forecasting. The proposed method utilizes time-varying quantile regression at the median, favorably inheriting the robustness of median regression in contrast to the widely used mean-based methods. Motivated by a working Laplace likelihood approach in Bayesian quantile regression, BayesMAR adopts a parametric model bearing the same structure as autoregressive models by altering the Gaussian error to Laplace, leading to a simple, robust, and interpretable modeling strategy for time series forecasting. We estimate model parameters by Markov chain Monte Carlo. Bayesian model averaging is used to account for model uncertainty, including the uncertainty in the autoregressive order, in addition to a Bayesian model selection approach. The proposed methods are illustrated using simulations and real data applications. An application to U.S. macroeconomic data forecasting shows that BayesMAR leads to favorable and often superior predictive performance compared to the selected mean-based alternatives under various loss functions that encompass both point and probabilistic forecasts. The proposed methods are generic and can be used to complement a rich class of methods that build on autoregressive models."
http://arxiv.org/abs/2001.08615v1,Knowledge Graphs for Innovation Ecosystems,2020-01-09 08:02:32+00:00,"['Alberto Tejero', 'Victor Rodriguez-Doncel', 'Ivan Pau']",cs.IR,"Innovation ecosystems can be naturally described as a collection of networked entities, such as experts, institutions, projects, technologies and products. Representing in a machine-readable form these entities and their relations is not entirely attainable, due to the existence of abstract concepts such as knowledge and due to the confidential, non-public nature of this information, but even its partial depiction is of strong interest. The representation of innovation ecosystems incarnated as knowledge graphs would enable the generation of reports with new insights, the execution of advanced data analysis tasks. An ontology to capture the essential entities and relations is presented, as well as the description of data sources, which can be used to populate innovation knowledge graphs. Finally, the application case of the Universidad Politecnica de Madrid is presented, as well as an insight of future applications."
http://arxiv.org/abs/2003.04066v3,Unit Root Testing with Slowly Varying Trends,2020-03-09 12:30:31+00:00,['Sven Otto'],econ.EM,"A unit root test is proposed for time series with a general nonlinear deterministic trend component. It is shown that asymptotically the pooled OLS estimator of overlapping blocks filters out any trend component that satisfies some Lipschitz condition. Under both fixed-$b$ and small-$b$ block asymptotics, the limiting distribution of the t-statistic for the unit root hypothesis is derived. Nuisance parameter corrections provide heteroskedasticity-robust tests, and serial correlation is accounted for by pre-whitening. A Monte Carlo study that considers slowly varying trends yields both good size and improved power results for the proposed tests when compared to conventional unit root tests."
http://arxiv.org/abs/2003.04129v1,Effect of segregation on inequality in kinetic models of wealth exchange,2020-03-04 16:23:56+00:00,"['Lennart Fernandes', 'Jacques Tempere']",physics.soc-ph,"Empirical distributions of wealth and income can be reproduced using simplified agent-based models of economic interactions, analogous to microscopic collisions of gas particles. Building upon these models of freely interacting agents, we explore the effect of a segregated economic network in which interactions are restricted to those between agents of similar wealth. Agents on a 2D lattice undergo kinetic exchanges with their nearest neighbours, while continuously switching places to minimize local wealth differences. A spatial concentration of wealth leads to a steady state with increased global inequality and a magnified distinction between local and global measures of combatting poverty. Individual saving propensity proves ineffective in the segregated economy, while redistributive taxation transcends the spatial inhomogeneity and greatly reduces inequality. Adding fluctuations to the segregation dynamics, we observe a sharp phase transition to lower inequality at a critical temperature, accompanied by a sudden change in the distribution of the wealthy elite."
http://arxiv.org/abs/2003.03540v2,SkillCheck: An Incentive-based Certification System using Blockchains,2020-03-07 09:12:46+00:00,"['Jay Gupta', 'Swaprava Nath']",cs.CR,"Skill verification is a central problem in workforce hiring. Companies and academia often face the difficulty of ascertaining the skills of an applicant since the certifications of the skills claimed by a candidate are generally not immediately verifiable and costly to test. Blockchains have been proposed in the literature for skill verification and tamper-proof information storage in a decentralized manner. However, most of these approaches deal with storing the certificates issued by traditional universities on the blockchain. Among the few techniques that consider the certification procedure itself, questions like (a) scalability with limited staff, (b) uniformity of grades over multiple evaluators, or (c) honest effort extraction from the evaluators are usually not addressed. We propose a blockchain-based platform named SkillCheck, which considers the questions above, and ensure several desirable properties. The platform incentivizes effort in grading via payments with tokens which it generates from the payments of the users of the platform, e.g., the recruiters and test-takers. We provide a detailed description of the design of the platform along with the provable properties of the algorithm."
http://arxiv.org/abs/2002.09037v2,Sustainability and Fairness Simulations Based on Decision-Making Model of Utility Function and Norm Function,2020-02-19 10:20:10+00:00,"['Takeshi Kato', 'Yasuyuki Kudo', 'Junichi Miyakoshi', 'Jun Otsuka', 'Hayato Saigo', 'Kaori Karasawa', 'Hiroyuki Yamaguchi', 'Yoshinori Hiroi', 'Yasuo Deguchi']",econ.GN,"We introduced a decision-making model based on value functions that included individualistic utility function and socio-constructivistic norm function and proposed a norm-fostering process that recursively updates norm function through mutual recognition between the self and others. As an example, we looked at the resource-sharing problem typical of economic activities and assumed the distribution of individual actions to define the (1) norm function fostered through mutual comparison of value/action ratio based on the equity theory (progressive tax-like), (2) norm function proportional to resource utilization (proportional tax-like) and (3) fixed norm function independent of resource utilization (fixed tax-like). By carrying out numerical simulation, we showed that the progressive tax-like norm function (i) does not increase disparity for the distribution of the actions, unlike the other norm functions, and (ii) has high resource productivity and low Gini coefficient. Therefore the progressive tax-like norm function has the highest sustainability and fairness."
http://arxiv.org/abs/2003.00276v1,Identification of Random Coefficient Latent Utility Models,2020-02-29 15:23:50+00:00,"['Roy Allen', 'John Rehbeck']",econ.EM,"This paper provides nonparametric identification results for random coefficient distributions in perturbed utility models. We cover discrete and continuous choice models. We establish identification using variation in mean quantities, and the results apply when an analyst observes aggregate demands but not whether goods are chosen together. We require exclusion restrictions and independence between random slope coefficients and random intercepts. We do not require regressors to have large supports or parametric assumptions."
http://arxiv.org/abs/2002.04470v1,Generalized Poisson Difference Autoregressive Processes,2020-02-11 15:21:24+00:00,"['Giulia Carallo', 'Roberto Casarin', 'Christian P. Robert']",stat.ME,This paper introduces a new stochastic process with values in the set Z of integers with sign. The increments of process are Poisson differences and the dynamics has an autoregressive structure. We study the properties of the process and exploit the thinning representation to derive stationarity conditions and the stationary distribution of the process. We provide a Bayesian inference method and an efficient posterior approximation procedure based on Monte Carlo. Numerical illustrations on both simulated and real data show the effectiveness of the proposed inference.
http://arxiv.org/abs/2002.04508v1,Ramsey Optimal Policy versus Multiple Equilibria with Fiscal and Monetary Interactions,2020-02-11 16:10:47+00:00,"['Jean-Bernard Chatelain', 'Kirsten Ralf']",econ.GN,"We consider a frictionless constant endowment economy based on Leeper (1991). In this economy, it is shown that, under an ad-hoc monetary rule and an ad-hoc fiscal rule, there are two equilibria. One has active monetary policy and passive fiscal policy, while the other has passive monetary policy and active fiscal policy. We consider an extended setup in which the policy maker minimizes a loss function under quasi-commitment, as in Schaumburg and Tambalotti (2007). Under this formulation there exists a unique Ramsey equilibrium, with an interest rate peg and a passive fiscal policy. We thank John P. Conley, Luis de Araujo and one referree for their very helpful comments."
http://arxiv.org/abs/2002.05384v1,Long-term prediction intervals of economic time series,2020-02-13 08:11:18+00:00,"['Marek Chudy', 'Sayar Karmakar', 'Wei Biao Wu']",econ.EM,We construct long-term prediction intervals for time-aggregated future values of univariate economic time series. We propose computational adjustments of the existing methods to improve coverage probability under a small sample constraint. A pseudo-out-of-sample evaluation shows that our methods perform at least as well as selected alternative methods based on model-implied Bayesian approaches and bootstrapping. Our most successful method yields prediction intervals for eight macroeconomic indicators over a horizon spanning several decades.
http://arxiv.org/abs/2002.07163v1,Satellite reveals age and extent of oil palm plantations in Southeast Asia,2020-02-17 09:02:36+00:00,"['Olha Danylo', 'Johannes Pirker', 'Guido Lemoine', 'Guido Ceccherini', 'Linda See', 'Ian McCallum', 'Hadi', 'Florian Kraxner', 'Frédéric Achard', 'Steffen Fritz']",econ.GN,"In recent decades, global oil palm production has shown an abrupt increase, with almost 90% produced in Southeast Asia alone. Monitoring oil palm is largely based on national surveys and inventories or one-off mapping studies. However, they do not provide detailed spatial extent or timely updates and trends in oil palm expansion or age. Palm oil yields vary significantly with plantation age, which is critical for landscape-level planning. Here we show the extent and age of oil palm plantations for the year 2017 across Southeast Asia using remote sensing. Satellites reveal a total of 11.66 (+/- 2.10) million hectares (Mha) of plantations with more than 45% located in Sumatra. Plantation age varies from ~7 years in Kalimantan to ~13 in Insular Malaysia. More than half the plantations on Kalimantan are young (<7 years) and not yet in full production compared to Insular Malaysia where 45% of plantations are older than 15 years, with declining yields. For the first time, these results provide a consistent, independent, and transparent record of oil palm plantation extent and age structure, which are complementary to national statistics."
http://arxiv.org/abs/2002.06341v2,The structure of two-valued strategy-proof social choice functions with indifference,2020-02-15 08:41:18+00:00,"['Achille Basile', 'Surekha Rao', 'K. P. S. Bhaskara Rao']",econ.TH,"We give a structure theorem for all coalitionally strategy-proof social choice functions whose range is a subset of cardinality two of a given larger set of alternatives.
  We provide this in the case where the voters/agents are allowed to express indifference and the domain consists of profiles of preferences over a society of arbitrary cardinality. The theorem, that takes the form of a representation formula, can be used to construct all functions under consideration."
http://arxiv.org/abs/2001.05095v2,Production externalities and dispersion process in a multi-region economy,2020-01-15 01:18:39+00:00,"['Minoru Osawa', 'José M. Gaspar']",econ.GN,"We consider an economic geography model with two inter-regional proximity structures: one governing goods trade and the other governing production externalities across regions. We investigate how the introduction of the latter affects the timing of endogenous agglomeration and the spatial distribution of workers across regions. As transportation costs decline, the economy undergoes a progressive dispersion process. Mono-centric agglomeration emerges when inter-regional trade and/or production externalities incur high transportation costs, while uniform dispersion occurs when these costs become negligibly small (i.e., when distance dies). In multi-regional geography, the network structure of production externalities can determine the geographical distribution of workers as economic integration increases. If production externalities are governed solely by geographical distance, a mono-centric spatial distribution emerges in the form of suburbanization. However, if geographically distant pairs of regions are connected through tight production linkages, multi-centric spatial distribution can be sustainable."
http://arxiv.org/abs/2002.05209v3,Decreasing market value of variable renewables can be avoided by policy action,2020-02-07 11:34:05+00:00,"['T. Brown', 'L. Reichenberg']",q-fin.GN,"Although recent studies have shown that electricity systems with shares of wind and solar above 80% can be affordable, economists have raised concerns about market integration. Correlated generation from variable renewable sources depresses market prices, which can cause wind and solar to cannibalise their own revenues and prevent them from covering their costs from the market. This cannibalisation appears to set limits on the integration of wind and solar, and thus to contradict studies that show that high shares are cost effective. Here we show from theory and with simulation examples how market incentives interact with prices, revenue and costs for renewable electricity systems. The decline in average revenue seen in some recent literature is due to an implicit policy assumption that technologies are forced into the system, whether it be with subsidies or quotas. This decline is mathematically guaranteed regardless of whether the subsidised technology is variable or not. If instead the driving policy is a carbon dioxide cap or tax, wind and solar shares can rise without cannibalising their own market revenue, even at penetrations of wind and solar above 80%. The strong dependence of market value on the policy regime means that market value needs to be used with caution as a measure of market integration. Declining market value is not necessarily a sign of integration problems, but rather a result of policy choices."
http://arxiv.org/abs/2002.00208v3,Variable-lag Granger Causality and Transfer Entropy for Time Series Analysis,2020-02-01 14:03:01+00:00,"['Chainarong Amornbunchornvej', 'Elena Zheleva', 'Tanya Berger-Wolf']",cs.LG,"Granger causality is a fundamental technique for causal inference in time series data, commonly used in the social and biological sciences. Typical operationalizations of Granger causality make a strong assumption that every time point of the effect time series is influenced by a combination of other time series with a fixed time delay. The assumption of fixed time delay also exists in Transfer Entropy, which is considered to be a non-linear version of Granger causality. However, the assumption of the fixed time delay does not hold in many applications, such as collective behavior, financial markets, and many natural phenomena. To address this issue, we develop Variable-lag Granger causality and Variable-lag Transfer Entropy, generalizations of both Granger causality and Transfer Entropy that relax the assumption of the fixed time delay and allow causes to influence effects with arbitrary time delays. In addition, we propose methods for inferring both variable-lag Granger causality and Transfer Entropy relations. In our approaches, we utilize an optimal warping path of Dynamic Time Warping (DTW) to infer variable-lag causal relations. We demonstrate our approaches on an application for studying coordinated collective behavior and other real-world casual-inference datasets and show that our proposed approaches perform better than several existing methods in both simulated and real-world datasets. Our approaches can be applied in any domain of time series analysis. The software of this work is available in the R-CRAN package: VLTimeCausality."
http://arxiv.org/abs/2002.04346v2,Identifiability and Estimation of Possibly Non-Invertible SVARMA Models: A New Parametrisation,2020-02-11 12:35:14+00:00,['Bernd Funovits'],econ.EM,"This article deals with parameterisation, identifiability, and maximum likelihood (ML) estimation of possibly non-invertible structural vector autoregressive moving average (SVARMA) models driven by independent and non-Gaussian shocks. In contrast to previous literature, the novel representation of the MA polynomial matrix using the Wiener-Hopf factorisation (WHF) focuses on the multivariate nature of the model, generates insights into its structure, and uses this structure for devising optimisation algorithms. In particular, it allows to parameterise the location of determinantal zeros inside and outside the unit circle, and it allows for MA zeros at zero, which can be interpreted as informational delays. This is highly relevant for data-driven evaluation of Dynamic Stochastic General Equilibrium (DSGE) models. Typically imposed identifying restrictions on the shock transmission matrix as well as on the determinantal root location are made testable. Furthermore, we provide low level conditions for asymptotic normality of the ML estimator and analytic expressions for the score and the information matrix. As application, we estimate the Blanchard and Quah model and show that our method provides further insights regarding non-invertibility using a standard macroeconometric model. These and further analyses are implemented in a well documented R-package."
http://arxiv.org/abs/2003.06023v5,Causal Spillover Effects Using Instrumental Variables,2020-03-12 21:17:21+00:00,['Gonzalo Vazquez-Bare'],econ.EM,"I set up a potential outcomes framework to analyze spillover effects using instrumental variables. I characterize the population compliance types in a setting in which spillovers can occur on both treatment take-up and outcomes, and provide conditions for identification of the marginal distribution of compliance types. I show that intention-to-treat (ITT) parameters aggregate multiple direct and spillover effects for different compliance types, and hence do not have a clear link to causally interpretable parameters. Moreover, rescaling ITT parameters by first-stage estimands generally recovers a weighted combination of average effects where the sum of weights is larger than one. I then analyze identification of causal direct and spillover effects under one-sided noncompliance, and show that causal effects can be estimated by 2SLS in this case. I illustrate the proposed methods using data from an experiment on social interactions and voting behavior. I also introduce an alternative assumption, independence of peers' types, that identifies parameters of interest under two-sided noncompliance by restricting the amount of heterogeneity in average potential outcomes."
http://arxiv.org/abs/2001.04324v3,Panel Data Quantile Regression for Treatment Effect Models,2020-01-13 15:05:52+00:00,['Takuya Ishihara'],stat.ME,"In this study, we develop a novel estimation method for quantile treatment effects (QTE) under rank invariance and rank stationarity assumptions. Ishihara (2020) explores identification of the nonseparable panel data model under these assumptions and proposes a parametric estimation based on the minimum distance method. However, when the dimensionality of the covariates is large, the minimum distance estimation using this process is computationally demanding. To overcome this problem, we propose a two-step estimation method based on the quantile regression and minimum distance methods. We then show the uniform asymptotic properties of our estimator and the validity of the nonparametric bootstrap. The Monte Carlo studies indicate that our estimator performs well in finite samples. Finally, we present two empirical illustrations, to estimate the distributional effects of insurance provision on household production and TV watching on child cognitive development."
http://arxiv.org/abs/2002.02097v4,Dependence-Robust Inference Using Resampled Statistics,2020-02-06 04:49:51+00:00,['Michael P. Leung'],econ.EM,"We develop inference procedures robust to general forms of weak dependence. The procedures utilize test statistics constructed by resampling in a manner that does not depend on the unknown correlation structure of the data. We prove that the statistics are asymptotically normal under the weak requirement that the target parameter can be consistently estimated at the parametric rate. This holds for regular estimators under many well-known forms of weak dependence and justifies the claim of dependence-robustness. We consider applications to settings with unknown or complicated forms of dependence, with various forms of network dependence as leading examples. We develop tests for both moment equalities and inequalities."
http://arxiv.org/abs/2001.03197v3,Competition between shared autonomous vehicles and public transit: A case study in Singapore,2020-01-09 19:34:54+00:00,"['Baichuan Mo', 'Zhejing Cao', 'Hongmou Zhang', 'Yu Shen', 'Jinhua Zhao']",physics.soc-ph,"Emerging autonomous vehicles (AV) can either supplement the public transportation (PT) system or compete with it. This study examines the competitive perspective where both AV and PT operators are profit-oriented with dynamic adjustable supply strategies under five regulatory structures regarding whether the AV operator is allowed to change the fleet size and whether the PT operator is allowed to adjust headway. Four out of the five scenarios are constrained competition while the other one focuses on unconstrained competition to find the Nash Equilibrium. We evaluate the competition process as well as the system performance from the standpoints of four stakeholders -- the AV operator, the PT operator, passengers, and the transport authority. We also examine the impact of PT subsidies on the competition results including both demand-based and supply-based subsidies. A heuristic algorithm is proposed to update supply strategies for AV and PT based on the operators' historical actions and profits. An agent-based simulation model is implemented in the first-mile scenario in Tampines, Singapore. We find that the competition can result in higher profits and higher system efficiency for both operators compared to the status quo. After the supply updates, the PT services are spatially concentrated to shorter routes feeding directly to the subway station and temporally concentrated to peak hours. On average, the competition reduces the travel time of passengers but increases their travel costs. Nonetheless, the generalized travel cost is reduced when incorporating the value of time. With respect to the system efficiency, the bus supply adjustment increases the average vehicle load and reduces the total vehicle kilometer traveled measured by the passenger car equivalent (PCE), while the AV supply adjustment does the opposite."
http://arxiv.org/abs/2003.07545v4,Interpretable Personalization via Policy Learning with Linear Decision Boundaries,2020-03-17 05:48:27+00:00,"['Zhaonan Qu', 'Isabella Qian', 'Zhengyuan Zhou']",cs.LG,"With the rise of the digital economy and an explosion of available information about consumers, effective personalization of goods and services has become a core business focus for companies to improve revenues and maintain a competitive edge. This paper studies the personalization problem through the lens of policy learning, where the goal is to learn a decision-making rule (a policy) that maps from consumer and product characteristics (features) to recommendations (actions) in order to optimize outcomes (rewards). We focus on using available historical data for offline learning with unknown data collection procedures, where a key challenge is the non-random assignment of recommendations. Moreover, in many business and medical applications, interpretability of a policy is essential. We study the class of policies with linear decision boundaries to ensure interpretability, and propose learning algorithms using tools from causal inference to address unbalanced treatments. We study several optimization schemes to solve the associated non-convex, non-smooth optimization problem, and find that a Bayesian optimization algorithm is effective. We test our algorithm with extensive simulation studies and apply it to an anonymized online marketplace customer purchase dataset, where the learned policy outputs a personalized discount recommendation based on customer and product features in order to maximize gross merchandise value (GMV) for sellers. Our learned policy improves upon the platform's baseline by 88.2\% in net sales revenue, while also providing informative insights on which features are important for the decision-making process. Our findings suggest that our proposed policy learning framework using tools from causal inference and Bayesian optimization provides a promising practical approach to interpretable personalization across a wide range of applications."
http://arxiv.org/abs/2002.07229v3,How Do Expectations Affect Learning About Fundamentals? Some Experimental Evidence,2020-02-17 20:06:42+00:00,"['Kieran Marray', 'Nikhil Krishna', 'Jarel Tang']",econ.GN,"We test how individuals with incorrect beliefs about their ability learn about an external parameter (`fundamental') when they cannot separately identify the effects of their ability, actions, and the parameter on their output. Heidhues et al. (2018) argue that learning makes overconfident individuals worse off as their beliefs about the fundamental get less accurate, causing them to take worse actions. In our experiment, subjects take incorrectly-marked tests, and we measure how they learn about the marker's accuracy over time. Overconfident subjects put in less effort, and their beliefs about the marker's accuracy got worse, as they learnt. Beliefs about the proportion of correct answers marked as correct fell by 0.05 over the experiment. We find no effect in underconfident subjects."
http://arxiv.org/abs/2001.10996v1,Functional Sequential Treatment Allocation with Covariates,2020-01-29 18:08:53+00:00,"['Anders Bredahl Kock', 'David Preinerstorfer', 'Bezirgen Veliyev']",stat.ML,"We consider a multi-armed bandit problem with covariates. Given a realization of the covariate vector, instead of targeting the treatment with highest conditional expectation, the decision maker targets the treatment which maximizes a general functional of the conditional potential outcome distribution, e.g., a conditional quantile, trimmed mean, or a socio-economic functional such as an inequality, welfare or poverty measure. We develop expected regret lower bounds for this problem, and construct a near minimax optimal assignment policy."
http://arxiv.org/abs/2006.03301v1,Inflation Dynamics of Financial Shocks,2020-06-05 08:39:17+00:00,['Olli Palmén'],econ.EM,"We study the effects of financial shocks on the United States economy by using a Bayesian structural vector autoregressive (SVAR) model that exploits the non-normalities in the data. We use this method to uniquely identify the model and employ inequality constraints to single out financial shocks. The results point to the existence of two distinct financial shocks that have opposing effects on inflation, which supports the idea that financial shocks are transmitted to the real economy through both demand and supply side channels."
http://arxiv.org/abs/2006.14243v1,Matching Multidimensional Types: Theory and Application,2020-06-25 08:19:32+00:00,['Veli Safak'],econ.EM,"Becker (1973) presents a bilateral matching model in which scalar types describe agents. For this framework, he establishes the conditions under which positive sorting between agents' attributes is the unique market outcome. Becker's celebrated sorting result has been applied to address many economic questions. However, recent empirical studies in the fields of health, household, and labor economics suggest that agents have multiple outcome-relevant attributes. In this paper, I study a matching model with multidimensional types. I offer multidimensional generalizations of concordance and supermodularity to construct three multidimensional sorting patterns and two classes of multidimensional complementarities. For each of these sorting patterns, I identify the sufficient conditions which guarantee its optimality. In practice, we observe sorting patterns between observed attributes that are aggregated over unobserved characteristics. To reconcile theory with practice, I establish the link between production complementarities and the aggregated sorting patterns. Finally, I examine the relationship between agents' health status and their spouses' education levels among U.S. households within the framework for multidimensional matching markets. Preliminary analysis reveals a weak positive association between agents' health status and their spouses' education levels. This weak positive association is estimated to be a product of three factors: (a) an attraction between better-educated individuals, (b) an attraction between healthier individuals, and (c) a weak positive association between agents' health status and their education levels. The attraction channel suggests that the insurance risk associated with a two-person family plan is higher than the aggregate risk associated with two individual policies."
http://arxiv.org/abs/2004.02706v1,What do online listings tell us about the housing market?,2020-04-06 14:40:09+00:00,"['Michele Loberto', 'Andrea Luciani', 'Marco Pangallo']",econ.EM,"Traditional data sources for the analysis of housing markets show several limitations, that recently started to be overcome using data coming from housing sales advertisements (ads) websites. In this paper, using a large dataset of ads in Italy, we provide the first comprehensive analysis of the problems and potential of these data. The main problem is that multiple ads (""duplicates"") can correspond to the same housing unit. We show that this issue is mainly caused by sellers' attempt to increase visibility of their listings. Duplicates lead to misrepresentation of the volume and composition of housing supply, but this bias can be corrected by identifying duplicates with machine learning tools. We then focus on the potential of these data. We show that the timeliness, granularity, and online nature of these data allow monitoring of housing demand, supply and liquidity, and that the (asking) prices posted on the website can be more informative than transaction prices."
http://arxiv.org/abs/2006.01852v3,Subjective Complexity Under Uncertainty,2020-06-02 18:03:11+00:00,['Quitzé Valenzuela-Stookey'],econ.TH,"Complexity of the problem of choosing among uncertain acts is a salient feature of many of the environments in which departures from expected utility theory are observed. I propose and axiomatize a model of choice under uncertainty in which the size of the partition with respect to which an act is measurable arises endogenously as a measure of subjective complexity. I derive a representation of incomplete Simple Bounds preferences in which acts that are complex from the perspective of the decision maker are bracketed by simple acts to which they are related by statewise dominance. The key axioms are motivated by a model of learning from limited data. I then consider choice behavior characterized by a ""cautious completion"" of Simple Bounds preferences, and discuss the relationship between this model and models of ambiguity aversion. I develop general comparative statics results, and explore applications to portfolio choice, contracting, and insurance choice."
http://arxiv.org/abs/2004.03865v1,Manipulation-Proof Machine Learning,2020-04-08 08:04:01+00:00,"['Daniel Björkegren', 'Joshua E. Blumenstock', 'Samsun Knight']",econ.TH,"An increasing number of decisions are guided by machine learning algorithms. In many settings, from consumer credit to criminal justice, those decisions are made by applying an estimator to data on an individual's observed behavior. But when consequential decisions are encoded in rules, individuals may strategically alter their behavior to achieve desired outcomes. This paper develops a new class of estimator that is stable under manipulation, even when the decision rule is fully transparent. We explicitly model the costs of manipulating different behaviors, and identify decision rules that are stable in equilibrium. Through a large field experiment in Kenya, we show that decision rules estimated with our strategy-robust method outperform those based on standard supervised learning approaches."
http://arxiv.org/abs/2004.06289v1,On Vickrey's Income Averaging,2020-04-14 03:30:19+00:00,"['Stefan Steinerberger', 'Aleh Tsyvinski']",econ.TH,"We consider a small set of axioms for income averaging -- recursivity, continuity, and the boundary condition for the present. These properties yield a unique averaging function that is the density of the reflected Brownian motion with a drift started at the current income and moving over the past incomes. When averaging is done over the short past, the weighting function is asymptotically converging to a Gaussian. When averaging is done over the long horizon, the weighing function converges to the exponential distribution. For all intermediate averaging scales, we derive an explicit solution that interpolates between the two."
http://arxiv.org/abs/2006.04968v2,Heterogeneous Effects of Job Displacement on Earnings,2020-06-08 22:27:05+00:00,"['Afrouz Azadikhah Jahromi', 'Brantly Callaway']",econ.EM,"This paper considers how the effect of job displacement varies across different individuals. In particular, our interest centers on features of the distribution of the individual-level effect of job displacement. Identifying features of this distribution is particularly challenging -- e.g., even if we could randomly assign workers to be displaced or not, many of the parameters that we consider would not be point identified. We exploit our access to panel data, and our approach relies on comparing outcomes of displaced workers to outcomes the same workers would have experienced if they had not been displaced and if they maintained the same rank in the distribution of earnings as they had before they were displaced. Using data from the Displaced Workers Survey, we find that displaced workers earn about $157 per week less, on average, than they would have earned if they had not been displaced. We also find that there is substantial heterogeneity. We estimate that 42% of workers have higher earnings than they would have had if they had not been displaced and that a large fraction of workers have experienced substantially more negative effects than the average effect of displacement. Finally, we also document major differences in the distribution of the effect of job displacement across education levels, sex, age, and counterfactual earnings levels. Throughout the paper, we rely heavily on quantile regression. First, we use quantile regression as a flexible (yet feasible) first step estimator of conditional distributions and quantile functions that our main results build on. We also use quantile regression to study how covariates affect the distribution of the individual-level effect of job displacement."
http://arxiv.org/abs/2006.14023v1,Asset Prices and Capital Share Risks: Theory and Evidence,2020-06-24 19:59:47+00:00,"['Joseph P. Byrne', 'Boulis M. Ibrahim', 'Xiaoyu Zong']",econ.EM,"An asset pricing model using long-run capital share growth risk has recently been found to successfully explain U.S. stock returns. Our paper adopts a recursive preference utility framework to derive an heterogeneous asset pricing model with capital share risks.While modeling capital share risks, we account for the elevated consumption volatility of high income stockholders. Capital risks have strong volatility effects in our recursive asset pricing model. Empirical evidence is presented in which capital share growth is also a source of risk for stock return volatility. We uncover contrasting unconditional and conditional asset pricing evidence for capital share risks."
http://arxiv.org/abs/2006.03441v3,Capital and Labor Income Pareto Exponents across Time and Space,2020-06-03 20:44:36+00:00,"['Tjeerd de Vries', 'Alexis Akira Toda']",econ.EM,"We estimate capital and labor income Pareto exponents across 475 country-year observations that span 52 countries over half a century (1967-2018). We document two stylized facts: (i) capital income is more unequally distributed than labor income in the tail; namely, the capital exponent (1-3, median 1.46) is smaller than labor (2-5, median 3.35), and (ii) capital and labor exponents are nearly uncorrelated. To explain these findings, we build an incomplete market model with job ladders and capital income risk that gives rise to a capital income Pareto exponent smaller than but nearly unrelated to the labor exponent. Our results suggest the importance of distinguishing income and wealth inequality."
http://arxiv.org/abs/2006.14047v1,Dynamic Effects of Persistent Shocks,2020-06-24 21:02:47+00:00,"['Mario Alloza', 'Jesus Gonzalo', 'Carlos Sanz']",econ.EM,"We provide evidence that many narrative shocks used by prominent literature are persistent. We show that the two leading methods to estimate impulse responses to an independently identified shock (local projections and distributed lag models) treat persistence differently, hence identifying different objects. We propose corrections to re-establish the equivalence between local projections and distributed lag models, providing applied researchers with methods and guidance to estimate their desired object of interest. We apply these methods to well-known empirical work and find that how persistence is treated has a sizable impact on the estimates of dynamic effects."
http://arxiv.org/abs/2006.13036v1,Vocational Training Programs and Youth Labor Market Outcomes: Evidence from Nepal,2020-06-22 08:19:24+00:00,"['S. Chakravarty', 'M. Lundberg', 'P. Nikolov', 'J. Zenker']",econ.EM,"Lack of skills is arguably one of the most important determinants of high levels of unemployment and poverty. In response, policymakers often initiate vocational training programs in effort to enhance skill formation among the youth. Using a regression-discontinuity design, we examine a large youth training intervention in Nepal. We find, twelve months after the start of the training program, that the intervention generated an increase in non-farm employment of 10 percentage points (ITT estimates) and up to 31 percentage points for program compliers (LATE estimates). We also detect sizeable gains in monthly earnings. Women who start self-employment activities inside their homes largely drive these impacts. We argue that low baseline educational levels and non-farm employment levels and Nepal's social and cultural norms towards women drive our large program impacts. Our results suggest that the program enables otherwise underemployed women to earn an income while staying at home - close to household errands and in line with the socio-cultural norms that prevent them from taking up employment outside the house."
http://arxiv.org/abs/2006.05859v1,Trading Privacy for the Greater Social Good: How Did America React During COVID-19?,2020-06-10 14:36:17+00:00,"['Anindya Ghose', 'Beibei Li', 'Meghanath Macha', 'Chenshuo Sun', 'Natasha Ying Zhang Foutz']",econ.EM,"Digital contact tracing and analysis of social distancing from smartphone location data are two prime examples of non-therapeutic interventions used in many countries to mitigate the impact of the COVID-19 pandemic. While many understand the importance of trading personal privacy for the public good, others have been alarmed at the potential for surveillance via measures enabled through location tracking on smartphones. In our research, we analyzed massive yet atomic individual-level location data containing over 22 billion records from ten Blue (Democratic) and ten Red (Republican) cities in the U.S., based on which we present, herein, some of the first evidence of how Americans responded to the increasing concerns that government authorities, the private sector, and public health experts might use individual-level location data to track the COVID-19 spread. First, we found a significant decreasing trend of mobile-app location-sharing opt-out. Whereas areas with more Democrats were more privacy-concerned than areas with more Republicans before the advent of the COVID-19 pandemic, there was a significant decrease in the overall opt-out rates after COVID-19, and this effect was more salient among Democratic than Republican cities. Second, people who practiced social distancing (i.e., those who traveled less and interacted with fewer close contacts during the pandemic) were also less likely to opt-out, whereas the converse was true for people who practiced less social-distancing. This relationship also was more salient among Democratic than Republican cities. Third, high-income populations and males, compared with low-income populations and females, were more privacy-conscientious and more likely to opt-out of location tracking."
http://arxiv.org/abs/2006.06595v1,Confidence sets for dynamic poverty indexes,2020-06-11 16:46:40+00:00,"[""Guglielmo D'Amico"", 'Riccardo De Blasis', 'Philippe Regnault']",econ.EM,"In this study, we extend the research on the dynamic poverty indexes, namely the dynamic Headcount ratio, the dynamic income-gap ratio, the dynamic Gini and the dynamic Sen, proposed in D'Amico and Regnault (2018). The contribution is twofold. First, we extend the computation of the dynamic Gini index, thus the Sen index accordingly, with the inclusion of the inequality within each class of poverty where people are classified according to their income. Second, for each poverty index, we establish a central limit theorem that gives us the possibility to determine the confidence sets. An application to the Italian income data from 1998 to 2012 confirms the effectiveness of the considered approach and the possibility to determine the evolution of poverty and inequality in real economies."
http://arxiv.org/abs/2006.01290v3,Revisiting money and labor for valuing environmental goods and services in developing countries,2020-06-01 22:19:06+00:00,"['Habtamu Tilahun Kassahun', 'Jette Bredahl Jacobsen', 'Charles F. Nicholson']",econ.EM,"Many Stated Preference studies conducted in developing countries provide a low willingness to pay (WTP) for a wide range of goods and services. However, recent studies in these countries indicate that this may partly be a result of the choice of payment vehicle, not the preference for the good. Thus, low WTP may not indicate a low welfare effect for public projects in developing countries. We argue that in a setting where 1) there is imperfect substitutability between money and other measures of wealth (e.g. labor), and 2) institutions are perceived to be corrupt, including payment vehicles that are currently available to the individual and less pron to corruption may be needed to obtain valid welfare estimates. Otherwise, we risk underestimating the welfare benefit of projects. We demonstrate this through a rural household contingent valuation (CV) survey designed to elicit the value of access to reliable irrigation water in Ethiopia. Of the total average annual WTP for access to reliable irrigation service, cash contribution comprises only 24.41 %. The implication is that socially desirable projects might be rejected based on cost-benefit analysis as a result of welfare gain underestimation due to mismatch of payment vehicles choice in valuation study."
http://arxiv.org/abs/2004.14953v1,Soft Affirmative Action and Minority Recruitment,2020-04-30 17:01:35+00:00,"['Daniel Fershtman', 'Alessandro Pavan']",econ.TH,"We study search, evaluation, and selection of candidates of unknown quality for a position. We examine the effects of ""soft"" affirmative action policies increasing the relative percentage of minority candidates in the candidate pool. We show that, while meant to encourage minority hiring, such policies may backfire if the evaluation of minority candidates is noisier than that of non-minorities. This may occur even if minorities are at least as qualified and as valuable as non-minorities. The results provide a possible explanation for why certain soft affirmative action policies have proved counterproductive, even in the absence of (implicit) biases."
http://arxiv.org/abs/2006.15183v4,Real-Time Real Economic Activity: Entering and Exiting the Pandemic Recession of 2020,2020-06-26 19:19:10+00:00,['Francis X. Diebold'],econ.EM,"Entering and exiting the Pandemic Recession, I study the high-frequency real-activity signals provided by a leading nowcast, the ADS Index of Business Conditions produced and released in real time by the Federal Reserve Bank of Philadelphia. I track the evolution of real-time vintage beliefs and compare them to a later-vintage chronology. Real-time ADS plunges and then swings as its underlying economic indicators swing, but the ADS paths quickly converge to indicate a return to brisk positive growth by mid-May. I show, moreover, that the daily real activity path was highly correlated with the daily COVID-19 cases. Finally, I provide a comparative assessment of the real-time ADS signals provided when exiting the Great Recession."
http://arxiv.org/abs/2005.09605v6,Evaluating Policies Early in a Pandemic: Bounding Policy Effects with Nonrandomly Missing Data,2020-05-19 17:26:52+00:00,"['Brantly Callaway', 'Tong Li']",econ.EM,"During the early part of the Covid-19 pandemic, national and local governments introduced a number of policies to combat the spread of Covid-19. In this paper, we propose a new approach to bound the effects of such early-pandemic policies on Covid-19 cases and other outcomes while dealing with complications arising from (i) limited availability of Covid-19 tests, (ii) differential availability of Covid-19 tests across locations, and (iii) eligibility requirements for individuals to be tested. We use our approach study the effects of Tennessee's expansion of Covid-19 testing early in the pandemic and find that the policy decreased Covid-19 cases."
http://arxiv.org/abs/2006.00581v1,Shared value economics: an axiomatic approach,2020-05-31 18:25:31+00:00,"['Francisco Salas-Molina', 'Juan Antonio Rodríguez Aguilar', 'Filippo Bistaffa']",cs.GT,"The concept of shared value was introduced by Porter and Kramer as a new conception of capitalism. Shared value describes the strategy of organizations that simultaneously enhance their competitiveness and the social conditions of related stakeholders such as employees, suppliers and the natural environment. The idea has generated strong interest, but also some controversy due to a lack of a precise definition, measurement techniques and difficulties to connect theory to practice. We overcome these drawbacks by proposing an economic framework based on three key aspects: coalition formation, sustainability and consistency, meaning that conclusions can be tested by means of logical deductions and empirical applications. The presence of multiple agents to create shared value and the optimization of both social and economic criteria in decision making represent the core of our quantitative definition of shared value. We also show how economic models can be characterized as shared value models by means of logical deductions. Summarizing, our proposal builds on the foundations of shared value to improve its understanding and to facilitate the suggestion of economic hypotheses, hence accommodating the concept of shared value within modern economic theory."
http://arxiv.org/abs/2006.00160v1,Parametric Modeling of Quantile Regression Coefficient Functions with Longitudinal Data,2020-05-30 03:29:10+00:00,"['Paolo Frumento', 'Matteo Bottai', 'Iván Fernández-Val']",stat.ME,"In ordinary quantile regression, quantiles of different order are estimated one at a time. An alternative approach, which is referred to as quantile regression coefficients modeling (QRCM), is to model quantile regression coefficients as parametric functions of the order of the quantile. In this paper, we describe how the QRCM paradigm can be applied to longitudinal data. We introduce a two-level quantile function, in which two different quantile regression models are used to describe the (conditional) distribution of the within-subject response and that of the individual effects. We propose a novel type of penalized fixed-effects estimator, and discuss its advantages over standard methods based on $\ell_1$ and $\ell_2$ penalization. We provide model identifiability conditions, derive asymptotic properties, describe goodness-of-fit measures and model selection criteria, present simulation results, and discuss an application. The proposed method has been implemented in the R package qrcm."
http://arxiv.org/abs/2005.14442v1,Competition among Large and Heterogeneous Small Firms,2020-05-29 08:16:19+00:00,"['Lijun Pan', 'Yongjin Wang']",econ.GN,"We extend the model of Parenti (2018) on large and small firms by introducing cost heterogeneity among small firms. We propose a novel necessary and sufficient condition for the existence of such a mixed market structure. Furthermore, in contrast to Parenti (2018), we show that in the presence of cost heterogeneity among small firms, trade liberalization may raise or reduce the mass of small firms in operation."
http://arxiv.org/abs/2006.04280v1,On forward invariance in Lyapunov stability theorem for local stability,2020-06-07 22:08:33+00:00,['Dai Zusai'],math.OC,"Forward invariance of a basin of attraction is often overlooked when using a Lyapunov stability theorem to prove local stability; even if the Lyapunov function decreases monotonically in a neighborhood of an equilibrium, the dynamic may escape from this neighborhood. In this note, we fix this gap by finding a smaller neighborhood that is forward invariant. This helps us to prove local stability more naturally without tracking each solution path. Similarly, we prove a transitivity theorem about basins of attractions without requiring forward invariance.
  Keywords: Lyapunov function, local stability, forward invariance, evolutionary dynamics."
http://arxiv.org/abs/2006.04428v1,"Envy-free Relaxations for Goods, Chores, and Mixed Items",2020-06-08 09:25:31+00:00,"['Kristóf Bérczi', 'Erika R. Bérczi-Kovács', 'Endre Boros', 'Fekadu Tolessa Gedefa', 'Naoyuki Kamiyama', 'Telikepalli Kavitha', 'Yusuke Kobayashi', 'Kazuhisa Makino']",econ.TH,"In fair division problems, we are given a set $S$ of $m$ items and a set $N$ of $n$ agents with individual preferences, and the goal is to find an allocation of items among agents so that each agent finds the allocation fair. There are several established fairness concepts and envy-freeness is one of the most extensively studied ones. However envy-free allocations do not always exist when items are indivisible and this has motivated relaxations of envy-freeness: envy-freeness up to one item (EF1) and envy-freeness up to any item (EFX) are two well-studied relaxations. We consider the problem of finding EF1 and EFX allocations for utility functions that are not necessarily monotone, and propose four possible extensions of different strength to this setting.
  In particular, we present a polynomial-time algorithm for finding an EF1 allocation for two agents with arbitrary utility functions. An example is given showing that EFX allocations need not exist for two agents with non-monotone, non-additive, identical utility functions. However, when all agents have monotone (not necessarily additive) identical utility functions, we prove that an EFX allocation of chores always exists. As a step toward understanding the general case, we discuss two subclasses of utility functions: Boolean utilities that are $\{0,+1\}$-valued functions, and negative Boolean utilities that are $\{0,-1\}$-valued functions. For the latter, we give a polynomial time algorithm that finds an EFX allocation when the utility functions are identical."
http://arxiv.org/abs/2006.05192v2,An Optimal Distributionally Robust Auction,2020-06-09 11:37:13+00:00,['Alex Suzdaltsev'],econ.TH,"An indivisible object may be sold to one of $n$ agents who know their valuations of the object. The seller would like to use a revenue-maximizing mechanism but her knowledge of the valuations' distribution is scarce: she knows only the means (which may be different) and an upper bound for valuations. Valuations may be correlated.
  Using a constructive approach based on duality, we prove that a mechanism that maximizes the worst-case expected revenue among all deterministic dominant-strategy incentive compatible, ex post individually rational mechanisms is such that the object should be awarded to the agent with the highest linear score provided it is nonnegative. Linear scores are bidder-specific linear functions of bids. The set of optimal mechanisms includes other mechanisms but all those have to be close to the optimal linear score auction in a certain sense. When means are high, all optimal mechanisms share the linearity property. Second-price auction without a reserve is an optimal mechanism when the number of symmetric bidders is sufficiently high."
http://arxiv.org/abs/2005.13596v1,"Breiman's ""Two Cultures"" Revisited and Reconciled",2020-05-27 19:02:56+00:00,"['Subhadeep', 'Mukhopadhyay', 'Kaijun Wang']",stat.ML,"In a landmark paper published in 2001, Leo Breiman described the tense standoff between two cultures of data modeling: parametric statistical and algorithmic machine learning. The cultural division between these two statistical learning frameworks has been growing at a steady pace in recent years. What is the way forward? It has become blatantly obvious that this widening gap between ""the two cultures"" cannot be averted unless we find a way to blend them into a coherent whole. This article presents a solution by establishing a link between the two cultures. Through examples, we describe the challenges and potential gains of this new integrated statistical thinking."
http://arxiv.org/abs/2005.13228v1,Oligopoly Dynamics,2020-05-27 08:11:58+00:00,['Bernardo Melo Pimentel'],cs.GT,"The present notes summarise the oligopoly dynamics lectures professor Luís Cabral gave at the Bank of Portugal in September and October 2017. The lectures discuss a set industrial organisation problems in a dynamic environment, namely learning by doing, switching costs, price wars, networks and platforms, and ladder models of innovation. Methodologically, the materials cover analytical solutions of known points (e.g., $δ= 0$), the discussion of firms' strategies based on intuitions derived directly from their value functions with no model solving, and the combination of analytical and numerical procedures to reach model solutions. State space analysis is done for both continuous and discrete cases. All errors are my own."
http://arxiv.org/abs/2006.08069v1,Repeated Communication with Private Lying Cost,2020-06-15 01:08:33+00:00,['Harry Pei'],econ.TH,"I study repeated communication games between a patient sender and a sequence of receivers. The sender has persistent private information about his psychological cost of lying, and in every period, can privately observe the realization of an i.i.d. state before communication takes place. I characterize every type of sender's highest equilibrium payoff. When the highest lying cost in the support of the receivers' prior belief approaches the sender's benefit from lying, every type's highest equilibrium payoff in the repeated communication game converges to his equilibrium payoff in a one-shot Bayesian persuasion game. I also show that in every sender-optimal equilibrium, no type of sender mixes between telling the truth and lying at every history. When there exist ethical types whose lying costs outweigh their benefits, I provide necessary and sufficient conditions for all non-ethical type senders to attain their optimal commitment payoffs. I identify an outside option effect through which the possibility of being ethical decreases every non-ethical type's payoff."
http://arxiv.org/abs/2006.07911v1,Loss Rate Forecasting Framework Based on Macroeconomic Changes: Application to US Credit Card Industry,2020-06-14 14:22:59+00:00,"['Sajjad Taghiyeh', 'David C Lengacher', 'Robert B Handfield']",stat.ML,"A major part of the balance sheets of the largest US banks consists of credit card portfolios. Hence, managing the charge-off rates is a vital task for the profitability of the credit card industry. Different macroeconomic conditions affect individuals' behavior in paying down their debts. In this paper, we propose an expert system for loss forecasting in the credit card industry using macroeconomic indicators. We select the indicators based on a thorough review of the literature and experts' opinions covering all aspects of the economy, consumer, business, and government sectors. The state of the art machine learning models are used to develop the proposed expert system framework. We develop two versions of the forecasting expert system, which utilize different approaches to select between the lags added to each indicator. Among 19 macroeconomic indicators that were used as the input, six were used in the model with optimal lags, and seven indicators were selected by the model using all lags. The features that were selected by each of these models covered all three sectors of the economy. Using the charge-off data for the top 100 US banks ranked by assets from the first quarter of 1985 to the second quarter of 2019, we achieve mean squared error values of 1.15E-03 and 1.04E-03 using the model with optimal lags and the model with all lags, respectively. The proposed expert system gives a holistic view of the economy to the practitioners in the credit card industry and helps them to see the impact of different macroeconomic conditions on their future loss."
http://arxiv.org/abs/2006.07938v1,The energy representation of world GDP,2020-06-14 16:00:23+00:00,['Boris M. Dolgonosov'],econ.GN,The dependence of world GDP on current energy consumption and total energy produced over the previous period and materialized in the form of production infrastructure is studied. The dependence describes empirical data with high accuracy over the entire observation interval 1965-2018.
http://arxiv.org/abs/2006.06519v2,Reserve Price Optimization for First Price Auctions,2020-06-11 15:35:19+00:00,"['Zhe Feng', 'Sébastien Lahaie', 'Jon Schneider', 'Jinchao Ye']",cs.GT,"The display advertising industry has recently transitioned from second- to first-price auctions as its primary mechanism for ad allocation and pricing. In light of this, publishers need to re-evaluate and optimize their auction parameters, notably reserve prices. In this paper, we propose a gradient-based algorithm to adaptively update and optimize reserve prices based on estimates of bidders' responsiveness to experimental shocks in reserves. Our key innovation is to draw on the inherent structure of the revenue objective in order to reduce the variance of gradient estimates and improve convergence rates in both theory and practice. We show that revenue in a first-price auction can be usefully decomposed into a \emph{demand} component and a \emph{bidding} component, and introduce techniques to reduce the variance of each component. We characterize the bias-variance trade-offs of these techniques and validate the performance of our proposed algorithm through experiments on synthetic data and real display ad auctions data from Google ad exchange."
http://arxiv.org/abs/2007.07839v1,COVID-19 Induced Economic Uncertainty: A Comparison between the United Kingdom and the United States,2020-06-29 00:22:07+00:00,['Ugur Korkut Pata'],econ.GN,"The purpose of this study is to investigate the effects of the COVID-19 pandemic on economic policy uncertainty in the US and the UK. The impact of the increase in COVID-19 cases and deaths in the country, and the increase in the number of cases and deaths outside the country may vary. To examine this, the study employs bootstrap ARDL cointegration approach from March 8, 2020 to May 24, 2020. According to the bootstrap ARDL results, a long-run equilibrium relationship is confirmed for five out of the 10 models. The long-term coefficients obtained from the ARDL models suggest that an increase in COVID-19 cases and deaths outside of the UK and the US has a significant effect on economic policy uncertainty. The US is more affected by the increase in the number of COVID-19 cases. The UK, on the other hand, is more negatively affected by the increase in the number of COVID-19 deaths outside the country than the increase in the number of cases. Moreover, another important finding from the study demonstrates that COVID-19 is a factor of great uncertainty for both countries in the short-term."
http://arxiv.org/abs/2006.16939v1,The Equilibrium Existence Duality: Equilibrium with Indivisibilities & Income Effects,2020-06-30 16:21:12+00:00,"['Elizabeth Baldwin', 'Omer Edhan', 'Ravi Jagadeesan', 'Paul Klemperer', 'Alexander Teytelboym']",econ.TH,"We show that, with indivisible goods, the existence of competitive equilibrium fundamentally depends on agents' substitution effects, not their income effects. Our Equilibrium Existence Duality allows us to transport results on the existence of competitive equilibrium from settings with transferable utility to settings with income effects. One consequence is that net substitutability---which is a strictly weaker condition than gross substitutability---is sufficient for the existence of competitive equilibrium. We also extend the ``demand types'' classification of valuations to settings with income effects and give necessary and sufficient conditions for a pattern of substitution effects to guarantee the existence of competitive equilibrium."
http://arxiv.org/abs/2005.12173v1,On Evaluation of Risky Investment Projects. Investment Certainty Equivalence,2020-05-25 15:42:19+00:00,"['Andrey Leonidov', 'Ilya Tipunin', 'Ekaterina Serebryannikova']",q-fin.RM,"The purpose of the study is to propose a methodology for evaluation and ranking of risky investment projects.An investment certainty equivalence approach dual to the conventional separation of riskless and risky contributions based on cash flow certainty equivalence is introduced. Proposed ranking of investment projects is based on gauging them with the Omega measure, which is defined as the ratio of chances to obtain profit/return greater than some critical (minimal acceptable) profitability over the chances to obtain the profit/return less than the critical one.Detailed consideration of alternative riskless investment is presented. Various performance measures characterizing investment projects with a special focus on the role of reinvestment are discussed. Relation between the proposed methodology and the conventional approach based on utilization of risk-adjusted discount rate (RADR) is discussed. Findings are supported with an illustrative example.The methodology proposed can be used to rank projects of different nature, scale and lifespan. In contrast to the conventional RADR approach for investment project evaluation, in the proposed method a risk profile of a specific project is explicitly analyzed in terms of appropriate performance measure distribution. No ad-hoc assumption about suitable risk-premium is made."
http://arxiv.org/abs/2005.12638v2,Decisions and Performance Under Bounded Rationality: A Computational Benchmarking Approach,2020-05-26 11:39:39+00:00,"['Dainis Zegners', 'Uwe Sunde', 'Anthony Strittmatter']",econ.GN,"This paper presents a novel approach to analyze human decision-making that involves comparing the behavior of professional chess players relative to a computational benchmark of cognitively bounded rationality. This benchmark is constructed using algorithms of modern chess engines and allows investigating behavior at the level of individual move-by-move observations, thus representing a natural benchmark for computationally bounded optimization. The analysis delivers novel insights by isolating deviations from this benchmark of bounded rationality as well as their causes and consequences for performance. The findings document the existence of several distinct dimensions of behavioral deviations, which are related to asymmetric positional evaluation in terms of losses and gains, time pressure, fatigue, and complexity. The results also document that deviations from the benchmark do not necessarily entail worse performance. Faster decisions are associated with more frequent deviations from the benchmark, yet they are also associated with better performance. The findings are consistent with an important influence of intuition and experience, thereby shedding new light on the recent debate about computational rationality in cognitive processes."
http://arxiv.org/abs/2006.03023v3,Digital Currency and Economic Crises: Helping States Respond,2020-06-04 17:17:49+00:00,"['Geoffrey Goodell', 'Hazem Danny Al-Nakib', 'Paolo Tasca']",cs.CY,"The current crisis, at the time of writing, has had a profound impact on the financial world, introducing the need for creative approaches to revitalising the economy at the micro level as well as the macro level. In this informal analysis and design proposal, we describe how infrastructure for digital assets can serve as a useful monetary and fiscal policy tool and an enabler of existing tools in the future, particularly during crises, while aligning the trajectory of financial technology innovation toward a brighter future. We propose an approach to digital currency that would allow people without banking relationships to transact electronically and privately, including both internet purchases and point-of-sale purchases that are required to be cashless. We also propose an approach to digital currency that would allow for more efficient and transparent clearing and settlement, implementation of monetary and fiscal policy, and management of systemic risk. The digital currency could be implemented as central bank digital currency (CBDC), or it could be issued by the government and collateralised by public funds or Treasury assets. Our proposed architecture allows both manifestations and would be operated by banks and other money services businesses, operating within a framework overseen by government regulators. We argue that now is the time for action to undertake development of such a system, not only because of the current crisis but also in anticipation of future crises resulting from geopolitical risks, the continued globalisation of the digital economy, and the changing value and risks that technology brings."
http://arxiv.org/abs/2006.03723v1,What Factors Drive Individual Misperceptions of the Returns to Schooling in Tanzania? Some Lessons for Education Policy,2020-06-05 22:38:06+00:00,"['Plamen Nikolov', 'Nusrat Jimi']",econ.GN,"Evidence on educational returns and the factors that determine the demand for schooling in developing countries is extremely scarce. Building on previous studies that show individuals underestimating the returns to schooling, we use two surveys from Tanzania to estimate both the actual and perceived schooling returns and subsequently examine what factors drive individual misperceptions regarding actual returns. Using ordinary least squares and instrumental variable methods, we find that each additional year of schooling in Tanzania increases earnings, on average, by 9 to 11 percent. We find that on average individuals underestimate returns to schooling by 74 to 79 percent and three factors are associated with these misperceptions: income, asset poverty and educational attainment. Shedding light on what factors relate to individual beliefs about educational returns can inform policy on how to structure effective interventions in order to correct individual misperceptions."
http://arxiv.org/abs/2006.06078v1,Risk Attitudes and Human Mobility during the COVID-19 Pandemic,2020-06-10 21:47:34+00:00,"['Ho Fai Chan', 'Ahmed Skali', 'David Savage', 'David Stadelmann', 'Benno Torgler']",econ.GN,"Behavioral responses to pandemics are less shaped by actual mortality or hospitalization risks than they are by risk attitudes. We explore human mobility patterns as a measure of behavioral responses during the COVID-19 pandemic. Our results indicate that risk-taking attitude is a critical factor in predicting reduction in human mobility and increase social confinement around the globe. We find that the sharp decline in movement after the WHO (World Health Organization) declared COVID-19 to be a pandemic can be attributed to risk attitudes. Our results suggest that regions with risk-averse attitudes are more likely to adjust their behavioral activity in response to the declaration of a pandemic even before most official government lockdowns. Further understanding of the basis of responses to epidemics, e.g., precautionary behavior, will help improve the containment of the spread of the virus."
http://arxiv.org/abs/2006.07074v1,Seemingly Unrelated Regression with Measurement Error: Estimation via Markov chain Monte Carlo and Mean Field Variational Bayes Approximation,2020-06-12 10:58:10+00:00,"['Georges Bresson', 'Anoop Chaturvedi', 'Mohammad Arshad Rahman', 'Shalabh']",stat.ME,"Linear regression with measurement error in the covariates is a heavily studied topic, however, the statistics/econometrics literature is almost silent to estimating a multi-equation model with measurement error. This paper considers a seemingly unrelated regression model with measurement error in the covariates and introduces two novel estimation methods: a pure Bayesian algorithm (based on Markov chain Monte Carlo techniques) and its mean field variational Bayes (MFVB) approximation. The MFVB method has the added advantage of being computationally fast and can handle big data. An issue pertinent to measurement error models is parameter identification, and this is resolved by employing a prior distribution on the measurement error variance. The methods are shown to perform well in multiple simulation studies, where we analyze the impact on posterior estimates arising due to different values of reliability ratio or variance of the true unobserved quantity used in the data generating process. The paper further implements the proposed algorithms in an application drawn from the health literature and shows that modeling measurement error in the data can improve model fitting."
http://arxiv.org/abs/2006.11386v1,Valid Causal Inference with (Some) Invalid Instruments,2020-06-19 21:09:26+00:00,"['Jason Hartford', 'Victor Veitch', 'Dhanya Sridhar', 'Kevin Leyton-Brown']",stat.ME,"Instrumental variable methods provide a powerful approach to estimating causal effects in the presence of unobserved confounding. But a key challenge when applying them is the reliance on untestable ""exclusion"" assumptions that rule out any relationship between the instrument variable and the response that is not mediated by the treatment. In this paper, we show how to perform consistent IV estimation despite violations of the exclusion assumption. In particular, we show that when one has multiple candidate instruments, only a majority of these candidates---or, more generally, the modal candidate-response relationship---needs to be valid to estimate the causal effect. Our approach uses an estimate of the modal prediction from an ensemble of instrumental variable estimators. The technique is simple to apply and is ""black-box"" in the sense that it may be used with any instrumental variable estimator as long as the treatment effect is identified for each valid instrument independently. As such, it is compatible with recent machine-learning based estimators that allow for the estimation of conditional average treatment effects (CATE) on complex, high dimensional data. Experimentally, we achieve accurate estimates of conditional average treatment effects using an ensemble of deep network-based estimators, including on a challenging simulated Mendelian Randomization problem."
http://arxiv.org/abs/2006.11749v1,Shifting Policy Strategy in Keynesianism,2020-06-21 09:47:16+00:00,['Asahi Noguchi'],econ.GN,"This paper analyzes the evolution of Keynesianism making use of concepts offered by Imre Lakatos. The Keynesian ""hard core"" lies in its views regarding the instability of the market economy, its ""protective belt"" in the policy strategy for macroeconomic stabilization using fiscal policy and monetary policy. Keynesianism developed as a policy program to counter classical liberalism, which attributes priority to the autonomy of the market economy and tries to limit the role of government. In general, the core of every policy program consists in an unfalsifiable worldview and a value judgment that remain unchanged. On the other hand, a policy strategy with a protective belt inevitably evolves owing to changes in reality and advances in scientific knowledge. This is why the Keynesian policy strategy has shifted from being fiscal-led to one that is monetary-led because of the influence of monetarism; further, the Great Recession has even led to their integration."
http://arxiv.org/abs/2006.10946v1,A simple model of interbank trading with tiered remuneration,2020-06-19 03:43:39+00:00,['Toshifumi Nakamura'],econ.GN,"A negative interest rate policy is often accompanied by tiered remuneration, which allows for exemption from negative rates. This study proposes a basic model of interest rates formed in the interbank market with a tiering system. The results predicted by the model largely mirror actual market developments in late 2019, when the European Central Bank introduced, and the Switzerland National Bank modified, the tiering system."
http://arxiv.org/abs/2006.06237v2,Re-evaluating cryptocurrencies' contribution to portfolio diversification -- A portfolio analysis with special focus on German investors,2020-06-11 07:43:00+00:00,"['Tim Schmitz', 'Ingo Hoffmann']",q-fin.ST,"In this paper, we investigate whether mixing cryptocurrencies to a German investor portfolio improves portfolio diversification. We analyse this research question by applying a (mean variance) portfolio analysis using a toolbox consisting of (i) the comparison of descriptive statistics, (ii) graphical methods and (iii) econometric spanning tests. In contrast to most of the former studies we use a (broad) customized, Equally-Weighted Cryptocurrency Index (EWCI) to capture the average development of a whole ex ante defined cryptocurrency universe and to mitigate possible survivorship biases in the data. According to Glas/Poddig (2018), this bias could have led to misleading results in some already existing studies. We find that cryptocurrencies can improve portfolio diversification in a few of the analyzed windows from our dataset (consisting of weekly observations from 2014-01-01 to 2019-05-31). However, we cannot confirm this pattern as the normal case. By including cryptocurrencies in their portfolios, investors predominantly cannot reach a significantly higher efficient frontier. These results also hold, if the non-normality of cryptocurrency returns is considered. Moreover, we control for changes of the results, if transaction costs/illiquidities on the cryptocurrency market are additionally considered."
http://arxiv.org/abs/2006.13934v2,Investor Emotions and Earnings Announcements,2020-06-24 16:38:45+00:00,['Domonkos F. Vamossy'],q-fin.PM,"Armed with a decade of social media data, I explore the impact of investor emotions on earnings announcements. In particular, I test whether the emotional content of firm-specific messages posted on social media just prior to a firm's earnings announcement predicts its earnings and announcement returns. I find that investors are typically excited about firms that end up exceeding expectations, yet their enthusiasm results in lower announcement returns. Specifically, a standard deviation increase in excitement is associated with an 7.8 basis points lower announcement return, which translates into an approximately -5.8% annualized loss. My findings confirm that emotions and market dynamics are closely related and highlight the importance of considering investor emotions when assessing a firm's short-term value."
http://arxiv.org/abs/2006.14667v1,Empirical MSE Minimization to Estimate a Scalar Parameter,2020-06-25 19:20:37+00:00,"['Clément de Chaisemartin', ""Xavier D'Haultfœuille""]",math.ST,"We consider the estimation of a scalar parameter, when two estimators are available. The first is always consistent. The second is inconsistent in general, but has a smaller asymptotic variance than the first, and may be consistent if an assumption is satisfied. We propose to use the weighted sum of the two estimators with the lowest estimated mean-squared error (MSE). We show that this third estimator dominates the other two from a minimax-regret perspective: the maximum asymptotic-MSE-gain one may incur by using this estimator rather than one of the other estimators is larger than the maximum asymptotic-MSE-loss."
http://arxiv.org/abs/2006.08375v2,Modeling and Controlling the Spread of Epidemic with Various Social and Economic Scenarios,2020-06-12 16:52:58+00:00,"['S. P. Lukyanets', 'I. S. Gandzha', 'O. V. Kliushnichenko']",physics.soc-ph,"We propose a dynamical model for describing the spread of epidemics. This model is an extension of the SIQR (susceptible-infected-quarantined-recovered) and SIRP (susceptible-infected-recovered-pathogen) models used earlier to describe various scenarios of epidemic spreading. As compared to the basic SIR model, our model takes into account two possible routes of contagion transmission: direct from the infected compartment to the susceptible compartment and indirect via some intermediate medium or fomites. Transmission rates are estimated in terms of average distances between the individuals in selected social environments and characteristic time spans for which the individuals stay in each of these environments. We also introduce a collective economic resource associated with the average amount of money or income per individual to describe the socioeconomic interplay between the spreading process and the resource available to infected individuals. The epidemic-resource coupling is supposed to be of activation type, with the recovery rate governed by the Arrhenius-like law. Our model brings an advantage of building various control strategies to mitigate the effect of epidemic and can be applied, in particular, to modeling the spread of COVID-19."
http://arxiv.org/abs/2004.13614v3,COVID-19 causes record decline in global CO2 emissions,2020-04-28 15:48:56+00:00,"['Zhu Liu', 'Philippe Ciais', 'Zhu Deng', 'Ruixue Lei', 'Steven J. Davis', 'Sha Feng', 'Bo Zheng', 'Duo Cui', 'Xinyu Dou', 'Pan He', 'Biqing Zhu', 'Chenxi Lu', 'Piyu Ke', 'Taochun Sun', 'Yuan Wang', 'Xu Yue', 'Yilong Wang', 'Yadong Lei', 'Hao Zhou', 'Zhaonan Cai', 'Yuhui Wu', 'Runtao Guo', 'Tingxuan Han', 'Jinjun Xue', 'Olivier Boucher', 'Eulalie Boucher', 'Frederic Chevallier', 'Yimin Wei', 'Haiwang Zhong', 'Chongqing Kang', 'Ning Zhang', 'Bin Chen', 'Fengming Xi', 'François Marie', 'Qiang Zhang', 'Dabo Guan', 'Peng Gong', 'Daniel M. Kammen', 'Kebin He', 'Hans Joachim Schellnhuber']",econ.GN,"The considerable cessation of human activities during the COVID-19 pandemic has affected global energy use and CO2 emissions. Here we show the unprecedented decrease in global fossil CO2 emissions from January to April 2020 was of 7.8% (938 Mt CO2 with a +6.8% of 2-σ uncertainty) when compared with the period last year. In addition other emerging estimates of COVID impacts based on monthly energy supply or estimated parameters, this study contributes to another step that constructed the near-real-time daily CO2 emission inventories based on activity from power generation (for 29 countries), industry (for 73 countries), road transportation (for 406 cities), aviation and maritime transportation and commercial and residential sectors emissions (for 206 countries). The estimates distinguished the decline of CO2 due to COVID-19 from the daily, weekly and seasonal variations as well as the holiday events. The COVID-related decreases in CO2 emissions in road transportation (340.4 Mt CO2, -15.5%), power (292.5 Mt CO2, -6.4% compared to 2019), industry (136.2 Mt CO2, -4.4%), aviation (92.8 Mt CO2, -28.9%), residential (43.4 Mt CO2, -2.7%), and international shipping (35.9Mt CO2, -15%). Regionally, decreases in China were the largest and earliest (234.5 Mt CO2,-6.9%), followed by Europe (EU-27 & UK) (138.3 Mt CO2, -12.0%) and the U.S. (162.4 Mt CO2, -9.5%). The declines of CO2 are consistent with regional nitrogen oxides concentrations observed by satellites and ground-based networks, but the calculated signal of emissions decreases (about 1Gt CO2) will have little impacts (less than 0.13ppm by April 30, 2020) on the overserved global CO2 concertation. However, with observed fast CO2 recovery in China and partial re-opening globally, our findings suggest the longer-term effects on CO2 emissions are unknown and should be carefully monitored using multiple measures."
http://arxiv.org/abs/2004.02357v3,Final Topology for Preference Spaces,2020-04-06 00:17:10+00:00,['Pablo Schenone'],econ.TH,"We say a model is continuous in utilities (resp., preferences) if small perturbations of utility functions (resp., preferences) generate small changes in the model's outputs. While similar, these two questions are different. They are only equivalent when the following two sets are isomorphic: the set of continuous mappings from preferences to the model's outputs, and the set of continuous mappings from utilities to the model's outputs. In this paper, we study the topology for preference spaces defined by such an isomorphism. This study is practically significant, as continuity analysis is predominantly conducted through utility functions, rather than the underlying preference space. Our findings enable researchers to infer continuity in utility as indicative of continuity in underlying preferences."
http://arxiv.org/abs/2006.06674v4,"Corona Games: Masks, Social Distancing and Mechanism Design",2020-06-11 07:18:03+00:00,"['Balazs Pejo', 'Gergely Biczok']",econ.TH,"Pandemic response is a complex affair. Most governments employ a set of quasi-standard measures to fight COVID-19 including wearing masks, social distancing, virus testing and contact tracing. We argue that some non-trivial factors behind the varying effectiveness of these measures are selfish decision-making and the differing national implementations of the response mechanism. In this paper, through simple games, we show the effect of individual incentives on the decisions made with respect to wearing masks and social distancing, and how these may result in a sub-optimal outcome. We also demonstrate the responsibility of national authorities in designing these games properly regarding the chosen policies and their influence on the preferred outcome. We promote a mechanism design approach: it is in the best interest of every government to carefully balance social good and response costs when implementing their respective pandemic response mechanism."
http://arxiv.org/abs/2004.00493v3,Containment efficiency and control strategies for the Corona pandemic costs,2020-04-01 15:12:17+00:00,"['Claudius Gros', 'Roser Valenti', 'Lukas Schneider', 'Kilian Valenti', 'Daniel Gros']",physics.soc-ph,"The rapid spread of the Coronavirus (COVID-19) confronts policy makers with the problem of measuring the effectiveness of containment strategies, balancing public health considerations with the economic costs of social distancing measures. We introduce a modified epidemic model that we name the controlled-SIR model, in which the disease reproduction rate evolves dynamically in response to political and societal reactions. An analytic solution is presented. The model reproduces official COVID-19 cases counts of a large number of regions and countries that surpassed the first peak of the outbreak. A single unbiased feedback parameter is extracted from field data and used to formulate an index that measures the efficiency of containment strategies (the CEI index). CEI values for a range of countries are given. For two variants of the controlled-SIR model, detailed estimates of the total medical and socio-economic costs are evaluated over the entire course of the epidemic. Costs comprise medical care cost, the economic cost of social distancing, as well as the economic value of lives saved. Under plausible parameters, strict measures fare better than a hands-off policy. Strategies based on current case numbers lead to substantially higher total costs than strategies based on the overall history of the epidemic."
http://arxiv.org/abs/2006.00916v2,Renewable Power Trades and Network Congestion Externalities,2020-05-28 19:23:50+00:00,"['Nayara Aguiar', 'Indraneel Chakraborty', 'Vijay Gupta']",eess.SY,"Integrating renewable energy production into the electricity grid is an important policy goal to address climate change. However, such an integration faces economic and technological challenges. As power generation by renewable sources increases, power transmission patterns over the electric grid change. Due to physical laws, these new transmission patterns lead to non-intuitive grid congestion externalities. We derive the conditions under which negative network externalities due to power trades occur. Calibration using a stylized framework and data from Europe shows that each additional unit of power traded between northern and western Europe reduces transmission capacity for the southern and eastern regions by 27% per unit traded. Such externalities suggest that new investments in the electric grid infrastructure cannot be made piecemeal. In our example, power infrastructure investment in northern and western Europe needs an accompanying investment in southern and eastern Europe as well. An economic challenge is regions facing externalities do not always have the financial ability to invest in infrastructure. Power transit fares can help finance power infrastructure investment in regions facing network congestion externalities. The resulting investment in the overall electricity grid facilitates integration of renewable energy production."
http://arxiv.org/abs/2006.07443v5,Algorithm for Computing Approximate Nash Equilibrium in Continuous Games with Application to Continuous Blotto,2020-06-12 19:53:18+00:00,['Sam Ganzfried'],cs.GT,"Successful algorithms have been developed for computing Nash equilibrium in a variety of finite game classes. However, solving continuous games -- in which the pure strategy space is (potentially uncountably) infinite -- is far more challenging. Nonetheless, many real-world domains have continuous action spaces, e.g., where actions refer to an amount of time, money, or other resource that is naturally modeled as being real-valued as opposed to integral. We present a new algorithm for {approximating} Nash equilibrium strategies in continuous games. In addition to two-player zero-sum games, our algorithm also applies to multiplayer games and games with imperfect information. We experiment with our algorithm on a continuous imperfect-information Blotto game, in which two players distribute resources over multiple battlefields. Blotto games have frequently been used to model national security scenarios and have also been applied to electoral competition and auction theory. Experiments show that our algorithm is able to quickly compute close approximations of Nash equilibrium strategies for this game."
http://arxiv.org/abs/2006.05460v2,Designing Stable Elections: A Survey,2020-06-09 18:59:48+00:00,['Steven Heilman'],math.PR,"We survey the design of elections that are resilient to attempted interference by third parties. For example, suppose votes have been cast in an election between two candidates, and then each vote is randomly changed with a small probability, independently of the other votes. It is desirable to keep the outcome of the election the same, regardless of the changes to the votes. It is well known that the US electoral college system is about 5 times more likely to have a changed outcome due to vote corruption, when compared to a majority vote. In fact, Mossel, O'Donnell and Oleszkiewicz proved in 2005 that the majority voting method is most stable to this random vote corruption, among voting methods where each person has a small influence on the election. We discuss some recent progress on the analogous result for elections between more than two candidates. In this case, plurality should be most stable to corruption in votes. We also survey results on adversarial election manipulation (where an adversary can select particular votes to change, perhaps in a non-random way), and we briefly discuss ranked choice voting methods (where a vote is a ranked list of candidates)."
http://arxiv.org/abs/2005.01081v5,Multialternative Neural Decision Processes,2020-05-03 13:19:37+00:00,"['Carlo Baldassi', 'Simone Cerreia-Vioglio', 'Fabio Maccheroni', 'Massimo Marinacci', 'Marco Pirazzini']",cs.AI,"We introduce an algorithmic decision process for multialternative choice that combines binary comparisons and Markovian exploration. We show that a preferential property, transitivity, makes it testable."
http://arxiv.org/abs/2005.12225v2,An alternative to synthetic control for models with many covariates under sparsity,2020-05-25 16:56:45+00:00,"['Marianne Bléhaut', ""Xavier D'Haultfoeuille"", ""Jérémy L'Hour"", 'Alexandre B. Tsybakov']",econ.EM,"The synthetic control method is a an econometric tool to evaluate causal effects when only one unit is treated. While initially aimed at evaluating the effect of large-scale macroeconomic changes with very few available control units, it has increasingly been used in place of more well-known microeconometric tools in a broad range of applications, but its properties in this context are unknown. This paper introduces an alternative to the synthetic control method, which is developed both in the usual asymptotic framework and in the high-dimensional scenario. We propose an estimator of average treatment effect that is doubly robust, consistent and asymptotically normal. It is also immunized against first-step selection mistakes. We illustrate these properties using Monte Carlo simulations and applications to both standard and potentially high-dimensional settings, and offer a comparison with the synthetic control method."
http://arxiv.org/abs/2006.07729v2,Optimal Attention Management: A Tractable Framework,2020-06-13 22:11:31+00:00,"['Elliot Lipnowski', 'Laurent Mathevet', 'Dong Wei']",econ.TH,"A well-intentioned principal provides information to a rationally inattentive agent without internalizing the agent's cost of processing information. Whatever information the principal makes available, the agent may choose to ignore some. We study optimal information provision in a tractable model with quadratic payoffs where full disclosure is not optimal. We characterize incentive-compatible information policies, that is, those to which the agent willingly pays full attention. In a leading example with three states, optimal disclosure involves information distortion at intermediate costs of attention. As the cost increases, optimal information abruptly changes from downplaying the state to exaggerating the state."
http://arxiv.org/abs/2006.08469v6,"V-, U-, L-, or W-shaped economic recovery after COVID: Insights from an Agent Based Model",2020-06-15 15:20:37+00:00,"['Dhruv Sharma', 'Jean-Philippe Bouchaud', 'Stanislao Gualdi', 'Marco Tarzia', 'Francesco Zamponi']",econ.GN,"We discuss the impact of a Covid-19--like shock on a simple model economy, described by the previously developed Mark-0 Agent-Based Model. We consider a mixed supply and demand shock, and show that depending on the shock parameters (amplitude and duration), our model economy can display V-shaped, U-shaped or W-shaped recoveries, and even an L-shaped output curve with permanent output loss. This is due to the economy getting trapped in a self-sustained ""bad"" state. We then discuss two policies that attempt to moderate the impact of the shock: giving easy credit to firms, and the so-called helicopter money, i.e. injecting new money into the households savings. We find that both policies are effective if strong enough. We highlight the potential danger of terminating these policies too early, although inflation is substantially increased by lax access to credit. Finally, we consider the impact of a second lockdown. While we only discuss a limited number of scenarios, our model is flexible and versatile enough to accommodate a wide variety of situations, thus serving as a useful exploratory tool for a qualitative, scenario-based understanding of post-Covid recovery. The corresponding code is available on-line."
http://arxiv.org/abs/2006.03618v2,Coordinated Transaction Scheduling in Multi-Area Electricity Markets: Equilibrium and Learning,2020-06-05 18:12:50+00:00,"['Mariola Ndrio', 'Subhonmesh Bose', 'Lang Tong', 'Ye Guo']",cs.GT,"Tie-line scheduling in multi-area power systems in the US largely proceeds through a market-based mechanism called Coordinated Transaction Scheduling (CTS). We analyze this market mechanism through a game-theoretic lens. Our analysis characterizes the effect of market liquidity, market participants' forecasts about inter-area price spreads, transactions fees and coupling of CTS markets with up-to-congestion virtual transactions. Using real data, we empirically verify that CTS bidders can employ simple learning algorithms to discover Nash equilibria that support the conclusions drawn from equilibrium analysis."
http://arxiv.org/abs/2005.02379v5,A Theory of the Saving Rate of the Rich,2020-05-05 03:20:13+00:00,"['Qingyin Ma', 'Alexis Akira Toda']",econ.TH,"Empirical evidence suggests that the rich have higher propensity to save than do the poor. While this observation may appear to contradict the homotheticity of preferences, we theoretically show that that is not the case. Specifically, we consider an income fluctuation problem with homothetic preferences and general shocks and prove that consumption functions are asymptotically linear, with an exact analytical characterization of asymptotic marginal propensities to consume (MPC). We provide necessary and sufficient conditions for the asymptotic MPCs to be zero. We calibrate a model with standard constant relative risk aversion utility and show that zero asymptotic MPCs are empirically plausible, implying that our mechanism has the potential to accommodate a large saving rate of the rich and high wealth inequality (small Pareto exponent) as observed in the data."
http://arxiv.org/abs/2005.03226v3,Detecting Latent Communities in Network Formation Models,2020-05-07 03:34:29+00:00,"['Shujie Ma', 'Liangjun Su', 'Yichong Zhang']",econ.EM,"This paper proposes a logistic undirected network formation model which allows for assortative matching on observed individual characteristics and the presence of edge-wise fixed effects. We model the coefficients of observed characteristics to have a latent community structure and the edge-wise fixed effects to be of low rank. We propose a multi-step estimation procedure involving nuclear norm regularization, sample splitting, iterative logistic regression and spectral clustering to detect the latent communities. We show that the latent communities can be exactly recovered when the expected degree of the network is of order log n or higher, where n is the number of nodes in the network. The finite sample performance of the new estimation and inference methods is illustrated through both simulated and real datasets."
http://arxiv.org/abs/2005.00510v2,On the Equivalence of Neural and Production Networks,2020-05-01 17:28:50+00:00,"['Roy Gernhardt', 'Bjorn Persson']",econ.TH,"This paper identifies the mathematical equivalence between economic networks of Cobb-Douglas agents and Artificial Neural Networks. It explores two implications of this equivalence under general conditions. First, a burgeoning literature has established that network propagation can transform microeconomic perturbations into large aggregate shocks. Neural network equivalence amplifies the magnitude and complexity of this phenomenon. Second, if economic agents adjust their production and utility functions in optimal response to local conditions, market pricing is a sufficient and robust channel for information feedback leading to macro learning."
http://arxiv.org/abs/2004.01311v3,Predicting Skill Shortages in Labor Markets: A Machine Learning Approach,2020-04-03 00:15:10+00:00,"['Nik Dawson', 'Marian-Andrei Rizoiu', 'Benjamin Johnston', 'Mary-Anne Williams']",econ.GN,"Skill shortages are a drain on society. They hamper economic opportunities for individuals, slow growth for firms, and impede labor productivity in aggregate. Therefore, the ability to understand and predict skill shortages in advance is critical for policy-makers and educators to help alleviate their adverse effects. This research implements a high-performing Machine Learning approach to predict occupational skill shortages. In addition, we demonstrate methods to analyze the underlying skill demands of occupations in shortage and the most important features for predicting skill shortages. For this work, we compile a unique dataset of both Labor Demand and Labor Supply occupational data in Australia from 2012 to 2018. This includes data from 7.7 million job advertisements (ads) and 20 official labor force measures. We use these data as explanatory variables and leverage the XGBoost classifier to predict yearly skills shortage classifications for 132 standardized occupations. The models we construct achieve macro-F1 average performance scores of up to 83 per cent. Our results show that job ads data and employment statistics were the highest performing feature sets for predicting year-to-year skills shortage changes for occupations. We also find that features such as 'Hours Worked', years of 'Education', years of 'Experience', and median 'Salary' are highly important features for predicting occupational skill shortages. This research provides a robust data-driven approach for predicting and analyzing skill shortages, which can assist policy-makers, educators, and businesses to prepare for the future of work."
http://arxiv.org/abs/2005.01365v3,Ensemble Forecasting for Intraday Electricity Prices: Simulating Trajectories,2020-05-04 10:21:20+00:00,"['Michał Narajewski', 'Florian Ziel']",q-fin.ST,"Recent studies concerning the point electricity price forecasting have shown evidence that the hourly German Intraday Continuous Market is weak-form efficient. Therefore, we take a novel, advanced approach to the problem. A probabilistic forecasting of the hourly intraday electricity prices is performed by simulating trajectories in every trading window to receive a realistic ensemble to allow for more efficient intraday trading and redispatch. A generalized additive model is fitted to the price differences with the assumption that they follow a zero-inflated distribution, precisely a mixture of the Dirac and the Student's t-distributions. Moreover, the mixing term is estimated using a high-dimensional logistic regression with lasso penalty. We model the expected value and volatility of the series using i.a. autoregressive and no-trade effects or load, wind and solar generation forecasts and accounting for the non-linearities in e.g. time to maturity. Both the in-sample characteristics and forecasting performance are analysed using a rolling window forecasting study. Multiple versions of the model are compared to several benchmark models and evaluated using probabilistic forecasting measures and significance tests. The study aims to forecast the price distribution in the German Intraday Continuous Market in the last 3 hours of trading, but the approach allows for application to other continuous markets, especially in Europe. The results prove superiority of the mixture model over the benchmarks gaining the most from the modelling of the volatility. They also indicate that the introduction of XBID reduced the market volatility."
http://arxiv.org/abs/2005.05196v3,Choice with Endogenous Categorization,2020-05-11 15:39:47+00:00,"['Andrew Ellis', 'Yusufcan Masatlioglu']",econ.TH,"We propose and axiomatize the categorical thinking model (CTM) in which the framing of the decision problem affects how agents categorize alternatives, that in turn affects their evaluation of it. Prominent models of salience, status quo bias, loss-aversion, inequality aversion, and present bias all fit under the umbrella of CTM. This suggests categorization is an underlying mechanism of key departures from the neoclassical model of choice. We specialize CTM to provide a behavioral foundation for the salient thinking model of Bordalo et al. (2013) that highlights its strong predictions and distinctions from other models."
http://arxiv.org/abs/2006.03718v1,Past production constrains current energy demands: persistent scaling in global energy consumption and implications for climate change mitigation,2020-06-05 22:22:12+00:00,"['Timothy J. Garrett', 'Matheus R. Grasselli', 'Stephen Keen']",econ.GN,"Climate change has become intertwined with the global economy. Here, we describe the importance of inertia to continued growth in energy consumption. Drawing from thermodynamic arguments, and using 38 years of available statistics between 1980 to 2017, we find a persistent time-independent scaling between the historical time integral $W$ of world inflation-adjusted economic production $Y$, or $W\left(t\right) = \int_0^t Y\left(t'\right)dt'$, and current rates of world primary energy consumption $\mathcal E$, such that $λ= \mathcal{E}/W = 5.9\pm0.1$ Gigawatts per trillion 2010 US dollars. This empirical result implies that population expansion is a symptom rather than a cause of the current exponential rise in $\mathcal E$ and carbon dioxide emissions $C$, and that it is past innovation of economic production efficiency $Y/\mathcal{E}$ that has been the primary driver of growth, at predicted rates that agree well with data. Options for stabilizing $C$ are then limited to rapid decarbonization of $\mathcal E$ through sustained implementation of over one Gigawatt of renewable or nuclear power capacity per day. Alternatively, assuming continued reliance on fossil fuels, civilization could shift to a steady-state economy that devotes economic production exclusively to maintenance rather than expansion. If this were instituted immediately, continual energy consumption would still be required, so atmospheric carbon dioxide concentrations would not balance natural sinks until concentrations exceeded 500 ppmv, and double pre-industrial levels if the steady-state was attained by 2030."
http://arxiv.org/abs/2006.00739v2,The Importance of Cognitive Domains and the Returns to Schooling in South Africa: Evidence from Two Labor Surveys,2020-06-01 06:31:59+00:00,"['Plamen Nikolov', 'Nusrat Jimi']",econ.GN,"Numerous studies have considered the important role of cognition in estimating the returns to schooling. How cognitive abilities affect schooling may have important policy implications, especially in developing countries during periods of increasing educational attainment. Using two longitudinal labor surveys that collect direct proxy measures of cognitive skills, we study the importance of specific cognitive domains for the returns to schooling in two samples. We instrument for schooling levels and we find that each additional year of schooling leads to an increase in earnings by approximately 18-20 percent. The estimated effect sizes-based on the two-stage least squares estimates-are above the corresponding ordinary least squares estimates. Furthermore, we estimate and demonstrate the importance of specific cognitive domains in the classical Mincer equation. We find that executive functioning skills (i.e., memory and orientation) are important drivers of earnings in the rural sample, whereas higher-order cognitive skills (i.e., numeracy) are more important for determining earnings in the urban sample. Although numeracy is tested in both samples, it is only a statistically significant predictor of earnings in the urban sample."
http://arxiv.org/abs/2005.03843v1,An Emissions Trading System to reach NDC targets in the Chilean electric sector,2020-05-08 04:01:09+00:00,"['Pía Amigo', 'Sebastián Cea-Echenique', 'Felipe Feijoo']",econ.GN,"In the context of the Paris Agreement, Chile has pledged to reduce Greenhouse Gases (GHG) intensity by at least 30% below 2007 levels by 2030, and to phase out coal as a energy source by 2040, among other strategies. In pursue of these goals, Chile has implemented a $5 per tonne of CO2 emission tax, first of its kind in Latin America. However, such a low price has proven to be insufficient. In our work, we study an alternative approach for capping and pricing carbon emissions in the Chilean electric sector; the cap and trade paradigm. We model the Chilean electric market (generators and emissions auctioneer) as a two stage capacity expansion equilibrium problem, where we allow future investment and trading of emission permits among generator agents. The model studies generation and future investments in the Chilean electric sector in two regimes of demand: deterministic and stochastic. We show that the current Chilean Greenhouse Gases (GHG) intensity pledge does not drive an important shift in the future Chilean electric matrix. To encourage a shift to greener technologies, a more stringent carbon budget must be considered, resulting in a carbon price approximately ten times higher than the present one. We also show that achieving the emissions reduction goal does not necessarily results in further reductions of carbon generation, or phasing out coal in the longer term. Finally, we demonstrate that under technology change costs reductions, higher demand scenarios will relax the need for stringent carbon budgets to achieve new renewable energy investments and hence meet the Chilean pledges. These results suggest that some aspects of the Chilean pledge require further analysis, of the economic impact, particularly with the recent announcement of achieving carbon neutrality towards 2050."
http://arxiv.org/abs/2004.14485v1,Distress propagation on production networks: Coarse-graining and modularity of linkages,2020-04-29 21:18:36+00:00,"['Ashish Kumar', 'Anindya S. Chakrabarti', 'Anirban Chakraborti', 'Tushar Nandi']",physics.soc-ph,"Distress propagation occurs in connected networks, its rate and extent being dependent on network topology. To study this, we choose economic production networks as a paradigm. An economic network can be examined at many levels: linkages among individual agents (microscopic), among firms/sectors (mesoscopic) or among countries (macroscopic). New emergent dynamical properties appear at every level, so the granularity matters. For viral epidemics, even an individual node may act as an epicenter of distress and potentially affect the entire network. Economic networks, however, are known to be immune at the micro-levels and more prone to failure in the meso/macro-levels. We propose a dynamical interaction model to characterize the mechanism of distress propagation, across different modules of a network, initiated at different epicenters. Vulnerable modules often lead to large degrees of destabilization. We demonstrate our methodology using a unique empirical data-set of input-output linkages across 0.14 million firms in one administrative state of India, a developing economy. The network has multiple hub-and-spoke structures that exhibits moderate disassortativity, which varies with the level of coarse-graining. The novelty lies in characterizing the production network at different levels of granularity or modularity, and finding `too-big-to-fail' modules supersede `too-central-to-fail' modules in distress propagation."
http://arxiv.org/abs/2004.04867v1,The Benefits and Costs of Social Distancing in Rich and Poor Countries,2020-04-10 00:48:15+00:00,"['Zachary Barnett-Howell', 'Ahmed Mushfiq Mobarak']",econ.GN,"Social distancing is the primary policy prescription for combating the COVID-19 pandemic, and has been widely adopted in Europe and North America. We estimate the value of disease avoidance using an epidemiological model that projects the spread of COVID-19 across rich and poor countries. Social distancing measures that ""flatten the curve"" of the disease to bring demand within the capacity of healthcare systems are predicted to save many lives in high-income countries, such that practically any economic cost is worth bearing. These social distancing policies are estimated to be less effective in poor countries with younger populations less susceptible to COVID-19, and more limited healthcare systems, which were overwhelmed before the pandemic. Moreover, social distancing lowers disease risk by limiting people's economic opportunities. Poorer people are less willing to make those economic sacrifices. They place relatively greater value on their livelihood concerns compared to contracting COVID-19. Not only are the epidemiological and economic benefits of social distancing much smaller in poorer countries, such policies may exact a heavy toll on the poorest and most vulnerable. Workers in the informal sector lack the resources and social protections to isolate themselves and sacrifice economic opportunities until the virus passes. By limiting their ability to earn a living, social distancing can lead to an increase in hunger, deprivation, and related mortality and morbidity. Rather than a blanket adoption of social distancing measures, we advocate for the exploration of alternative harm-reduction strategies, including universal mask adoption and increased hygiene measures."
http://arxiv.org/abs/2004.07947v9,Correlates of the country differences in the infection and mortality rates during the first wave of the COVID-19 pandemic: Evidence from Bayesian model averaging,2020-04-14 19:12:57+00:00,"['Viktor Stojkoski', 'Zoran Utkovski', 'Petar Jolakoski', 'Dragan Tevdovski', 'Ljupco Kocarev']",physics.soc-ph,"In the initial wave of the COVID-19 pandemic we observed great discrepancies in both infection and mortality rates between countries. Besides the biological and epidemiological factors, a multitude of social and economic criteria also influence the extent to which these discrepancies appear. Consequently, there is an active debate regarding the critical socio-economic and health factors that correlate with the infection and mortality rates outcome of the pandemic. Here, we leverage Bayesian model averaging techniques and country level data to investigate the potential of 28 variables, describing a diverse set of health and socio-economic characteristics, in being correlates of the final number of infections and deaths during the first wave of the coronavirus pandemic. We show that only few variables are able to robustly correlate with these outcomes. To understand the relationship between the potential correlates in explaining the infection and death rates, we create a Jointness Space. Using this space, we conclude that the extent to which each variable is able to provide a credible explanation for the COVID-19 infections/mortality outcome varies between countries because of their heterogeneous features."
http://arxiv.org/abs/2006.12966v4,The unbearable lightness of equilibria in a low interest rate environment,2020-06-20 20:59:30+00:00,"['Guido Ascari', 'Sophocles Mavroeidis']",econ.GN,"Structural models with no solution are incoherent, and those with multiple solutions are incomplete. We show that models with occasionally binding constraints are not generically coherent. Coherency requires restrictions on the parameters or on the support of the distribution of the shocks. In presence of multiple shocks, the support restrictions cannot be independent from each other, so the assumption of orthogonality of structural shocks is incompatible with coherency. Models whose coherency is based on support restrictions are generically incomplete, admitting a very large number of minimum state variable solutions."
http://arxiv.org/abs/2005.10711v3,Sequential Fundraising and Mutual Insurance,2020-05-21 15:08:58+00:00,"['Amir Ban', 'Moran Koren']",cs.GT,"Seed fundraising for ventures often takes place by sequentially approaching potential contributors, who make observable decisions. The fundraising succeeds when a target number of investments is reached. Though resembling classic information cascades models, its behavior is radically different, exhibiting surprising complexities. Assuming a common distribution for contributors' levels of information, we show that participants rely on {\em mutual insurance}, i.e., invest despite unfavorable information, trusting future player strategies to protect them from loss. {\em Delegation} occurs when contributors invest unconditionally, empowering the decision to future players. Often, all early contributors delegate, in effect empowering the last few contributors to decide the outcome. Similar dynamics hold in sequential voting, as in voting in committees."
http://arxiv.org/abs/2005.03693v3,Belief-Averaged Relative Utilitarianism,2020-05-07 18:39:28+00:00,['Florian Brandl'],econ.TH,"We consider social welfare functions when the preferences of individual agents and society maximize subjective expected utility in the tradition of Savage. A system of axioms is introduced whose unique solution is the social welfare function that averages the agents' beliefs and sums up their utility functions, normalized to have the same range. The first distinguishing axiom requires positive association of society's preferences with the agents' preferences for acts about which beliefs agree. The second is a weakening of Arrow's independence of irrelevant alternatives that only applies to non-redundant acts."
http://arxiv.org/abs/2004.12022v2,Bayesian Clustered Coefficients Regression with Auxiliary Covariates Assistant Random Effects,2020-04-25 00:21:33+00:00,"['Guanyu Hu', 'Yishu Xue', 'Zhihua Ma']",stat.ME,"In regional economics research, a problem of interest is to detect similarities between regions, and estimate their shared coefficients in economics models. In this article, we propose a mixture of finite mixtures (MFM) clustered regression model with auxiliary covariates that account for similarities in demographic or economic characteristics over a spatial domain. Our Bayesian construction provides both inference for number of clusters and clustering configurations, and estimation for parameters for each cluster. Empirical performance of the proposed model is illustrated through simulation experiments, and further applied to a study of influential factors for monthly housing cost in Georgia."
http://arxiv.org/abs/2004.01788v2,Reselling Information,2020-04-03 21:50:33+00:00,"['S. Nageeb Ali', 'Ayal Chen-Zion', 'Erik Lillethun']",cs.GT,"Information is replicable in that it can be simultaneously consumed and sold to others. We study how resale affects a decentralized market for information. We show that even if the initial seller is an informational monopolist, she captures non-trivial rents from at most a single buyer: her payoffs converge to 0 as soon as a single buyer has bought information. By contrast, if the seller can also sell valueless tokens, there exists a ``prepay equilibrium'' where payment is extracted from all buyers before the information good is released. By exploiting resale possibilities, this prepay equilibrium gives the seller as high a payoff as she would achieve if resale were prohibited."
http://arxiv.org/abs/2006.16099v3,Pay Transparency and Gender Equality,2020-06-25 11:21:57+00:00,"['Emma Duchini', 'Stefania Simion', 'Arthur Turrell', 'Jack Blundell']",econ.GN,"Since 2018 UK firms with at least 250 employees have been mandated to publicly disclose gender equality indicators. Exploiting variations in this mandate across firm size and time we show that pay transparency closes 18 percent of the gender pay gap by reducing men's wage growth. The public availability of the equality indicators seems to influence employers' response as worse performing firms and industries more exposed to public scrutiny reduce their gender pay gap the most. Employers are also 9 percent more likely to post wages in job vacancies, potentially in an effort to improve gender equality at entry level."
http://arxiv.org/abs/2005.11285v2,Identifying Key Sectors in the Regional Economy: A Network Analysis Approach Using Input-Output Data,2020-05-22 17:14:59+00:00,"['Fernando DePaolis', 'Phil Murphy', 'M. Clara DePaolis Kaluza']",econ.GN,"By applying network analysis techniques to large input-output system, we identify key sectors in the local/regional economy. We overcome the limitations of traditional measures of centrality by using random-walk based measures, as an extension of Blochl et al. (2011). These are more appropriate to analyze very dense networks, i.e. those in which most nodes are connected to all other nodes. These measures also allow for the presence of recursive ties (loops), since these are common in economic systems (depending to the level of aggregation, most firms buy from and sell to other firms in the same industrial sector). The centrality measures we present are well suited for capturing sectoral effects missing from the usual output and employment multipliers. We also develop an R package (xtranat) for the processing of data from IMPLAN(R) models and for computing the newly developed measures."
http://arxiv.org/abs/2004.04683v1,On the Factors Influencing the Choices of Weekly Telecommuting Frequencies of Post-secondary Students in Toronto,2020-04-09 17:08:22+00:00,"['Khandker Nurul Habib', 'Ph. D.', 'PEng']",econ.EM,"The paper presents an empirical investigation of telecommuting frequency choices by post-secondary students in Toronto. It uses a dataset collected through a large-scale travel survey conducted on post-secondary students of four major universities in Toronto and it employs multiple alternative econometric modelling techniques for the empirical investigation. Results contribute on two fronts. Firstly, it presents empirical investigations of factors affecting telecommuting frequency choices of post-secondary students that are rare in literature. Secondly, it identifies better a performing econometric modelling technique for modelling telecommuting frequency choices. Empirical investigation clearly reveals that telecommuting for school related activities is prevalent among post-secondary students in Toronto. Around 80 percent of 0.18 million of the post-secondary students of the region, who make roughly 36,000 trips per day, also telecommute at least once a week. Considering that large numbers of students need to spend a long time travelling from home to campus with around 33 percent spending more than two hours a day on travelling, telecommuting has potential to enhance their quality of life. Empirical investigations reveal that car ownership and living farther from the campus have similar positive effects on the choice of higher frequency of telecommuting. Students who use a bicycle for regular travel are least likely to telecommute, compared to those using transit or a private car."
http://arxiv.org/abs/2004.08167v1,Mean Field Game Approach to Bitcoin Mining,2020-04-17 10:57:33+00:00,"['Charles Bertucci', 'Louis Bertucci', 'Jean-Michel Lasry', 'Pierre-Louis Lions']",econ.TH,"We present an analysis of the Proof-of-Work consensus algorithm, used on the Bitcoin blockchain, using a Mean Field Game framework. Using a master equation, we provide an equilibrium characterization of the total computational power devoted to mining the blockchain (hashrate). From a simple setting we show how the master equation approach allows us to enrich the model by relaxing most of the simplifying assumptions. The essential structure of the game is preserved across all the enrichments. In deterministic settings, the hashrate ultimately reaches a steady state in which it increases at the rate of technological progress. In stochastic settings, there exists a target for the hashrate for every possible random state. As a consequence, we show that in equilibrium the security of the underlying blockchain is either $i)$ constant, or $ii)$ increases with the demand for the underlying cryptocurrency."
http://arxiv.org/abs/2004.08460v2,Estimating and Projecting Air Passenger Traffic during the COVID-19 Coronavirus Outbreak and its Socio-Economic Impact,2020-04-17 21:40:31+00:00,"['Stefano Maria Iacus', 'Fabrizio Natale', 'Carlos Satamaria', 'Spyridon Spyratos', 'Michele Vespe']",stat.AP,"The main focus of this study is to collect and prepare data on air passengers traffic worldwide with the scope of analyze the impact of travel ban on the aviation sector. Based on historical data from January 2010 till October 2019, a forecasting model is implemented in order to set a reference baseline. Making use of airplane movements extracted from online flight tracking platforms and on-line booking systems, this study presents also a first assessment of recent changes in flight activity around the world as a result of the COVID-19 pandemic. To study the effects of air travel ban on aviation and in turn its socio-economic, several scenarios are constructed based on past pandemic crisis and the observed flight volumes. It turns out that, according to this hypothetical scenarios, in the first Quarter of 2020 the impact of aviation losses could have negatively reduced World GDP by 0.02% to 0.12% according to the observed data and, in the worst case scenarios, at the end of 2020 the loss could be as high as 1.41-1.67% and job losses may reach the value of 25-30 millions. Focusing on EU27, the GDP loss may amount to 1.66-1.98% by the end of 2020 and the number of job losses from 4.2 to 5 millions in the worst case scenarios. Some countries will be more affected than others in the short run and most European airlines companies will suffer from the travel ban."
http://arxiv.org/abs/2004.01411v4,Targeting predictors in random forest regression,2020-04-03 07:42:11+00:00,"['Daniel Borup', 'Bent Jesper Christensen', 'Nicolaj Nørgaard Mühlbach', 'Mikkel Slot Nielsen']",econ.EM,"Random forest regression (RF) is an extremely popular tool for the analysis of high-dimensional data. Nonetheless, its benefits may be lessened in sparse settings due to weak predictors, and a pre-estimation dimension reduction (targeting) step is required. We show that proper targeting controls the probability of placing splits along strong predictors, thus providing an important complement to RF's feature sampling. This is supported by simulations using representative finite samples. Moreover, we quantify the immediate gain from targeting in terms of increased strength of individual trees. Macroeconomic and financial applications show that the bias-variance trade-off implied by targeting, due to increased correlation among trees in the forest, is balanced at a medium degree of targeting, selecting the best 10--30\% of commonly applied predictors. Improvements in predictive accuracy of targeted RF relative to ordinary RF are considerable, up to 12-13\%, occurring both in recessions and expansions, particularly at long horizons."
http://arxiv.org/abs/2004.06759v1,Supply and demand shocks in the COVID-19 pandemic: An industry and occupation perspective,2020-04-14 19:19:15+00:00,"['R. Maria del Rio-Chanona', 'Penny Mealy', 'Anton Pichler', 'Francois Lafond', 'Doyne Farmer']",econ.GN,"We provide quantitative predictions of first order supply and demand shocks for the U.S. economy associated with the COVID-19 pandemic at the level of individual occupations and industries. To analyze the supply shock, we classify industries as essential or non-essential and construct a Remote Labor Index, which measures the ability of different occupations to work from home. Demand shocks are based on a study of the likely effect of a severe influenza epidemic developed by the US Congressional Budget Office. Compared to the pre-COVID period, these shocks would threaten around 22% of the US economy's GDP, jeopardise 24% of jobs and reduce total wage income by 17%. At the industry level, sectors such as transport are likely to have output constrained by demand shocks, while sectors relating to manufacturing, mining and services are more likely to be constrained by supply shocks. Entertainment, restaurants and tourism face large supply and demand shocks. At the occupation level, we show that high-wage occupations are relatively immune from adverse supply and demand-side shocks, while low-wage occupations are much more vulnerable. We should emphasize that our results are only first-order shocks -- we expect them to be substantially amplified by feedback effects in the production network."
http://arxiv.org/abs/2004.09087v1,Effects of the COVID-19 Pandemic on Population Mobility under Mild Policies: Causal Evidence from Sweden,2020-04-20 06:56:43+00:00,"['Matz Dahlberg', 'Per-Anders Edin', 'Erik Grönqvist', 'Johan Lyhagen', 'John Östh', 'Alexey Siretskiy', 'Marina Toger']",econ.GN,"Sweden has adopted far less restrictive social distancing policies than most countries following the COVID-19 pandemic. This paper uses data on all mobile phone users, from one major Swedish mobile phone network, to examine the impact of the Coronavirus outbreak under the Swedish mild recommendations and restrictions regime on individual mobility and if changes in geographical mobility vary over different socio-economic strata. Having access to data for January-March in both 2019 and 2020 enables the estimation of causal effects of the COVID-19 outbreak by adopting a Difference-in-Differences research design. The paper reaches four main conclusions: (i) The daytime population in residential areas increased significantly (64 percent average increase); (ii) The daytime presence in industrial and commercial areas decreased significantly (33 percent average decrease); (iii) The distance individuals move from their homes during a day was substantially reduced (38 percent decrease in the maximum distance moved and 36 percent increase in share of individuals who move less than one kilometer from home); (iv) Similar reductions in mobility were found for residents in areas with different socioeconomic and demographic characteristics. These results show that mild government policies can compel people to adopt social distancing behavior."
http://arxiv.org/abs/2004.09627v3,Inference by Stochastic Optimization: A Free-Lunch Bootstrap,2020-04-20 20:43:28+00:00,"['Jean-Jacques Forneron', 'Serena Ng']",econ.EM,Assessing sampling uncertainty in extremum estimation can be challenging when the asymptotic variance is not analytically tractable. Bootstrap inference offers a feasible solution but can be computationally costly especially when the model is complex. This paper uses iterates of a specially designed stochastic optimization algorithm as draws from which both point estimates and bootstrap standard errors can be computed in a single run. The draws are generated by the gradient and Hessian computed from batches of data that are resampled at each iteration. We show that these draws yield consistent estimates and asymptotically valid frequentist inference for a large class of regular problems. The algorithm provides accurate standard errors in simulation examples and empirical applications at low computational costs. The draws from the algorithm also provide a convenient way to detect data irregularities.
http://arxiv.org/abs/2004.09835v1,How Much Income Inequality Is Too Much?,2020-04-21 09:08:28+00:00,['Jean-Philippe Bouchaud'],econ.GN,"We propose a highly schematic economic model in which, in some cases, wage inequalities lead to higher overall social welfare. This is due to the fact that high earners can consume low productivity, non essential products, which allows everybody to remain employed even when the productivity of essential goods is high and producing them does not require everybody to work. We derive a relation between heterogeneities in technologies and the minimum Gini coefficient required to maximize global welfare. Stronger inequalities appear to be economically unjustified. Our model may shed light on the role of non-essential goods in the economy, a topical issue when thinking about the post-Covid-19 world."
http://arxiv.org/abs/2004.11486v1,Machine Learning Econometrics: Bayesian algorithms and methods,2020-04-23 23:15:33+00:00,"['Dimitris Korobilis', 'Davide Pettenuzzo']",stat.CO,"As the amount of economic and other data generated worldwide increases vastly, a challenge for future generations of econometricians will be to master efficient algorithms for inference in empirical models with large information sets. This Chapter provides a review of popular estimation algorithms for Bayesian inference in econometrics and surveys alternative algorithms developed in machine learning and computing science that allow for efficient computation in high-dimensional settings. The focus is on scalability and parallelizability of each algorithm, as well as their ability to be adopted in various empirical settings in economics and finance."
http://arxiv.org/abs/2005.00137v1,How average is average? Temporal patterns in human behaviour as measured by mobile phone data -- or why chose Thursdays,2020-04-30 23:06:15+00:00,"['Marina Toger', 'Ian Shuttleworth', 'John Östh']",econ.GN,"Mobile phone data -- with file sizes scaling into terabytes -- easily overwhelm the computational capacity available to some researchers. Moreover, for ethical reasons, data access is often granted only to particular subsets, restricting analyses to cover single days, weeks, or geographical areas. Consequently, it is frequently impossible to set a particular analysis or event in its context and know how typical it is, compared to other days, weeks or months. This is important for academic referees questioning research on mobile phone data and for the analysts in deciding how to sample, how much data to process, and which events are anomalous. All these issues require an understanding of variability in Big Data to answer the question of how average is average? This paper provides a method, using a large mobile phone dataset, to answer these basic but necessary questions. We show that file size is a robust proxy for the activity level of phone users by profiling the temporal variability of the data at an hourly, daily and monthly level. We then apply time-series analysis to isolate temporal periodicity. Finally, we discuss confidence limits to anomalous events in the data. We recommend an analytical approach to mobile phone data selection which suggests that ideally data should be sampled across days, across working weeks, and across the year, to obtain a representative average. However, where this is impossible, the temporal variability is such that specific weekdays' data can provide a fair picture of other days in their general structure."
http://arxiv.org/abs/2004.14640v1,Stable Roommate Problem with Diversity Preferences,2020-04-30 08:59:27+00:00,"['Niclas Boehmer', 'Edith Elkind']",cs.GT,"In the multidimensional stable roommate problem, agents have to be allocated to rooms and have preferences over sets of potential roommates. We study the complexity of finding good allocations of agents to rooms under the assumption that agents have diversity preferences [Bredereck et al., 2019]: each agent belongs to one of the two types (e.g., juniors and seniors, artists and engineers), and agents' preferences over rooms depend solely on the fraction of agents of their own type among their potential roommates. We consider various solution concepts for this setting, such as core and exchange stability, Pareto optimality and envy-freeness. On the negative side, we prove that envy-free, core stable or (strongly) exchange stable outcomes may fail to exist and that the associated decision problems are NP-complete. On the positive side, we show that these problems are in FPT with respect to the room size, which is not the case for the general stable roommate problem. Moreover, for the classic setting with rooms of size two, we present a linear-time algorithm that computes an outcome that is core and exchange stable as well as Pareto optimal. Many of our results for the stable roommate problem extend to the stable marriage problem."
http://arxiv.org/abs/2005.09100v1,The Effects of Smartphones on Well-Being: Theoretical Integration and Research Agenda,2020-05-18 21:25:51+00:00,"['Kostadin Kushlev', 'Matthew R Leitao']",cs.HC,"As smartphones become ever more integrated in peoples lives, a burgeoning new area of research has emerged on their well-being effects. We propose that disparate strands of research and apparently contradictory findings can be integrated under three basic hypotheses, positing that smartphones influence well-being by (1) replacing other activities (displacement hypothesis), (2) interfering with concurrent activities (interference hypothesis), and (3) affording access to information and activities that would otherwise be unavailable (complementarity hypothesis). Using this framework, we highlight methodological issues and go beyond net effects to examine how and when phones boost versus hurt well-being. We examine both psychological and contextual mediators and moderators of the effects, thus outlining an agenda for future research."
http://arxiv.org/abs/2005.07521v1,Exploring Weak Strategy-Proofness in Voting Theory,2020-05-13 19:53:08+00:00,['Anne Carlstein'],econ.TH,"Voting is the aggregation of individual preferences in order to select a winning alternative. Selection of a winner is accomplished via a voting rule, e.g., rank-order voting, majority rule, plurality rule, approval voting. Which voting rule should be used? In social choice theory, desirable properties of voting rules are expressed as axioms to be satisfied. This thesis focuses on axioms concerning strategic manipulation by voters. Sometimes, voters may intentionally misstate their true preferences in order to alter the outcome for their own advantage. For example, in plurality rule, if a voter knows that their top-choice candidate will lose, then they might instead vote for their second-choice candidate just to avoid an even less desirable result. When no coalition of voters can strategically manipulate, then the voting rule is said to satisfy the axiom of Strategy-Proofness. A less restrictive axiom is Weak Strategy-Proofness (as defined by Dasgupta and Maskin (2019)), which allows for strategic manipulation by all but the smallest coalitions. Under certain intuitive conditions, Dasgupta and Maskin (2019) proved that the only voting rules satisfying Strategy-Proofness are rank-order voting and majority rule. In my thesis, I generalize their result, by proving that rank-order voting and majority rule are surprisingly still the only voting rules satisfying Weak Strategy-Proofness."
http://arxiv.org/abs/2005.01103v1,Dynamic Reserves in Matching Markets,2020-05-03 14:42:41+00:00,"['Orhan Aygün', 'Bertan Turhan']",econ.TH,"We study a school choice problem under affirmative action policies where authorities reserve a certain fraction of the slots at each school for specific student groups, and where students have preferences not only over the schools they are matched to but also the type of slots they receive. Such reservation policies might cause waste in instances of low demand from some student groups. To propose a solution to this issue, we construct a family of choice functions, dynamic reserves choice functions, for schools that respect within-group fairness and allow the transfer of otherwise vacant slots from low-demand groups to high-demand groups. We propose the cumulative offer mechanism (COM) as an allocation rule where each school uses a dynamic reserves choice function and show that it is stable with respect to schools' choice functions, is strategy-proof, and respects improvements. Furthermore, we show that transferring more of the otherwise vacant slots leads to strategy-proof Pareto improvement under the COM."
http://arxiv.org/abs/2005.01835v1,The Murphy Decomposition and the Calibration-Resolution Principle: A New Perspective on Forecast Evaluation,2020-05-04 20:41:00+00:00,['Marc-Oliver Pohle'],stat.ME,"I provide a unifying perspective on forecast evaluation, characterizing accurate forecasts of all types, from simple point to complete probabilistic forecasts, in terms of two fundamental underlying properties, autocalibration and resolution, which can be interpreted as describing a lack of systematic mistakes and a high information content. This ""calibration-resolution principle"" gives a new insight into the nature of forecasting and generalizes the famous sharpness principle by Gneiting et al. (2007) from probabilistic to all types of forecasts. It amongst others exposes the shortcomings of several widely used forecast evaluation methods. The principle is based on a fully general version of the Murphy decomposition of loss functions, which I provide. Special cases of this decomposition are well-known and widely used in meteorology.
  Besides using the decomposition in this new theoretical way, after having introduced it and the underlying properties in a proper theoretical framework, accompanied by an illustrative example, I also employ it in its classical sense as a forecast evaluation method as the meteorologists do: As such, it unveils the driving forces behind forecast errors and complements classical forecast evaluation methods. I discuss estimation of the decomposition via kernel regression and then apply it to popular economic forecasts. Analysis of mean forecasts from the US Survey of Professional Forecasters and quantile forecasts derived from Bank of England fan charts indeed yield interesting new insights and highlight the potential of the method."
http://arxiv.org/abs/2005.01839v1,Equilibria of nonatomic anonymous games,2020-05-04 20:45:24+00:00,"['Simone Cerreia-Vioglio', 'Fabio Maccheroni', 'David Schmeidler']",econ.TH,"We add here another layer to the literature on nonatomic anonymous games started with the 1973 paper by Schmeidler. More specifically, we define a new notion of equilibrium which we call $\varepsilon$-estimated equilibrium and prove its existence for any positive $\varepsilon$. This notion encompasses and brings to nonatomic games recent concepts of equilibrium such as self-confirming, peer-confirming, and Berk--Nash. This augmented scope is our main motivation. At the same time, our approach also resolves some conceptual problems present in Schmeidler (1973), pointed out by Shapley. In that paper\ the existence of pure-strategy Nash equilibria has been proved for any nonatomic game with a continuum of players, endowed with an atomless countably additive probability. But, requiring Borel measurability of strategy profiles may impose some limitation on players' choices and introduce an exogenous dependence among\ players' actions, which clashes with the nature of noncooperative game theory. Our suggested solution is to consider every subset of players as measurable. This leads to a nontrivial purely finitely additive component which might prevent the existence of equilibria and requires a novel mathematical approach to prove the existence of $\varepsilon$-equilibria."
http://arxiv.org/abs/2005.06576v1,Short-Term Investments and Indices of Risk,2020-05-13 20:35:39+00:00,"['Yuval Heller', 'Amnon Schreiber']",q-fin.PM,"We study various decision problems regarding short-term investments in risky assets whose returns evolve continuously in time. We show that in each problem, all risk-averse decision makers have the same (problem-dependent) ranking over short-term risky assets. Moreover, in each problem, the ranking is represented by the same risk index as in the case of CARA utility agents and normally distributed risky assets."
http://arxiv.org/abs/2004.13261v2,Matching with Generalized Lexicographic Choice Rules,2020-04-28 03:33:30+00:00,"['Orhan Aygün', 'Bertan Turhan']",econ.TH,"Motivated by the need for real-world matching problems, this paper formulates a large class of practical choice rules, Generalized Lexicographic Choice Rules (GLCR), for institutions that consist of multiple divisions. Institutions fill their divisions sequentially, and each division is endowed with a sub-choice rule that satisfies classical substitutability and size monotonicity in conjunction with a new property that we introduce, quota monotonicity. We allow rich interactions between divisions in the form of capacity transfers. The overall choice rule of an institution is defined as the union of the sub-choices of its divisions. The cumulative offer mechanism (COM) with respect to GLCR is the unique stable and strategy-proof mechanism. We define a choice-based improvement notion and show that the COM respects improvements. We employ the theory developed in this paper in our companion paper, Aygün and Turhan (2020), to design satisfactory matching mechanisms for India with comprehensive affirmative action constraints."
http://arxiv.org/abs/2004.13620v1,Wealth distribution under the spread of infectious diseases,2020-04-28 16:00:30+00:00,"['G. Dimarco', 'L. Pareschi', 'G. Toscani', 'M. Zanella']",physics.soc-ph,"We develop a mathematical framework to study the economic impact of infectious diseases by integrating epidemiological dynamics with a kinetic model of wealth exchange. The multi-agent description leads to study the evolution over time of a system of kinetic equations for the wealth densities of susceptible, infectious and recovered individuals, whose proportions are driven by a classical compartmental model in epidemiology. Explicit calculations show that the spread of the disease seriously affects the distribution of wealth, which, unlike the situation in the absence of epidemics, can converge towards a stationary state with a bimodal form. Furthermore, simulations confirm the ability of the model to describe different phenomena characteristics of economic trends in situations compromised by the rapid spread of an epidemic, such as the unequal impact on the various wealth classes and the risk of a shrinking middle class."
http://arxiv.org/abs/2004.05563v1,Closing Gaps in Asymptotic Fair Division,2020-04-12 08:21:09+00:00,"['Pasin Manurangsi', 'Warut Suksompong']",cs.GT,"We study a resource allocation setting where $m$ discrete items are to be divided among $n$ agents with additive utilities, and the agents' utilities for individual items are drawn at random from a probability distribution. Since common fairness notions like envy-freeness and proportionality cannot always be satisfied in this setting, an important question is when allocations satisfying these notions exist. In this paper, we close several gaps in the line of work on asymptotic fair division. First, we prove that the classical round-robin algorithm is likely to produce an envy-free allocation provided that $m=Ω(n\log n/\log\log n)$, matching the lower bound from prior work. We then show that a proportional allocation exists with high probability as long as $m\geq n$, while an allocation satisfying envy-freeness up to any item (EFX) is likely to be present for any relation between $m$ and $n$. Finally, we consider a related setting where each agent is assigned exactly one item and the remaining items are left unassigned, and show that the transition from non-existence to existence with respect to envy-free assignments occurs at $m=en$."
http://arxiv.org/abs/2006.14838v1,Kuhn's Equivalence Theorem for Games in Intrinsic Form,2020-06-26 07:35:21+00:00,"['Benjamin Heymann', 'Michel de Lara', 'Jean-Philippe Chancelier']",math.OC,"We state and prove Kuhn's equivalence theorem for a new representation of games, the intrinsic form. First, we introduce games in intrinsic form where information is represented by $σ$-fields over a product set. For this purpose, we adapt to games the intrinsic representation that Witsenhausen introduced in control theory. Those intrinsic games do not require an explicit description of the play temporality, as opposed to extensive form games on trees. Second, we prove, for this new and more general representation of games, that behavioral and mixed strategies are equivalent under perfect recall (Kuhn's theorem). As the intrinsic form replaces the tree structure with a product structure, the handling of information is easier. This makes the intrinsic form a new valuable tool for the analysis of games with information."
http://arxiv.org/abs/2005.09095v4,Role models and revealed gender-specific costs of STEM in an extended Roy model of major choice,2020-05-18 21:17:33+00:00,"['Marc Henry', 'Romuald Meango', 'Ismael Mourifie']",econ.EM,"We derive sharp bounds on the non consumption utility component in an extended Roy model of sector selection. We interpret this non consumption utility component as a compensating wage differential. The bounds are derived under the assumption that potential utilities in each sector are (jointly) stochastically monotone with respect to an observed selection shifter. The research is motivated by the analysis of women's choice of university major, their under representation in mathematics intensive fields, and the impact of role models on choices and outcomes. To illustrate our methodology, we investigate the cost of STEM fields with data from a German graduate survey, and using the mother's education level and the proportion of women on the STEM faculty at the time of major choice as selection shifters."
http://arxiv.org/abs/2005.05772v3,"Evolution, Heritable Risk, and Skewness Loving",2020-05-12 13:50:26+00:00,"['Yuval Heller', 'Arthur Robson']",econ.TH,"Our understanding of risk preferences can be sharpened by considering their evolutionary basis. The existing literature has focused on two sources of risk: idiosyncratic risk and aggregate risk. We introduce a new source of risk, heritable risk, in which there is a positive correlation between the fitness of a newborn agent and the fitness of her parent. Heritable risk was plausibly common in our evolutionary past and it leads to a strictly higher growth rate than the other sources of risk. We show that the presence of heritable risk in the evolutionary past may explain the tendency of people to exhibit skewness loving today."
http://arxiv.org/abs/2005.03906v2,Dynamic Shrinkage Priors for Large Time-varying Parameter Regressions using Scalable Markov Chain Monte Carlo Methods,2020-05-08 08:40:09+00:00,"['Niko Hauzenberger', 'Florian Huber', 'Gary Koop']",econ.EM,"Time-varying parameter (TVP) regression models can involve a huge number of coefficients. Careful prior elicitation is required to yield sensible posterior and predictive inferences. In addition, the computational demands of Markov Chain Monte Carlo (MCMC) methods mean their use is limited to the case where the number of predictors is not too large. In light of these two concerns, this paper proposes a new dynamic shrinkage prior which reflects the empirical regularity that TVPs are typically sparse (i.e. time variation may occur only episodically and only for some of the coefficients). A scalable MCMC algorithm is developed which is capable of handling very high dimensional TVP regressions or TVP Vector Autoregressions. In an exercise using artificial data we demonstrate the accuracy and computational efficiency of our methods. In an application involving the term structure of interest rates in the eurozone, we find our dynamic shrinkage prior to effectively pick out small amounts of parameter change and our methods to forecast well."
http://arxiv.org/abs/2004.00196v2,"Equilibrium Selection in Data Markets: Multiple-Principal, Multiple-Agent Problems with Non-Rivalrous Goods",2020-04-01 02:07:19+00:00,"['Samir Wadhwa', 'Roy Dong']",cs.GT,"There are several aspects of data markets that distinguish them from a typical commodity market: asymmetric information, the non-rivalrous nature of data, and informational externalities. Formally, this gives rise to a new class of games which we call multiple-principal, multiple-agent problem with non-rivalrous goods. Under the assumption that the principal's payoff is quasilinear in the payments given to agents, we show that there is a fundamental degeneracy in the market of non-rivalrous goods. This multiplicity of equilibria also affects common refinements of equilibrium definitions intended to uniquely select an equilibrium: both variational equilibria and normalized equilibria will be non-unique in general. This implies that most existing equilibrium concepts cannot provide predictions on the outcomes of data markets emerging today. The results support the idea that modifications to payment contracts themselves are unlikely to yield a unique equilibrium, and either changes to the models of study or new equilibrium concepts will be required to determine unique equilibria in settings with multiple principals and a non-rivalrous good."
http://arxiv.org/abs/2008.10217v2,Finite-Sample Average Bid Auction,2020-08-24 06:40:09+00:00,['Haitian Xie'],econ.EM,"The paper studies the problem of auction design in a setting where the auctioneer accesses the knowledge of the valuation distribution only through statistical samples. A new framework is established that combines the statistical decision theory with mechanism design. Two optimality criteria, maxmin, and equivariance, are studied along with their implications on the form of auctions. The simplest form of the equivariant auction is the average bid auction, which set individual reservation prices proportional to the average of other bids and historical samples. This form of auction can be motivated by the Gamma distribution, and it sheds new light on the estimation of the optimal price, an irregular parameter. Theoretical results show that it is often possible to use the regular parameter population mean to approximate the optimal price. An adaptive average bid estimator is developed under this idea, and it has the same asymptotic properties as the empirical Myerson estimator. The new proposed estimator has a significantly better performance in terms of value at risk and expected shortfall when the sample size is small."
http://arxiv.org/abs/2007.09213v4,How Flexible is that Functional Form? Quantifying the Restrictiveness of Theories,2020-07-17 20:08:53+00:00,"['Drew Fudenberg', 'Wayne Gao', 'Annie Liang']",econ.TH,"We propose a restrictiveness measure for economic models based on how well they fit synthetic data from a pre-defined class. This measure, together with a measure for how well the model fits real data, outlines a Pareto frontier, where models that rule out more regularities, yet capture the regularities that are present in real data, are preferred. To illustrate our approach, we evaluate the restrictiveness of popular models in two laboratory settings -- certainty equivalents and initial play -- and in one field setting -- takeup of microfinance in Indian villages. The restrictiveness measure reveals new insights about each of the models, including that some economic models with only a few parameters are very flexible."
http://arxiv.org/abs/2009.11917v8,Learning in a Small/Big World,2020-09-24 19:25:02+00:00,['Benson Tsz Kin Leung'],econ.TH,"Complexity and limited ability have profound effect on how we learn and make decisions under uncertainty. Using the theory of finite automaton to model belief formation, this paper studies the characteristics of optimal learning behavior in small and big worlds, where the complexity of the environment is low and high, respectively, relative to the cognitive ability of the decision maker. Optimal behavior is well approximated by the Bayesian benchmark in very small world but is more different as the world gets bigger. In addition, in big worlds, the optimal learning behavior could exhibit a wide range of well-documented non-Bayesian learning behavior, including the use of heuristics, correlation neglect, persistent over-confidence, inattentive learning, and other behaviors of model simplification or misspecification. These results establish a clear and testable relationship among the prominence of non-Bayesian learning behavior, complexity, and cognitive ability."
http://arxiv.org/abs/2007.10564v2,The impact of economic policy uncertainties on the volatility of European carbon market,2020-07-21 02:17:03+00:00,"['Peng-Fei Dai', 'Xiong Xiong', 'Toan Luu Duc Huynh', 'Jiqiang Wang']",econ.GN,"The European Union Emission Trading Scheme is a carbon emission allowance trading system designed by Europe to achieve emission reduction targets. The amount of carbon emission caused by production activities is closely related to the socio-economic environment. Therefore, from the perspective of economic policy uncertainty, this article constructs the GARCH-MIDAS-EUEPU and GARCH-MIDAS-GEPU models for investigating the impact of European and global economic policy uncertainty on carbon price fluctuations. The results show that both European and global economic policy uncertainty will exacerbate the long-term volatility of European carbon spot return, with the latter having a stronger impact when the change is the same. Moreover, the volatility of the European carbon spot return can be forecasted better by the predictor, global economic policy uncertainty. This research can provide some implications for market managers in grasping carbon market trends and helping participants control the risk of fluctuations in carbon allowances."
http://arxiv.org/abs/2009.03379v1,Counterfactual and Welfare Analysis with an Approximate Model,2020-09-07 19:20:59+00:00,"['Roy Allen', 'John Rehbeck']",econ.EM,"We propose a conceptual framework for counterfactual and welfare analysis for approximate models. Our key assumption is that model approximation error is the same magnitude at new choices as the observed data. Applying the framework to quasilinear utility, we obtain bounds on quantities at new prices using an approximate law of demand. We then bound utility differences between bundles and welfare differences between prices. All bounds are computable as linear programs. We provide detailed analytical results describing how the data map to the bounds including shape restrictions that provide a foundation for plug-in estimation. An application to gasoline demand illustrates the methodology."
http://arxiv.org/abs/2008.04229v10,Decision Conflict and Deferral in A Class of Logit Models with a Context-Dependent Outside Option,2020-08-10 16:03:21+00:00,['Georgios Gerasimou'],econ.TH,"Decision makers often opt for the deferral outside option when they find it difficult to make an active choice. Contrary to existing logit models with an outside option where the latter is assigned a fixed value exogenously, this paper introduces and analyzes a class of logit models where that option's value is menu-dependent, may be determined endogenously, and could be interpreted as proxying the varying dgree of decision difficulty at different menus. We focus on the *power logit* special class of these models. We show that these predict some observed choice-deferral effects that are caused by hard decisions, including non-monotonic ""roller-coaster"" choice-overload phenomena that are regulated by the presence or absence of a clearly dominant feasible alternative. We illustrate the usability, novel insights and explanatory gains of the proposed framework for empirical discrete choice analysis and theoretical modelling of imperfectly competitive markets in the presence of potentially indecisive consumers."
http://arxiv.org/abs/2008.02581v1,Teaching Economics with Interactive Browser-Based Models,2020-08-06 11:24:46+00:00,"['Juan Dominguez-Moran', 'Rouven Geismar']",q-fin.GN,"Interactive simulation toolkits come in handy when teaching macroeconomic models by facilitating an easy understanding of underlying economic concepts and offering an intuitive approach to the models' comparative statics. Based on the example of the IS-LM model, this paper demonstrates innovative browser-based features well-suited for the shift in education to online platforms accelerated by COVID-19. The free and open-source code can be found alongside the standalone HTML files for the AD-AS and the Solow growth model at https://gitlab.tu-berlin.de/chair-of-macroeconomics/."
http://arxiv.org/abs/2008.04256v1,Purely Bayesian counterfactuals versus Newcomb's paradox,2020-08-10 16:56:48+00:00,['Lê Nguyên Hoang'],econ.TH,"This paper proposes a careful separation between an entity's epistemic system and their decision system. Crucially, Bayesian counterfactuals are estimated by the epistemic system; not by the decision system. Based on this remark, I prove the existence of Newcomb-like problems for which an epistemic system necessarily expects the entity to make a counterfactually bad decision. I then address (a slight generalization of) Newcomb's paradox. I solve the specific case where the player believes that the predictor applies Bayes rule with a supset of all the data available to the player. I prove that the counterfactual optimality of the 1-Box strategy depends on the player's prior on the predictor's additional data. If these additional data are not expected to reduce sufficiently the predictor's uncertainty on the player's decision, then the player's epistemic system will counterfactually prefer to 2-Box. But if the predictor's data is believed to make them quasi-omniscient, then 1-Box will be counterfactually preferred. Implications of the analysis are then discussed. More generally, I argue that, to better understand or design an entity, it is useful to clearly separate the entity's epistemic, decision, but also data collection, reward and maintenance systems, whether the entity is human, algorithmic or institutional."
http://arxiv.org/abs/2008.04850v5,Counting the costs of COVID-19: why future treatment option values matter,2020-07-06 12:51:00+00:00,['Adrian Kent'],econ.GN,"I critique a recent analysis (Miles, Stedman & Heald, 2020) of COVID-19 lockdown costs and benefits, focussing on the United Kingdom (UK). Miles et al. (2020) argue that the March-June UK lockdown was more costly than the benefit of lives saved, evaluated using the NICE threshold of £30000 for a quality-adjusted life year (QALY) and that the costs of a lockdown for 13 weeks from mid-June would be vastly greater than any plausible estimate of the benefits, even if easing produced a second infection wave causing over 7000 deaths weekly by mid-September.
  I note here two key problems that significantly affect their estimates and cast doubt on their conclusions. Firstly, their calculations arbitrarily cut off after 13 weeks, without costing the epidemic end state. That is, they assume indifference between mid-September states of 13 or 7500 weekly deaths and corresponding infection rates. This seems indefensible unless one assumes that (a) there is little chance of any effective vaccine or improved medical or social interventions for the foreseeable future, (b) notwithstanding temporary lockdowns, COVID-19 will very likely propagate until herd immunity. Even under these assumptions it is very questionable. Secondly, they ignore the costs of serious illness, possible long-term lowering of life quality and expectancy for survivors. These are uncertain, but plausibly at least as large as the costs in deaths.
  In summary, policy on tackling COVID-19 cannot be rationally made without estimating probabilities of future medical interventions and long-term illness costs. More work on modelling these uncertainties is urgently needed."
http://arxiv.org/abs/2008.06564v1,Optimal selection of the number of control units in kNN algorithm to estimate average treatment effects,2020-08-14 20:18:16+00:00,"['Andrés Ramírez-Hassan', 'Raquel Vargas-Correa', 'Gustavo García', 'Daniel Londoño']",econ.EM,"We propose a simple approach to optimally select the number of control units in k nearest neighbors (kNN) algorithm focusing in minimizing the mean squared error for the average treatment effects. Our approach is non-parametric where confidence intervals for the treatment effects were calculated using asymptotic results with bias correction. Simulation exercises show that our approach gets relative small mean squared errors, and a balance between confidence intervals length and type I error. We analyzed the average treatment effects on treated (ATET) of participation in 401(k) plans on accumulated net financial assets confirming significant effects on amount and positive probability of net asset. Our optimal k selection produces significant narrower ATET confidence intervals compared with common practice of using k=1."
http://arxiv.org/abs/2007.11606v1,The Mode Treatment Effect,2020-07-22 18:05:56+00:00,['Neng-Chieh Chang'],econ.EM,"Mean, median, and mode are three essential measures of the centrality of probability distributions. In program evaluation, the average treatment effect (mean) and the quantile treatment effect (median) have been intensively studied in the past decades. The mode treatment effect, however, has long been neglected in program evaluation. This paper fills the gap by discussing both the estimation and inference of the mode treatment effect. I propose both traditional kernel and machine learning methods to estimate the mode treatment effect. I also derive the asymptotic properties of the proposed estimators and find that both estimators follow the asymptotic normality but with the rate of convergence slower than the regular rate $\sqrt{N}$, which is different from the rates of the classical average and quantile treatment effect estimators."
http://arxiv.org/abs/2007.04962v1,"Análise dos fatores determinantes para o decreto de prisão preventiva em casos envolvendo acusações por roubo, tráfico de drogas e furto: Um estudo no âmbito das cidades mais populosas do Paraná",2020-07-09 17:51:27+00:00,"['Giovane Cerezuela Policeno', 'Mario Edson Passerino Fischer da Silva', 'Vitor Pestana Ostrensky']",econ.GN,"Based on the theoretical assumptions of the Labeling Approach, Critical Criminology and Behavioral Economics, taking into account that almost half of the Brazilian prison population is composed of individuals who are serving pre-trial detention, it was sought to assess the characteristics of the flagranteated were presented as determinants to influence the subjectivity of the judges in the decision to determine, or not, the custody of these. The research initially adopted a deductive methodology, based on the principle that external objective factors encourage magistrates to decide in a certain sense. It was then focused on the identification of which characteristics of the flagranteated and allegedly committed crimes would be relevant to guide such decisions. Subsequently, after deduction of such factors, an inductive methodology was adopted, analyzing the data from the theoretical assumptions pointed out. During the research, 277 decisions were analyzed, considering decision as individual decision and not custody hearing. This sample embarked decisions of six judges among the largest cities of the State of Paraná and concerning the crimes of theft, robbery and drug trafficking. It was then concluded that the age, gender, social class and type of accusation that the flagranteated suffered are decisive for the decree of his provisional arrest, being that, depending on the judge competent in the case, the chances of the decree can increase in Up to 700%, taking into account that the circumstantial and causal variables are constant. Given the small sample size, as far as the number of judges is concerned, more extensive research is needed so that conclusions of national validity can be developed."
http://arxiv.org/abs/2007.07091v1,Time-Equitable Dynamic Tolling Scheme For Single Bottlenecks,2020-07-11 19:19:57+00:00,"['John W Helsel', 'Venktesh Pandey', 'Stephen D. Boyles']",econ.TH,"Dynamic tolls present an opportunity for municipalities to eliminate congestion and fund infrastructure. Imposing tolls that regulate travel along a public highway through monetary fees raise worries of inequity. In this article, we introduce the concept of time poverty, emphasize its value in policy-making in the same ways income poverty is already considered, and argue the potential equity concern posed by time-varying tolls that produce time poverty. We also compare the cost burdens of a no-toll, system optimal toll, and a proposed ``time-equitable"" toll on heterogeneous traveler groups using an analytical Vickrey bottleneck model where travelers make departure time decisions to arrive at their destination at a fixed time. We show that the time-equitable toll is able to eliminate congestion while creating equitable travel patterns amongst traveler groups."
http://arxiv.org/abs/2008.00718v1,Estimating TVP-VAR models with time invariant long-run multipliers,2020-08-03 08:45:01+00:00,"['Denis Belomestny', 'Ekaterina Krymova', 'Andrey Polbin']",econ.EM,"The main goal of this paper is to develop a methodology for estimating time varying parameter vector auto-regression (TVP-VAR) models with a timeinvariant long-run relationship between endogenous variables and changes in exogenous variables. We propose a Gibbs sampling scheme for estimation of model parameters as well as time-invariant long-run multiplier parameters. Further we demonstrate the applicability of the proposed method by analyzing examples of the Norwegian and Russian economies based on the data on real GDP, real exchange rate and real oil prices. Our results show that incorporating the time invariance constraint on the long-run multipliers in TVP-VAR model helps to significantly improve the forecasting performance."
http://arxiv.org/abs/2008.01618v2,Distributionally Robust Pricing in Independent Private Value Auctions,2020-08-04 14:59:45+00:00,['Alex Suzdaltsev'],econ.TH,"A seller chooses a reserve price in a second-price auction to maximize worst-case expected revenue when she knows only the mean of value distribution and an upper bound on either values themselves or variance. Values are private and iid. Using an indirect technique, we prove that it is always optimal to set the reserve price to the seller's own valuation. However, the maxmin reserve price may not be unique. If the number of bidders is sufficiently high, all prices below the seller's valuation, including zero, are also optimal. A second-price auction with the reserve equal to seller's value (or zero) is an asymptotically optimal mechanism (among all ex post individually rational mechanisms) as the number of bidders grows without bound."
http://arxiv.org/abs/2007.15265v1,Equilibrium Oil Market Share under the COVID-19 Pandemic,2020-07-30 07:08:39+00:00,"['Xiaojun Chen', 'Yun Shi', 'Xiaozhou Wang']",math.OC,Equilibrium models for energy markets under uncertain demand and supply have attracted considerable attentions. This paper focuses on modelling crude oil market share under the COVID-19 pandemic using two-stage stochastic equilibrium. We describe the uncertainties in the demand and supply by random variables and provide two types of production decisions (here-and-now and wait-and-see). The here-and-now decision in the first stage does not depend on the outcome of random events to be revealed in the future and the wait-and-see decision in the second stage is allowed to depend on the random events in the future and adjust the feasibility of the here-and-now decision in rare unexpected scenarios such as those observed during the COVID-19 pandemic. We develop a fast algorithm to find a solution of the two-stage stochastic equilibrium. We show the robustness of the two-stage stochastic equilibrium model for forecasting the oil market share using the real market data from January 2019 to May 2020.
http://arxiv.org/abs/2007.01896v2,Spatial Iterated Prisoner's Dilemma as a Transformation Semigroup,2020-07-03 18:20:30+00:00,"['Isaiah Farahbakhsh', 'Chrystopher L. Nehaniv']",math.DS,"The prisoner's dilemma (PD) is a game-theoretic model studied in a wide array of fields to understand the emergence of cooperation between rational self-interested agents. In this work, we formulate a spatial iterated PD as a discrete-event dynamical system where agents play the game in each time-step and analyse it algebraically using Krohn-Rhodes algebraic automata theory using a computational implementation of the holonomy decomposition of transformation semigroups. In each iteration all players adopt the most profitable strategy in their immediate neighbourhood. Perturbations resetting the strategy of a given player provide additional generating events for the dynamics. Our initial study shows that the algebraic structure, including how natural subsystems comprising permutation groups acting on the spatial distributions of strategies, arise in certain parameter regimes for the pay-off matrix, and are absent for other parameter regimes. Differences in the number of group levels in the holonomy decomposition (an upper bound for Krohn-Rhodes complexity) are revealed as more pools of reversibility appear when the temptation to defect is at an intermediate level. Algebraic structure uncovered by this analysis can be interpreted to shed light on the dynamics of the spatial iterated PD."
http://arxiv.org/abs/2007.02141v2,Off-Policy Exploitability-Evaluation in Two-Player Zero-Sum Markov Games,2020-07-04 16:51:56+00:00,"['Kenshi Abe', 'Yusuke Kaneko']",cs.LG,"Off-policy evaluation (OPE) is the problem of evaluating new policies using historical data obtained from a different policy. In the recent OPE context, most studies have focused on single-player cases, and not on multi-player cases. In this study, we propose OPE estimators constructed by the doubly robust and double reinforcement learning estimators in two-player zero-sum Markov games. The proposed estimators project exploitability that is often used as a metric for determining how close a policy profile (i.e., a tuple of policies) is to a Nash equilibrium in two-player zero-sum games. We prove the exploitability estimation error bounds for the proposed estimators. We then propose the methods to find the best candidate policy profile by selecting the policy profile that minimizes the estimated exploitability from a given policy profile class. We prove the regret bounds of the policy profiles selected by our methods. Finally, we demonstrate the effectiveness and performance of the proposed estimators through experiments."
http://arxiv.org/abs/2007.11388v1,The impact of life-saving interventions on fertility,2020-07-16 00:45:00+00:00,['David Roodman'],econ.GN,"Many interventions in global health save lives. One criticism sometimes lobbed at these interventions invokes the spirit of Malthus. The good done, the charge goes, is offset by the harm of spreading the earth's limited resources more thinly: more people, and more misery per person. To the extent this holds, the net benefit of savings lives is lower than it appears at first. On the other hand, if lower mortality, especially in childhood, leads families to have fewer children, life-saving interventions could reduce population. This document critically reviews the evidence. It finds that the impact of life-saving interventions on fertility and population growth varies by context, and is rarely greater than 1:1. In places where lifetime births/woman has been converging to 2 or lower, saving one child's life should lead parents to avert a birth they would otherwise have. The impact of mortality drops on fertility will be nearly 1:1, so population growth will hardly change. In the increasingly exceptional locales where couples appear not to limit fertility much, such as Niger and Mali, the impact of saving a life on total births will be smaller, and may come about mainly through the biological channel of lactational amenorrhea. Here, mortality-drop-fertility-drop ratios of 1:0.5 and 1:0.33 appear more plausible. But in the long-term, it would be surprising if these few countries do not join the rest of the world in the transition to lower and more intentionally controlled fertility."
http://arxiv.org/abs/2007.13549v1,The Wage Premium of Communist Party Membership: Evidence from China,2020-07-23 18:35:25+00:00,"['Plamen Nikolov', 'Hongjian Wang', 'Kevin Acker']",econ.GN,"Social status and political connections could confer large economic benefits to an individual. Previous studies focused on China examine the relationship between Communist party membership and earnings and find a positive correlation. However, this correlation may be partly or totally spurious, thereby generating upwards-biased estimates of the importance of political party membership. Using data from three surveys spanning more than three decades, we estimate the causal effect of Chinese party membership on monthly earnings in in China. We find that, on average, membership in the Communist party of China increases monthly earnings and we find evidence that the wage premium has grown in recent years. We explore for potential mechanisms and we find suggestive evidence that improvements in one's social network, acquisition of job-related qualifications and improvement in one's social rank and life satisfaction likely play an important role. (JEL D31, J31, P2)"
http://arxiv.org/abs/2007.04346v1,Efficient Covariate Balancing for the Local Average Treatment Effect,2020-07-08 18:04:46+00:00,['Phillip Heiler'],econ.EM,"This paper develops an empirical balancing approach for the estimation of treatment effects under two-sided noncompliance using a binary conditionally independent instrumental variable. The method weighs both treatment and outcome information with inverse probabilities to produce exact finite sample balance across instrument level groups. It is free of functional form assumptions on the outcome or the treatment selection step. By tailoring the loss function for the instrument propensity scores, the resulting treatment effect estimates exhibit both low bias and a reduced variance in finite samples compared to conventional inverse probability weighting methods. The estimator is automatically weight normalized and has similar bias properties compared to conventional two-stage least squares estimation under constant causal effects for the compliers. We provide conditions for asymptotic normality and semiparametric efficiency and demonstrate how to utilize additional information about the treatment selection step for bias reduction in finite samples. The method can be easily combined with regularization or other statistical learning approaches to deal with a high-dimensional number of observed confounding variables. Monte Carlo simulations suggest that the theoretical advantages translate well to finite samples. The method is illustrated in an empirical example."
http://arxiv.org/abs/2008.08511v1,Are temporary value-added tax reductions passed on to consumers? Evidence from Germany's stimulus,2020-08-19 15:42:15+00:00,"['Felix Montag', 'Alina Sagimuldina', 'Monika Schnitzer']",econ.GN,"This paper provides the first estimates of the pass-through rate of the ongoing temporary value-added tax (VAT) reduction, which is part of the German fiscal response to COVID-19. Using a unique dataset containing the universe of price changes at fuel stations in Germany and France in June and July 2020, we employ a difference-in-differences strategy and find that pass-through is fast and substantial but remains incomplete for all fuel types. Furthermore, we find a high degree of heterogeneity between the pass-through estimates for different fuel types. Our results are consistent with the interpretation that pass-through rates are higher for customer groups who are more likely to exert competitive pressure by shopping for lower prices. Our results have important implications for the effectiveness of the stimulus measure and the cost-effective design of unconventional fiscal policy."
http://arxiv.org/abs/2009.12838v1,On the Continuity of the Feasible Set Mapping in Optimal Transport,2020-09-27 13:17:26+00:00,"['Mario Ghossoub', 'David Saunders']",q-fin.RM,"Consider the set of probability measures with given marginal distributions on the product of two complete, separable metric spaces, seen as a correspondence when the marginal distributions vary. In problems of optimal transport, continuity of this correspondence from marginal to joint distributions is often desired, in light of Berge's Maximum Theorem, to establish continuity of the value function in the marginal distributions, as well as stability of the set of optimal transport plans. Bergin (1999) established the continuity of this correspondence, and in this note, we present a novel and considerably shorter proof of this important result. We then examine an application to an assignment game (transferable utility matching problem) with unknown type distributions."
http://arxiv.org/abs/2008.09932v2,Lindahl Equilibrium as a Collective Choice Rule,2020-08-23 00:37:13+00:00,"['Faruk Gul', 'Wolfgang Pesendorfer']",econ.TH,"A collective choice problem is a finite set of social alternatives and a finite set of economic agents with vNM utility functions. We associate a public goods economy with each collective choice problem and establish the existence and efficiency of (equal income) Lindahl equilibrium allocations. We interpret collective choice problems as cooperative bargaining problems and define a set-valued solution concept, {\it the equitable solution} (ES). We provide axioms that characterize ES and show that ES contains the Nash bargaining solution. Our main result shows that the set of ES payoffs is the same a the set of Lindahl equilibrium payoffs. We consider two applications: in the first, we show that in a large class of matching problems without transfers the set of Lindahl equilibrium payoffs is the same as the set of (equal income) Walrasian equilibrium payoffs. In our second application, we show that in any discrete exchange economy without transfers every Walrasian equilibrium payoff is a Lindahl equilibrium payoff of the corresponding collective choice market. Moreover, for any cooperative bargaining problem, it is possible to define a set of commodities so that the resulting economy's utility possibility set is that bargaining problem {\it and} the resulting economy's set of Walrasian equilibrium payoffs is the same as the set of Lindahl equilibrium payoffs of the corresponding collective choice market."
http://arxiv.org/abs/2009.10834v1,Analysis of the main factors for the configuration of green ports in Colombia,2020-09-22 21:51:26+00:00,"['Abraham Londono Pineda', 'Tatiana Arias Naranjo', 'Jose Alejandro Cano Arenas']",econ.GN,"This study analyzes the factors affecting the configuration and consolidation of green ports in Colombia. For this purpose a case stady of maritime cargo ports of Cartagena, Barranquilla and Santa Marta is performed addressing semiestructured interviews to identify the factors contributing to the consolidation of green ports and the factors guiding the sustainability management in the ports that have not yet been certified as green ports. The results show that environmental regulations are atarting point not the key factor to consolidate asgreen ports. As a conclusions, the conversion of Colombian to green ports should not be limited to the attaiment of certifications, such as Ecoport certification, but should ensure the contribution to sustainable development through economic, social and environmental dimensions and the achievement of the SDGs"
http://arxiv.org/abs/2009.11235v1,"A step-by-step guide to design, implement, and analyze a discrete choice experiment",2020-09-23 16:13:10+00:00,['Daniel Pérez-Troncoso'],econ.EM,"Discrete Choice Experiments (DCE) have been widely used in health economics, environmental valuation, and other disciplines. However, there is a lack of resources disclosing the whole procedure of carrying out a DCE. This document aims to assist anyone wishing to use the power of DCEs to understand people's behavior by providing a comprehensive guide to the procedure. This guide contains all the code needed to design, implement, and analyze a DCE using only free software."
http://arxiv.org/abs/2009.09998v3,On the Existence of Conditional Maximum Likelihood Estimates of the Binary Logit Model with Fixed Effects,2020-09-21 16:24:44+00:00,['Martin Mugnier'],econ.EM,"By exploiting McFadden (1974)'s results on conditional logit estimation, we show that there exists a one-to-one mapping between existence and uniqueness of conditional maximum likelihood estimates of the binary logit model with fixed effects and the configuration of data points. Our results extend those in Albert and Anderson (1984) for the cross-sectional case and can be used to build a simple algorithm that detects spurious estimates in finite samples. As an illustration, we exhibit an artificial dataset for which the STATA's command \texttt{clogit} returns spurious estimates."
http://arxiv.org/abs/2009.10103v1,Recent Developments on Factor Models and its Applications in Econometric Learning,2020-09-21 18:02:20+00:00,"['Jianqing Fan', 'Kunpeng Li', 'Yuan Liao']",econ.EM,"This paper makes a selective survey on the recent development of the factor model and its application on statistical learnings. We focus on the perspective of the low-rank structure of factor models, and particularly draws attentions to estimating the model from the low-rank recovery point of view. The survey mainly consists of three parts: the first part is a review on new factor estimations based on modern techniques on recovering low-rank structures of high-dimensional models. The second part discusses statistical inferences of several factor-augmented models and applications in econometric learning models. The final part summarizes new developments dealing with unbalanced panels from the matrix completion perspective."
http://arxiv.org/abs/2009.08045v1,Identification and Estimation of A Rational Inattention Discrete Choice Model with Bayesian Persuasion,2020-09-17 03:41:44+00:00,['Moyu Liao'],econ.EM,"This paper studies the semi-parametric identification and estimation of a rational inattention model with Bayesian persuasion. The identification requires the observation of a cross-section of market-level outcomes. The empirical content of the model can be characterized by three moment conditions. A two-step estimation procedure is proposed to avoid computation complexity in the structural model. In the empirical application, I study the persuasion effect of Fox News in the 2000 presidential election. Welfare analysis shows that persuasion will not influence voters with high school education but will generate higher dispersion in the welfare of voters with a partial college education and decrease the dispersion in the welfare of voters with a bachelors degree."
http://arxiv.org/abs/2009.04037v1,The Impact of COVID-19 and Policy Responses on Australian Income Distribution and Poverty,2020-09-08 23:55:45+00:00,"['Jinjing Li', 'Yogi Vidyattama', 'Hai Anh La', 'Riyana Miranti', 'Denisa M Sologon']",econ.GN,"This paper undertakes a near real-time analysis of the income distribution effects of the COVID-19 crisis in Australia to understand the ongoing changes in the income distribution as well as the impact of policy responses. By semi-parametrically combining incomplete observed data from three different sources, namely, the Monthly Longitudinal Labour Force Survey, the Survey of Income and Housing and the administrative payroll data, we estimate the impact of COVID-19 and the associated policy responses on the Australian income distribution between February and June 2020, covering the immediate periods before and after the initial outbreak. Our results suggest that despite the growth in unemployment, the Gini of the equalised disposable income inequality dropped by nearly 0.03 point since February. The reduction is because of the additional wage subsidies and welfare supports offered as part of the policy response, offsetting a potential surge in income inequality. Additionally, the poverty rate, which could have been doubled in the absence of the government response, also reduced by 3 to 4 percentage points. The result shows the effectiveness of temporary policy measures in maintaining both the living standards and the level of income inequality. However, the heavy reliance on the support measures raises the possibility that the changes in the income distribution may be reversed and even substantially worsened off should the measures be withdrawn."
http://arxiv.org/abs/2009.05455v1,Object Recognition for Economic Development from Daytime Satellite Imagery,2020-09-11 14:07:12+00:00,"['Klaus Ackermann', 'Alexey Chernikov', 'Nandini Anantharama', 'Miethy Zaman', 'Paul A Raschky']",econ.GN,"Reliable data about the stock of physical capital and infrastructure in developing countries is typically very scarce. This is particular a problem for data at the subnational level where existing data is often outdated, not consistently measured or coverage is incomplete. Traditional data collection methods are time and labor-intensive costly, which often prohibits developing countries from collecting this type of data. This paper proposes a novel method to extract infrastructure features from high-resolution satellite images. We collected high-resolution satellite images for 5 million 1km $\times$ 1km grid cells covering 21 African countries. We contribute to the growing body of literature in this area by training our machine learning algorithm on ground-truth data. We show that our approach strongly improves the predictive accuracy. Our methodology can build the foundation to then predict subnational indicators of economic development for areas where this data is either missing or unreliable."
http://arxiv.org/abs/2009.05771v1,Application of a system of indicatirs for assessing the socio-economic situation of a subject based on digital shadows,2020-09-12 11:47:07+00:00,['Olga G. Lebedinskaya'],econ.GN,"The development of Digital Economy sets its own requirements for the formation and development of so-called digital doubles and digital shadows of real objects (subjects/regions). An integral element of their development and application is a multi-level matrix of targets and resource constraints (time, financial, technological, production, etc.). The volume of statistical information collected for a digital double must meet several criteria: be objective, characterize the real state of the managed object as accurately as possible, contain all the necessary information on all managed parameters, and at the same time avoid unnecessary and duplicate indicators (""information garbage""). The relevance of forming the profile of the ""digital shadow of the region"" in the context of multitasking and conflict of departmental and Federal statistics predetermined the goal of the work-to form a system of indicators of the socio-economic situation of regions based on the harmonization of information resources. In this study, an inventory of the composition of indicators of statistical forms for their relevance and relevance was carried out on the example of assessing the economic health of the subject and the level of provision of banking services"
http://arxiv.org/abs/2008.09653v2,Search for a moving target in a competitive environment,2020-08-21 19:08:16+00:00,"['Benoit Duvocelle', 'János Flesch', 'Hui Min Shi', 'Dries Vermeulen']",math.OC,"We consider a discrete-time dynamic search game in which a number of players compete to find an invisible object that is moving according to a time-varying Markov chain. We examine the subgame perfect equilibria of these games. The main result of the paper is that the set of subgame perfect equilibria is exactly the set of greedy strategy profiles, i.e. those strategy profiles in which the players always choose an action that maximizes their probability of immediately finding the object. We discuss various variations and extensions of the model."
http://arxiv.org/abs/2008.11850v1,Changes in mobility and socioeconomic conditions in Bogotá city during the COVID-19 outbreak,2020-08-26 22:38:22+00:00,"['Marco Dueñas', 'Mercedes Campi', 'Luis Olmos']",physics.soc-ph,"We analyze mobility changes following the implementation of containment measures aimed at mitigating the spread of COVID-19 in Bogotá, Colombia. We characterize the mobility network before and during the pandemic and analyze its evolution and changes between January and July 2020. We then link the observed mobility changes to socioeconomic conditions, estimating a gravity model to assess the effect of socioeconomic conditions on mobility flows. We observe an overall reduction in mobility trends, but the overall connectivity between different areas of the city remains after the lockdown, reflecting the mobility network's resilience. We find that the responses to lockdown policies depend on socioeconomic conditions. Before the pandemic, the population with better socioeconomic conditions shows higher mobility flows. Since the lockdown, mobility presents a general decrease, but the population with worse socioeconomic conditions shows lower decreases in mobility flows. We conclude deriving policy implications."
http://arxiv.org/abs/2009.13802v1,"Expectations, Networks, and Conventions",2020-09-29 06:21:16+00:00,"['Benjamin Golub', 'Stephen Morris']",econ.TH,"In coordination games and speculative over-the-counter financial markets, solutions depend on higher-order average expectations: agents' expectations about what counterparties, on average, expect their counterparties to think, etc. We offer a unified analysis of these objects and their limits, for general information structures, priors, and networks of counterparty relationships. Our key device is an interaction structure combining the network and agents' beliefs, which we analyze using Markov methods. This device allows us to nest classical beauty contests and network games within one model and unify their results. Two applications illustrate the techniques: The first characterizes when slight optimism about counterparties' average expectations leads to contagion of optimism and extreme asset prices. The second describes the tyranny of the least-informed: agents coordinating on the prior expectations of the one with the worst private information, despite all having nearly common certainty, based on precise private signals, of the ex post optimal action."
http://arxiv.org/abs/2007.07580v1,Prophylaxis of Epidemic Spreading with Transient Dynamics,2020-07-15 09:54:54+00:00,"['Geraldine Bouveret', 'Antoine Mandel']",econ.TH,"We investigate the containment of epidemic spreading in networks from a normative point of view. We consider a susceptible/infected model in which agents can invest in order to reduce the contagiousness of network links. In this setting, we study the relationships between social efficiency, individual behaviours and network structure. First, we exhibit an upper bound on the Price of Anarchy and prove that the level of inefficiency can scale up to linearly with the number of agents. Second, we prove that policies of uniform reduction of interactions satisfy some optimality conditions in a vast range of networks. In setting where no central authority can enforce such stringent policies, we consider as a type of second-best policy the shift from a local to a global game by allowing agents to subsidise investments in contagiousness reduction in the global rather than in the local network. We then characterise the scope for Pareto improvement opened by such policies through a notion of Price of Autarky, measuring the ratio between social welfare at a global and a local equilibrium. Overall, our results show that individual behaviours can be extremely inefficient in the face of epidemic propagation but that policy can take advantage of the network structure to design efficient containment policies."
http://arxiv.org/abs/2007.07352v1,"Degrees of individual and groupwise backward and forward responsibility in extensive-form games with ambiguity, and their application to social choice problems",2020-07-09 13:19:13+00:00,"['Jobst Heitzig', 'Sarah Hiller']",econ.TH,"Many real-world situations of ethical relevance, in particular those of large-scale social choice such as mitigating climate change, involve not only many agents whose decisions interact in complicated ways, but also various forms of uncertainty, including quantifiable risk and unquantifiable ambiguity. In such problems, an assessment of individual and groupwise moral responsibility for ethically undesired outcomes or their responsibility to avoid such is challenging and prone to the risk of under- or overdetermination of responsibility. In contrast to existing approaches based on strict causation or certain deontic logics that focus on a binary classification of `responsible' vs `not responsible', we here present several different quantitative responsibility metrics that assess responsibility degrees in units of probability. For this, we use a framework based on an adapted version of extensive-form game trees and an axiomatic approach that specifies a number of potentially desirable properties of such metrics, and then test the developed candidate metrics by their application to a number of paradigmatic social choice situations. We find that while most properties one might desire of such responsibility metrics can be fulfilled by some variant, an optimal metric that clearly outperforms others has yet to be found."
http://arxiv.org/abs/2007.06169v3,An Adversarial Approach to Structural Estimation,2020-07-13 03:31:02+00:00,"['Tetsuya Kaji', 'Elena Manresa', 'Guillaume Pouliot']",econ.EM,"We propose a new simulation-based estimation method, adversarial estimation, for structural models. The estimator is formulated as the solution to a minimax problem between a generator (which generates simulated observations using the structural model) and a discriminator (which classifies whether an observation is simulated). The discriminator maximizes the accuracy of its classification while the generator minimizes it. We show that, with a sufficiently rich discriminator, the adversarial estimator attains parametric efficiency under correct specification and the parametric rate under misspecification. We advocate the use of a neural network as a discriminator that can exploit adaptivity properties and attain fast rates of convergence. We apply our method to the elderly's saving decision model and show that our estimator uncovers the bequest motive as an important source of saving across the wealth distribution, not only for the rich."
http://arxiv.org/abs/2007.12877v1,Catastrophe by Design in Population Games: Destabilizing Wasteful Locked-in Technologies,2020-07-25 07:55:15+00:00,"['Stefanos Leonardos', 'Iosif Sakos', 'Costas Courcoubetis', 'Georgios Piliouras']",cs.GT,"In multi-agent environments in which coordination is desirable, the history of play often causes lock-in at sub-optimal outcomes. Notoriously, technologies with a significant environmental footprint or high social cost persist despite the successful development of more environmentally friendly and/or socially efficient alternatives. The displacement of the status quo is hindered by entrenched economic interests and network effects. To exacerbate matters, the standard mechanism design approaches based on centralized authorities with the capacity to use preferential subsidies to effectively dictate system outcomes are not always applicable to modern decentralized economies. What other types of mechanisms are feasible? In this paper, we develop and analyze a mechanism that induces transitions from inefficient lock-ins to superior alternatives. This mechanism does not exogenously favor one option over another -- instead, the phase transition emerges endogenously via a standard evolutionary learning model, Q-learning, where agents trade-off exploration and exploitation. Exerting the same transient influence to both the efficient and inefficient technologies encourages exploration and results in irreversible phase transitions and permanent stabilization of the efficient one. On a technical level, our work is based on bifurcation and catastrophe theory, a branch of mathematics that deals with changes in the number and stability properties of equilibria. Critically, our analysis is shown to be structurally robust to significant and even adversarially chosen perturbations to the parameters of both our game and our behavioral model."
http://arxiv.org/abs/2009.00553v6,A Vector Monotonicity Assumption for Multiple Instruments,2020-09-01 16:38:54+00:00,['Leonard Goff'],econ.EM,"When a researcher combines multiple instrumental variables for a single binary treatment, the monotonicity assumption of the local average treatment effects (LATE) framework can become restrictive: it requires that all units share a common direction of response even when separate instruments are shifted in opposing directions. What I call vector monotonicity, by contrast, simply assumes treatment uptake to be monotonic in all instruments. I characterize the class of causal parameters that are point identified under vector monotonicity, when the instruments are binary. This class includes, for example, the average treatment effect among units that are in any way responsive to the collection of instruments, or those that are responsive to a given subset of them. The identification results are constructive and yield a simple estimator for the identified treatment effect parameters. An empirical application revisits the labor market returns to college."
http://arxiv.org/abs/2009.04462v2,A Survey on Data Pricing: from Economics to Data Science,2020-09-09 19:31:38+00:00,['Jian Pei'],econ.TH,"Data are invaluable. How can we assess the value of data objectively, systematically and quantitatively? Pricing data, or information goods in general, has been studied and practiced in dispersed areas and principles, such as economics, marketing, electronic commerce, data management, data mining and machine learning. In this article, we present a unified, interdisciplinary and comprehensive overview of this important direction. We examine various motivations behind data pricing, understand the economics of data pricing and review the development and evolution of pricing models according to a series of fundamental principles. We discuss both digital products and data products. We also consider a series of challenges and directions for future work."
http://arxiv.org/abs/2009.12665v2,Nonclassical Measurement Error in the Outcome Variable,2020-09-26 18:44:18+00:00,"['Christoph Breunig', 'Stephan Martin']",econ.EM,"We study a semi-/nonparametric regression model with a general form of nonclassical measurement error in the outcome variable. We show equivalence of this model to a generalized regression model. Our main identifying assumptions are a special regressor type restriction and monotonicity in the nonlinear relationship between the observed and unobserved true outcome. Nonparametric identification is then obtained under a normalization of the unknown link function, which is a natural extension of the classical measurement error case. We propose a novel sieve rank estimator for the regression function and establish its rate of convergence.
  In Monte Carlo simulations, we find that our estimator corrects for biases induced by nonclassical measurement error and provides numerically stable results. We apply our method to analyze belief formation of stock market expectations with survey data from the German Socio-Economic Panel (SOEP) and find evidence for nonclassical measurement error in subjective belief data."
http://arxiv.org/abs/2007.11386v2,A Canon of Probabilistic Rationality,2020-07-17 21:43:23+00:00,"['Simone Cerreia-Vioglio', 'Per Olov Lindberg', 'Fabio Maccheroni', 'Massimo Marinacci', 'Aldo Rustichini']",econ.TH,"We prove that a random choice rule satisfies Luce's Choice Axiom if and only if its support is a choice correspondence that satisfies the Weak Axiom of Revealed Preference, thus it consists of alternatives that are optimal according to some preference, and random choice then occurs according to a tie breaking among such alternatives that satisfies Renyi's Conditioning Axiom. Our result shows that the Choice Axiom is, in a precise formal sense, a probabilistic version of the Weak Axiom. It thus supports Luce's view of his own axiom as a ""canon of probabilistic rationality."""
http://arxiv.org/abs/2007.09193v3,Tractable Profit Maximization over Multiple Attributes under Discrete Choice Models,2020-07-17 19:20:34+00:00,"['Hongzhang Shao', 'Anton J. Kleywegt']",math.OC,"A fundamental problem in revenue management is to optimally choose the attributes of products, such that the total profit or revenue or market share is maximized. Usually, these attributes can affect both a product's market share (probability to be chosen) and its profit margin. For example, if a smart phone has a better battery, then it is more costly to be produced, but is more likely to be purchased by a customer. The decision maker then needs to choose an optimal vector of attributes for each product that balances this trade-off. In spite of the importance of such problems, there is not yet a method to solve it efficiently in general. Past literature in revenue management and discrete choice models focus on pricing problems, where price is the only attribute to be chosen for each product. Existing approaches to solve pricing problems tractably cannot be generalized to the optimization problem with multiple product attributes as decision variables. On the other hand, papers studying product line design with multiple attributes all result in intractable optimization problems. Then we found a way to reformulate the static multi-attribute optimization problem, as well as the multi-stage fluid optimization problem with both resource constraints and upper and lower bounds of attributes, as a tractable convex conic optimization problem. Our result applies to optimization problems under the multinomial logit (MNL) model, the Markov chain (MC) choice model, and with certain conditions, the nested logit (NL) model."
http://arxiv.org/abs/2007.02411v3,Assessing External Validity Over Worst-case Subpopulations,2020-07-05 18:34:14+00:00,"['Sookyo Jeong', 'Hongseok Namkoong']",stat.ML,"Study populations are typically sampled from limited points in space and time, and marginalized groups are underrepresented. To assess the external validity of randomized and observational studies, we propose and evaluate the worst-case treatment effect (WTE) across all subpopulations of a given size, which guarantees positive findings remain valid over subpopulations. We develop a semiparametrically efficient estimator for the WTE that analyzes the external validity of the augmented inverse propensity weighted estimator for the average treatment effect. Our cross-fitting procedure leverages flexible nonparametric and machine learning-based estimates of nuisance parameters and is a regular root-$n$ estimator even when nuisance estimates converge more slowly. On real examples where external validity is of core concern, our proposed framework guards against brittle findings that are invalidated by unanticipated population shifts."
http://arxiv.org/abs/2008.13651v3,Causal Inference in Possibly Nonlinear Factor Models,2020-08-31 14:39:36+00:00,['Yingjie Feng'],econ.EM,"This paper develops a general causal inference method for treatment effects models with noisily measured confounders. The key feature is that a large set of noisy measurements are linked with the underlying latent confounders through an unknown, possibly nonlinear factor structure. The main building block is a local principal subspace approximation procedure that combines $K$-nearest neighbors matching and principal component analysis. Estimators of many causal parameters, including average treatment effects and counterfactual distributions, are constructed based on doubly-robust score functions. Large-sample properties of these estimators are established, which only require relatively mild conditions on the principal subspace approximation. The results are illustrated with an empirical application studying the effect of political connections on stock returns of financial firms, and a Monte Carlo experiment. The main technical and methodological results regarding the general local principal subspace approximation method may be of independent interest."
http://arxiv.org/abs/2007.05983v3,Contracting over persistent information,2020-07-12 13:19:27+00:00,"['Wei Zhao', 'Claudio Mezzetti', 'Ludovic Renou', 'Tristan Tomala']",econ.TH,"We consider a dynamic moral hazard problem between a principal and an agent, where the sole instrument the principal has to incentivize the agent is the disclosure of information. The principal aims at maximizing the (discounted) number of times the agent chooses a particular action, e.g., to work hard. We show that there exists an optimal contract, where the principal stops disclosing information as soon as its most preferred action is a static best reply for the agent or else continues disclosing information until the agent perfectly learns the principal's private information. If the agent perfectly learns the state, he learns it in finite time with probability one; the more patient the agent, the later he learns it."
http://arxiv.org/abs/2009.06558v2,Vector copulas,2020-09-14 16:40:17+00:00,"['Yanqin Fan', 'Marc Henry']",econ.EM,"This paper introduces vector copulas associated with multivariate distributions with given multivariate marginals, based on the theory of measure transportation, and establishes a vector version of Sklar's theorem. The latter provides a theoretical justification for the use of vector copulas to characterize nonlinear or rank dependence between a finite number of random vectors (robust to within vector dependence), and to construct multivariate distributions with any given non overlapping multivariate marginals. We construct Elliptical and Kendall families of vector copulas, derive their densities, and present algorithms to generate data from them. The use of vector copulas is illustrated with a stylized analysis of international financial contagion."
http://arxiv.org/abs/2008.10666v3,On the equivalence between the Kinetic Ising Model and discrete autoregressive processes,2020-08-24 19:20:14+00:00,"['Carlo Campajola', 'Fabrizio Lillo', 'Piero Mazzarisi', 'Daniele Tantari']",cond-mat.stat-mech,"Binary random variables are the building blocks used to describe a large variety of systems, from magnetic spins to financial time series and neuron activity. In Statistical Physics the Kinetic Ising Model has been introduced to describe the dynamics of the magnetic moments of a spin lattice, while in time series analysis discrete autoregressive processes have been designed to capture the multivariate dependence structure across binary time series. In this article we provide a rigorous proof of the equivalence between the two models in the range of a unique and invertible map unambiguously linking one model parameters set to the other. Our result finds further justification acknowledging that both models provide maximum entropy distributions of binary time series with given means, auto-correlations, and lagged cross-correlations of order one. We further show that the equivalence between the two models permits to exploit the inference methods originally developed for one model in the inference of the other."
http://arxiv.org/abs/2009.05311v2,Strategy-proof allocation with outside option,2020-09-11 09:51:25+00:00,['Jun Zhang'],econ.TH,"Strategy-proof mechanisms are widely used in market design. In an abstract allocation framework where outside options are available to agents, we obtain two results for strategy-proof mechanisms. They provide a unified foundation for several existing results in distinct models and imply new results in some models. The first result proves that, for individually rational and strategy-proof mechanisms, pinning down every agent's probability of choosing his outside option is equivalent to pinning down a mechanism. The second result provides a sufficient condition for two strategy-proof mechanisms to be equivalent when the number of possible allocations is finite."
http://arxiv.org/abs/2007.07061v5,Polarization in Networks: Identification-alienation Framework,2020-07-14 14:39:48+00:00,"['Kenan Huremovic', 'Ali Ozkes']",econ.TH,"We introduce a model of polarization in networks as a unifying framework for the measurement of polarization that covers a wide range of applications. We consider a sufficiently general setup for this purpose: node- and edge-weighted, undirected, and connected networks. We generalize the axiomatic characterization of Esteban and Ray (1994) and show that only a particular instance within this class can be used justifiably to measure polarization in networks."
http://arxiv.org/abs/2007.02934v1,The Effects of Taxes on Wealth Inequality in Artificial Chemistry Models of Economic Activity,2020-07-03 18:00:18+00:00,['Wolfgang Banzhaf'],q-fin.GN,"We consider a number of Artificial Chemistry models for economic activity and what consequences they have for the formation of economic inequality. We are particularly interested in what tax measures are effective in dampening economic inequality. By starting from well-known kinetic exchange models, we examine different scenarios for reducing the tendency of economic activity models to form unequal wealth distribution in equilibrium."
http://arxiv.org/abs/2008.04131v2,Aggression in the workplace makes social distance difficult,2020-08-10 13:48:42+00:00,['Keisuke Kokubun'],econ.GN,"The spread of new coronavirus (COVID-19) infections continues to increase. The practice of social distance attracts attention as a measure to prevent the spread of infection, but it is difficult for some occupations. Therefore, in previous studies, the scale of factors that determine social distance has been developed. However, it was not clear how to select the items among them, and it seemed to be somewhat arbitrary. In response to this trend, this paper extracted eight scales by performing exploratory factor analysis based on certain rules while eliminating arbitrariness as much as possible. They were Adverse Conditions, Leadership, Information Processing, Response to Aggression, Mechanical Movement, Autonomy, Communication with the Outside, and Horizontal Teamwork. Of these, Adverse Conditions, Response to Aggression, and Horizontal Teamwork had a positive correlation with Physical Proximity, and Information Processing, Mechanical Movement, Autonomy, and Communication with the Outside had a negative correlation with Physical Proximity. Furthermore, as a result of multiple regression analysis, it was shown that Response to Aggression, not the mere teamwork assumed in previous studies, had the greatest influence on Physical Proximity."
http://arxiv.org/abs/2008.13561v3,Sustainable Border Control Policy in the COVID-19 Pandemic: A Math Modeling Study,2020-08-28 17:07:53+00:00,"['Zhen Zhu', 'Enzo Weber', 'Till Strohsal', 'Duaa Serhan']",q-bio.PE,"Imported COVID-19 cases, if unchecked, can jeopardize the effort of domestic containment. We aim to find out what sustainable border control options for different entities (e.g., countries, states) exist during the reopening phases, given their own choice of domestic control measures and new technologies such as contact tracing. We propose a SUIHR model, which represents an extension to the discrete time SIR models. The model focuses on studying the spreading of virus predominantly by asymptomatic and pre-symptomatic patients. Imported risk and (1-tier) contact tracing are both built into the model. Under plausible parameter assumptions, we seek sustainable border control policies, in combination with sufficient internal measures, which allow entities to confine the virus without the need to revert back to more restrictive life styles or to rely on herd immunity. When the base reproduction number of COVID-19 exceeds 2.5, even 100% effective contact tracing alone is not enough to contain the spreading. For an entity that has completely eliminated the virus domestically, and resumes ""normal"", very strict pre-departure screening and test and isolation upon arrival combined with effective contact tracing can only delay another outbreak by 6 months. However, if the total net imported cases are non-increasing, and the entity employs a confining domestic control policy, then the total new cases can be contained even without border control."
http://arxiv.org/abs/2008.02318v2,On the Size Control of the Hybrid Test for Predictive Ability,2020-08-05 18:52:44+00:00,['Deborah Kim'],econ.EM,"We analyze theoretical properties of the hybrid test for superior predictability. We demonstrate with a simple example that the test may not be pointwise asymptotically of level $α$ at commonly used significance levels and may lead to rejection rates over $11\%$ when the significance level $α$ is $5\%$. Generalizing this observation, we provide a formal result that pointwise asymptotic invalidity of the hybrid test persists in a setting under reasonable conditions. As an easy alternative, we propose a modified hybrid test based on the generalized moment selection method and show that the modified test enjoys pointwise asymptotic validity. Monte Carlo simulations support the theoretical findings."
http://arxiv.org/abs/2008.13042v2,Efficiency Loss of Asymptotically Efficient Tests in an Instrumental Variables Regression,2020-08-29 19:37:11+00:00,"['Marcelo J. Moreira', 'Geert Ridder']",math.ST,"In an instrumental variable model, the score statistic can be bounded for any alternative in parts of the parameter space. These regions involve a constraint on the first-stage regression coefficients and the reduced-form covariance matrix. Consequently, the Lagrange Multiplier test can have power close to size, despite being efficient under standard asymptotics. This information loss limits the power of conditional tests which use only the Anderson-Rubin and the score statistic. The conditional quasi-likelihood ratio test also suffers severe losses because it can be bounded for any alternative.
  A necessary condition for drastic power loss to occur is that the Hermitian of the reduced-form covariance matrix has eigenvalues of opposite signs. These cases are denoted impossibility designs (ID). We show this happens in practice, by applying our theory to the problem of inference on the intertemporal elasticity of substitution (IES). Of eleven countries studied by Yogo (2004} and Andrews (2016), nine are consistent with ID at the 95\% level."
http://arxiv.org/abs/2009.10320v3,One-Sided Matching Markets with Endowments: Equilibria and Algorithms,2020-09-22 05:06:43+00:00,"['Jugal Garg', 'Thorben Tröbst', 'Vijay V. Vazirani']",cs.GT,"The Arrow-Debreu extension of the classic Hylland-Zeckhauser scheme for a one-sided matching market -- called ADHZ in this paper -- has natural applications but has instances which do not admit equilibria. By introducing approximation, we define the $ε$-approximate ADHZ model, and we give the following results.
  * Existence of equilibrium under linear utility functions. We prove that the equilibrium satisfies Pareto optimality, approximate envy-freeness, and approximate weak core stability.
  * A combinatorial polynomial-time algorithm for an $ε$-approximate ADHZ equilibrium for the case of dichotomous, and more generally bi-valued, utilities.
  * An instance of ADHZ, with dichotomous utilities and a strongly connected demand graph, which does not admit an equilibrium.
  Since computing an equilibrium for HZ is likely to be highly intractable and because of the difficulty of extending HZ to more general utility functions, Hosseini and Vazirani proposed (a rich collection of) Nash-bargaining-based matching market models. For the dichotomous-utilities case of their model linear Arrow-Debreu Nash bargaining one-sided matching market (1LAD), we give a combinatorial, strongly polynomial-time algorithm and show that it admits a rational convex program."
http://arxiv.org/abs/2009.09198v3,On the implementation of the Universal Basic Income as a response to technological unemployment,2020-09-19 09:37:27+00:00,['Le Dong Hai Nguyen'],econ.GN,"The effects of automation on our economy and society are more palpable than ever, with nearly half of jobs at risk of being fully executed by machines over the next decade or two. Policymakers and scholars alike have championed the Universal Basic Income (UBI) as a catch-all solution to this problem. This paper examines the shortcomings of UBI in addressing the automation-led large-scale displacement of labor by analyzing empirical data from previous UBI-comparable experiments and presenting theoretical projections that highlight disappointing impacts of UBI in the improvement of relevant living standards and employability metrics among pensioners. Finally, a recommendation shall be made for the retainment of existing means-tested welfare programs while bolstering funding and R&D for more up-to-date worker training schemes as a more effective solution to technological unemployment."
http://arxiv.org/abs/2008.00963v3,Existence and uniqueness of recursive utilities without boundedness,2020-07-30 20:24:56+00:00,['Timothy M. Christensen'],econ.TH,"This paper derives primitive, easily verifiable sufficient conditions for existence and uniqueness of (stochastic) recursive utilities for several important classes of preferences. In order to accommodate models commonly used in practice, we allow both the state-space and per-period utilities to be unbounded. For many of the models we study, existence and uniqueness is established under a single, primitive ""thin tail"" condition on the distribution of growth in per-period utilities. We present several applications to robust preferences, models of ambiguity aversion and learning about hidden states, and Epstein-Zin preferences."
http://arxiv.org/abs/2008.13276v2,Proportional Participatory Budgeting with Additive Utilities,2020-08-30 21:04:11+00:00,"['Dominik Peters', 'Grzegorz Pierczyński', 'Piotr Skowron']",cs.GT,"We study voting rules for participatory budgeting, where a group of voters collectively decides which projects should be funded using a common budget. We allow the projects to have arbitrary costs, and the voters to have arbitrary additive valuations over the projects. We formulate an axiom (Extended Justified Representation, EJR) that guarantees proportional representation to groups of voters with common interests. We propose a simple and attractive voting rule called the Method of Equal Shares that satisfies this axiom for arbitrary costs and approval utilities, and that satisfies the axiom up to one project for arbitrary additive valuations. This method can be computed in polynomial time. In contrast, we show that the standard method for achieving proportionality in committee elections, Proportional Approval Voting (PAV), cannot be extended to work with arbitrary costs. Finally, we introduce a strengthened axiom (Full Justified Representation, FJR) and show that it is also satisfiable, though by a computationally more expensive and less natural voting rule."
http://arxiv.org/abs/2007.05529v2,Is there a Golden Parachute in Sannikov's principal-agent problem?,2020-07-10 12:50:49+00:00,"['Dylan Possamaï', 'Nizar Touzi']",econ.TH,"This paper provides a complete review of the continuous-time optimal contracting problem introduced by Sannikov, in the extended context allowing for possibly different discount rates for both parties. The agent's problem is to seek for optimal effort, given the compensation scheme proposed by the principal over a random horizon. Then, given the optimal agent's response, the principal determines the best compensation scheme in terms of running payment, retirement, and lump-sum payment at retirement. A Golden Parachute is a situation where the agent ceases any effort at some positive stopping time, and receives a payment afterwards, possibly under the form of a lump sum payment, or of a continuous stream of payments. We show that a Golden Parachute only exists in certain specific circumstances. This is in contrast with the results claimed by Sannikov, where the only requirement is a positive agent's marginal cost of effort at zero. Namely, we show that there is no Golden Parachute if this parameter is too large. Similarly, in the context of a concave marginal utility, there is no Golden Parachute if the agent's utility function has a too negative curvature at zero. In the general case, we prove that an agent with positive reservation utility is either never retired by the principal, or retired above some given threshold (as in Sannikov's solution). We show that different discount factors induce a face-lifted utility function, which allows to reduce the analysis to a setting similar to the equal discount rates one. Finally, we also confirm that an agent with small reservation utility may have an informational rent, meaning that the principal optimally offers him a contract with strictly higher utility than his participation value."
http://arxiv.org/abs/2009.03844v2,Exact Computation of Maximum Rank Correlation Estimator,2020-09-08 16:21:58+00:00,"['Youngki Shin', 'Zvezdomir Todorov']",econ.EM,In this paper we provide a computation algorithm to get a global solution for the maximum rank correlation estimator using the mixed integer programming (MIP) approach. We construct a new constrained optimization problem by transforming all indicator functions into binary parameters to be estimated and show that it is equivalent to the original problem. We also consider an application of the best subset rank prediction and show that the original optimization problem can be reformulated as MIP. We derive the non-asymptotic bound for the tail probability of the predictive performance measure. We investigate the performance of the MIP algorithm by an empirical example and Monte Carlo simulations.
http://arxiv.org/abs/2008.01071v4,Making Decisions under Model Misspecification,2020-08-01 10:09:07+00:00,"['Simone Cerreia-Vioglio', 'Lars Peter Hansen', 'Fabio Maccheroni', 'Massimo Marinacci']",econ.TH,"We use decision theory to confront uncertainty that is sufficiently broad to incorporate ""models as approximations."" We presume the existence of a featured collection of what we call ""structured models"" that have explicit substantive motivations. The decision maker confronts uncertainty through the lens of these models, but also views these models as simplifications, and hence, as misspecified. We extend the max-min analysis under model ambiguity to incorporate the uncertainty induced by acknowledging that the models used in decision-making are simplified approximations. Formally, we provide an axiomatic rationale for a decision criterion that incorporates model misspecification concerns."
http://arxiv.org/abs/2009.01575v2,Deep Learning in Science,2020-09-03 10:41:29+00:00,"['Stefano Bianchini', 'Moritz Müller', 'Pierre Pelletier']",cs.CY,"Much of the recent success of Artificial Intelligence (AI) has been spurred on by impressive achievements within a broader family of machine learning methods, commonly referred to as Deep Learning (DL). This paper provides insights on the diffusion and impact of DL in science. Through a Natural Language Processing (NLP) approach on the arXiv.org publication corpus, we delineate the emerging DL technology and identify a list of relevant search terms. These search terms allow us to retrieve DL-related publications from Web of Science across all sciences. Based on that sample, we document the DL diffusion process in the scientific system. We find i) an exponential growth in the adoption of DL as a research tool across all sciences and all over the world, ii) regional differentiation in DL application domains, and iii) a transition from interdisciplinary DL applications to disciplinary research within application domains. In a second step, we investigate how the adoption of DL methods affects scientific development. Therefore, we empirically assess how DL adoption relates to re-combinatorial novelty and scientific impact in the health sciences. We find that DL adoption is negatively correlated with re-combinatorial novelty, but positively correlated with expectation as well as variance of citation performance. Our findings suggest that DL does not (yet?) work as an autopilot to navigate complex knowledge landscapes and overthrow their structure. However, the 'DL principle' qualifies for its versatility as the nucleus of a general scientific method that advances science in a measurable way."
http://arxiv.org/abs/2009.11075v2,A Deep Learning Approach for Dynamic Balance Sheet Stress Testing,2020-09-23 11:47:34+00:00,"['Anastasios Petropoulos', 'Vassilis Siakoulis', 'Konstantinos P. Panousis', 'Loukas Papadoulas', 'Sotirios Chatzis']",q-fin.CP,"In the aftermath of the financial crisis, supervisory authorities have considerably altered the mode of operation of financial stress testing. Despite these efforts, significant concerns and extensive criticism have been raised by market participants regarding the considered unrealistic methodological assumptions and simplifications. Current stress testing methodologies attempt to simulate the risks underlying a financial institution's balance sheet by using several satellite models. This renders their integration a really challenging task, leading to significant estimation errors. Moreover, advanced statistical techniques that could potentially capture the non-linear nature of adverse shocks are still ignored. This work aims to address these criticisms and shortcomings by proposing a novel approach based on recent advances in Deep Learning towards a principled method for Dynamic Balance Sheet Stress Testing. Experimental results on a newly collected financial/supervisory dataset, provide strong empirical evidence that our paradigm significantly outperforms traditional approaches; thus, it is capable of more accurately and efficiently simulating real world scenarios."
http://arxiv.org/abs/2009.08010v1,Tail behavior of stopped Lévy processes with Markov modulation,2020-09-17 01:50:28+00:00,"['Brendan K. Beare', 'Won-Ki Seo', 'Alexis Akira Toda']",math.PR,This article concerns the tail probabilities of a light-tailed Markov-modulated Lévy process stopped at a state-dependent Poisson rate. The tails are shown to decay exponentially at rates given by the unique positive and negative roots of the spectral abscissa of a certain matrix-valued function. We illustrate the use of our results with an application to the stationary distribution of wealth in a simple economic model in which agents with constant absolute risk aversion are subject to random mortality and income fluctuation.
http://arxiv.org/abs/2009.07144v1,What factors have caused Japanese prefectures to attract a larger population influx?,2020-09-15 14:54:18+00:00,['Keisuke Kokubun'],econ.GN,"Regional promotion and centralized correction in Tokyo have long been the goals of the Government of Japan. Furthermore, in the wake of the recent new coronavirus (COVID-19) epidemic, the momentum for rural migration is increasing, to prevent the risk of infection with the help of penetration of remote work. However, there is not enough debate about what kind of land will attract the population. Therefore, in this paper, we will consider this problem by performing correlation analysis and multiple regression analysis with the inflow rate and the excess inflow rate of the population as the dependent variables, using recent government statistics for each prefecture. As a result of the analysis, in addition to economic factor variables, variables of climatic, amenity, and human factors correlated with the inflow rate, and it was shown that the model has the greatest explanatory power when multiple factors were used in addition to specific factors. Therefore, local prefectures are required to take regional promotion measures focusing on not only economic factors but also multifaceted factors to attract the outside population."
http://arxiv.org/abs/2007.14620v2,Epidemic response to physical distancing policies and their impact on the outbreak risk,2020-07-29 06:12:25+00:00,"['Fabio Vanni', 'David Lambert', 'Luigi Palatella']",physics.soc-ph,"We introduce a theoretical framework that highlights the impact of physical distancing variables such as human mobility and physical proximity on the evolution of epidemics and, crucially, on the reproduction number. In particular, in response to the coronavirus disease (CoViD-19) pandemic, countries have introduced various levels of 'lockdown' to reduce the number of new infections. Specifically we use a collisional approach to an infection-age structured model described by a renewal equation for the time homogeneous evolution of epidemics. As a result, we show how various contributions of the lockdown policies, namely physical proximity and human mobility, reduce the impact of SARS-CoV-2 and mitigate the risk of disease resurgence. We check our theoretical framework using real-world data on physical distancing with two different data repositories, obtaining consistent results. Finally, we propose an equation for the effective reproduction number which takes into account types of interactions among people, which may help policy makers to improve remote-working organizational structure."
http://arxiv.org/abs/2009.14367v2,Local Regression Distribution Estimators,2020-09-30 01:10:44+00:00,"['Matias D. Cattaneo', 'Michael Jansson', 'Xinwei Ma']",econ.EM,"This paper investigates the large sample properties of local regression distribution estimators, which include a class of boundary adaptive density estimators as a prime example. First, we establish a pointwise Gaussian large sample distributional approximation in a unified way, allowing for both boundary and interior evaluation points simultaneously. Using this result, we study the asymptotic efficiency of the estimators, and show that a carefully crafted minimum distance implementation based on ""redundant"" regressors can lead to efficiency gains. Second, we establish uniform linearizations and strong approximations for the estimators, and employ these results to construct valid confidence bands. Third, we develop extensions to weighted distributions with estimated weights and to local $L^{2}$ least squares estimation. Finally, we illustrate our methods with two applications in program evaluation: counterfactual density testing, and IV specification and heterogeneity density analysis. Companion software packages in Stata and R are available."
http://arxiv.org/abs/2007.09568v3,Only Time Will Tell: Credible Dynamic Signaling,2020-07-19 03:02:25+00:00,['Egor Starkov'],econ.TH,"This paper characterizes informational outcomes in a model of dynamic signaling with vanishing commitment power. It shows that contrary to popular belief, informative equilibria with payoff-relevant signaling can exist without requiring unreasonable off-path beliefs. The paper provides a sharp characterization of possible separating equilibria: all signaling must take place through attrition, when the weakest type mixes between revealing own type and pooling with the stronger types. The framework explored in the paper is general, imposing only minimal assumptions on payoff monotonicity and single-crossing. Applications to bargaining, monopoly price signaling, and labor market signaling are developed to demonstrate the results in specific contexts."
http://arxiv.org/abs/2009.11689v3,A characterization of absorbing sets in coalition formation games,2020-09-24 13:48:18+00:00,"['Agustin G. Bonifacio', 'Elena Inarra', 'Pablo Neme']",econ.TH,"Given a standard myopic dynamic process among coalition structures, an absorbing set is a minimal collection of such structures that is never left once entered through that process. Absorbing sets are an important solution concept in coalition formation games, but they have drawbacks: they can be large and hard to obtain. In this paper, we characterize an absorbing set in terms of a collection consisting of a small number of sets of coalitions that we refer to as a ""reduced form"" of a game. We apply our characterization to study convergence to stability in several economic environments."
http://arxiv.org/abs/2009.03436v5,Globalization? Trade War? A Counterbalance Perspective,2020-09-04 12:38:51+00:00,['Xingwei Hu'],econ.GN,"This paper explores the exciting dynamics of international trade, focusing on the strategic balance between competition and cooperation in the global trade network. It highlights how competitive advantages, rather than comparative advantages, drive trade conflicts and deglobalization. By using the balance of power, the paper introduces a quantitative measure of competitiveness that serves as a common goal for all countries, alongside trade balance. It then examines how nations can boost their competitive strength and trade balance through globalization, protectionism, trade collaboration, or conflict. The study provides practical insights for policymakers on managing trade relationships, resolving conflicts, and determining the right level of globalization by analyzing trade data. The analysis is supported by a comparison between theory-driven quantitative evidence and historical events, using real-world trade data from 2000 to 2019."
http://arxiv.org/abs/2009.12217v2,Latent Causal Socioeconomic Health Index,2020-09-24 07:00:46+00:00,"['Swen Kuh', 'Grace S. Chiu', 'Anton H. Westveld']",stat.ME,"This research develops a model-based LAtent Causal Socioeconomic Health (LACSH) index at the national level. Motivated by the need for a holistic national well-being index, we build upon the latent health factor index (LHFI) approach that has been used to assess the unobservable ecological/ecosystem health. LHFI integratively models the relationship between metrics, latent health, and covariates that drive the notion of health. In this paper, the LHFI structure is integrated with spatial modeling and statistical causal modeling. Our efforts are focused on developing the integrated framework to facilitate the understanding of how an observational continuous variable might have causally affected a latent trait that exhibits spatial correlation. A novel visualization technique to evaluate covariate balance is also introduced for the case of a continuous policy (treatment) variable. Our resulting LACSH framework and visualization tool are illustrated through two global case studies on national socioeconomic health (latent trait), each with various metrics and covariates pertaining to different aspects of societal health, and the treatment variable being mandatory maternity leave days and government expenditure on healthcare, respectively. We validate our model by two simulation studies. All approaches are structured in a Bayesian hierarchical framework and results are obtained by Markov chain Monte Carlo techniques."
http://arxiv.org/abs/2009.06350v4,Upstreamness and downstreamness in input-output analysis from local and aggregate information,2020-09-14 12:17:46+00:00,"['Silvia Bartolucci', 'Fabio Caccioli', 'Francesco Caravelli', 'Pierpaolo Vivo']",physics.soc-ph,"Ranking sectors and countries within global value chains is of paramount importance to estimate risks and forecast growth in large economies. However, this task is often non-trivial due to the lack of complete and accurate information on the flows of money and goods between sectors and countries, which are encoded in Input-Output (I-O) tables. In this work, we show that an accurate estimation of the role played by sectors and countries in supply chain networks can be achieved without full knowledge of the I-O tables, but only relying on local and aggregate information, e.g., the total intermediate demand per sector. Our method, based on a rank-$1$ approximation to the I-O table, shows consistently good performance in reconstructing rankings (i.e., upstreamness and downstreamness measures for countries and sectors) when tested on empirical data from the World Input-Output Database. Moreover, we connect the accuracy of our approximate framework with the spectral properties of the I-O tables, which ordinarily exhibit relatively large spectral gaps. Our approach provides a fast and analytical tractable framework to rank constituents of a complex economy without the need of matrix inversions and the knowledge of finer intersectorial details."
http://arxiv.org/abs/2008.04401v2,Connected Incomplete Preferences,2020-08-10 20:16:51+00:00,"['Leandro Gorno', 'Alessandro Rivello']",econ.TH,"This paper explores a new class of incomplete preferences, termed ""connected preferences"", in which maximal domains of comparability are topologically connected. We provide necessary and sufficient conditions for continuous preferences to be connected. We also characterize their maximal domains of comparability. Our results extend classical findings in decision theory by linking topological properties of the choice space with the structure of preferences, offering a novel perspective on incompleteness in economic models."
http://arxiv.org/abs/2008.03283v1,COVID-19: What If Immunity Wanes?,2020-08-07 17:32:21+00:00,"['M. Alper Çenesiz', 'Luís Guimarães']",econ.GN,"Using a simple economic model in which social-distancing reduces contagion, we study the implications of waning immunity for the epidemiological dynamics and social activity. If immunity wanes, we find that COVID-19 likely becomes endemic and that social-distancing is here to stay until the discovery of a vaccine or cure. But waning immunity does not necessarily change optimal actions on the onset of the pandemic. Decentralized equilibria are virtually independent of waning immunity until close to peak infections. For centralized equilibria, the relevance of waning immunity decreases in the probability of finding a vaccine or cure, the costs of infection (e.g., infection-fatality rate), and the presence of other NPIs that lower contagion (e.g., quarantining and mask use). In simulations calibrated to July 2020, our model suggests that waning immunity is virtually unimportant for centralized equilibria until at least 2021. This provides vital time for individuals and policymakers to learn about immunity against SARS-CoV-2 before it becomes critical."
http://arxiv.org/abs/2008.10926v1,The Impact of Sodomy Law Repeals on Crime,2020-08-25 10:15:38+00:00,"['Riccardo Ciacci', 'Dario Sansone']",econ.GN,"We exploit variation in the timing of decriminalization of same-sex sexual intercourse across U.S. states to estimate the impact of these law changes on crime through difference-in-difference and event-study models. We provide the first evidence that sodomy law repeals led to a decline in the number of arrests for disorderly conduct, prostitution, and other sex offenses. Furthermore, we show that these repeals led to a reduction in arrests for drug and alcohol consumption."
http://arxiv.org/abs/2007.10270v1,The impacts of alcohol taxes: A replication review,2020-07-16 00:40:48+00:00,['David Roodman'],econ.GN,"This paper reviews the research on the impacts of alcohol taxation outcomes such as heavy drinking and mortality. Where data availability permits, reviewed studies are replicated and reanalyzed. Despite weaknesses in the majority of studies, and despite seeming disagreements among the more credible one--ones based on natural experiments--we can be reasonably confident that taxing alcohol reduces drinking in general and problem drinking in particular. The larger and cleaner the underlying natural experiment, the more apt a study is to detect impacts on drinking. Estimates from the highest-powered study settings, such as in Alaska in 2002 and Finland in 2004, suggest an elasticity of mortality with respect to price of -1 to -3. A 10% price increase in the US would, according to this estimate, save 2,000-6,000 lives and 48,000-130,000 years of life each year."
http://arxiv.org/abs/2007.09566v1,Authoritarian Governments Appear to Manipulate COVID Data,2020-07-19 02:54:01+00:00,"['Mudit Kapoor', 'Anup Malani', 'Shamika Ravi', 'Arnav Agrawal']",econ.GN,"Because SARS-Cov-2 (COVID-19) statistics affect economic policies and political outcomes, governments have an incentive to control them. Manipulation may be less likely in democracies, which have checks to ensure transparency. We show that data on disease burden bear indicia of data modification by authoritarian governments relative to democratic governments. First, data on COVID-19 cases and deaths from authoritarian governments show significantly less variation from a 7 day moving average. Because governments have no reason to add noise to data, lower deviation is evidence that data may be massaged. Second, data on COVID-19 deaths from authoritarian governments do not follow Benford's law, which describes the distribution of leading digits of numbers. Deviations from this law are used to test for accounting fraud. Smoothing and adjustments to COVID-19 data may indicate other alterations to these data and a need to account for such alterations when tracking the disease."
http://arxiv.org/abs/2007.10160v1,Variable Selection in Macroeconomic Forecasting with Many Predictors,2020-07-20 14:33:57+00:00,"['Zhenzhong Wang', 'Zhengyuan Zhu', 'Cindy Yu']",econ.EM,"In the data-rich environment, using many economic predictors to forecast a few key variables has become a new trend in econometrics. The commonly used approach is factor augment (FA) approach. In this paper, we pursue another direction, variable selection (VS) approach, to handle high-dimensional predictors. VS is an active topic in statistics and computer science. However, it does not receive as much attention as FA in economics. This paper introduces several cutting-edge VS methods to economic forecasting, which includes: (1) classical greedy procedures; (2) l1 regularization; (3) gradient descent with sparsification and (4) meta-heuristic algorithms. Comprehensive simulation studies are conducted to compare their variable selection accuracy and prediction performance under different scenarios. Among the reviewed methods, a meta-heuristic algorithm called sequential Monte Carlo algorithm performs the best. Surprisingly the classical forward selection is comparable to it and better than other more sophisticated algorithms. In addition, we apply these VS methods on economic forecasting and compare with the popular FA approach. It turns out for employment rate and CPI inflation, some VS methods can achieve considerable improvement over FA, and the selected predictors can be well explained by economic theories."
http://arxiv.org/abs/2007.09230v1,Absentee and Economic Impact of Low-Level Fine Particulate Matter and Ozone Exposure in K-12 Students,2020-07-16 01:27:27+00:00,"['Daniel L. Mendoza', 'Cheryl S. Pirozzi', 'Erik T. Crosman', 'Theodore G. Liou', 'Yue Zhang', 'Jessica J. Cleeves', 'Stephen C. Bannister', 'William R. L. Anderegg', 'Robert Paine']",econ.GN,"High air pollution levels are associated with school absences. However, low level pollution impact on individual school absences are under-studied. We modelled PM2.5 and ozone concentrations at 36 schools from July 2015 to June 2018 using data from a dense, research grade regulatory sensor network. We determined exposures and daily absences at each school. We used generalized estimating equations model to retrospectively estimate rate ratios for association between outdoor pollutant concentrations and school absences. We estimated lost school revenue, productivity, and family economic burden. PM2.5 and ozone concentrations and absence rates vary across the School District. Pollution exposure were associated with as high a rate ratio of 1.02 absences per ug/m$^3$ and 1.01 per ppb increase for PM2.5 and ozone, respectively. Significantly, even PM2.5 and ozone exposure below regulatory standards (<12.1 ug/m$^3$ and <55 ppb) was associated with positive rate ratios of absences: 1.04 per ug/m$^3$ and 1.01 per ppb increase, respectively. Granular local measurements enabled demonstration of air pollution impacts that varied between schools undetectable with averaged pollution levels. Reducing pollution by 50% would save $452,000 per year districtwide. Pollution reduction benefits would be greatest in schools located in socioeconomically disadvantaged areas. Exposures to air pollution, even at low levels, are associated with increased school absences. Heterogeneity in exposure, disproportionately affecting socioeconomically disadvantaged schools, points to the need for fine resolution exposure estimation. The economic cost of absences associated with air pollution is substantial even excluding indirect costs such as hospital visits and medication. These findings may help inform decisions about recess during severe pollution events and regulatory considerations for localized pollution sources."
http://arxiv.org/abs/2007.11580v1,How happy are my neighbours? Modelling spatial spillover effects of well-being,2020-07-22 11:24:09+00:00,"['Thanasis Ziogas', 'Dimitris Ballas', 'Sierdjan Koster', 'Arjen Edzes']",econ.GN,"This article uses data of subjective Life Satisfaction aggregated to the community level in Canada and examines the spatial interdependencies and spatial spillovers of community happiness. A theoretical model of utility is presented. Using spatial econometric techniques, we find that the utility of community, proxied by subjective measures of life satisfaction, is affected both by the utility of neighbouring communities as well as by the latter's average household income and unemployment rate. Shared cultural traits and institutions may justify such spillovers. The results are robust to the different binary contiguity spatial weights matrices used and to the various econometric models. Clusters of both high-high and low-low in Life Satisfaction communities are also found based on the Moran's I test"
http://arxiv.org/abs/2007.11439v1,"A comprehensive view of the manifestations of aggregate demand and aggregate supply shocks in Greece, Ireland, Italy and Portugal",2020-07-20 14:42:51+00:00,['Ionut Jianu'],econ.GN,"The main goal of the paper is to extract the aggregate demand and aggregate supply shocks in Greece, Ireland, Italy and Portugal, as well as to examine the correlation among the two types of shocks. The decomposition of the shocks was achieved by using a structural vector autoregression that analyses the relationship between the evolution of the gross domestic product and inflation in the period 1997-2015. The goal of the paper is to confirm the aggregate demand - aggregate supply model in the above-mentioned economies."
http://arxiv.org/abs/2007.06994v1,Do Online Courses Provide an Equal Educational Value Compared to In-Person Classroom Teaching? Evidence from US Survey Data using Quantile Regression,2020-07-14 12:23:18+00:00,"['Manini Ojha', 'Mohammad Arshad Rahman']",econ.GN,"Education has traditionally been classroom-oriented with a gradual growth of online courses in recent times. However, the outbreak of the COVID-19 pandemic has dramatically accelerated the shift to online classes. Associated with this learning format is the question: what do people think about the educational value of an online course compared to a course taken in-person in a classroom? This paper addresses the question and presents a Bayesian quantile analysis of public opinion using a nationally representative survey data from the United States. Our findings show that previous participation in online courses and full-time employment status favor the educational value of online courses. We also find that the older demographic and females have a greater propensity for online education. In contrast, highly educated individuals have a lower willingness towards online education vis-à-vis traditional classes. Besides, covariate effects show heterogeneity across quantiles which cannot be captured using probit or logit models."
http://arxiv.org/abs/2007.15980v1,The Hansen ratio in mean--variance portfolio theory,2020-07-31 11:38:07+00:00,['Aleš Černý'],q-fin.PM,"It is shown that the ratio between the mean and the $L^2$-norm leads to a particularly parsimonious description of the mean-variance efficient frontier and the dual pricing kernel restrictions known as the Hansen-Jagannathan (HJ) bounds. Because this ratio has not appeared in economic theory previously, it seems appropriate to name it the Hansen ratio. The initial treatment of the mean-variance theory via the Hansen ratio is extended in two directions, to monotone mean-variance preferences and to arbitrary Hilbert space setting. A multiperiod example with IID returns is also discussed."
http://arxiv.org/abs/2008.07650v1,Mobility and Social Efficiency,2020-08-17 22:23:32+00:00,['Ryan Steven Kostiuk'],econ.TH,"This is a general competitive analysis paper. A model is presented that describes how an individual with a physical disability, or mobility impairment, would go about utility maximization. These results are then generalized. Subsequently, a selection of disability policies from Canada and the United States are compared to the insights of the model, and it is shown that there are sources of inefficiency in many North American disability support systems."
http://arxiv.org/abs/2007.03682v1,A Dynamic Choice Model with Heterogeneous Decision Rules: Application in Estimating the User Cost of Rail Crowding,2020-07-07 10:26:56+00:00,"['Prateek Bansal', 'Daniel Hörcher', 'Daniel J. Graham']",stat.AP,"Crowding valuation of subway riders is an important input to various supply-side decisions of transit operators. The crowding cost perceived by a transit rider is generally estimated by capturing the trade-off that the rider makes between crowding and travel time while choosing a route. However, existing studies rely on static compensatory choice models and fail to account for inertia and the learning behaviour of riders. To address these challenges, we propose a new dynamic latent class model (DLCM) which (i) assigns riders to latent compensatory and inertia/habit classes based on different decision rules, (ii) enables transitions between these classes over time, and (iii) adopts instance-based learning theory to account for the learning behaviour of riders. We use the expectation-maximisation algorithm to estimate DLCM, and the most probable sequence of latent classes for each rider is retrieved using the Viterbi algorithm. The proposed DLCM can be applied in any choice context to capture the dynamics of decision rules used by a decision-maker. We demonstrate its practical advantages in estimating the crowding valuation of an Asian metro's riders. To calibrate the model, we recover the daily route preferences and in-vehicle crowding experiences of regular metro riders using a two-month-long smart card and vehicle location data. The results indicate that the average rider follows the compensatory rule on only 25.5% of route choice occasions. DLCM estimates also show an increase of 47% in metro riders' valuation of travel time under extremely crowded conditions relative to that under uncrowded conditions."
http://arxiv.org/abs/2007.03654v1,The Cathedral and the Starship: Learning from the Middle Ages for Future Long-Duration Projects,2020-07-07 17:43:49+00:00,['Andreas M. Hein'],econ.GN,"A popular analogue used in the space domain is that of historical building projects, notably cathedrals that took decades and in some cases centuries to complete. Cathedrals are often taken as archetypes for long-term projects. In this article, I will explore the cathedral from the point of view of project management and systems architecting and draw implications for long-term projects in the space domain, notably developing a starship. I will show that the popular image of a cathedral as a continuous long-term project is in contradiction to the current state of research. More specifically, I will show that for the following propositions: The cathedrals were built based on an initial detailed master plan; Building was a continuous process that adhered to the master plan; Investments were continuously provided for the building process. Although initial plans might have existed, the construction process took often place in multiple campaigns, sometimes separated by decades. Such interruptions made knowledge-preservation very challenging. The reason for the long stretches of inactivity was mostly due to a lack of funding. Hence, the availability of funding coincided with construction activity. These findings paint a much more relevant picture of cathedral building for long-duration projects today: How can a project be completed despite a range of uncertainties regarding loss in skills, shortage in funding, and interruptions? It is concluded that long-term projects such as an interstellar exploration program can take inspiration from cathedrals by developing a modular architecture, allowing for extensibility and flexibility, thinking about value delivery at an early point, and establishing mechanisms and an organization for stable funding."
http://arxiv.org/abs/2007.02823v1,Dynamic Awareness,2020-07-06 15:28:22+00:00,"['Joseph Y. Halpern', 'Evan Piermont']",cs.AI,"We investigate how to model the beliefs of an agent who becomes more aware. We use the framework of Halpern and Rego (2013) by adding probability, and define a notion of a model transition that describes constraints on how, if an agent becomes aware of a new formula $φ$ in state $s$ of a model $M$, she transitions to state $s^*$ in a model $M^*$. We then discuss how such a model can be applied to information disclosure."
http://arxiv.org/abs/2008.05417v2,Bookmakers' mispricing of the disappeared home advantage in the German Bundesliga after the COVID-19 break,2020-08-12 16:19:51+00:00,"['Christian Deutscher', 'David Winkelmann', 'Marius Ötting']",econ.GN,"The outbreak of COVID-19 in March 2020 led to a shutdown of economic activities in Europe. This included the sports sector, since public gatherings were prohibited. The German Bundesliga was among the first sport leagues realising a restart without spectators. Several recent studies suggest that the home advantage of teams was eroded for the remaining matches. Our paper analyses the reaction by bookmakers to the disappearance of such home advantage. We show that bookmakers had problems to adjust the betting odds in accordance to the disappeared home advantage, opening opportunities for profitable betting strategies."
http://arxiv.org/abs/2008.12477v1,How is Machine Learning Useful for Macroeconomic Forecasting?,2020-08-28 04:23:52+00:00,"['Philippe Goulet Coulombe', 'Maxime Leroux', 'Dalibor Stevanovic', 'Stéphane Surprenant']",econ.EM,"We move beyond ""Is Machine Learning Useful for Macroeconomic Forecasting?"" by adding the ""how"". The current forecasting literature has focused on matching specific variables and horizons with a particularly successful algorithm. In contrast, we study the usefulness of the underlying features driving ML gains over standard macroeconometric methods. We distinguish four so-called features (nonlinearities, regularization, cross-validation and alternative loss function) and study their behavior in both the data-rich and data-poor environments. To do so, we design experiments that allow to identify the ""treatment"" effects of interest. We conclude that (i) nonlinearity is the true game changer for macroeconomic prediction, (ii) the standard factor model remains the best regularization, (iii) K-fold cross-validation is the best practice and (iv) the $L_2$ is preferred to the $\bar ε$-insensitive in-sample loss. The forecasting gains of nonlinear techniques are associated with high macroeconomic uncertainty, financial stress and housing bubble bursts. This suggests that Machine Learning is useful for macroeconomic forecasting by mostly capturing important nonlinearities that arise in the context of uncertainty and financial frictions."
http://arxiv.org/abs/2008.12910v1,Implication of Natal Care and Maternity Leave on Child Morbidity: Evidence from Ghana,2020-08-29 05:11:00+00:00,"['Danny Turkson', 'Joy Kafui Ahiabor']",econ.GN,Failure to receive post-natal care within first week of delivery causes a 3% increase in the possibility of Acute Respiratory Infection in children under five. Mothers with unpaid maternity leave put their children at a risk of 3.9% increase in the possibility of ARI compared to those with paid maternity leave.
http://arxiv.org/abs/2009.07124v2,An Agent-Based Model of Delegation Relationships With Hidden-Action: On the Effects of Heterogeneous Memory on Performance,2020-09-09 07:08:42+00:00,"['Patrick Reinwald', 'Stephan Leitner', 'Friederike Wall']",cs.MA,"We introduce an agent-based model of delegation relationships between a principal and an agent, which is based on the standard-hidden action model introduced by Holmström and, by doing so, provide a model which can be used to further explore theoretical topics in managerial economics, such as the efficiency of incentive mechanisms. We employ the concept of agentization, i.e., we systematically transform the standard hidden-action model into an agent-based model. Our modeling approach allows for a relaxation of some of the rather ""heroic"" assumptions included in the standard hidden-action model, whereby we particularly focus on assumptions related to the (i) availability of information about the environment and the (ii) principal's and agent's cognitive capabilities (with a particular focus on their learning capabilities and their memory). Our analysis focuses on how close and how fast the incentive scheme, which endogenously emerges from the agent-based model, converges to the solution proposed by the standard hidden-action model. Also, we investigate whether a stable solution can emerge from the agent-based model variant. The results show that in stable environments the emergent result can nearly reach the solution proposed by the standard hidden-action model. Surprisingly, the results indicate that turbulence in the environment leads to stability in earlier time periods."
http://arxiv.org/abs/2009.06391v1,Capital Flows and the Stabilizing Role of Macroprudential Policies in CESEE,2020-09-10 18:17:11+00:00,"['Markus Eller', 'Niko Hauzenberger', 'Florian Huber', 'Helene Schuberth', 'Lukas Vashold']",econ.EM,"In line with the recent policy discussion on the use of macroprudential measures to respond to cross-border risks arising from capital flows, this paper tries to quantify to what extent macroprudential policies (MPPs) have been able to stabilize capital flows in Central, Eastern and Southeastern Europe (CESEE) -- a region that experienced a substantial boom-bust cycle in capital flows amid the global financial crisis and where policymakers had been quite active in adopting MPPs already before that crisis. To study the dynamic responses of capital flows to MPP shocks, we propose a novel regime-switching factor-augmented vector autoregressive (FAVAR) model. It allows to capture potential structural breaks in the policy regime and to control -- besides domestic macroeconomic quantities -- for the impact of global factors such as the global financial cycle. Feeding into this model a novel intensity-adjusted macroprudential policy index, we find that tighter MPPs may be effective in containing domestic private sector credit growth and the volumes of gross capital inflows in a majority of the countries analyzed. However, they do not seem to generally shield CESEE countries from capital flow volatility."
http://arxiv.org/abs/2009.00131v1,InClass Nets: Independent Classifier Networks for Nonparametric Estimation of Conditional Independence Mixture Models and Unsupervised Classification,2020-08-31 22:24:09+00:00,"['Konstantin T. Matchev', 'Prasanth Shyamsundar']",stat.ML,"We introduce a new machine-learning-based approach, which we call the Independent Classifier networks (InClass nets) technique, for the nonparameteric estimation of conditional independence mixture models (CIMMs). We approach the estimation of a CIMM as a multi-class classification problem, since dividing the dataset into different categories naturally leads to the estimation of the mixture model. InClass nets consist of multiple independent classifier neural networks (NNs), each of which handles one of the variates of the CIMM. Fitting the CIMM to the data is performed by simultaneously training the individual NNs using suitable cost functions. The ability of NNs to approximate arbitrary functions makes our technique nonparametric. Further leveraging the power of NNs, we allow the conditionally independent variates of the model to be individually high-dimensional, which is the main advantage of our technique over existing non-machine-learning-based approaches. We derive some new results on the nonparametric identifiability of bivariate CIMMs, in the form of a necessary and a (different) sufficient condition for a bivariate CIMM to be identifiable. We provide a public implementation of InClass nets as a Python package called RainDancesVI and validate our InClass nets technique with several worked out examples. Our method also has applications in unsupervised and semi-supervised classification problems."
http://arxiv.org/abs/2009.04767v1,Using Nudges to Prevent Student Dropouts in the Pandemic,2020-09-10 10:51:02+00:00,"['Guilherme Lichand', 'Julien Christen']",econ.GN,"The impacts of COVID-19 reach far beyond the hundreds of lives lost to the disease; in particular, the pre-existing learning crisis is expected to be magnified during school shutdown. Despite efforts to put distance learning strategies in place, the threat of student dropouts, especially among adolescents, looms as a major concern. Are interventions to motivate adolescents to stay in school effective amidst the pandemic? Here we show that, in Brazil, nudges via text messages to high-school students, to motivate them to stay engaged with school activities, substantially reduced dropouts during school shutdown, and greatly increased their motivation to go back to school when classes resume. While such nudges had been shown to decrease dropouts during normal times, it is surprising that those impacts replicate in the absence of regular classes because their effects are typically mediated by teachers (whose effort in the classroom changes in response to the nudges). Results show that insights from the science of adolescent psychology can be leveraged to shift developmental trajectories at a critical juncture. They also qualify those insights: effects increase with exposure and gradually fade out once communication stops, providing novel evidence that motivational interventions work by redirecting adolescents' attention."
http://arxiv.org/abs/2009.05360v1,Inferring hidden potentials in analytical regions: uncovering crime suspect communities in Medellín,2020-09-11 11:57:38+00:00,"['Alejandro Puerta', 'Andrés Ramírez-Hassan']",econ.EM,"This paper proposes a Bayesian approach to perform inference regarding the size of hidden populations at analytical region using reported statistics. To do so, we propose a specification taking into account one-sided error components and spatial effects within a panel data structure. Our simulation exercises suggest good finite sample performance. We analyze rates of crime suspects living per neighborhood in Medellín (Colombia) associated with four crime activities. Our proposal seems to identify hot spots or ""crime communities"", potential neighborhoods where under-reporting is more severe, and also drivers of crime schools. Statistical evidence suggests a high level of interaction between homicides and drug dealing in one hand, and motorcycle and car thefts on the other hand."
http://arxiv.org/abs/2009.04173v1,Random Non-Expected Utility: Non-Uniqueness,2020-09-09 09:16:22+00:00,['Yi-Hsuan Lin'],econ.TH,"In random expected utility (Gul and Pesendorfer, 2006), the distribution of preferences is uniquely recoverable from random choice. This paper shows through two examples that such uniqueness fails in general if risk preferences are random but do not conform to expected utility theory. In the first, non-uniqueness obtains even if all preferences are confined to the betweenness class (Dekel, 1986) and are suitably monotone. The second example illustrates random choice behavior consistent with random expected utility that is also consistent with random non-expected utility. On the other hand, we find that if risk preferences conform to weighted utility theory (Chew, 1983) and are monotone in first-order stochastic dominance, random choice again uniquely identifies the distribution of preferences. Finally, we argue that, depending on the domain of risk preferences, uniqueness may be restored if joint distributions of choice across a limited number of feasible sets are available."
http://arxiv.org/abs/2009.07341v1,Encompassing Tests for Value at Risk and Expected Shortfall Multi-Step Forecasts based on Inference on the Boundary,2020-09-15 20:18:53+00:00,"['Timo Dimitriadis', 'Xiaochun Liu', 'Julie Schnaitmann']",econ.EM,"We propose forecast encompassing tests for the Expected Shortfall (ES) jointly with the Value at Risk (VaR) based on flexible link (or combination) functions. Our setup allows testing encompassing for convex forecast combinations and for link functions which preclude crossings of the combined VaR and ES forecasts. As the tests based on these link functions involve parameters which are on the boundary of the parameter space under the null hypothesis, we derive and base our tests on nonstandard asymptotic theory on the boundary. Our simulation study shows that the encompassing tests based on our new link functions outperform tests based on unrestricted linear link functions for one-step and multi-step forecasts. We further illustrate the potential of the proposed tests in a real data analysis for forecasting VaR and ES of the S&P 500 index."
http://arxiv.org/abs/2009.11660v2,Association between COVID-19 cases and international equity indices,2020-09-24 13:12:55+00:00,"['Nick James', 'Max Menzies']",physics.soc-ph,"This paper analyzes the impact of COVID-19 on the populations and equity markets of 92 countries. We compare country-by-country equity market dynamics to cumulative COVID-19 case and death counts and new case trajectories. First, we examine the multivariate time series of cumulative cases and deaths, particularly regarding their changing structure over time. We reveal similarities between the case and death time series, and key dates that the structure of the time series changed. Next, we classify new case time series, demonstrate five characteristic classes of trajectories, and quantify discrepancy between them with respect to the behavior of waves of the disease. Finally, we show there is no relationship between countries' equity market performance and their success in managing COVID-19. Each country's equity index has been unresponsive to the domestic or global state of the pandemic. Instead, these indices have been highly uniform, with most movement in March."
http://arxiv.org/abs/2011.00520v1,"Social networks, confirmation bias and shock elections",2020-11-01 15:01:45+00:00,"['Edoardo Gallo', 'Alastair Langtry']",econ.TH,"In recent years online social networks have become increasingly prominent in political campaigns and, concurrently, several countries have experienced shock election outcomes. This paper proposes a model that links these two phenomena. In our set-up, the process of learning from others on a network is influenced by confirmation bias, i.e. the tendency to ignore contrary evidence and interpret it as consistent with one's own belief. When agents pay enough attention to themselves, confirmation bias leads to slower learning in any symmetric network, and it increases polarization in society. We identify a subset of agents that become more/less influential with confirmation bias. The socially optimal network structure depends critically on the information available to the social planner. When she cannot observe agents' beliefs, the optimal network is symmetric, vertex-transitive and has no self-loops. We explore the implications of these results for electoral outcomes and media markets. Confirmation bias increases the likelihood of shock elections, and it pushes fringe media to take a more extreme ideology."
http://arxiv.org/abs/2011.09640v1,Belief Error and Non-Bayesian Social Learning: Experimental Evidence,2020-11-19 04:06:32+00:00,"['Boğaçhan Çelen', 'Sen Geng', 'Huihui Li']",econ.GN,"This paper experimentally studies whether individuals hold a first-order belief that others apply Bayes' rule to incorporate private information into their beliefs, which is a fundamental assumption in many Bayesian and non-Bayesian social learning models. We design a novel experimental setting in which the first-order belief assumption implies that social information is equivalent to private information. Our main finding is that participants' reported reservation prices of social information are significantly lower than those of private information, which provides evidence that casts doubt on the first-order belief assumption. We also build a novel belief error model in which participants form a random posterior belief with a Bayesian posterior belief kernel to explain the experimental findings. A structural estimation of the model suggests that participants' sophisticated consideration of others' belief error and their exaggeration of the error both contribute to the difference in reservation prices."
http://arxiv.org/abs/2012.15007v4,Evolutionarily Stable (Mis)specifications: Theory and Applications,2020-12-30 02:33:15+00:00,"['Kevin He', 'Jonathan Libgober']",econ.TH,"Toward explaining the persistence of biased inferences, we propose a framework to evaluate competing (mis)specifications in strategic settings. Agents with heterogeneous (mis)specifications coexist and draw Bayesian inferences about their environment through repeated play. The relative stability of (mis)specifications depends on their adherents' equilibrium payoffs. A key mechanism is the learning channel: the endogeneity of perceived best replies due to inference. We characterize when a rational society is only vulnerable to invasion by some misspecification through the learning channel. The learning channel leads to new stability phenomena, and can confer an evolutionary advantage to otherwise detrimental biases in economically relevant applications."
http://arxiv.org/abs/2011.03695v3,Endogenous structural transformation in economic development,2020-11-07 04:56:55+00:00,"['Justin Y. F. Lin', 'Haipeng Xing']",econ.TH,"This paper extends Xing's (2023abcd) optimal growth models of catching-up economies from the case of production function switching to that of economic structure switching and argues how a country develops its economy by endogenous structural transformation and efficient resource allocation in a market mechanism. To achieve this goal, the paper first summarizes three attributes of economic structures from the literature, namely, structurality, durationality, and transformality, and discuss their implications for methods of economic modeling. Then, with the common knowledge assumption, the paper extends Xing's (2023a) optimal growth model that is based on production function switching and considers an extended Ramsey model with endogenous structural transformation in which the social planner chooses the optimal industrial structure, recource allocation with the chosen structure, and consumption to maximize the representative household's total utility subject to the resource constraint. The paper next establishes the mathematical underpinning of the static, dynamic, and switching equilibria. The Ramsey growth model and its equilibria are then extended to economies with complicated economic structures consisting of hierarchical production, technology adoption and innovation, infrastructure, and economic and political institutions. The paper concludes with a brief discussion of applications of the proposed methodology to economic development problems in other scenarios."
http://arxiv.org/abs/2010.05058v1,Valid t-ratio Inference for IV,2020-10-10 17:59:00+00:00,"['David S. Lee', 'Justin McCrary', 'Marcelo J. Moreira', 'Jack Porter']",econ.EM,"In the single IV model, current practice relies on the first-stage F exceeding some threshold (e.g., 10) as a criterion for trusting t-ratio inferences, even though this yields an anti-conservative test. We show that a true 5 percent test instead requires an F greater than 104.7. Maintaining 10 as a threshold requires replacing the critical value 1.96 with 3.43. We re-examine 57 AER papers and find that corrected inference causes half of the initially presumed statistically significant results to be insignificant. We introduce a more powerful test, the tF procedure, which provides F-dependent adjusted t-ratio critical values."
http://arxiv.org/abs/2012.07238v1,Misspecified Beliefs about Time Lags,2020-12-14 03:45:43+00:00,"['Yingkai Li', 'Harry Pei']",econ.TH,"We examine the long-term behavior of a Bayesian agent who has a misspecified belief about the time lag between actions and feedback, and learns about the payoff consequences of his actions over time. Misspecified beliefs about time lags result in attribution errors, which have no long-term effect when the agent's action converges, but can lead to arbitrarily large long-term inefficiencies when his action cycles. Our proof uses concentration inequalities to bound the frequency of action switches, which are useful to study learning problems with history dependence. We apply our methods to study a policy choice game between a policy-maker who has a correctly specified belief about the time lag and the public who has a misspecified belief."
http://arxiv.org/abs/2012.03327v1,"Competition, Politics, & Social Media",2020-12-06 17:15:55+00:00,"['Benson Tsz Kin Leung', 'Pinar Yildirim']",econ.GN,"An increasing number of politicians are relying on cheaper, easier to access technologies such as online social media platforms to communicate with their constituency. These platforms present a cheap and low-barrier channel of communication to politicians, potentially intensifying political competition by allowing many to enter political races. In this study, we demonstrate that lowering costs of communication, which allows many entrants to come into a competitive market, can strengthen an incumbent's position when the newcomers compete by providing more information to the voters. We show an asymmetric bad-news-good-news effect where early negative news hurts the challengers more than the positive news benefit them, such that in aggregate, an incumbent politician's chances of winning is higher with more entrants in the market. Our findings indicate that communication through social media and other platforms can intensify competition, how-ever incumbency advantage may be strengthened rather than weakened as an outcome of higher number of entrants into a political market."
http://arxiv.org/abs/2010.01079v6,On Statistical Discrimination as a Failure of Social Learning: A Multi-Armed Bandit Approach,2020-10-02 16:20:14+00:00,"['Junpei Komiyama', 'Shunya Noda']",econ.TH,"We analyze statistical discrimination in hiring markets using a multi-armed bandit model. Myopic firms face workers arriving with heterogeneous observable characteristics. The association between the worker's skill and characteristics is unknown ex ante; thus, firms need to learn it. Laissez-faire causes perpetual underestimation: minority workers are rarely hired, and therefore, the underestimation tends to persist. Even a marginal imbalance in the population ratio frequently results in perpetual underestimation. We propose two policy solutions: a novel subsidy rule (the hybrid mechanism) and the Rooney Rule. Our results indicate that temporary affirmative actions effectively alleviate discrimination stemming from insufficient data."
http://arxiv.org/abs/2011.06778v2,Most likely retail agglomeration patterns: Potential maximization and stochastic stability of spatial equilibria,2020-11-13 06:31:07+00:00,"['Minoru Osawa', 'Takashi Akamatsu', 'Yosuke Kogure']",econ.TH,"We study a model of retail agglomeration where consumers are more likely to visit zones with a higher concentration of shops. This agglomerative effect makes zones with many retailers more attractive. The spatial distribution of retailers in equilibrium is endogenously determined in response to the spatial pattern of shopping demand. In such a setting, multiple locally stable equilibria may arise, and the outcome can depend on the initial distribution of shops. To address this issue, we apply an approach from evolutionary game theory, selecting the equilibrium that maximizes a potential function representing the incentives of retailers. We demonstrate the method in a two-dimensional spatial setting. Compared to local stability based on gradual, myopic adjustments, this global maximization leads to a unique and more robust prediction. As expected, the number of retail clusters decreases either when shopping costs for immobile consumers fall or when the attractiveness of larger retail concentrations increases."
http://arxiv.org/abs/2010.05342v2,Using Information to Amplify Competition,2020-10-11 21:02:42+00:00,['Wenhao Li'],econ.GN,"I characterize the consumer-optimal market segmentation in competitive markets where multiple firms selling differentiated products to consumers with unit demand. This segmentation is public---in that each firm observes the same market segments---and takes a simple form: in each market segment, there is a dominant firm favored by all consumers in that segment. By segmenting the market, all but the dominant firm maximally compete to poach the consumer's business, setting price to equal marginal cost. Information, thus, is being used to amplify competition. This segmentation simultaneously generates an efficient allocation and delivers to each firm its minimax profit."
http://arxiv.org/abs/2011.06753v2,Weak Identification in Discrete Choice Models,2020-11-13 04:21:15+00:00,"['David T. Frazier', 'Eric Renault', 'Lina Zhang', 'Xueyan Zhao']",econ.EM,"We study the impact of weak identification in discrete choice models, and provide insights into the determinants of identification strength in these models. Using these insights, we propose a novel test that can consistently detect weak identification in commonly applied discrete choice models, such as probit, logit, and many of their extensions. Furthermore, we demonstrate that when the null hypothesis of weak identification is rejected, Wald-based inference can be carried out using standard formulas and critical values. A Monte Carlo study compares our proposed testing approach against commonly applied weak identification tests. The results simultaneously demonstrate the good performance of our approach and the fundamental failure of using conventional weak identification tests for linear models in the discrete choice model context. Furthermore, we compare our approach against those commonly applied in the literature in two empirical examples: married women labor force participation, and US food aid and civil conflicts."
http://arxiv.org/abs/2012.11935v1,Split-then-Combine simplex combination and selection of forecasters,2020-12-22 11:19:38+00:00,"['Antonio Martin Arroyo', 'Aranzazu de Juan Fernandez']",econ.EM,"This paper considers the Split-Then-Combine (STC) approach (Arroyo and de Juan, 2014) to combine forecasts inside the simplex space, the sample space of positive weights adding up to one. As it turns out, the simplicial statistic given by the center of the simplex compares favorably against the fixed-weight, average forecast. Besides, we also develop a Combine-After-Selection (CAS) method to get rid of redundant forecasters. We apply these two approaches to make out-of-sample one-step ahead combinations and subcombinations of forecasts for several economic variables. This methodology is particularly useful when the sample size is smaller than the number of forecasts, a case where other methods (e.g., Least Squares (LS) or Principal Component Analysis (PCA)) are not applicable."
http://arxiv.org/abs/2011.04587v1,Short Term Electricity Market Designs: Identified Challenges and Promising Solutions,2020-11-09 17:32:12+00:00,"['Lina Silva-Rodriguez', 'Anibal Sanjab', 'Elena Fumagalli', 'Ana Virag', 'Madeleine Gibescu']",econ.GN,"The electricity market, which was initially designed for dispatchable power plants and inflexible demand, is being increasingly challenged by new trends, such as the high penetration of intermittent renewables and the transformation of the consumers energy space. To accommodate these new trends and improve the performance of the market, several modifications to current market designs have been proposed in the literature. Given the vast variety of these proposals, this paper provides a comprehensive investigation of the modifications proposed in the literature as well as a detailed assessment of their suitability for improving market performance under the continuously evolving electricity landscape. To this end, first, a set of criteria for an ideal market design is proposed, and the barriers present in current market designs hindering the fulfillment of these criteria are identified. Then, the different market solutions proposed in the literature, which could potentially mitigate these barriers, are extensively explored. Finally, a taxonomy of the proposed solutions is presented, highlighting the barriers addressed by each proposal and the associated implementation challenges. The outcomes of this analysis show that even though each barrier is addressed by at least one proposed solution, no single proposal is able to address all the barriers simultaneously. In this regard, a future-proof market design must combine different elements of proposed solutions to comprehensively mitigate market barriers and overcome the identified implementation challenges. Thus, by thoroughly reviewing this rich body of literature, this paper introduces key contributions enabling the advancement of the state-of-the-art towards increasingly efficient electricity market."
http://arxiv.org/abs/2010.05221v1,Identifying causal channels of policy reforms with multiple treatments and different types of selection,2020-10-11 11:13:41+00:00,"['Annabelle Doerr', 'Anthony Strittmatter']",econ.EM,"We study the identification of channels of policy reforms with multiple treatments and different types of selection for each treatment. We disentangle reform effects into policy effects, selection effects, and time effects under the assumption of conditional independence, common trends, and an additional exclusion restriction on the non-treated. Furthermore, we show the identification of direct- and indirect policy effects after imposing additional sequential conditional independence assumptions on mediating variables. We illustrate the approach using the German reform of the allocation system of vocational training for unemployed persons. The reform changed the allocation of training from a mandatory system to a voluntary voucher system. Simultaneously, the selection criteria for participants changed, and the reform altered the composition of course types. We consider the course composition as a mediator of the policy reform. We show that the empirical evidence from previous studies reverses when considering the course composition. This has important implications for policy conclusions."
http://arxiv.org/abs/2011.12869v2,Implementation of a cost-benefit analysis of Demand-Responsive Transport with a Multi-Agent Transport Simulation,2020-11-25 16:49:07+00:00,"['Conny Grunicke', 'Jan Christian Schlüter', 'Jani-Pekka Jokinen']",econ.GN,"In this paper, the technical requirements to perform a cost-benefit analysis of a Demand Responsive Transport (DRT) service with the traffic simulation software MATSim are elaborated in order to achieve the long-term goal of assessing the introduction of a DRT service in Göttingen and the surrounding area. The aim was to determine if the software is suitable for a cost-benefit analysis while providing a user manual for building a basic simulation that can be extended with public transport and DRT. The main result is that the software is suitable for a cost-benefit analysis of a DRT service. In particular, the most important internal and external costs, such as usage costs of the various modes of transport and emissions, can be integrated into the simulation scenarios. Thus, the scenarios presented in this paper can be extended by data from a mobility study of Göttingen and its surroundings in order to achieve the long-term goal. This paper is aimed at transport economists and researchers who are not familiar with MATSim, to provide them with a guide for the first steps in working with a traffic simulation software."
http://arxiv.org/abs/2011.13900v4,Persuasion Produces the (Diamond) Paradox,2020-11-27 18:42:03+00:00,['Mark Whitmeyer'],econ.TH,"This paper extends the sequential search model of Wolinsky (1986) by allowing firms to choose how much match value information to disclose to visiting consumers. This restores the Diamond paradox (Diamond 1971): there exist no symmetric equilibria in which consumers engage in active search, so consumers obtain zero surplus and firms obtain monopoly profits. Modifying the scenario to one in which prices are advertised, we discover that the no-active-search result persists, although the resulting symmetric equilibria are ones in which firms price at marginal cost."
http://arxiv.org/abs/2010.05311v3,Interpretable Neural Networks for Panel Data Analysis in Economics,2020-10-11 18:40:22+00:00,"['Yucheng Yang', 'Zhong Zheng', 'Weinan E']",econ.EM,"The lack of interpretability and transparency are preventing economists from using advanced tools like neural networks in their empirical research. In this paper, we propose a class of interpretable neural network models that can achieve both high prediction accuracy and interpretability. The model can be written as a simple function of a regularized number of interpretable features, which are outcomes of interpretable functions encoded in the neural network. Researchers can design different forms of interpretable functions based on the nature of their tasks. In particular, we encode a class of interpretable functions named persistent change filters in the neural network to study time series cross-sectional data. We apply the model to predicting individual's monthly employment status using high-dimensional administrative data. We achieve an accuracy of 94.5% in the test set, which is comparable to the best performed conventional machine learning methods. Furthermore, the interpretability of the model allows us to understand the mechanism that underlies the prediction: an individual's employment status is closely related to whether she pays different types of insurances. Our work is a useful step towards overcoming the black-box problem of neural networks, and provide a new tool for economists to study administrative and proprietary big data."
http://arxiv.org/abs/2010.12351v1,Modeling the US-China trade conflict: a utility theory approach,2020-10-23 12:31:23+00:00,"['Yuhan Zhang', 'Cheng Chang']",econ.GN,"This paper models the US-China trade conflict and attempts to analyze the (optimal) strategic choices. In contrast to the existing literature on the topic, we employ the expected utility theory and examine the conflict mathematically. In both perfect information and incomplete information games, we show that expected net gains diminish as the utility of winning increases because of the costs incurred during the struggle. We find that the best response function exists for China but not for the US during the conflict. We argue that the less the US coerces China to change its existing trade practices, the higher the US expected net gains. China's best choice is to maintain the status quo, and any further aggression in its policy and behavior will aggravate the situation."
http://arxiv.org/abs/2012.13657v1,Negative votes to depolarize politics,2020-12-26 01:05:24+00:00,['Karthik H. Shankar'],econ.TH,"The controversies around the 2020 US presidential elections certainly casts serious concerns on the efficiency of the current voting system in representing the people's will. Is the naive Plurality voting suitable in an extremely polarized political environment? Alternate voting schemes are gradually gaining public support, wherein the voters rank their choices instead of just voting for their first preference. However they do not capture certain crucial aspects of voter preferences like disapprovals and negativities against candidates. I argue that these unexpressed negativities are the predominant source of polarization in politics. I propose a voting scheme with an explicit expression of these negative preferences, so that we can simultaneously decipher the popularity as well as the polarity of each candidate. The winner is picked by an optimal tradeoff between the most popular and the least polarizing candidate. By penalizing the candidates for their polarization, we can discourage the divisive campaign rhetorics and pave way for potential third party candidates."
http://arxiv.org/abs/2012.04485v3,Occupational segregation in a Roy model with composition preferences,2020-12-08 15:24:47+00:00,"['Haoning Chen', 'Miaomiao Dong', 'Marc Henry', 'Ivan Sidorov']",econ.TH,"We propose a model of labor market sector self-selection that combines comparative advantage, as in the Roy model, and sector composition preference. Two groups choose between two sectors based on heterogeneous potential incomes and group compositions in each sector. Potential incomes incorporate group specific human capital accumulation and wage discrimination. Composition preferences are interpreted as reflecting group specific amenity preferences as well as homophily and aversion to minority status. We show that occupational segregation is amplified by the composition preferences and we highlight a resulting tension between redistribution and diversity. The model also exhibits tipping from extreme compositions to more balanced ones. Tipping occurs when a small nudge, associated with affirmative action, pushes the system to a very different equilibrium, and when the set of equilibria changes abruptly when a parameter governing the relative importance of pecuniary and composition preferences crosses a threshold."
http://arxiv.org/abs/2012.11542v1,Uncertainty on the Reproduction Ratio in the SIR Model,2020-12-21 18:21:36+00:00,"['Sean Elliott', 'Christian Gourieroux']",stat.ME,"The aim of this paper is to understand the extreme variability on the estimated reproduction ratio $R_0$ observed in practice. For expository purpose we consider a discrete time stochastic version of the Susceptible-Infected-Recovered (SIR) model, and introduce different approximate maximum likelihood (AML) estimators of $R_0$. We carefully discuss the properties of these estimators and illustrate by a Monte-Carlo study the width of confidence intervals on $R_0$."
http://arxiv.org/abs/2012.11595v1,Valuation Models Applied to Value-Based Management. Application to the Case of UK Companies with Problems,2020-12-19 14:09:14+00:00,['Marcel Ausloos'],q-fin.GN,"Many still rightly wonder whether accounting numbers affect business value. Basic questions are why? and how? I aim at promoting an objective choice on how optimizing the most suitable valuation methods under a value-based management framework through some performance measurement systems. First, I present a comprehensive review of valuation methods. Three valuations methods, (i) Free Cash Flow Valuation Model (FCFVM), (ii) Residual Earning Valuation Model (REVM) and (iii) Abnormal Earning Growth Model (AEGM), are presented. I point out to advantages and limitations. As applications, the proofs of the findings are illustrated on three study cases: Marks & Spencer's business pattern (size and growth prospect), which had a recently advertised valuation problem, and two comparable companies, Tesco and Sainsbury's, all three chosen for multiple-based valuation. For the purpose, two value drivers are chosen, EnV/EBIT (entity value/earnings before interests and taxes) and the corresponding EnV/Sales. Thus, the question whether accounting numbers through models based on mathematical economics truly affect business value has an answer: Maybe, yes."
http://arxiv.org/abs/2012.12951v1,"If Global or Local Investor Sentiments are Prone to Developing an Impact on Stock Returns, is there an Industry Effect?",2020-12-21 22:51:42+00:00,"['Jing Shi', 'Marcel Ausloos', 'Tingting Zhu']",econ.GN,"This paper investigates the heterogeneous impacts of either Global or Local Investor Sentiments on stock returns. We study 10 industry sectors through the lens of 6 (so called) emerging countries: China, Brazil, India, Mexico, Indonesia and Turkey, over the 2000 to 2014 period. Using a panel data framework, our study sheds light on a significant effect of Local Investor Sentiments on expected returns for basic materials, consumer goods, industrial, and financial industries. Moreover, our results suggest that from Global Investor Sentiments alone, one cannot predict expected stock returns in these markets."
http://arxiv.org/abs/2012.11900v2,Expanding on Repeated Consumer Search Using Multi-Armed Bandits and Secretaries,2020-12-22 09:53:55+00:00,['Tung Yu Marco Chan'],econ.TH,"We seek to take a different approach in deriving the optimal search policy for the repeated consumer search model found in Fishman and Rob (1995) with the main motivation of dropping the assumption of prior knowledge of the price distribution $F(p)$ in each period. We will do this by incorporating the famous multi-armed bandit problem (MAB). We start by modifying the MAB framework to fit the setting of the repeated consumer search model and formulate the objective as a dynamic optimization problem. Then, given any sequence of exploration, we assign a value to each store in that sequence using Bellman equations. We then proceed to break down the problem into individual optimal stopping problems for each period which incidentally coincides with the framework of the famous secretary problem where we proceed to derive the optimal stopping policy. We will see that implementing the optimal stopping policy in each period solves the original dynamic optimization by `forward induction' reasoning."
http://arxiv.org/abs/2012.13710v1,Analysis of Randomized Experiments with Network Interference and Noncompliance,2020-12-26 10:11:20+00:00,['Bora Kim'],econ.EM,"Randomized experiments have become a standard tool in economics. In analyzing randomized experiments, the traditional approach has been based on the Stable Unit Treatment Value (SUTVA: \cite{rubin}) assumption which dictates that there is no interference between individuals. However, the SUTVA assumption fails to hold in many applications due to social interaction, general equilibrium, and/or externality effects. While much progress has been made in relaxing the SUTVA assumption, most of this literature has only considered a setting with perfect compliance to treatment assignment. In practice, however, noncompliance occurs frequently where the actual treatment receipt is different from the assignment to the treatment. In this paper, we study causal effects in randomized experiments with network interference and noncompliance. Spillovers are allowed to occur at both treatment choice stage and outcome realization stage. In particular, we explicitly model treatment choices of agents as a binary game of incomplete information where resulting equilibrium treatment choice probabilities affect outcomes of interest. Outcomes are further characterized by a random coefficient model to allow for general unobserved heterogeneity in the causal effects. After defining our causal parameters of interest, we propose a simple control function estimator and derive its asymptotic properties under large-network asymptotics. We apply our methods to the randomized subsidy program of \cite{dupas} where we find evidence of spillover effects on both short-run and long-run adoption of insecticide-treated bed nets. Finally, we illustrate the usefulness of our methods by analyzing the impact of counterfactual subsidy policies."
http://arxiv.org/abs/2012.13816v1,"Value of agreement in decision analysis: Concept, measures and application",2020-12-26 21:26:39+00:00,['Tom Pape'],econ.GN,"In multi-criteria decision analysis workshops, participants often appraise the options individually before discussing the scoring as a group. The individual appraisals lead to score ranges within which the group then seeks the necessary agreement to identify their preferred option. Preference programming enables some options to be identified as dominated even before the group agrees on a precise scoring for them. Workshop participants usually face time pressure to make a decision. Decision support can be provided by flagging options for which further agreement on their scores seems particularly valuable. By valuable, we mean the opportunity to identify other options as dominated (using preference programming) without having their precise scores agreed beforehand. The present paper quantifies this Value of Agreement and extends the concept to portfolio decision analysis and criterion weights. The new concept is validated through a case study in recruitment."
http://arxiv.org/abs/2010.03455v1,No data? No problem! A Search-based Recommendation System with Cold Starts,2020-10-07 14:48:58+00:00,"['Pedro M. Gardete', 'Carlos D. Santos']",econ.GN,"Recommendation systems are essential ingredients in producing matches between products and buyers. Despite their ubiquity, they face two important challenges. First, they are data-intensive, a feature that precludes sophisticated recommendations by some types of sellers, including those selling durable goods. Second, they often focus on estimating fixed evaluations of products by consumers while ignoring state-dependent behaviors identified in the Marketing literature.
  We propose a recommendation system based on consumer browsing behaviors, which bypasses the ""cold start"" problem described above, and takes into account the fact that consumers act as ""moving targets,"" behaving differently depending on the recommendations suggested to them along their search journey. First, we recover the consumers' search policy function via machine learning methods. Second, we include that policy into the recommendation system's dynamic problem via a Bellman equation framework.
  When compared with the seller's own recommendations, our system produces a profit increase of 33%. Our counterfactual analyses indicate that browsing history along with past recommendations feature strong complementary effects in value creation. Moreover, managing customer churn effectively is a big part of value creation, whereas recommending alternatives in a forward-looking way produces moderate effects."
http://arxiv.org/abs/2011.01374v2,Synthetic Data Generation for Economists,2020-11-02 23:05:55+00:00,"['Allison Koenecke', 'Hal Varian']",econ.GN,"As more tech companies engage in rigorous economic analyses, we are confronted with a data problem: in-house papers cannot be replicated due to use of sensitive, proprietary, or private data. Readers are left to assume that the obscured true data (e.g., internal Google information) indeed produced the results given, or they must seek out comparable public-facing data (e.g., Google Trends) that yield similar results. One way to ameliorate this reproducibility issue is to have researchers release synthetic datasets based on their true data; this allows external parties to replicate an internal researcher's methodology. In this brief overview, we explore synthetic data generation at a high level for economic analyses."
http://arxiv.org/abs/2011.00239v1,"When ""Better"" is better than ""Best""",2020-10-31 10:58:02+00:00,"['Ben Amiet', 'Andrea Collevecchio', 'Kais Hamza']",econ.TH,"We consider two-player normal form games where each player has the same finite strategy set.
  The payoffs of each player are assumed to be i.i.d. random variables with a continuous distribution.
  We show that, with high probability, the better-response dynamics converge to pure Nash equilibrium whenever there is one, whereas best-response dynamics fails to converge, as it is trapped."
http://arxiv.org/abs/2010.05712v1,"Twin Estimates of the Effects of Prenatal Environment, Child Biology, and Parental Bias on Sex Differences in Early Age Mortality",2020-10-12 13:49:12+00:00,['Roland Pongou'],econ.GN,"Sex differences in early age mortality have been explained in prior literature by differences in biological make-up and gender discrimination in the allocation of household resources. Studies estimating the effects of these factors have generally assumed that offspring sex ratio is random, which is implausible in view of recent evidence that the sex of a child is partly determined by prenatal environmental factors. These factors may also affect child health and survival in utero or after birth, which implies that conventional approaches to explaining sex differences in mortality are likely to yield biased estimates. We propose a methodology for decomposing these differences into the effects of prenatal environment, child biology, and parental preferences. Using a large sample of twins, we compare mortality rates in male-female twin pairs in India, a region known for discriminating against daughters, and sub-Saharan Africa, a region where sons and daughters are thought to be valued by their parents about equally. We find that: (1) prenatal environment positively affects the mortality of male children; (2) biological make-up of the latter contributes to their excess mortality, but its effect has been previously overestimated; and (3) parental discrimination against female children in India negatively affects their survival; but failure to control for the effects of prenatal and biological factors leads conventional approaches to underestimating its effect by 237 percent during infancy, and 44 percent during childhood."
http://arxiv.org/abs/2011.00143v1,"Nonparametric Identification of Production Function, Total Factor Productivity, and Markup from Revenue Data",2020-10-30 23:34:40+00:00,"['Hiroyuki Kasahara', 'Yoichi Sugita']",econ.EM,"Commonly used methods of production function and markup estimation assume that a firm's output quantity can be observed as data, but typical datasets contain only revenue, not output quantity. We examine the nonparametric identification of production function and markup from revenue data when a firm faces a general nonparametri demand function under imperfect competition. Under standard assumptions, we provide the constructive nonparametric identification of various firm-level objects: gross production function, total factor productivity, price markups over marginal costs, output prices, output quantities, a demand system, and a representative consumer's utility function."
http://arxiv.org/abs/2010.15889v1,"Disparities in ridesourcing demand for mobility resilience: A multilevel analysis of neighborhood effects in Chicago, Illinois",2020-10-29 19:06:37+00:00,"['Elisa Borowski', 'Jason Soria', 'Joseph Schofer', 'Amanda Stathopoulos']",physics.soc-ph,"Mobility resilience refers to the ability of individuals to complete their desired travel despite unplanned disruptions to the transportation system. The potential of new on-demand mobility options, such as ridesourcing services, to fill unpredicted gaps in mobility is an underexplored source of adaptive capacity. Applying a natural experiment approach to newly released ridesourcing data, we examine variation in the gap-filling role of on-demand mobility during sudden shocks to a transportation system by analyzing the change in use of ridesourcing during unexpected rail transit service disruptions across the racially and economically diverse city of Chicago. Using a multilevel mixed model, we control not only for the immediate station attributes where the disruption occurs, but also for the broader context of the community area and city quadrant in a three-level structure. Thereby the unobserved variability across neighborhoods can be associated with differences in factors such as transit ridership, or socio-economic status of residents, in addition to controlling for station level effects. Our findings reveal that individuals use ridesourcing as a gap-filling mechanism during rail transit disruptions, but there is strong variation across situational and locational contexts. Specifically, our results show larger increases in transit disruption responsive ridesourcing during weekdays, nonholidays, and more severe disruptions, as well as in community areas that have higher percentages of White residents and transit commuters, and on the more affluent northside of the city. These findings point to new insights with far-reaching implications on how ridesourcing complements existing transport networks by providing added capacity during disruptions but does not appear to bring equitable gap-filling benefits to low-income communities of color that typically have more limited mobility options."
http://arxiv.org/abs/2010.12350v1,Love Thy Neighbor? Perceived Community Abidance and Private Compliance to COVID-19 Norms in India,2020-10-21 18:20:55+00:00,"['Upasak Das', 'Prasenjit Sarkhel', 'Sania Ashraf']",econ.GN,"Compliance with measures like social distancing, hand-washing and wearing masks have emerged as the dominant strategy to combat health risk from the COVID-19 pandemic. These behaviors are often argued to be pro-social, where one must incur private cost to benefit or protect others. Using self-reported data across India (n=934) through online survey, we assess if changes in perceived community compliance can predict changes in individual compliance behavior, controlling for the potential confounders. We observe statistically significant and positive relationship between the two, even after accounting for omitted variable bias, plausibly allowing us to view the results from a plausible causal lens. Further, we find subsequent lockdowns such as the ones imposed in India, have a detrimental effect on individual compliance though the gains from higher perceived community compliance seems to offset this loss. We also find that sensitization through community can be particularly effective for people with pre-existing co-morbidities. Our findings underscore the need for multi-level behavioral interventions involving local actors and community institutions to sustain private compliance during the pandemic."
http://arxiv.org/abs/2010.12470v1,A Practical Guide of Off-Policy Evaluation for Bandit Problems,2020-10-23 15:11:19+00:00,"['Masahiro Kato', 'Kenshi Abe', 'Kaito Ariu', 'Shota Yasui']",cs.LG,"Off-policy evaluation (OPE) is the problem of estimating the value of a target policy from samples obtained via different policies. Recently, applying OPE methods for bandit problems has garnered attention. For the theoretical guarantees of an estimator of the policy value, the OPE methods require various conditions on the target policy and policy used for generating the samples. However, existing studies did not carefully discuss the practical situation where such conditions hold, and the gap between them remains. This paper aims to show new results for bridging the gap. Based on the properties of the evaluation policy, we categorize OPE situations. Then, among practical applications, we mainly discuss the best policy selection. For the situation, we propose a meta-algorithm based on existing OPE estimators. We investigate the proposed concepts using synthetic and open real-world datasets in experiments."
http://arxiv.org/abs/2010.01335v1,Wealth and Poverty: The Effect of Poverty on Communities,2020-10-03 12:23:36+00:00,"['Merrick Wang', 'Robert Johnston']",econ.GN,"This paper analyzes the differences in poverty in high wealth communities and low wealth communities. We first discuss methods of measuring poverty and analyze the causes of individual poverty and poverty in the Bay Area. Three cases are considered regarding relative poverty. The first two cases involve neighborhoods in the Bay Area while the third case evaluates two neighborhoods within the city of San Jose, CA. We find that low wealth communities have more crime, more teen births, and more cost-burdened renters because of high concentrations of temporary and seasonal workers, extensive regulations on greenhouse gas emissions, minimum wage laws, and limited housing supply. In the conclusion, we review past attempts to alleviate the effects of poverty and give suggestions on how future policy can be influenced to eventually create a future free of poverty."
http://arxiv.org/abs/2010.01492v1,A Class of Time-Varying Vector Moving Average Models: Nonparametric Kernel Estimation and Application,2020-10-04 06:52:54+00:00,"['Yayi Yan', 'Jiti Gao', 'Bin Peng']",econ.EM,"Multivariate dynamic time series models are widely encountered in practical studies, e.g., modelling policy transmission mechanism and measuring connectedness between economic agents. To better capture the dynamics, this paper proposes a wide class of multivariate dynamic models with time-varying coefficients, which have a general time-varying vector moving average (VMA) representation, and nest, for instance, time-varying vector autoregression (VAR), time-varying vector autoregression moving-average (VARMA), and so forth as special cases. The paper then develops a unified estimation method for the unknown quantities before an asymptotic theory for the proposed estimators is established. In the empirical study, we investigate the transmission mechanism of monetary policy using U.S. data, and uncover a fall in the volatilities of exogenous shocks. In addition, we find that (i) monetary policy shocks have less influence on inflation before and during the so-called Great Moderation, (ii) inflation is more anchored recently, and (iii) the long-run level of inflation is below, but quite close to the Federal Reserve's target of two percent after the beginning of the Great Moderation period."
http://arxiv.org/abs/2010.11030v1,"Fire Sales, the LOLR and Bank Runs with Continuous Asset Liquidity",2020-10-21 14:22:01+00:00,"['Ulrich Bindseil', 'Edoardo Lanari']",econ.GN,"Bank's asset fire sales and recourse to central bank credit are modelled with continuous asset liquidity, allowing to derive the liability structure of a bank. Both asset sales liquidity and the central bank collateral framework are modeled as power functions within the unit interval. Funding stability is captured as a strategic bank run game in pure strategies between depositors. Fire sale liquidity and the central bank collateral framework determine jointly the ability of the banking system to deliver maturity transformation without endangering financial stability. The model also explains why banks tend to use the least liquid eligible collateral with the central bank and why a sudden non-anticipated reduction of asset liquidity, or a tightening of the collateral framework, can trigger a bank run. The model also shows that the collateral framework can be understood, beyond its aim to protect the central bank, as financial stability and non-conventional monetary policy instrument."
http://arxiv.org/abs/2010.05984v2,An Extension of the Birkhoff-von Neumann Theorem to Non-Bipartite Graphs,2020-10-12 19:20:46+00:00,['Vijay V. Vazirani'],cs.DS,"We prove that a fractional perfect matching in a non-bipartite graph can be written, in polynomial time, as a convex combination of perfect matchings. This extends the Birkhoff-von Neumann Theorem from bipartite to non-bipartite graphs.
  The algorithm of Birkhoff and von Neumann is greedy; it starts with the given fractional perfect matching and successively ""removes"" from it perfect matchings, with appropriate coefficients. This fails in non-bipartite graphs -- removing perfect matchings arbitrarily can lead to a graph that is non-empty but has no perfect matchings. Using odd cuts appropriately saves the day."
http://arxiv.org/abs/2010.08386v1,Individual Heterogeneity and Cultural Attitudes in Credence Goods Provision,2020-10-16 13:40:17+00:00,['Johnny Tang'],econ.GN,"I study the heterogeneity of credence goods provision in taxi drivers taking detours in New York City. First, I document that there is significant detouring on average by drivers. Second, there is significant heterogeneity in cheating across individuals, yet each individual's propensity to take detours is stable: drivers who detour almost always detour, while those who do not detour almost never do. Drivers who take longer detours on each trip also take such trips more often. Third, cultural attitudes plausibly explain some of this heterogeneity in behavior across individuals."
http://arxiv.org/abs/2012.06192v1,Bihar Assembly Elections 2020: An Analysis,2020-12-11 08:36:54+00:00,"['Mudit Kapoor', 'Shamika Ravi']",econ.GN,"We analyse the Bihar assembly elections of 2020, and find that poverty was the key driving factor, over and above female voters as determinants. The results show that the poor were more likely to support the NDA. The relevance of this result for an election held in the midst of a pandemic, is very crucial, given that the poor were the hardest hit. Secondly, in contrast to conventional commentary, the empirical results show that the AIMIM-factor and the LJP-factor hurt the NDA while benefitting the MGB, with their presence in these elections. The methodological novelty in this paper is combining elections data with wealth index data to study the effect of poverty on elections outcomes."
http://arxiv.org/abs/2012.07669v1,The structure of multiplex networks predicts play in economic games and real-world cooperation,2020-12-14 16:16:32+00:00,"['Curtis Atkisson', 'Monique Borgerhoff Mulder']",econ.GN,"Explaining why humans cooperate in anonymous contexts is a major goal of human behavioral ecology, cultural evolution, and related fields. What predicts cooperation in anonymous contexts is inconsistent across populations, levels of analysis, and games. For instance, market integration is a key predictor across ethnolinguistic groups but has inconsistent predictive power at the individual level. We adapt an idea from 19th-century sociology: people in societies with greater overlap in ties across domains among community members (Durkheim's ""mechanical"" solidarity) will cooperate more with their network partners and less in anonymous contexts than people in societies with less overlap (""organic"" solidarity). This hypothesis, which can be tested at the individual and community level, assumes that these two types of societies differ in the importance of keeping existing relationships as opposed to recruiting new partners. Using multiplex networks, we test this idea by comparing cooperative tendencies in both anonymous experimental games and real-life communal labor tasks across 9 Makushi villages in Guyana that vary in the degree of within-village overlap. Average overlap in a village predicts both real-world cooperative and anonymous interactions in the predicted direction; individual overlap also has effects in the expected direction. These results reveal a consistent patterning of cooperative tendencies at both individual and local levels and contribute to the debate over the emergence of norms for cooperation among humans. Multiplex overlap can help us understand inconsistencies in previous studies of cooperation in anonymous contexts and is an unexplored dimension with explanatory power at multiple levels of analysis."
http://arxiv.org/abs/2012.10735v1,The role of time estimation in decreased impatience in Intertemporal Choice,2020-12-19 16:33:07+00:00,"['Camila S. Agostino Peter M. E. Claessens', 'Fuat Balci', 'Yossi Zana']",econ.TH,"The role of specific cognitive processes in deviations from constant discounting in intertemporal choice is not well understood. We evaluated decreased impatience in intertemporal choice tasks independent of discounting rate and non-linearity in long-scale time representation; nonlinear time representation was expected to explain inconsistencies in discounting rate. Participants performed temporal magnitude estimation and intertemporal choice tasks. Psychophysical functions for time intervals were estimated by fitting linear and power functions, while discounting functions were estimated by fitting exponential and hyperbolic functions. The temporal magnitude estimates of 65% of the participants were better fit with power functions (mostly compression). 63% of the participants had intertemporal choice patterns corresponding best to hyperbolic functions. Even when the perceptual bias in the temporal magnitude estimations was compensated in the discounting rate computation, the data of 8 out of 14 participants continued exhibiting temporal inconsistency. The results suggest that temporal inconsistency in discounting rate can be explained to different degrees by the bias in temporal representations. Non-linearity in temporal representation and discounting rate should be evaluated on an individual basis. Keywords: Intertemporal choice, temporal magnitude, model comparison, impatience, time inconsistency"
http://arxiv.org/abs/2011.05255v1,On social networks that support learning,2020-11-10 17:22:45+00:00,"['Itai Arieli', 'Fedor Sandomirskiy', 'Rann Smorodinsky']",econ.TH,"It is well understood that the structure of a social network is critical to whether or not agents can aggregate information correctly. In this paper, we study social networks that support information aggregation when rational agents act sequentially and irrevocably. Whether or not information is aggregated depends, inter alia, on the order in which agents decide. Thus, to decouple the order and the topology, our model studies a random arrival order.
  Unlike the case of a fixed arrival order, in our model, the decision of an agent is unlikely to be affected by those who are far from him in the network. This observation allows us to identify a local learning requirement, a natural condition on the agent's neighborhood that guarantees that this agent makes the correct decision (with high probability) no matter how well other agents perform. Roughly speaking, the agent should belong to a multitude of mutually exclusive social circles.
  We illustrate the power of the local learning requirement by constructing a family of social networks that guarantee information aggregation despite that no agent is a social hub (in other words, there are no opinion leaders). Although the common wisdom of the social learning literature suggests that information aggregation is very fragile, another application of the local learning requirement demonstrates the existence of networks where learning prevails even if a substantial fraction of the agents are not involved in the learning process. On a technical level, the networks we construct rely on the theory of expander graphs, i.e., highly connected sparse graphs with a wide range of applications from pure mathematics to error-correcting codes."
http://arxiv.org/abs/2012.04571v1,How Covid-19 Pandemic Changes the Theory of Economics?,2020-12-08 17:16:34+00:00,['Matti Estola'],econ.TH,"During its history, the ultimate goal of economics has been to develop similar frameworks for modeling economic behavior as invented in physics. This has not been successful, however, and current state of the process is the neoclassical framework that bases on static optimization. By using a static framework, however, we cannot model and forecast the time paths of economic quantities because for a growing firm or a firm going into bankruptcy, a positive profit maximizing flow of production does not exist. Due to these problems, we present a dynamic theory for the production of a profit-seeking firm where the adjustment may be stable or unstable. This is important, currently, because we should be able to forecast the possible future bankruptcies of firms due to the Covid-19 pandemic. By using the model, we can solve the time moment of bankruptcy of a firm as a function of several parameters. The proposed model is mathematically identical with Newtonian model of a particle moving in a resisting medium, and so the model explains the reasons that stop the motion too. The frameworks for modeling dynamic events in physics are thus applicable in economics, and we give reasons why physics is more important for the development of economics than pure mathematics. (JEL D21, O12)
  Keywords: Limitations of neoclassical framework, Dynamics of production, Economic force, Connections between economics and physics."
http://arxiv.org/abs/2012.02662v1,Imperfect Credibility versus No Credibility of Optimal Monetary Policy,2020-12-04 15:37:06+00:00,"['Jean-Bernard Chatelain', 'Kirsten Ralf']",econ.GN,"A minimal central bank credibility, with a non-zero probability of not renegning his commitment (""quasi-commitment""), is a necessary condition for anchoring inflation expectations and stabilizing inflation dynamics. By contrast, a complete lack of credibility, with the certainty that the policy maker will renege his commitment (""optimal discretion""), leads to the local instability of inflation dynamics. In the textbook example of the new-Keynesian Phillips curve, the response of the policy instrument to inflation gaps for optimal policy under quasi-commitment has an opposite sign than in optimal discretion, which explains this bifurcation."
http://arxiv.org/abs/2012.01903v1,Impact of COVID-19 on the trade of goods and services in Spain,2020-12-03 13:35:54+00:00,['Asier Minondo'],econ.GN,"The COVID-19 crisis has led to the sharpest collapse in the Spanish trade of goods and services in recent decades. The containment measures adopted to arrest the spread of the virus have caused an especially intense fall of trade in services. Spain's export specialization in transport equipment, capital and outdoor goods, and services that rely on the movement of people has made the COVID-19 trade crisis more intense in Spain than in the rest of the European Union. However, the nature of the collapse suggests that trade in goods can recover swiftly when the health crisis ends. On the other hand, COVID-19 may have a long-term negative impact on the trade of services that rely on the movement of people."
http://arxiv.org/abs/2012.02394v1,Biased Programmers? Or Biased Data? A Field Experiment in Operationalizing AI Ethics,2020-12-04 04:12:33+00:00,"['Bo Cowgill', ""Fabrizio Dell'Acqua"", 'Samuel Deng', 'Daniel Hsu', 'Nakul Verma', 'Augustin Chaintreau']",econ.GN,"Why do biased predictions arise? What interventions can prevent them? We evaluate 8.2 million algorithmic predictions of math performance from $\approx$400 AI engineers, each of whom developed an algorithm under a randomly assigned experimental condition. Our treatment arms modified programmers' incentives, training data, awareness, and/or technical knowledge of AI ethics. We then assess out-of-sample predictions from their algorithms using randomized audit manipulations of algorithm inputs and ground-truth math performance for 20K subjects. We find that biased predictions are mostly caused by biased training data. However, one-third of the benefit of better training data comes through a novel economic mechanism: Engineers exert greater effort and are more responsive to incentives when given better training data. We also assess how performance varies with programmers' demographic characteristics, and their performance on a psychological test of implicit bias (IAT) concerning gender and careers. We find no evidence that female, minority and low-IAT engineers exhibit lower bias or discrimination in their code. However, we do find that prediction errors are correlated within demographic groups, which creates performance improvements through cross-demographic averaging. Finally, we quantify the benefits and tradeoffs of practical managerial or policy interventions such as technical advice, simple reminders, and improved incentives for decreasing algorithmic bias."
http://arxiv.org/abs/2011.14823v1,Gender diversity in research teams and citation impact in Economics and Management,2020-11-23 21:16:35+00:00,"['Abdelghani Maddi', 'Yves Gingras']",econ.GN,"The aim of this paper is twofold:1)contribute to a better understanding of the place of women in Economics and Management disciplines by characterizing the difference in levels of scientific collaboration between men and women at the specialties level;2) Investigate the relationship between gender diversity and citation impact in Economics and Management. Our data, extracted from the Web of Science database, cover global production as indexed in 302 journals in Economics and 370 journals in Management, with respectively 153 667 and 163 567 articles published between 2008 and 2018. Results show that collaborative practices between men and women are quite different in Economics and Management. We also find that there is a positive and significant effect of gender diversity on the academic impact of publications. Mixed-gender publications (co-authored by men and women) receive more citations than non-mixed papers (written by same-gender author teams) or single-author publications. The effect is slightly stronger in Management. The regression analysis also indicates that there is, for both disciplines, a small negative effect on citations received if the corresponding author is a woman."
http://arxiv.org/abs/2011.11017v1,Machine Predictions and Human Decisions with Variation in Payoffs and Skill,2020-11-22 13:53:24+00:00,"['Michael Allan Ribers', 'Hannes Ullrich']",econ.GN,"Human decision-making differs due to variation in both incentives and available information. This constitutes a substantial challenge for the evaluation of whether and how machine learning predictions can improve decision outcomes. We propose a framework that incorporates machine learning on large-scale data into a choice model featuring heterogeneity in decision maker payoff functions and predictive skill. We apply this framework to the major health policy problem of improving the efficiency in antibiotic prescribing in primary care, one of the leading causes of antibiotic resistance. Our analysis reveals large variation in physicians' skill to diagnose bacterial infections and in how physicians trade off the externality inherent in antibiotic use against its curative benefit. Counterfactual policy simulations show that the combination of machine learning predictions with physician diagnostic skill results in a 25.4 percent reduction in prescribing and achieves the largest welfare gains compared to alternative policies for both estimated physician as well as conservative social planner preference weights on the antibiotic resistance externality."
http://arxiv.org/abs/2011.04013v1,Screening and Information-Sharing Externalities,2020-11-08 16:25:42+00:00,['Quitzé Valenzuela-Stookey'],econ.TH,"In many settings, multiple uninformed agents bargain simultaneously with a single informed agent in each of multiple periods. For example, workers and firms negotiate each year over salaries, and the firm has private information about the value of workers' output. I study the effects of transparency in these settings; uninformed agents may observe others' past bargaining outcomes, e.g. wages. I show that in equilibrium, each uninformed agent will choose in each period whether to try to separate the informed agent's types (screen) or receive the same outcome regardless of type (pool). In other words, the agents engage in a form of experimentation via their bargaining strategies. There are two main theoretical insights. First, there is a complementary screening effect: the more agents screen in equilibrium, the lower the information rents that each will have to pay. Second, the payoff of the informed agent will have a certain supermodularity property, which implies that equilibria with screening are ""fragile"" to deviations by uninformed agents. I apply the results to study pay-secrecy regulations and anti-discrimination policy. I show that, surprisingly, penalties for pay discrimination have no impact on bargaining outcomes. I discuss how this result depends on the legal framework for discrimination cases, and suggest changes to enhance the efficacy of anti-discrimination regulations. In particular, anti-discrimination law should preclude the so-called ""salary negotiation defense""."
http://arxiv.org/abs/2011.03297v1,Agent-based Computational Economics in Management Accounting Research: Opportunities and Difficulties,2020-11-06 11:39:06+00:00,"['Friederike Wall', 'Stephan Leitner']",econ.GN,"Agent-based computational economics (ACE) - while adopted comparably widely in other domains of managerial science - is a rather novel paradigm for management accounting research (MAR). This paper provides an overview of opportunities and difficulties that ACE may have for research in management accounting and, in particular, introduces a framework that researchers in management accounting may employ when considering ACE as a paradigm for their particular research endeavor. The framework builds on the two interrelated paradigmatic elements of ACE: a set of theoretical assumptions on economic agents and the approach of agent-based modeling. Particular focus is put on contrasting opportunities and difficulties of ACE in comparison to other research methods employed in MAR."
http://arxiv.org/abs/2011.05588v1,Deep Neural Networks and Neuro-Fuzzy Networks for Intellectual Analysis of Economic Systems,2020-11-11 06:21:08+00:00,"['Alexey Averkin', 'Sergey Yarushev']",cs.NE,"In tis paper we consider approaches for time series forecasting based on deep neural networks and neuro-fuzzy nets. Also, we make short review of researches in forecasting based on various models of ANFIS models. Deep Learning has proven to be an effective method for making highly accurate predictions from complex data sources. Also, we propose our models of DL and Neuro-Fuzzy Networks for this task. Finally, we show possibility of using these models for data science tasks. This paper presents also an overview of approaches for incorporating rule-based methodology into deep learning neural networks."
http://arxiv.org/abs/2011.05658v1,Disentangling Community-level Changes in Crime Trends During the COVID-19 Pandemic in Chicago,2020-11-11 09:34:49+00:00,"['Gian Maria Campedelli', 'Serena Favarin', 'Alberto Aziani', 'Alex R. Piquero']",econ.GN,"Recent studies exploiting city-level time series have shown that, around the world, several crimes declined after COVID-19 containment policies have been put in place. Using data at the community-level in Chicago, this work aims to advance our understanding on how public interventions affected criminal activities at a finer spatial scale. The analysis relies on a two-step methodology. First, it estimates the community-wise causal impact of social distancing and shelter-in-place policies adopted in Chicago via Structural Bayesian Time-Series across four crime categories (i.e., burglary, assault, narcotics-related offenses, and robbery). Once the models detected the direction, magnitude and significance of the trend changes, Firth's Logistic Regression is used to investigate the factors associated to the statistically significant crime reduction found in the first step of the analyses. Statistical results first show that changes in crime trends differ across communities and crime types. This suggests that beyond the results of aggregate models lies a complex picture characterized by diverging patterns. Second, regression models provide mixed findings regarding the correlates associated with significant crime reduction: several relations have opposite directions across crimes with population being the only factor that is stably and positively associated with significant crime reduction."
http://arxiv.org/abs/2011.05809v1,Competition between simultaneous demand-side flexibility options: The case of community electricity storage systems,2020-11-11 14:19:05+00:00,"['Fabian Scheller', 'Robert Burkhardt', 'Robert Schwarzeit', 'Russell McKenna', 'Thomas Bruckner']",eess.SY,"Community electricity storage systems for multiple applications promise benefits over household electricity storage systems. More economical flexibility options such as demand response and sector coupling might reduce the market size for storage facilities. This paper assesses the economic performance of community electricity storage systems by taking competitive flexibility options into account. For this purpose, an actor-related, scenario-based optimization framework is applied. The results are in line with the literature and show that community storage systems are economically more efficient than household storage systems. Relative storage capacity reductions of community storage systems over household storage systems are possible, as the demand and generation profiles are balanced out among end users. On average, storage capacity reductions of 9% per household are possible in the base case, resulting in lower specific investments. The simultaneous application of demand-side flexibility options such as sector coupling and demand response enable a further capacity reduction of the community storage size by up to 23%. At the same time, the competition between flexibility options leads to smaller benefits regarding the community storage flexibility potential, which reduces the market viability for these applications. In the worst case, the cannibalization effects reach up to 38% between the flexibility measures. The losses of the flexibility benefits outweigh the savings of the capacity reduction whereby sector coupling constitutes a far greater influencing factor than demand response. Overall, in consideration of the stated cost trends, the economies of scale, and the reduction possibilities, a profitable community storage model might be reached between 2025 and 2035. Future work should focus on the analysis of policy frameworks."
http://arxiv.org/abs/2011.07797v1,Double blind vs. open review: an evolutionary game logit-simulating the behavior of authors and reviewers,2020-11-16 08:59:30+00:00,"['Mantas Radzvilas', 'Francesco De Pretis', 'William Peden', 'Daniele Tortoli', 'Barbara Osimani']",econ.TH,"Despite the tremendous successes of science in providing knowledge and technologies, the Replication Crisis has highlighted that scientific institutions have much room for improvement. Peer-review is one target of criticism and suggested reforms. However, despite numerous controversies peer review systems, plus the obvious complexity of the incentives affecting the decisions of authors and reviewers, there is very little systematic and strategic analysis of peer-review systems. In this paper, we begin to address this feature of the peer-review literature by applying the tools of game theory. We use simulations to develop an evolutionary model based around a game played by authors and reviewers, before exploring some of its tendencies. In particular, we examine the relative impact of double-blind peer-review and open review on incentivising reviewer effort under a variety of parameters. We also compare (a) the impact of one review system versus another with (b) other alterations, such as higher costs of reviewing. We find that is no reliable difference between peer-review systems in our model. Furthermore, under some conditions, higher payoffs for good reviewing can lead to less (rather than more) author effort under open review. Finally, compared to the other parameters that we vary, it is the exogenous utility of author effort that makes an important and reliable difference in our model, which raises the possibility that peer-review might not be an important target for institutional reforms."
http://arxiv.org/abs/2011.05830v1,From passive to active: Flexibility from electric vehicles in the context of transmission system development,2020-11-11 14:49:30+00:00,"['Philipp Andreas Gunkel', 'Claire Bergaentzlé', 'Ida Græsted Jensen', 'Fabian Scheller']",econ.GN,"Electrification of transport in RES-based power system will support the decarbonisation of the transportsector.  However, due to the increase in energy demand and the large peak effects of charging, the passiveintegration of electric cars is likely to undermine sustainability efforts.  This study investigates three differentcharging strategies for electric vehicle in Europe offering various degrees of flexibility:  passive charging,smart charging and vehicle-to-grid, and puts this flexibility in perspective with the flexibility offered byinterconnections.  We use the Balmorel optimization tool to represent the short-term dispatch and long-terminvestment in the energy system and we contribute to the state-of-the-art in developing new methodologiesto represent home charging and battery degradation.  Our results show how each step of increased chargingflexibility reduces system costs, affects energy mix, impacts spot prices and reduces CO2 emissions untilthe horizon 2050.  We quantify how flexible charging and variable generation mutually support each other(¿100TWh from wind and solar energy in 2050) and restrict the business case for stationary batteries, whereaspassive charging results in a substitution of wind by solar energy.  The comparison of each charging schemewith and without interconnection expansion highlights the interplay between European countries in terms ofelectricity prices and CO2 emissions in the context of electrified transport.  Although the best outcome isreached under the most flexible scenario at the EU level, the situation of the countries with the cheapest andmost decarbonised electricity mix is damaged, which calls for adapted coordination policy at the EU level."
http://arxiv.org/abs/2012.08355v1,A mathematical model of national-level food system sustainability,2020-12-14 16:06:35+00:00,"['Conor Goold', 'Simone Pfuderer', 'William H. M. James', 'Nik Lomax', 'Fiona Smith', 'Lisa M. Collins']",econ.GN,"The global food system faces various endogeneous and exogeneous, biotic and abiotic risk factors, including a rising human population, higher population densities, price volatility and climate change. Quantitative models play an important role in understanding food systems' expected responses to shocks and stresses. Here, we present a stylised mathematical model of a national-level food system that incorporates domestic supply of a food commodity, international trade, consumer demand, and food commodity price. We derive a critical compound parameter signalling when domestic supply will become unsustainable and the food system entirely dependent on imports, which results in higher commodity prices, lower consumer demand and lower inventory levels. Using Bayesian estimation, we apply the dynamic food systems model to infer the sustainability of the UK pork industry. We find that the UK pork industry is currently sustainable but because the industry is dependent on imports to meet demand, a decrease in self-sufficiency below 50% (current levels are 60-65%) would lead it close to the critical boundary signalling its collapse. Our model provides a theoretical foundation for future work to determine more complex causal drivers of food system vulnerability."
http://arxiv.org/abs/2010.03898v2,Consistent Specification Test of the Quantile Autoregression,2020-10-08 10:50:28+00:00,['Anthoulla Phella'],econ.EM,"This paper proposes a test for the joint hypothesis of correct dynamic specification and no omitted latent factors for the Quantile Autoregression. If the composite null is rejected we proceed to disentangle the cause of rejection, i.e., dynamic misspecification or an omitted variable. We establish the asymptotic distribution of the test statistics under fairly weak conditions and show that factor estimation error is negligible. A Monte Carlo study shows that the suggested tests have good finite sample properties. Finally, we undertake an empirical illustration of modelling GDP growth and CPI inflation in the United Kingdom, where we find evidence that factor augmented models are correctly specified in contrast with their non-augmented counterparts when it comes to GDP growth, while also exploring the asymmetric behaviour of the growth and inflation distributions."
http://arxiv.org/abs/2011.13846v5,Persuading a Wishful Thinker,2020-11-27 17:25:01+00:00,"['Victor Augias', 'Daniel M. A. Barreto']",econ.TH,"We study a persuasion problem in which a sender designs an information structure to induce a non-Bayesian receiver to take a particular action. The receiver, who is privately informed about his preferences, is a wishful thinker: he is systematically overoptimistic about the most favorable outcomes. We show that wishful thinking can lead to a qualitative shift in the structure of optimal persuasion compared to the Bayesian case, whenever the sender is uncertain about what the receiver perceives as the best-case outcome in his decision problem."
http://arxiv.org/abs/2010.04129v3,The English Patient: Evaluating Local Lockdowns Using Real-Time COVID-19 & Consumption Data,2020-10-08 17:14:21+00:00,"['John Gathergood', 'Benedict Guttman-Kenney']",econ.GN,"We find UK 'local lockdowns' of cities and small regions, focused on limiting how many people a household can interact with and in what settings, are effective in turning the tide on rising positive COVID-19 cases. Yet, by focusing on household mixing within the home, these local lockdowns have not inflicted the large declines in consumption observed in March 2020 when the first virus wave and first national lockdown occurred. Our study harnesses a new source of real-time, transaction-level consumption data that we show to be highly correlated with official statistics. The effectiveness of local lockdowns are evaluated applying a difference-in-difference approach which exploits nearby localities not subject to local lockdowns as comparison groups. Our findings indicate that policymakers may be able to contain virus outbreaks without killing local economies. However, the ultimate effectiveness of local lockdowns is expected to be highly dependent on co-ordination between regions and an effective system of testing."
http://arxiv.org/abs/2011.06158v3,Mostly Harmless Machine Learning: Learning Optimal Instruments in Linear IV Models,2020-11-12 01:55:11+00:00,"['Jiafeng Chen', 'Daniel L. Chen', 'Greg Lewis']",econ.EM,"We offer straightforward theoretical results that justify incorporating machine learning in the standard linear instrumental variable setting. The key idea is to use machine learning, combined with sample-splitting, to predict the treatment variable from the instrument and any exogenous covariates, and then use this predicted treatment and the covariates as technical instruments to recover the coefficients in the second-stage. This allows the researcher to extract non-linear co-variation between the treatment and instrument that may dramatically improve estimation precision and robustness by boosting instrument strength. Importantly, we constrain the machine-learned predictions to be linear in the exogenous covariates, thus avoiding spurious identification arising from non-linear relationships between the treatment and the covariates. We show that this approach delivers consistent and asymptotically normal estimates under weak conditions and that it may be adapted to be semiparametrically efficient (Chamberlain, 1992). Our method preserves standard intuitions and interpretations of linear instrumental variable methods, including under weak identification, and provides a simple, user-friendly upgrade to the applied economics toolbox. We illustrate our method with an example in law and criminal justice, examining the causal effect of appellate court reversals on district court sentencing decisions."
http://arxiv.org/abs/2011.12544v2,On the benefits of index insurance in US agriculture: a large-scale analysis using satellite data,2020-11-25 06:42:05+00:00,"['Matthieu Stigler', 'David Lobell']",econ.GN,"Index insurance has been promoted as a promising solution for reducing agricultural risk compared to traditional farm-based insurance. By linking payouts to a regional factor instead of individual loss, index insurance reduces monitoring costs, and alleviates the problems of moral hazard and adverse selection. Despite its theoretical appeal, demand for index insurance has remained low in many developing countries, triggering a debate on the causes of the low uptake. Surprisingly, there has been little discussion in this debate about the experience in the United States. The US is an unique case as both farm-based and index-based products have been available for more than two decades. Furthermore, the number of insurance zones is very large, allowing interesting comparisons over space. As in developing countries, the adoption of index insurance is rather low -- less than than 5\% of insured acreage. Does this mean that we should give up on index insurance?
  In this paper, we investigate the low take-up of index insurance in the US leveraging a field-level dataset for corn and soybean obtained from satellite predictions. While previous studies were based either on county aggregates or on relatively small farm-level dataset, our satellite-derived data gives us a very large number of fields (close to 1.8 million) comprised within a large number of index zones (600) observed over 20 years. To evaluate the suitability of index insurance, we run a large-scale simulation comparing the benefits of both insurance schemes using a new measure of farm-equivalent risk coverage of index insurance. We make two main contributions. First, we show that in our simulations, demand for index insurance is unexpectedly high, at about 30\% to 40\% of total demand. This result is robust to relaxing several assumptions of the model and to using prospect theory instead of expected utility."
http://arxiv.org/abs/2012.03182v2,Binary Response Models for Heterogeneous Panel Data with Interactive Fixed Effects,2020-12-06 04:46:00+00:00,"['Jiti Gao', 'Fei Liu', 'Bin Peng', 'Yayi Yan']",econ.EM,"In this paper, we investigate binary response models for heterogeneous panel data with interactive fixed effects by allowing both the cross-sectional dimension and the temporal dimension to diverge. From a practical point of view, the proposed framework can be applied to predict the probability of corporate failure, conduct credit rating analysis, etc. Theoretically and methodologically, we establish a link between a maximum likelihood estimation and a least squares approach, provide a simple information criterion to detect the number of factors, and achieve the asymptotic distributions accordingly. In addition, we conduct intensive simulations to examine the theoretical findings. In the empirical study, we focus on the sign prediction of stock returns, and then use the results of sign forecast to conduct portfolio analysis."
http://arxiv.org/abs/2012.07739v2,The economics of stop-and-go epidemic control,2020-12-14 17:38:50+00:00,"['Claudius Gros', 'Daniel Gros']",econ.TH,"We analyse 'stop-and-go' containment policies that produce infection cycles as periods of tight lockdowns are followed by periods of falling infection rates. The subsequent relaxation of containment measures allows cases to increase again until another lockdown is imposed and the cycle repeats. The policies followed by several European countries during the Covid-19 pandemic seem to fit this pattern. We show that 'stop-and-go' should lead to lower medical costs than keeping infections at the midpoint between the highs and lows produced by 'stop-and-go'. Increasing the upper and reducing the lower limits of a stop-and-go policy by the same amount would lower the average medical load. But increasing the upper and lowering the lower limit while keeping the geometric average constant would have the opposite effect. We also show that with economic costs proportional to containment, any path that brings infections back to the original level (technically a closed cycle) has the same overall economic cost."
http://arxiv.org/abs/2010.06452v2,Competition versus Cooperation: A class of solvable mean field impulse control problems,2020-10-13 15:09:16+00:00,"['Sören Christensen', 'Berenice Anne Neumann', 'Tobias Sohr']",math.OC,"We discuss a class of explicitly solvable mean field type control problems/mean field games with a clear economic interpretation. More precisely, we consider long term average impulse control problems with underlying general one-dimensional diffusion processes motivated by optimal harvesting problems in natural resource management. We extend the classical stochastic Faustmann models by allowing the prices to depend on the state of the market using a mean field structure. In a competitive market model, we prove that, under natural conditions, there exists an equilibrium strategy of threshold-type and furthermore characterize the threshold explicitly. If the agents cooperate with each other, we are faced with the mean field type control problem. Using a Lagrange-type argument, we prove that the optimizer of this non-standard impulse control problem is of threshold-type as well and characterize the optimal threshold. Furthermore, we compare the solutions and illustrate the findings in an example."
http://arxiv.org/abs/2010.10901v2,On Information Asymmetry in Competitive Multi-Agent Reinforcement Learning: Convergence and Optimality,2020-10-21 11:19:53+00:00,"['Ezra Tampubolon', 'Haris Ceribasic', 'Holger Boche']",cs.LG,"In this work, we study the system of interacting non-cooperative two Q-learning agents, where one agent has the privilege of observing the other's actions. We show that this information asymmetry can lead to a stable outcome of population learning, which generally does not occur in an environment of general independent learners. The resulting post-learning policies are almost optimal in the underlying game sense, i.e., they form a Nash equilibrium. Furthermore, we propose in this work a Q-learning algorithm, requiring predictive observation of two subsequent opponent's actions, yielding an optimal strategy given that the latter applies a stationary strategy, and discuss the existence of the Nash equilibrium in the underlying information asymmetrical game."
http://arxiv.org/abs/2011.09119v2,Getting to a feasible income equality,2020-11-18 07:03:34+00:00,"['Ji-Won Park', 'Chae Un Kim']",econ.TH,"Income inequality is known to have negative impacts on an economic system, thus has been debated for a hundred years past or more. Numerous ideas have been proposed to quantify income inequality, and the Gini coefficient is a prevalent index. However, the concept of perfect equality in the Gini coefficient is rather idealistic and cannot provide realistic guidance on whether government interventions are needed to adjust income inequality. In this paper, we first propose the concept of a more realistic and feasible income equality that maximizes total social welfare. Then we show that an optimal income distribution representing the feasible equality could be modeled using the sigmoid welfare function and the Boltzmann income distribution. Finally, we carry out an empirical analysis of four countries and demonstrate how optimal income distributions could be evaluated. Our results show that the feasible income equality could be used as a practical guideline for government policies and interventions."
http://arxiv.org/abs/2012.00370v5,Evaluating (weighted) dynamic treatment effects by double machine learning,2020-12-01 09:55:40+00:00,"['Hugo Bodory', 'Martin Huber', 'Lukáš Lafférs']",econ.EM,"We consider evaluating the causal effects of dynamic treatments, i.e. of multiple treatment sequences in various periods, based on double machine learning to control for observed, time-varying covariates in a data-driven way under a selection-on-observables assumption. To this end, we make use of so-called Neyman-orthogonal score functions, which imply the robustness of treatment effect estimation to moderate (local) misspecifications of the dynamic outcome and treatment models. This robustness property permits approximating outcome and treatment models by double machine learning even under high dimensional covariates and is combined with data splitting to prevent overfitting. In addition to effect estimation for the total population, we consider weighted estimation that permits assessing dynamic treatment effects in specific subgroups, e.g. among those treated in the first treatment period. We demonstrate that the estimators are asymptotically normal and $\sqrt{n}$-consistent under specific regularity conditions and investigate their finite sample properties in a simulation study. Finally, we apply the methods to the Job Corps study in order to assess different sequences of training programs under a large set of covariates."
http://arxiv.org/abs/2012.08155v3,Real-time Inflation Forecasting Using Non-linear Dimension Reduction Techniques,2020-12-15 08:55:18+00:00,"['Niko Hauzenberger', 'Florian Huber', 'Karin Klieber']",econ.EM,"In this paper, we assess whether using non-linear dimension reduction techniques pays off for forecasting inflation in real-time. Several recent methods from the machine learning literature are adopted to map a large dimensional dataset into a lower dimensional set of latent factors. We model the relationship between inflation and the latent factors using constant and time-varying parameter (TVP) regressions with shrinkage priors. Our models are then used to forecast monthly US inflation in real-time. The results suggest that sophisticated dimension reduction methods yield inflation forecasts that are highly competitive to linear approaches based on principal components. Among the techniques considered, the Autoencoder and squared principal components yield factors that have high predictive power for one-month- and one-quarter-ahead inflation. Zooming into model performance over time reveals that controlling for non-linear relations in the data is of particular importance during recessionary episodes of the business cycle or the current COVID-19 pandemic."
http://arxiv.org/abs/2011.07920v3,Forecasting CPI Inflation Components with Hierarchical Recurrent Neural Networks,2020-11-16 13:13:05+00:00,"['Oren Barkan', 'Jonathan Benchimol', 'Itamar Caspi', 'Eliya Cohen', 'Allon Hammer', 'Noam Koenigstein']",econ.GN,"We present a hierarchical architecture based on Recurrent Neural Networks (RNNs) for predicting disaggregated inflation components of the Consumer Price Index (CPI). While the majority of existing research is focused mainly on predicting the inflation headline, many economic and financial entities are more interested in its partial disaggregated components. To this end, we developed the novel Hierarchical Recurrent Neural Network (HRNN) model that utilizes information from higher levels in the CPI hierarchy to improve predictions at the more volatile lower levels. Our evaluations, based on a large data-set from the US CPI-U index, indicate that the HRNN model significantly outperforms a vast array of well-known inflation prediction baselines."
http://arxiv.org/abs/2010.02378v3,Protectionism and economic growth: Causal evidence from the first era of globalization,2020-10-05 22:50:28+00:00,"['Niklas Potrafke', 'Fabian Ruthardt', 'Kaspar Wüthrich']",econ.GN,"We investigate how protectionist policies influence economic growth. Our empirical strategy exploits an extraordinary tax scandal that gave rise to an unexpected change of government in Sweden. A free-trade majority in parliament was overturned by a protectionist majority in 1887. The protectionist government increased tariffs. We employ the synthetic control method to select control countries against which economic growth in Sweden can be compared. We do not find evidence suggesting that protectionist policies influenced economic growth and examine channels why. The new tariff laws increased government revenue. However, the results do not suggest that the protectionist government stimulated the economy by increasing government expenditure."
http://arxiv.org/abs/2012.15367v3,Assessing the Sensitivity of Synthetic Control Treatment Effect Estimates to Misspecification Error,2020-12-30 23:35:30+00:00,"['Billy Ferguson', 'Brad Ross']",econ.EM,"We propose a sensitivity analysis for Synthetic Control (SC) treatment effect estimates to interrogate the assumption that the SC method is well-specified, namely that choosing weights to minimize pre-treatment prediction error yields accurate predictions of counterfactual post-treatment outcomes. Our data-driven procedure recovers the set of treatment effects consistent with the assumption that the misspecification error incurred by the SC method is at most the observable misspecification error incurred when using the SC estimator to predict the outcomes of some control unit. We show that under one definition of misspecification error, our procedure provides a simple, geometric motivation for comparing the estimated treatment effect to the distribution of placebo residuals to assess estimate credibility. When we apply our procedure to several canonical studies that report SC estimates, we broadly confirm the conclusions drawn by the source papers."
http://arxiv.org/abs/2012.15144v1,What is the impact of labor displacement on management consulting services?,2020-12-30 13:12:34+00:00,['Edouard Ribes'],econ.GN,"Labor displacement off-or nearshore is a performance improvement instrument that currently sparks a lot of interest in the service sector. This article proposes a model to understand the consequences of such a decision on management consulting firms. Its calibration on the market of consulting services for the German transportation industry highlights that, under realistic assumptions, labor displacement translates in price decrease by-0.5% on average per year and that for MC practices to remain competitive/profitable they have to at least increase the amount of work they off/nears shore by +0.7% a year."
http://arxiv.org/abs/2012.14976v1,"REME -- Renewable Energy and Materials Economy -- The Path to Energy Security, Prosperity and Climate Stability",2020-12-29 23:34:44+00:00,['Peter Eisenberger'],physics.soc-ph,"A Renewable Energy and Materials Economy (REME) is proposed as the solution to the climate change threat. REME mimics nature to produce carbon neutral liquid fuels and chemicals as well as carbon negative materials by using water, CO$_2$ from the atmosphere and renewable energy as inputs. By being in harmony with nature REME has a positive feedback between economic development and climate change protection. In REME the feedback driven accelerated rate of economic growth enables the climate change threat to be addressed in a timely manner. It is also cost-effective protection because it sequesters by monetizing the carbon removed from the air in carbon-based building materials. Thus, addressing the climate change threat is not a cost to the economy but a result of REME driven prosperity."
http://arxiv.org/abs/2011.07809v1,"Do tar roads bring tourism? Growth corridor policy and tourism development in the Zambezi region, Namibia",2020-11-16 09:23:52+00:00,"['Linus Kalvelage', 'Javier Revilla Diez', 'Michael Bollig']",econ.GN,"There are high aspirations to foster growth in Namibia's Zambezi region via the development of tourism. The Zambezi region is a core element of the Kavango-Zambezi Transfrontier Conservation Area (KAZA), a mosaic of areas with varying degrees of protection, which is designed to combine nature conservation and rural development. These conservation areas serve as a resource base for wildlife tourism, and growth corridor policy aims to integrate the region into tourism global production networks (GPNs) by means of infrastructure development. Despite the increasing popularity of growth corridors, little is known about the effectiveness of this development strategy at local level. The mixed-methods approach reveals that the improvement of infrastructure has led to increased tourism in the region. However, the establishment of a territorial conservation imaginary that results in the designation of conservation areas is a necessary precondition for such a development. Despite the far-reaching territorial claims associated with tourism, the benefits for rural residents are limited."
http://arxiv.org/abs/2012.11215v3,"Binary Classification Tests, Imperfect Standards, and Ambiguous Information",2020-12-21 09:51:30+00:00,['Gabriel Ziegler'],econ.EM,"New binary classification tests are often evaluated relative to a pre-established test. For example, rapid Antigen tests for the detection of SARS-CoV-2 are assessed relative to more established PCR tests. In this paper, I argue that the new test can be described as producing ambiguous information when the pre-established is imperfect. This allows for a phenomenon called dilation -- an extreme form of non-informativeness. As an example, I present hypothetical test data satisfying the WHO's minimum quality requirement for rapid Antigen tests which leads to dilation. The ambiguity in the information arises from a missing data problem due to imperfection of the established test: the joint distribution of true infection and test results is not observed. Using results from Copula theory, I construct the (usually non-singleton) set of all these possible joint distributions, which allows me to assess the new test's informativeness. This analysis leads to a simple sufficient condition to make sure that a new test is not a dilation. I illustrate my approach with applications to data from three COVID-19 related tests. Two rapid Antigen tests satisfy my sufficient condition easily and are therefore informative. However, less accurate procedures, like chest CT scans, may exhibit dilation."
http://arxiv.org/abs/2012.12802v3,Machine Learning Advances for Time Series Forecasting,2020-12-23 17:01:56+00:00,"['Ricardo P. Masini', 'Marcelo C. Medeiros', 'Eduardo F. Mendes']",econ.EM,"In this paper we survey the most recent advances in supervised machine learning and high-dimensional models for time series forecasting. We consider both linear and nonlinear alternatives. Among the linear methods we pay special attention to penalized regressions and ensemble of models. The nonlinear methods considered in the paper include shallow and deep neural networks, in their feed-forward and recurrent versions, and tree-based methods, such as random forests and boosted trees. We also consider ensemble and hybrid models by combining ingredients from different alternatives. Tests for superior predictive ability are briefly reviewed. Finally, we discuss application of machine learning in economics and finance and provide an illustration with high-frequency financial data."
http://arxiv.org/abs/2011.10242v3,A Stationary Kyle Setup: Microfounding propagator models,2020-11-20 07:23:35+00:00,"['Michele Vodret', 'Iacopo Mastromatteo', 'Bence Tóth', 'Michael Benzaquen']",q-fin.TR,"We provide an economically sound micro-foundation to linear price impact models, by deriving them as the equilibrium of a suitable agent-based system. Our setup generalizes the well-known Kyle model, by dropping the assumption of a terminal time at which fundamental information is revealed so to describe a stationary market, while retaining agents' rationality and asymmetric information. We investigate the stationary equilibrium for arbitrary Gaussian noise trades and fundamental information, and show that the setup is compatible with universal price diffusion at small times, and non-universal mean-reversion at time scales at which fluctuations in fundamentals decay. Our model provides a testable relation between volatility of prices, magnitude of fluctuations in fundamentals and level of volume traded in the market."
http://arxiv.org/abs/2011.08275v2,"Fat tails arise endogenously in asset prices from supply/demand, with or without jump processes",2020-11-16 21:03:27+00:00,['Gunduz Caginalp'],econ.GN,"We show that the quotient of Levy processes of jump-diffusion type has a fat-tailed distribution. An application is to price theory in economics. We show that fat tails arise endogenously from modeling of price change based on an excess demand analysis resulting in a quotient of arbitrarily correlated demand and supply whether or not jump discontinuities are present. The assumption is that supply and demand are described by drift terms, Brownian (i.e., Gaussian) and compound Poisson jump processes. If $P^{-1}dP/dt$ (the relative price change in an interval $dt$) is given by a suitable function of relative excess demand, $\left( \mathcal{D}% -\mathcal{S}\right) /\mathcal{S}$ (where $\mathcal{D}$ and $\mathcal{S}$ are demand and supply), then the distribution has tail behavior $F\left( x\right) \sim x^{-ζ}$ for a power $ζ$ that depends on the function $G$ in $P^{-1}dP/dt=G\left( \mathcal{D}/\mathcal{S}\right) $. For $G\left( x\right) \sim\left\vert x\right\vert ^{1/q}$ one has $ζ=q.$ The empirical data for assets typically yields a value, $ζ\tilde{=}3,$ or $\ ζ\in\left[ 3,5\right] $ for some markets.
  The discrepancy between the empirical result and theory never arises if one models price dynamics using basic economics methodology, i.e., generalized Walrasian adjustment, rather than the usual starting point for classical finance which assumes a normal distribution of price changes. The function $G$ is deterministic, and can be calibrated with a smaller data set. The results establish a simple link between the decay exponent of the density function and the price adjustment function, a feature that can improve methodology for risk assessment.
  The mathematical results can be applied to other problems involving the relative difference or quotient of Levy processes of jump-diffusion type."
http://arxiv.org/abs/2012.06481v2,Equitable preference relations on infinite utility streams,2020-11-12 20:00:19+00:00,"['Ram S. Dubey', 'Giorgio Laguzzi']",econ.TH,We propose generalized versions of strong equity and Pigou-Dalton transfer principle. We study the existence and the real valued representation of social welfare relations satisfying these two generalized equity principles. Our results characterize the restrictions on one period utility domains for the equitable social welfare relation (i) to exist; and (ii) to admit real-valued representations.
http://arxiv.org/abs/2011.02612v2,Bitcoin's future carbon footprint,2020-11-05 01:58:16+00:00,"['Shize Qin', 'Lena Klaaßen', 'Ulrich Gallersdörfer', 'Christian Stoll', 'Da Zhang']",econ.GN,"The carbon footprint of Bitcoin has drawn wide attention, but Bitcoin's long-term impact on the climate remains uncertain. Here we present a framework to overcome uncertainties in previous estimates and project Bitcoin's electricity consumption and carbon footprint in the long term. If we assume Bitcoin's market capitalization grows in line with the one of gold, we find that the annual electricity consumption of Bitcoin may increase from 60 to 400 TWh between 2020 and 2100. The future carbon footprint of Bitcoin strongly depends on the decarbonization pathway of the electricity sector. If the electricity sector achieves carbon neutrality by 2050, Bitcoin's carbon footprint has peaked already. However, in the business-as-usual scenario, emissions sum up to 2 gigatons until 2100, an amount comparable to 7% of global emissions in 2019. The Bitcoin price spike at the end of 2020 shows, however, that progressive development of market capitalization could yield an electricity consumption of more than 100 TWh already in 2021, and lead to cumulative emissions of over 5 gigatons by 2100. Therefore, we also discuss policy instruments to reduce Bitcoin's future carbon footprint."
http://arxiv.org/abs/2012.12263v3,Challenges of Equitable Vaccine Distribution in the COVID-19 Pandemic,2020-11-24 17:16:29+00:00,"['Joseph Bae', 'Darshan Gandhi', 'Jil Kothari', 'Sheshank Shankar', 'Jonah Bae', 'Parth Patwa', 'Rohan Sukumaran', 'Aviral Chharia', 'Sanjay Adhikesaven', 'Shloak Rathod', 'Irene Nandutu', 'Sethuraman TV', 'Vanessa Yu', 'Krutika Misra', 'Srinidhi Murali', 'Aishwarya Saxena', 'Kasia Jakimowicz', 'Vivek Sharma', 'Rohan Iyer', 'Ashley Mehra', 'Alex Radunsky', 'Priyanshi Katiyar', 'Ananthu James', 'Jyoti Dalal', 'Sunaina Anand', 'Shailesh Advani', 'Jagjit Dhaliwal', 'Ramesh Raskar']",econ.GN,"The COVID-19 pandemic has led to a need for widespread and rapid vaccine development. As several vaccines have recently been approved for human use or are in different stages of development, governments across the world are preparing comprehensive guidelines for vaccine distribution and monitoring. In this early article, we identify challenges in logistics, health outcomes, user-centric matters, and communication associated with disease-related, individual, societal, economic, and privacy consequences. Primary challenges include difficulty in equitable distribution, vaccine efficacy, duration of immunity, multi-dose adherence, and privacy-focused record-keeping to be HIPAA compliant. While many of these challenges have been previously identified and addressed, some have not been acknowledged from a comprehensive view accounting for unprecedented interactions between challenges and specific populations. The logistics of equitable widespread vaccine distribution in disparate populations and countries of various economic, racial, and cultural constitutions must be thoroughly examined and accounted for. We also describe unique challenges regarding the efficacy of vaccines in specialized populations including children, the elderly, and immunocompromised individuals. Furthermore, we report the potential for understudied drug-vaccine interactions as well as the possibility that certain vaccine platforms may increase susceptibility to HIV. Given these complicated issues, the importance of privacy-focused, user-centric systems for vaccine education and incentivization along with clear communication from governments, organizations, and academic institutions is imperative. These challenges are by no means insurmountable, but require careful attention to avoid consequences spanning a range of disease-related, individual, societal, economic, and security domains."
http://arxiv.org/abs/2011.03996v3,Do We Exploit all Information for Counterfactual Analysis? Benefits of Factor Models and Idiosyncratic Correction,2020-11-08 15:07:48+00:00,"['Jianqing Fan', 'Ricardo P. Masini', 'Marcelo C. Medeiros']",econ.EM,"Optimal pricing, i.e., determining the price level that maximizes profit or revenue of a given product, is a vital task for the retail industry. To select such a quantity, one needs first to estimate the price elasticity from the product demand. Regression methods usually fail to recover such elasticities due to confounding effects and price endogeneity. Therefore, randomized experiments are typically required. However, elasticities can be highly heterogeneous depending on the location of stores, for example. As the randomization frequently occurs at the municipal level, standard difference-in-differences methods may also fail. Possible solutions are based on methodologies to measure the effects of treatments on a single (or just a few) treated unit(s) based on counterfactuals constructed from artificial controls. For example, for each city in the treatment group, a counterfactual may be constructed from the untreated locations. In this paper, we apply a novel high-dimensional statistical method to measure the effects of price changes on daily sales from a major retailer in Brazil. The proposed methodology combines principal components (factors) and sparse regressions, resulting in a method called Factor-Adjusted Regularized Method for Treatment evaluation (\texttt{FarmTreat}). The data consist of daily sales and prices of five different products over more than 400 municipalities. The products considered belong to the \emph{sweet and candies} category and experiments have been conducted over the years of 2016 and 2017. Our results confirm the hypothesis of a high degree of heterogeneity yielding very different pricing strategies over distinct municipalities."
http://arxiv.org/abs/2011.07131v3,Rank Determination in Tensor Factor Model,2020-11-13 21:04:47+00:00,"['Yuefeng Han', 'Rong Chen', 'Cun-Hui Zhang']",stat.ME,"Factor model is an appealing and effective analytic tool for high-dimensional time series, with a wide range of applications in economics, finance and statistics. This paper develops two criteria for the determination of the number of factors for tensor factor models where the signal part of an observed tensor time series assumes a Tucker decomposition with the core tensor as the factor tensor. The task is to determine the dimensions of the core tensor. One of the proposed criteria is similar to information based criteria of model selection, and the other is an extension of the approaches based on the ratios of consecutive eigenvalues often used in factor analysis for panel time series. Theoretically results, including sufficient conditions and convergence rates, are established. The results include the vector factor models as special cases, with an additional convergence rates. Simulation studies provide promising finite sample performance for the two criteria."
http://arxiv.org/abs/2011.05840v4,Selling two complementary goods,2020-11-11 15:08:38+00:00,"['Komal Malik', 'Kolagani Paramahamsa']",econ.TH,"A seller is selling a pair of divisible complementary goods to an agent. The agent consumes the goods only in a specific ratio and freely disposes of excess in either goods. The value of the bundle and the ratio are private information of the agent. In this two-dimensional type space model, we characterize the incentive constraints and show that the optimal (expected revenue-maximizing) mechanism is a ratio-dependent posted price or a posted price mechanism for a class of distributions. We also show that the optimal mechanism is a posted price mechanism when the value and the ratio are independently distributed."
http://arxiv.org/abs/2010.04814v5,When Is Parallel Trends Sensitive to Functional Form?,2020-10-09 21:25:43+00:00,"['Jonathan Roth', ""Pedro H. C. Sant'Anna""]",econ.EM,"This paper assesses when the validity of difference-in-differences depends on functional form. We provide a novel characterization: the parallel trends assumption holds under all strictly monotonic transformations of the outcome if and only if a stronger ``parallel trends''-type condition holds for the cumulative distribution function of untreated potential outcomes. This condition for parallel trends to be insensitive to functional form is satisfied if and essentially only if the population can be partitioned into a subgroup for which treatment is effectively randomly assigned and a remaining subgroup for which the distribution of untreated potential outcomes is stable over time. These conditions have testable implications, and we introduce falsification tests for the null that parallel trends is insensitive to functional form."
http://arxiv.org/abs/2012.08133v4,Kicking You When You're Already Down: The Multipronged Impact of Austerity on Crime,2020-12-15 07:39:39+00:00,"['Corrado Giulietti', 'Brendon McConnell']",econ.GN,"The UK Welfare Reform Act 2012 imposed a series of deep welfare cuts, which disproportionately affected ex-ante poorer areas. In this paper, we provide the first evidence of the impact of these austerity measures on two different but complementary elements of crime -- the crime rate and the less-studied concentration of crime -- over the period 2011-2015 in England and Wales, and document four new facts. First, areas more exposed to the welfare reforms experience increased levels of crime, an effect driven by a rise in violent crime. Second, both violent and property crime become more concentrated within an area due to the welfare reforms. Third, it is ex-ante more deprived neighborhoods that bear the brunt of the crime increases over this period. Fourth, we find no evidence that the welfare reforms increased recidivism, suggesting that the changes in crime we find are likely driven by new criminals. Combining these results, we document unambiguous evidence of a negative spillover of the welfare reforms at the heart of the UK government's austerity program on social welfare, which reinforced the direct inequality-worsening effect of this program. Guided by a hedonic house price model, we calculate the welfare effects implied by the cuts in order to provide a financial quantification of the impact of the reform. We document an implied welfare loss of the policy -- borne by the public -- that far exceeds the savings made to government coffers."
http://arxiv.org/abs/2010.05087v2,Proportional resource allocation in dynamic n-player Blotto games,2020-10-10 20:37:10+00:00,"['Nejat Anbarcı', 'Kutay Cingiz', 'Mehmet S. Ismail']",econ.TH,"A variety of social, economic, and political interactions have long been modelled after Blotto games. In this paper, we introduce a general model of dynamic $n$-player Blotto contests. The players have asymmetric resources, and the battlefield prizes are not necessarily homogeneous. Each player's probability of winning the prize in a battlefield is governed by a contest success function and players' resource allocation on that battlefield. We show that there exists a subgame perfect equilibrium in which players allocate their resources proportional to the battlefield prizes for every history. This result is robust to exogenous resource shocks throughout the game."
http://arxiv.org/abs/2010.13438v2,Pooling for First and Last Mile: Integrating Carpooling and Transit,2020-10-26 09:18:21+00:00,"['Andrea Araldo', 'André de Palma', 'Souhila Arib', 'Vincent Gauthier', 'Romain Sere', 'Youssef Chaabouni', 'Oussama Kharouaa', 'Ado Adamou Abba Ari']",cs.MA,"While carpooling is widely adopted for long travels, it is by construction inefficient for daily commuting, where it is difficult to match drivers and riders, sharing similar origin, destination and time. To overcome this limitation, we present an Integrated system, which integrates carpooling into transit, in the line of the philosophy of Mobility as a Service. Carpooling acts as feeder to transit and transit stations act as consolidation points, where trips of riders and drivers meet, increasing potential matching. We present algorithms to construct multimodal rider trips (including transit and carpooling legs) and driver detours. Simulation shows that our Integrated system increases transit ridership and reduces auto-dependency, with respect to current practice, in which carpooling and transit are operated separately. Indeed, the Integrated system decreases the number of riders who are left with no feasible travel option and would thus be forced to use private cars. The simulation code is available as open source."
http://arxiv.org/abs/2012.11649v3,On the Aggregation of Probability Assessments: Regularized Mixtures of Predictive Densities for Eurozone Inflation and Real Interest Rates,2020-12-21 19:22:29+00:00,"['Francis X. Diebold', 'Minchul Shin', 'Boyuan Zhang']",econ.EM,"We propose methods for constructing regularized mixtures of density forecasts. We explore a variety of objectives and regularization penalties, and we use them in a substantive exploration of Eurozone inflation and real interest rate density forecasts. All individual inflation forecasters (even the ex post best forecaster) are outperformed by our regularized mixtures. From the Great Recession onward, the optimal regularization tends to move density forecasts' probability mass from the centers to the tails, correcting for overconfidence."
http://arxiv.org/abs/2010.01105v8,Policy evaluation of waste pricing programs using heterogeneous causal effect estimation,2020-10-02 17:03:26+00:00,['Marica Valente'],econ.GN,"Using machine learning methods in a quasi-experimental setting, I study the heterogeneous effects of introducing waste prices - unit prices on household unsorted waste disposal on - waste demands, municipal costs and pollution. Using a unique panel of Italian municipalities with large variation in prices and observables, I show that waste demands are nonlinear. I find evidence of constant elasticities at low prices, and increasing elasticities at high prices driven by income effects and waste habits before policy. The policy reduces waste management costs and pollution in all municipalities after three years of adoption, when prices cause significant waste avoidance."
http://arxiv.org/abs/2010.11644v1,Theory-based residual neural networks: A synergy of discrete choice models and deep neural networks,2020-10-22 12:31:01+00:00,"['Shenhao Wang', 'Baichuan Mo', 'Jinhua Zhao']",cs.LG,"Researchers often treat data-driven and theory-driven models as two disparate or even conflicting methods in travel behavior analysis. However, the two methods are highly complementary because data-driven methods are more predictive but less interpretable and robust, while theory-driven methods are more interpretable and robust but less predictive. Using their complementary nature, this study designs a theory-based residual neural network (TB-ResNet) framework, which synergizes discrete choice models (DCMs) and deep neural networks (DNNs) based on their shared utility interpretation. The TB-ResNet framework is simple, as it uses a ($δ$, 1-$δ$) weighting to take advantage of DCMs' simplicity and DNNs' richness, and to prevent underfitting from the DCMs and overfitting from the DNNs. This framework is also flexible: three instances of TB-ResNets are designed based on multinomial logit model (MNL-ResNets), prospect theory (PT-ResNets), and hyperbolic discounting (HD-ResNets), which are tested on three data sets. Compared to pure DCMs, the TB-ResNets provide greater prediction accuracy and reveal a richer set of behavioral mechanisms owing to the utility function augmented by the DNN component in the TB-ResNets. Compared to pure DNNs, the TB-ResNets can modestly improve prediction and significantly improve interpretation and robustness, because the DCM component in the TB-ResNets stabilizes the utility functions and input gradients. Overall, this study demonstrates that it is both feasible and desirable to synergize DCMs and DNNs by combining their utility specifications under a TB-ResNet framework. Although some limitations remain, this TB-ResNet framework is an important first step to create mutual benefits between DCMs and DNNs for travel behavior modeling, with joint improvement in prediction, interpretation, and robustness."
http://arxiv.org/abs/2011.06528v5,Treatment Allocation with Strategic Agents,2020-11-12 17:40:53+00:00,['Evan Munro'],econ.EM,"There is increasing interest in allocating treatments based on observed individual characteristics: examples include targeted marketing, individualized credit offers, and heterogeneous pricing. Treatment personalization introduces incentives for individuals to modify their behavior to obtain a better treatment. Strategic behavior shifts the joint distribution of covariates and potential outcomes. The optimal rule without strategic behavior allocates treatments only to those with a positive Conditional Average Treatment Effect. With strategic behavior, we show that the optimal rule can involve randomization, allocating treatments with less than 100% probability even to those who respond positively on average to the treatment. We propose a sequential experiment based on Bayesian Optimization that converges to the optimal treatment rule without parametric assumptions on individual strategic behavior."
http://arxiv.org/abs/2010.12415v2,Exploring investor behavior in Bitcoin: a study of the disposition effect,2020-10-23 14:06:19+00:00,"['Jürgen E. Schatzmann', 'Bernhard Haslhofer']",econ.GN,"Investors commonly exhibit the disposition effect - the irrational tendency to sell their winning investments and hold onto their losing ones. While this phenomenon has been observed in many traditional markets, it remains unclear whether it also applies to atypical markets like cryptoassets. This paper investigates the prevalence of the disposition effect in Bitcoin using transactions targeting cryptoasset exchanges as proxies for selling transactions. Our findings suggest that investors in Bitcoin were indeed subject to the disposition effect, with varying intensity. They also show that the disposition effect was not consistently present throughout the observation period. Its prevalence was more evident from the boom and bust year 2017 onwards, as confirmed by various technical indicators. Our study suggests irrational investor behavior is also present in atypical markets like Bitcoin."
http://arxiv.org/abs/2012.09422v4,The Variational Method of Moments,2020-12-17 07:21:06+00:00,"['Andrew Bennett', 'Nathan Kallus']",cs.LG,"The conditional moment problem is a powerful formulation for describing structural causal parameters in terms of observables, a prominent example being instrumental variable regression. A standard approach reduces the problem to a finite set of marginal moment conditions and applies the optimally weighted generalized method of moments (OWGMM), but this requires we know a finite set of identifying moments, can still be inefficient even if identifying, or can be theoretically efficient but practically unwieldy if we use a growing sieve of moment conditions. Motivated by a variational minimax reformulation of OWGMM, we define a very general class of estimators for the conditional moment problem, which we term the variational method of moments (VMM) and which naturally enables controlling infinitely-many moments. We provide a detailed theoretical analysis of multiple VMM estimators, including ones based on kernel methods and neural nets, and provide conditions under which these are consistent, asymptotically normal, and semiparametrically efficient in the full conditional moment model. We additionally provide algorithms for valid statistical inference based on the same kind of variational reformulations, both for kernel- and neural-net-based varieties. Finally, we demonstrate the strong performance of our proposed estimation and inference algorithms in a detailed series of synthetic experiments."
http://arxiv.org/abs/2010.04265v1,Debreu's open gap lemma for semiorders,2020-10-01 22:30:52+00:00,['A. Estevan'],econ.TH,"The problem of finding a (continuous) utility function for a semiorder has been studied since in 1956 R.D. Luce introduced in \emph{Econometrica} the notion. There was almost no results on the continuity of the representation. A similar result to Debreu's Lemma, but for semiorders, was never achieved. Recently, some necessary conditions for the existence of a continuous representation as well as some conjectures were presented by A. Estevan. In the present paper we prove these conjectures, achieving the desired version of Debreu's Open Gap Lemma for bounded semiorders. This result allows to remove the open-closed and closed-open gaps of a subset $S\subseteq \mathbb{R}$, but now keeping the constant threshold, so that $x+1<y$ if and only if $g(x)+1<g(y) \, (x,y\in S)$. Therefore, the continuous representation (in the sense of Scott-Suppes) of bounded semiorders is characterized. These results are achieved thanks to the key notion of $ε$-continuity, which generalizes the idea of continuity for semiorders."
http://arxiv.org/abs/2011.09052v3,Visual Time Series Forecasting: An Image-driven Approach,2020-11-18 02:51:37+00:00,"['Srijan Sood', 'Zhen Zeng', 'Naftali Cohen', 'Tucker Balch', 'Manuela Veloso']",cs.CV,"Time series forecasting is essential for agents to make decisions. Traditional approaches rely on statistical methods to forecast given past numeric values. In practice, end-users often rely on visualizations such as charts and plots to reason about their forecasts. Inspired by practitioners, we re-imagine the topic by creating a novel framework to produce visual forecasts, similar to the way humans intuitively do. In this work, we leverage advances in deep learning to extend the field of time series forecasting to a visual setting. We capture input data as an image and train a model to produce the subsequent image. This approach results in predicting distributions as opposed to pointwise values. We examine various synthetic and real datasets with diverse degrees of complexity. Our experiments show that visual forecasting is effective for cyclic data but somewhat less for irregular data such as stock price. Importantly, when using image-based evaluation metrics, we find the proposed visual forecasting method to outperform various numerical baselines, including ARIMA and a numerical variation of our method. We demonstrate the benefits of incorporating vision-based approaches in forecasting tasks -- both for the quality of the forecasts produced, as well as the metrics that can be used to evaluate them."
http://arxiv.org/abs/2011.01380v2,Instrumental Variable Identification of Dynamic Variance Decompositions,2020-11-02 23:32:44+00:00,"['Mikkel Plagborg-Møller', 'Christian K. Wolf']",econ.EM,"Macroeconomists increasingly use external sources of exogenous variation for causal inference. However, unless such external instruments (proxies) capture the underlying shock without measurement error, existing methods are silent on the importance of that shock for macroeconomic fluctuations. We show that, in a general moving average model with external instruments, variance decompositions for the instrumented shock are interval-identified, with informative bounds. Various additional restrictions guarantee point identification of both variance and historical decompositions. Unlike SVAR analysis, our methods do not require invertibility. Applied to U.S. data, they give a tight upper bound on the importance of monetary shocks for inflation dynamics."
http://arxiv.org/abs/2012.02708v3,A Multivariate Realized GARCH Model,2020-12-04 16:32:21+00:00,"['Ilya Archakov', 'Peter Reinhard Hansen', 'Asger Lunde']",econ.EM,"We propose a novel class of multivariate GARCH models that incorporate realized measures of volatility and correlations. The key innovation is an unconstrained vector parametrization of the conditional correlation matrix, which enables the use of factor models for correlations. This approach elegantly addresses the main challenge faced by multivariate GARCH models in high-dimensional settings. As an illustration, we explore block correlation matrices that naturally simplify to linear factor models for the conditional correlations. The model is applied to the returns of nine assets, and its in-sample and out-of-sample performance compares favorably against several popular benchmarks."
http://arxiv.org/abs/2012.04591v4,Are Men Less Generous to a Smarter Woman? Evidence from a Dictator Game Experiment,2020-12-08 17:46:05+00:00,['Yuki Takahashi'],econ.GN,"Although evidence suggests men are more generous to women than to men, it may stem from paternalism and could reverse when women excel in important skills for one's career success, such as cognitive skills. Using a dictator game, this paper studies whether male dictators allocate less to female receivers than to male receivers when these receivers have higher IQs than dictators. By exogenously varying the receivers' IQ relative to the dictators', I do not find evidence consistent with this hypothesis; if anything, male dictators allocate slightly more to female receivers with higher IQs than to male receivers with equivalent IQs. The results hold both in mean and distribution and are robust to the so-called ``beauty premium.'' Also, female dictators' allocations are qualitatively similar to male dictators. These findings suggest that women who excel in cognitive skills may not receive less favorable treatment than equally intelligent men in the labor market."
http://arxiv.org/abs/2101.05847v5,Using Monotonicity Restrictions to Identify Models with Partially Latent Covariates,2021-01-14 19:53:32+00:00,"['Minji Bang', 'Wayne Yuan Gao', 'Andrew Postlewaite', 'Holger Sieg']",econ.EM,"This paper develops a new method for identifying econometric models with partially latent covariates. Such data structures arise in industrial organization and labor economics settings where data are collected using an input-based sampling strategy, e.g., if the sampling unit is one of multiple labor input factors. We show that the latent covariates can be nonparametrically identified, if they are functions of a common shock satisfying some plausible monotonicity assumptions. With the latent covariates identified, semiparametric estimation of the outcome equation proceeds within a standard IV framework that accounts for the endogeneity of the covariates. We illustrate the usefulness of our method using a new application that focuses on the production functions of pharmacies. We find that differences in technology between chains and independent pharmacies may partially explain the observed transformation of the industry structure."
http://arxiv.org/abs/2103.12779v2,Identification at the Zero Lower Bound,2021-03-23 18:24:13+00:00,['Sophocles Mavroeidis'],econ.EM,"I show that the Zero Lower Bound (ZLB) on interest rates can be used to identify the causal effects of monetary policy. Identification depends on the extent to which the ZLB limits the efficacy of monetary policy. I propose a simple way to test the efficacy of unconventional policies, modelled via a `shadow rate'. I apply this method to U.S. monetary policy using a three-equation SVAR model of inflation, unemployment and the federal funds rate. I reject the null hypothesis that unconventional monetary policy has no effect at the ZLB, but find some evidence that it is not as effective as conventional monetary policy."
http://arxiv.org/abs/2102.04758v1,Lowest-cost virus suppression,2021-02-09 11:18:30+00:00,"['Jacob Janssen', 'Yaneer Bar-Yam']",econ.GN,"Analysis of policies for managing epidemics require simultaneously an economic and epidemiological perspective. We adopt a cost-of-policy framework to model both the virus spread and the cost of handling the pandemic. Because it is harder and more costly to fight the pandemic when the circulation is higher, we find that the optimal policy is to go to zero or near-zero case numbers. Without imported cases, if a region is willing to implement measures to prevent spread at one level in number of cases, it must also be willing to prevent the spread with at a lower level, since it will be cheaper to do so and has only positive other effects. With imported cases, if a region is not coordinating with other regions, we show the cheapest policy is continually low but nonzero cases due to decreasing cost of halting imported cases. When it is coordinating, zero is cost-optimal. Our analysis indicates that within Europe cooperation targeting a reduction of both within country transmission, and between country importation risk, should help achieve lower transmission and reduced costs."
http://arxiv.org/abs/2103.02754v5,Beyond Unbounded Beliefs: How Preferences and Information Interplay in Social Learning,2021-03-03 23:31:19+00:00,"['Navin Kartik', 'SangMok Lee', 'Tianhao Liu', 'Daniel Rappoport']",econ.TH,"When does society eventually learn the truth, or take the correct action, via observational learning? In a general model of sequential learning over social networks, we identify a simple condition for learning dubbed excludability. Excludability is a joint property of agents' preferences and their information. We develop two classes of preferences and information that jointly satisfy excludability: (i) for a one-dimensional state, preferences with single-crossing differences and a new informational condition, directionally unbounded beliefs; and (ii) for a multi-dimensional state, intermediate preferences and subexponential location-shift information. These applications exemplify that with multiple states ""unbounded beliefs"" is not only unnecessary for learning, but incompatible with familiar informational structures like normal information. Unbounded beliefs demands that a single agent can identify the correct action. Excludability, on the other hand, only requires that a single agent must be able to displace any wrong action, even if she cannot take the correct action."
http://arxiv.org/abs/2101.09543v2,Inference on the New Keynesian Phillips Curve with Very Many Instrumental Variables,2021-01-23 17:56:44+00:00,['Max-Sebastian Dovì'],econ.GN,"Limited-information inference on New Keynesian Phillips Curves (NKPCs) and other single-equation macroeconomic relations is characterised by weak and high-dimensional instrumental variables (IVs). Beyond the efficiency concerns previously raised in the literature, I show by simulation that ad-hoc selection procedures can lead to substantial biases in post-selection inference. I propose a Sup Score test that remains valid under dependent data, arbitrarily weak identification, and a number of IVs that increases exponentially with the sample size. Conducting inference on a standard NKPC with 359 IVs and 179 observations, I find substantially wider confidence sets than those commonly found."
http://arxiv.org/abs/2101.09373v3,Relief and Stimulus in A Cross-sector Multi-product Scarce Resource Supply Chain Network,2021-01-22 22:48:41+00:00,"['Xiaowei Hu', 'Peng Li']",econ.TH,"In the era of a growing population, systemic changes to the world, and the rising risk of crises, humanity has been facing an unprecedented challenge of resource scarcity. Confronting and addressing the issues concerning the scarce resource's conservation, competition, and stimulation by grappling its characteristics and adopting viable policy instruments calls the decision-maker's attention with a paramount priority. In this paper, we develop the first general decentralized cross-sector supply chain network model that captures the unique features of scarce resources under a unifying fiscal policy scheme. We formulate the problem as a network equilibrium model with finite-dimensional variational inequality theories. We then characterize the network equilibrium with a set of classic theoretical properties, as well as with a set of properties that are novel to the network games application literature, namely, the lowest eigenvalue of the game Jacobian. Lastly, we provide a series of illustrative examples, including a medical glove supply network, to showcase how our model can be used to investigate the efficacy of the imposed policies in relieving supply chain distress and stimulating welfare. Our managerial insights inform and expand the political dialogues on fiscal policy design, public resource legislation, social welfare redistribution, and supply chain practice toward sustainability."
http://arxiv.org/abs/2102.07222v7,Exclusion of Extreme Jurors and Minority Representation: The Effect of Jury Selection Procedures,2021-02-14 18:56:46+00:00,"['Andrea Moro', 'Martin Van der Linden']",econ.GN,"We compare two jury selection procedures meant to safeguard against the inclusion of biased jurors that are perceived as causing minorities to be under-represented. The Strike and Replace procedure presents potential jurors one-by-one to the parties, while the Struck procedure presents all potential jurors before the parties exercise their challenges. Struck more effectively excludes extreme jurors but leads to a worse representation of minorities. The advantage of Struck in terms of excluding extremes is sizable in a wide range of cases. In contrast, Strike and Replace better represents minorities only if the minority and majority are polarized. Results are robust to assuming the parties statistically discriminate against jurors based on group identity."
http://arxiv.org/abs/2101.10941v1,Identifying and Estimating Perceived Returns to Binary Investments,2021-01-26 17:10:41+00:00,['Clint Harris'],econ.EM,"I describe a method for estimating agents' perceived returns to investments that relies on cross-sectional data containing binary choices and prices, where prices may be imperfectly known to agents. This method identifies the scale of perceived returns by assuming agent knowledge of an identity that relates profits, revenues, and costs rather than by eliciting or assuming agent beliefs about structural parameters that are estimated by researchers. With this assumption, modest adjustments to standard binary choice estimators enable consistent estimation of perceived returns when using price instruments that are uncorrelated with unobserved determinants of agents' price misperceptions as well as other unobserved determinants of their perceived returns. I demonstrate the method, and the importance of using price variation that is known to agents, in a series of data simulations."
http://arxiv.org/abs/2101.01170v3,"Better Bunching, Nicer Notching",2021-01-04 18:58:39+00:00,"['Marinho Bertanha', 'Andrew H. McCallum', 'Nathan Seegert']",econ.EM,"This paper studies the bunching identification strategy for an elasticity parameter that summarizes agents' responses to changes in slope (kink) or intercept (notch) of a schedule of incentives. We show that current bunching methods may be very sensitive to implicit assumptions in the literature about unobserved individual heterogeneity. We overcome this sensitivity concern with new non- and semi-parametric estimators. Our estimators allow researchers to show how bunching elasticities depend on different identifying assumptions and when elasticities are robust to them. We follow the literature and derive our methods in the context of the iso-elastic utility model and an income tax schedule that creates a piece-wise linear budget constraint. We demonstrate bunching behavior provides robust estimates for self-employed and not-married taxpayers in the context of the U.S. Earned Income Tax Credit. In contrast, estimates for self-employed and married taxpayers depend on specific identifying assumptions, which highlight the value of our approach. We provide the Stata package ""bunching"" to implement our procedures."
http://arxiv.org/abs/2103.00734v2,Welfare v. Consent: On the Optimal Penalty for Harassment,2021-03-01 03:52:41+00:00,"['Ratul Das Chaudhury', 'Birendra Rai', 'Liang Choon Wang', 'Dyuti Banerjee']",econ.GN,The economic approach to determine optimal legal policies involves maximizing a social welfare function. We propose an alternative: a consent-approach that seeks to promote consensual interactions and deter non-consensual interactions. The consent-approach does not rest upon inter-personal utility comparisons or value judgments about preferences. It does not require any additional information relative to the welfare-approach. We highlight the contrast between the welfare-approach and the consent-approach using a stylized model inspired by seminal cases of harassment and the #MeToo movement. The social welfare maximizing penalty for harassment in our model can be zero under the welfare-approach but not under the consent-approach.
http://arxiv.org/abs/2103.00591v1,Epidemics with Behavior,2021-02-28 19:11:31+00:00,"['Satoshi Fukuda', 'Nenad Kos', 'Christoph Wolf']",econ.GN,"We study equilibrium distancing during epidemics. Distancing reduces the individual's probability of getting infected but comes at a cost. It creates a single-peaked epidemic, flattens the curve and decreases the size of the epidemic. We examine more closely the effects of distancing on the outset, the peak and the final size of the epidemic. First, we define a behavioral basic reproduction number and show that it is concave in the transmission rate. The infection, therefore, spreads only if the transmission rate is in the intermediate region. Second, the peak of the epidemic is non-monotonic in the transmission rate. A reduction in the transmission rate can lead to an increase of the peak. On the other hand, a decrease in the cost of distancing always flattens the curve. Third, both an increase in the infection rate as well as an increase in the cost of distancing increase the size of the epidemic. Our results have important implications on the modeling of interventions. Imposing restrictions on the infection rate has qualitatively different effects on the trajectory of the epidemics than imposing assumptions on the cost of distancing. The interventions that affect interactions rather than the transmission rate should, therefore, be modeled as changes in the cost of distancing."
http://arxiv.org/abs/2102.10145v2,Learning Epidemiology by Doing: The Empirical Implications of a Spatial-SIR Model with Behavioral Responses,2021-02-19 20:16:20+00:00,"['Alberto Bisin', 'Andrea Moro']",econ.GN,"We simulate a spatial behavioral model of the diffusion of an infection to understand the role of geographic characteristics: the number and distribution of outbreaks, population size, density, and agents' movements. We show that several invariance properties of the SIR model concerning these variables do not hold when agents interact with neighbors in a (two dimensional) geographical space. Indeed, the spatial model's local interactions generate matching frictions and local herd immunity effects, which play a fundamental role in the infection dynamics. We also show that geographical factors affect how behavioral responses affect the epidemics. We derive relevant implications for estimating the effects of the epidemics and policy interventions that use panel data from several geographical units."
http://arxiv.org/abs/2103.13789v3,Spatial-SIR with Network Structure and Behavior: Lockdown Rules and the Lucas Critique,2021-03-25 12:30:00+00:00,"['Alberto Bisin', 'Andrea Moro']",econ.GN,"We introduce a model of the diffusion of an epidemic with demographically heterogeneous agents interacting socially on a spatially structured network. Contagion-risk averse agents respond behaviorally to the diffusion of the infections by limiting their social interactions. Schools and workplaces also respond by allowing students and employees to attend and work remotely. The spatial structure induces local herd immunities along socio-demographic dimensions, which significantly affect the dynamics of infections. We study several non-pharmaceutical interventions; e.g., i) lockdown rules, which set thresholds on the spread of the infection for the closing and reopening of economic activities; ii) neighborhood lockdowns, leveraging granular (neighborhood-level) information to improve the effectiveness public health policies; iii) selective lockdowns, which restrict social interactions by location (in the network) and by the demographic characteristics of the agents. Substantiating a ""Lucas critique"" argument, we assess the cost of naive discretionary policies ignoring agents and firms' behavioral responses."
http://arxiv.org/abs/2103.11051v1,Differentiation in a Two-Dimensional Market with Endogenous Sequential Entry,2021-03-19 22:27:00+00:00,"['Jeffrey D. Michler', 'Benjamin M. Gramig']",econ.GN,"Previous research on two-dimensional extensions of Hotelling's location game has argued that spatial competition leads to maximum differentiation in one dimensions and minimum differentiation in the other dimension. We expand on existing models to allow for endogenous entry into the market. We find that competition may lead to the min/max finding of previous work but also may lead to maximum differentiation in both dimensions. The critical issue in determining the degree of differentiation is if existing firms are seeking to deter entry of a new firm or to maximizing profits within an existing, stable market."
http://arxiv.org/abs/2103.11504v5,Purchase history and product personalization,2021-03-21 22:17:35+00:00,"['Laura Doval', 'Vasiliki Skreta']",econ.TH,"Product personalization opens the door to price discrimination. A rich product line allows firms to better tailor products to consumers' tastes, but the mere choice of a product carries valuable information about consumers that can be leveraged for price discrimination. We study this trade-off in an upstream-downstream model, where a consumer buys a good of variable quality upstream, followed by an indivisible good downstream. The downstream firm's use of the consumer's purchase history for price discrimination introduces a novel distortion: The upstream firm offers a subset of the products that it would offer if, instead, it could jointly design its product line and downstream pricing. By controlling the degree of product personalization the upstream firm curbs ratcheting forces that result from the consumer facing downstream price discrimination."
http://arxiv.org/abs/2102.08174v1,LATE for History,2021-02-16 14:22:44+00:00,"['Alberto Bisin', 'Andrea Moro']",econ.GN,"In Historical Economics, Persistence studies document the persistence of some historical phenomenon or leverage this persistence to identify causal relationships of interest in the present. In this chapter, we analyze the implications of allowing for heterogeneous treatment effects in these studies. We delineate their common empirical structure, argue that heterogeneous treatment effects are likely in their context, and propose minimal abstract models that help interpret results and guide the development of empirical strategies to uncover the mechanisms generating the effects."
http://arxiv.org/abs/2103.01412v2,Some Finite Sample Properties of the Sign Test,2021-03-02 01:51:20+00:00,['Yong Cai'],econ.EM,"This paper contains two finite-sample results concerning the sign test. First, we show that the sign-test is unbiased with independent, non-identically distributed data for both one-sided and two-sided hypotheses. The proof for the two-sided case is based on a novel argument that relates the derivatives of the power function to a regular bipartite graph. Unbiasedness then follows from the existence of perfect matchings on such graphs. Second, we provide a simple theoretical counterexample to show that the sign test over-rejects when the data exhibits correlation. Our results can be useful for understanding the properties of approximate randomization tests in settings with few clusters."
http://arxiv.org/abs/2101.00399v8,The Law of Large Numbers for Large Stable Matchings,2021-01-02 08:05:05+00:00,"['Jacob Schwartz', 'Kyungchul Song']",econ.EM,"In many empirical studies of a large two-sided matching market (such as in a college admissions problem), the researcher performs statistical inference under the assumption that they observe a random sample from a large matching market. In this paper, we consider a setting in which the researcher observes either all or a nontrivial fraction of outcomes from a stable matching. We establish a concentration inequality for empirical matching probabilities assuming strong correlation among the colleges' preferences while allowing students' preferences to be fully heterogeneous. Our concentration inequality yields laws of large numbers for the empirical matching probabilities and other statistics commonly used in empirical analyses of a large matching market. To illustrate the usefulness of our concentration inequality, we prove consistency for estimators of conditional matching probabilities and measures of positive assortative matching."
http://arxiv.org/abs/2103.07066v2,Finding Subgroups with Significant Treatment Effects,2021-03-12 03:36:03+00:00,"['Jann Spiess', 'Vasilis Syrgkanis', 'Victor Yaneng Wang']",econ.EM,"Researchers often run resource-intensive randomized controlled trials (RCTs) to estimate the causal effects of interventions on outcomes of interest. Yet these outcomes are often noisy, and estimated overall effects can be small or imprecise. Nevertheless, we may still be able to produce reliable evidence of the efficacy of an intervention by finding subgroups with significant effects. In this paper, we propose a machine-learning method that is specifically optimized for finding such subgroups in noisy data. Unlike available methods for personalized treatment assignment, our tool is fundamentally designed to take significance testing into account: it produces a subgroup that is chosen to maximize the probability of obtaining a statistically significant positive treatment effect. We provide a computationally efficient implementation using decision trees and demonstrate its gain over selecting subgroups based on positive (estimated) treatment effects. Compared to standard tree-based regression and classification tools, this approach tends to yield higher power in detecting subgroups affected by the treatment."
http://arxiv.org/abs/2103.06928v3,Conditional strategy equilibrium,2021-03-11 19:59:57+00:00,"['Lorenzo Bastianello', 'Mehmet S. Ismail']",econ.TH,"In this note, we prove the existence of an equilibrium concept, dubbed conditional strategy equilibrium, for non-cooperative games in which a strategy of a player is a function from the other players' actions to her own actions. We study the properties of efficiency and coalition-proofness of the conditional strategy equilibrium in $n$-person games."
http://arxiv.org/abs/2103.16681v4,On-Chain Auctions with Deposits,2021-03-30 21:05:29+00:00,"['Jan Christoph Schlegel', 'Akaki Mamageishvili']",cs.GT,"Second-price auctions with deposits are frequently used in blockchain environments. An auction takes place on-chain: bidders deposit an amount that fully covers their bid (but possibly exceeds it) in a smart contract. The deposit is used as insurance against bidders not honoring their bid if they win. The deposit, but not the bid, is publicly observed during the bidding phase of the auction.
  The visibility of deposits can fundamentally change the strategic structure of the auction if bidding happens sequentially: Bidding is costly since deposit are costly to make. Thus, deposits can be used as a costly signal for a high valuation. This is the source of multiple inefficiencies: To engage in costly signalling, a bidder who bids first and has a high valuation will generally over-deposit in equilibrium, i.e.~deposit more than he will bid. If high valuations are likely there can, moreover, be entry deterrence through high deposits: a bidder who bids first can deter subsequent bidders from entering the auction. Pooling can happen in equilibrium, where bidders of different valuations deposit the same amount. The auction fails to allocate the item to the bidder with the highest valuation."
http://arxiv.org/abs/2103.11075v2,Limited Cognitive Abilities and Dominance Hierarchy,2021-03-20 02:45:13+00:00,"['Hanyuan Huang', 'Jiabin Wu']",q-bio.PE,"We propose a novel model to explain the mechanisms underlying dominance hierarchical structures. Guided by a predetermined social convention, agents with limited cognitive abilities optimize their strategies in a Hawk-Dove game. We find that several commonly observed hierarchical structures in nature such as linear hierarchy and despotism, emerge as the total fitness-maximizing social structures given different levels of cognitive abilities."
http://arxiv.org/abs/2102.00143v2,Dynamic Random Choice,2021-01-30 03:32:56+00:00,['Ricky Li'],econ.TH,"I study dynamic random utility with finite choice sets and exogenous total menu variation, which I refer to as stochastic utility (SU). First, I characterize SU when each choice set has three elements. Next, I prove several mathematical identities for joint, marginal, and conditional Block--Marschak sums, which I use to obtain two characterizations of SU when each choice set but the last has three elements. As a corollary under the same cardinality restrictions, I sharpen an axiom to obtain a characterization of SU with full support over preference tuples. I conclude by characterizing SU without cardinality restrictions. All of my results hold over an arbitrary finite discrete time horizon."
http://arxiv.org/abs/2102.11429v3,Ambiguity and Partial Bayesian Updating,2021-02-23 00:13:55+00:00,['Matthew Kovach'],econ.TH,"Models of updating a set of priors either do not allow a decision maker to make inference about her priors (full bayesian updating or FB) or require an extreme degree of selection (maximum likelihood updating or ML). I characterize a general method for updating a set of priors, partial bayesian updating (PB), in which the decision maker (i) utilizes an event-dependent threshold to determine whether a prior is likely enough, conditional on observed information, and then (ii) applies Bayes' rule to the sufficiently likely priors. I show that PB nests FB and ML and explore its behavioral properties."
http://arxiv.org/abs/2103.00295v2,Nash equilibrium mapping vs Hamiltonian dynamics vs Darwinian evolution for some social dilemma games in the thermodynamic limit,2021-02-27 19:13:49+00:00,"['Colin Benjamin', 'Arjun Krishnan U M']",cond-mat.stat-mech,"How cooperation evolves and manifests itself in the thermodynamic or infinite player limit of social dilemma games is a matter of intense speculation. Various analytical methods have been proposed to analyze the thermodynamic limit of social dilemmas. In this work, we compare two analytical methods, i.e., Darwinian evolution and Nash equilibrium mapping, with a numerical agent-based approach. For completeness, we also give results for another analytical method, Hamiltonian dynamics. In contrast to Hamiltonian dynamics, which involves the maximization of payoffs of all individuals, in Darwinian evolution, the payoff of a single player is maximized with respect to its interaction with the nearest neighbor. While the Hamiltonian dynamics method utterly fails as compared to Nash equilibrium mapping, the Darwinian evolution method gives a false positive for game magnetization -- the net difference between the fraction of cooperators and defectors -- when payoffs obey the condition a + d = b + c, wherein a,d represents the diagonal elements and b,c the off-diagonal elements in a symmetric social dilemma game payoff matrix. When either a + d =/= b + c or when one looks at the average payoff per player, the Darwinian evolution method fails, much like the Hamiltonian dynamics approach. On the other hand, the Nash equilibrium mapping and numerical agent-based method agree well for both game magnetization and average payoff per player for the social dilemmas in question, i.e., the Hawk-Dove game and the Public goods game. This paper thus brings to light the inconsistency of the Darwinian evolution method vis-a-vis both Nash equilibrium mapping and a numerical agent-based approach."
http://arxiv.org/abs/2102.04543v3,Sharp Sensitivity Analysis for Inverse Propensity Weighting via Quantile Balancing,2021-02-08 21:47:23+00:00,"['Jacob Dorn', 'Kevin Guo']",math.ST,"Inverse propensity weighting (IPW) is a popular method for estimating treatment effects from observational data. However, its correctness relies on the untestable (and frequently implausible) assumption that all confounders have been measured. This paper introduces a robust sensitivity analysis for IPW that estimates the range of treatment effects compatible with a given amount of unobserved confounding. The estimated range converges to the narrowest possible interval (under the given assumptions) that must contain the true treatment effect. Our proposal is a refinement of the influential sensitivity analysis by Zhao, Small, and Bhattacharya (2019), which we show gives bounds that are too wide even asymptotically. This analysis is based on new partial identification results for Tan (2006)'s marginal sensitivity model."
http://arxiv.org/abs/2102.12061v2,Deep Video Prediction for Time Series Forecasting,2021-02-24 04:27:23+00:00,"['Zhen Zeng', 'Tucker Balch', 'Manuela Veloso']",cs.CV,"Time series forecasting is essential for decision making in many domains. In this work, we address the challenge of predicting prices evolution among multiple potentially interacting financial assets. A solution to this problem has obvious importance for governments, banks, and investors. Statistical methods such as Auto Regressive Integrated Moving Average (ARIMA) are widely applied to these problems. In this paper, we propose to approach economic time series forecasting of multiple financial assets in a novel way via video prediction. Given past prices of multiple potentially interacting financial assets, we aim to predict the prices evolution in the future. Instead of treating the snapshot of prices at each time point as a vector, we spatially layout these prices in 2D as an image, such that we can harness the power of CNNs in learning a latent representation for these financial assets. Thus, the history of these prices becomes a sequence of images, and our goal becomes predicting future images. We build on a state-of-the-art video prediction method for forecasting future images. Our experiments involve the prediction task of the price evolution of nine financial assets traded in U.S. stock markets. The proposed method outperforms baselines including ARIMA, Prophet, and variations of the proposed method, demonstrating the benefits of harnessing the power of CNNs in the problem of economic time series forecasting."
http://arxiv.org/abs/2102.11929v4,PolicySpace2: modeling markets and endogenous public policies,2021-02-23 20:29:59+00:00,['Bernardo Alves Furtado'],cs.MA,"Policymakers decide on alternative policies facing restricted budgets and uncertain, ever-changing future. Designing public policies is further difficult due to the need to decide on priorities and handle effects across policies. Housing policies, specifically, involve heterogeneous characteristics of properties themselves and the intricacy of housing markets and the spatial context of cities. We propose PolicySpace2 (PS2) as an adapted and extended version of the open source PolicySpace agent-based model. PS2 is a computer simulation that relies on empirically detailed spatial data to model real estate, along with labor, credit, and goods and services markets. Interaction among workers, firms, a bank, households and municipalities follow the literature benchmarks to integrate economic, spatial and transport scholarship. PS2 is applied to a comparison among three competing public policies aimed at reducing inequality and alleviating poverty: (a) house acquisition by the government and distribution to lower income households, (b) rental vouchers, and (c) monetary aid. Within the model context, the monetary aid, that is, smaller amounts of help for a larger number of households, makes the economy perform better in terms of production, consumption, reduction of inequality, and maintenance of financial duties. PS2 as such is also a framework that may be further adapted to a number of related research questions."
http://arxiv.org/abs/2102.11780v2,Hierarchical Regularizers for Mixed-Frequency Vector Autoregressions,2021-02-23 16:30:15+00:00,"['Alain Hecq', 'Marie Ternes', 'Ines Wilms']",econ.EM,"Mixed-frequency Vector AutoRegressions (MF-VAR) model the dynamics between variables recorded at different frequencies. However, as the number of series and high-frequency observations per low-frequency period grow, MF-VARs suffer from the ""curse of dimensionality"". We curb this curse through a regularizer that permits hierarchical sparsity patterns by prioritizing the inclusion of coefficients according to the recency of the information they contain. Additionally, we investigate the presence of nowcasting relations by sparsely estimating the MF-VAR error covariance matrix. We study predictive Granger causality relations in a MF-VAR for the U.S. economy and construct a coincident indicator of GDP growth. Supplementary Materials for this article are available online."
http://arxiv.org/abs/2102.12454v1,Regional and Sectoral Structures and Their Dynamics of Chinese Economy: A Network Perspective from Multi-Regional Input-Output Tables,2021-02-24 18:38:03+00:00,"['Tao Wang', 'Shiying Xiao', 'Jun Yan', 'Panpan Zhang']",physics.soc-ph,"A multi-regional input-output table (MRIOT) containing the transactions among the region-sectors in an economy defines a weighted and directed network. Using network analysis tools, we analyze the regional and sectoral structure of the Chinese economy and their temporal dynamics from 2007 to 2012 via the MRIOTs of China. Global analyses are done with network topology measures. Growth-driving province-sector clusters are identified with community detection methods. Influential province-sectors are ranked by weighted PageRank scores. The results revealed a few interesting and telling insights. The level of inter-province-sector activities increased with the rapid growth of the national economy, but not as fast as that of intra-province economic activities. Regional community structures were deeply associated with geographical factors. The community heterogeneity across the regions was high and the regional fragmentation increased during the study period. Quantified metrics assessing the relative importance of the province-sectors in the national economy echo the national and regional economic development policies to a certain extent."
http://arxiv.org/abs/2102.12378v1,"Understanding the Farmers, Environmental Citizenship Behaviors Towards Climate Change. The Moderating Mediating Role of Environmental Knowledge and Ascribed Responsibility",2021-02-23 15:05:51+00:00,"['Immaculate Maumoh', 'Emmanuel H. Yindi']",econ.GN,"Knowledge is known to be a pre-condition for an individuals behavior. For the most efficient informational strategies for education, it is essential that we identify the types of knowledge that promote behavior effectively and investigate their structure. The purpose of this paper is therefore to examine the factors that affect Kenyan farmers, environmental citizenship behavior (ECB) in the context of Adaptation and mitigation (Climate smart agriculture). To achieve this objective, a theoretical framework has been developed based on value belief norm (VBN) theory. Design/methodology/approach, Data were obtained from 350 farmers using a survey method. Partial lease square structural equation modelling (PLS-SEM) was used to examine the hypothetical model. The results of PLS analysis confirm the direct and mediating effect of the causal sequences of the variables in the VBN model. The moderating role of Environmental knowledge has been seen to be impactful in Climate Smart Agriculture."
http://arxiv.org/abs/2103.11458v1,The Stable Marriage Problem: an Interdisciplinary Review from the Physicist's Perspective,2021-03-21 18:34:45+00:00,"['Enrico Maria Fenoaltea', 'Izat B. Baybusinov', 'Jianyang Zhao', 'Lei Zhou', 'Yi-Cheng Zhang']",physics.soc-ph,"We present a fascinating model that has lately caught attention among physicists working in complexity related fields. Though it originated from mathematics and later from economics, the model is very enlightening in many aspects that we shall highlight in this review. It is called The Stable Marriage Problem (though the marriage metaphor can be generalized to many other contexts), and it consists of matching men and women, considering preference-lists where individuals express their preference over the members of the opposite gender. This problem appeared for the first time in 1962 in the seminal paper of Gale and Shapley and has aroused interest in many fields of science, including economics, game theory, computer science, etc. Recently it has also attracted many physicists who, using the powerful tools of statistical mechanics, have also approached it as an optimization problem. Here we present a complete overview of the Stable Marriage Problem emphasizing its multidisciplinary aspect, and reviewing the key results in the disciplines that it has influenced most. We focus, in particular, in the old and recent results achieved by physicists, finally introducing two new promising models inspired by the philosophy of the Stable Marriage Problem. Moreover, we present an innovative reinterpretation of the problem, useful to highlight the revolutionary role of information in the contemporary economy."
http://arxiv.org/abs/2102.03239v1,Applications of Machine Learning in Document Digitisation,2021-02-05 15:35:28+00:00,"['Christian M. Dahl', 'Torben S. D. Johansen', 'Emil N. Sørensen', 'Christian E. Westermann', 'Simon F. Wittrock']",cs.CV,"Data acquisition forms the primary step in all empirical research. The availability of data directly impacts the quality and extent of conclusions and insights. In particular, larger and more detailed datasets provide convincing answers even to complex research questions. The main problem is that 'large and detailed' usually implies 'costly and difficult', especially when the data medium is paper and books. Human operators and manual transcription have been the traditional approach for collecting historical data. We instead advocate the use of modern machine learning techniques to automate the digitisation process. We give an overview of the potential for applying machine digitisation for data collection through two illustrative applications. The first demonstrates that unsupervised layout classification applied to raw scans of nurse journals can be used to construct a treatment indicator. Moreover, it allows an assessment of assignment compliance. The second application uses attention-based neural networks for handwritten text recognition in order to transcribe age and birth and death dates from a large collection of Danish death certificates. We describe each step in the digitisation pipeline and provide implementation insights."
http://arxiv.org/abs/2102.02379v1,"Airport Capacity and Performance in Europe -- A study of transport economics, service quality and sustainability",2021-02-04 02:42:36+00:00,['Branko Bubalo'],econ.GN,"The purpose of this dissertation is to present an overview of the operational and financial performance of airports in Europe. In benchmarking studies, airports are assessed and compared with other airports based on key indicators from a technical and an economic point of view. The interest lies primarily in the question, which key figures can best measure the perception of quality of service from the point of view of the passenger for the services at an airport."
http://arxiv.org/abs/2102.03436v3,"Non-rationalizable Individuals, Stochastic Rationalizability, and Sampling",2021-02-05 22:17:14+00:00,"['Changkuk Im', 'John Rehbeck']",econ.TH,"Experimental work regularly finds that individual choices are not deterministically rationalized by well-defined preferences. Nonetheless, recent work shows that data collected from many individuals can be stochastically rationalized by a distribution of well-defined preferences. We study the relationship between deterministic and stochastic rationalizability. We show that a population can be stochastically rationalized even when half of the individuals in the population cannot be deterministically rationalized. We also find the ability to detect individuals who are not deterministically rationalized from population level data can decrease as the number of observations increases."
http://arxiv.org/abs/2103.05453v1,Portfolio risk allocation through Shapley value,2021-03-09 14:36:16+00:00,"['Patrick S. Hagan', 'Andrew Lesniewski', 'Georgios E. Skoufis', 'Diana E. Woodward']",q-fin.CP,"We argue that using the Shapley value of cooperative game theory as the scheme for risk allocation among non-orthogonal risk factors is a natural way of interpreting the contribution made by each of such factors to overall portfolio risk. We discuss a Shapley value scheme for allocating risk to non-orthogonal greeks in a portfolio of derivatives. Such a situation arises, for example, when using a stochastic volatility model to capture option volatility smile. We also show that Shapley value allows for a natural method of interpreting components of enterprise risk measures such as VaR and ES. For all applications discussed, we derive explicit formulas and / or numerical algorithms to calculate the allocations."
http://arxiv.org/abs/2103.06691v3,Regression based thresholds in principal loading analysis,2021-03-11 14:25:44+00:00,"['J. O. Bauer', 'B. Drabant']",math.ST,"Principal loading analysis is a dimension reduction method that discards variables which have only a small distorting effect on the covariance matrix. As a special case, principal loading analysis discards variables that are not correlated with the remaining ones. In multivariate linear regression on the other hand, predictors that are neither correlated with both the remaining predictors nor with the dependent variables have a regression coefficients equal to zero. Hence, if the goal is to select a number of predictors, variables that do not correlate are discarded as it is also done in principal loading analysis. That both methods select the same variables occurs not only for the special case of zero correlation however. We contribute conditions under which both methods share the same variable selection. Further, we extend those conditions to provide a choice for the threshold in principal loading analysis which only follows recommendations based on simulation results so far."
http://arxiv.org/abs/2103.06530v1,Assessment of the Effectiveness of State Participation in Economic Clusters,2021-03-11 08:47:41+00:00,"['A. R. Baghirzade', 'B. Kushbakov']",econ.GN,"In the article we made an attempt to reveal the contents and development of the concept of economic clusters, to characterize the specificity of the regional cluster as a project. We have identified features of an estimation of efficiency of state participation in the cluster, where the state is an institution representing the interests of society."
http://arxiv.org/abs/2103.06227v1,Sensitivity analysis of an integrated climate-economic model,2021-03-10 17:57:13+00:00,"['Benjamin M. Bolker', 'Matheus R. Grasselli', 'Emma Holmes']",econ.GN,"We conduct a sensitivity analysis of a new type of integrated climate-economic model recently proposed in the literature, where the core economic component is based on the Goodwin-Keen dynamics instead of a neoclassical growth model. Because these models can exhibit much richer behaviour, including multiple equilibria, runaway trajectories and unbounded oscillations, it is crucial to determine how sensitive they are to changes in underlying parameters. We focus on four economic parameters (markup rate, speed of price adjustments, coefficient of money illusion, growth rate of productivity) and two climate parameters (size of upper ocean reservoir, equilibrium climate sensitivity) and show how their relative effects on the outcomes of the model can be quantified by methods that can be applied to an arbitrary number of parameters."
http://arxiv.org/abs/2102.03043v1,The Refined Assortment Optimization Problem,2021-02-05 07:59:34+00:00,"['Gerardo Berbeglia', 'Alvaro Flores', 'Guillermo Gallego']",econ.TH,"We introduce the refined assortment optimization problem where a firm may decide to make some of its products harder to get instead of making them unavailable as in the traditional assortment optimization problem. Airlines, for example, offer fares with severe restrictions rather than making them unavailable. This is a more subtle way of handling the trade-off between demand induction and demand cannibalization. For the latent class MNL model, a firm that engages in refined assortment optimization can make up to $\min(n,m)$ times more than one that insists on traditional assortment optimization, where $n$ is the number of products and $m$ the number of customer types. Surprisingly, the revenue-ordered assortment heuristic has the same performance guarantees relative to {\em personalized} refined assortment optimization as it does to traditional assortment optimization. Based on this finding, we construct refinements of the revenue-order heuristic and measure their improved performance relative to the revenue-ordered assortment and the optimal traditional assortment optimization problem. We also provide tight bounds on the ratio of the expected revenues for the refined versus the traditional assortment optimization for some well known discrete choice models."
http://arxiv.org/abs/2102.05876v3,On the Fragility of Third-party Punishment: The Context Effect of a Dominated Risky Investment Option,2021-02-11 07:40:31+00:00,"['Changkuk Im', 'Jinkwon Lee']",econ.GN,"Experimental studies regularly show that third-party punishment (TPP) substantially exists in various settings. This study further investigates the robustness of TPP under an environment where context effects are involved. In our experiment, we offer a third party an additional but unattractive risky investment option. We find that, when the dominated investment option irrelevant to prosocial behavior is available, the demand for punishment decreases, whereas the demand for investment increases. These findings support our hypothesis that the seemingly unrelated and dominated investment option may work as a compromise and suggest the fragility of TPP in this setting."
http://arxiv.org/abs/2102.06075v2,Local Utility and Multivariate Risk Aversion,2021-02-08 15:35:54+00:00,"['Arthur Charpentier', 'Alfred Galichon', 'Marc Henry']",econ.TH,"We revisit Machina's local utility as a tool to analyze attitudes to multivariate risks. We show that for non-expected utility maximizers choosing between multivariate prospects, aversion to multivariate mean preserving increases in risk is equivalent to the concavity of the local utility functions, thereby generalizing Machina's result in Machina (1982). To analyze comparative risk attitudes within the multivariate extension of rank dependent expected utility of Galichon and Henry (2011), we extend Quiggin's monotone mean and utility preserving increases in risk and show that the useful characterization given in Landsberger and Meilijson (1994) still holds in the multivariate case."
http://arxiv.org/abs/2102.02884v1,How the Massachusetts Assault Weapons Ban Enforcement Notice Changed Firearm Sales,2021-02-04 20:55:31+00:00,"['Meenakshi Balakrishna', 'Kenneth C. Wilbur']",econ.GN,"The Massachusetts Attorney General issued an Enforcement Notice in 2016 to announce a new interpretation of a key phrase in the state's assault weapons ban. The Enforcement Notice increased sales of tagged assault rifles by 616% in the first 5 days, followed by a 9% decrease over the next three weeks. Sales of Handguns and Shotguns did not change significantly. Tagged assault rifle sales fell 28-30% in 2017 compared to previous years, suggesting that the Enforcement Notice reduced assault weapon sales but also that many banned weapons continued to be sold. Tagged assault rifles sold most in 2017 in zip codes with higher household incomes and proportions of white males. Overall, the results suggest that the firearm market reacts rapidly to policy changes and partially complies with firearm restrictions."
http://arxiv.org/abs/2102.06232v1,Inference on two component mixtures under tail restrictions,2021-02-11 19:27:47+00:00,"['Marc Henry', 'Koen Jochmans', 'Bernard Salanié']",econ.EM,"Many econometric models can be analyzed as finite mixtures. We focus on two-component mixtures and we show that they are nonparametrically point identified by a combination of an exclusion restriction and tail restrictions. Our identification analysis suggests simple closed-form estimators of the component distributions and mixing proportions, as well as a specification test. We derive their asymptotic properties using results on tail empirical processes and we present a simulation study that documents their finite-sample performance."
http://arxiv.org/abs/2103.04123v2,Signaling and Employer Learning with Instruments,2021-03-06 14:35:04+00:00,"['Gaurab Aryal', 'Manudeep Bhuller', 'Fabian Lange']",econ.GN,"This paper considers the use of instruments to identify and estimate private and social returns to education within a model of employer learning. What an instrument identifies depends on whether it is hidden from, or transparent (i.e., observed) to, the employers. A hidden instrument identifies private returns to education, and a transparent instrument identifies social returns to education. We use variation in compulsory schooling laws across non-central and central municipalities in Norway to, respectively, construct hidden and transparent instruments. We estimate a private return of 7.9%, of which 70% is due to increased productivity and the remaining 30% is due to signaling."
http://arxiv.org/abs/2102.10528v1,A Novel Multi-Period and Multilateral Price Index,2021-02-21 06:44:18+00:00,"['Consuelo Rubina Nava', 'Maria Grazia Zoia']",econ.EM,"A novel approach to price indices, leading to an innovative solution in both a multi-period or a multilateral framework, is presented. The index turns out to be the generalized least squares solution of a regression model linking values and quantities of the commodities. The index reference basket, which is the union of the intersections of the baskets of all country/period taken in pair, has a coverage broader than extant indices. The properties of the index are investigated and updating formulas established. Applications to both real and simulated data provide evidence of the better index performance in comparison with extant alternatives."
http://arxiv.org/abs/2102.10347v1,Mechanism Design Powered by Social Interactions,2021-02-20 13:46:05+00:00,['Dengji Zhao'],cs.GT,"Mechanism design has traditionally assumed that the set of participants are fixed and known to the mechanism (the market owner) in advance. However, in practice, the market owner can only directly reach a small number of participants (her neighbours). Hence the owner often needs costly promotions to recruit more participants in order to get desirable outcomes such as social welfare or revenue maximization. In this paper, we propose to incentivize existing participants to invite their neighbours to attract more participants. However, they would not invite each other if they are competitors. We discuss how to utilize the conflict of interest between the participants to incentivize them to invite each other to form larger markets. We will highlight the early solutions and open the floor for discussing the fundamental open questions in the settings of auctions, coalitional games, matching and voting."
http://arxiv.org/abs/2103.11933v3,PatentSBERTa: A Deep NLP based Hybrid Model for Patent Distance and Classification using Augmented SBERT,2021-03-22 15:23:19+00:00,"['Hamid Bekamiri', 'Daniel S. Hain', 'Roman Jurowetzki']",cs.LG,"This study provides an efficient approach for using text data to calculate patent-to-patent (p2p) technological similarity, and presents a hybrid framework for leveraging the resulting p2p similarity for applications such as semantic search and automated patent classification. We create embeddings using Sentence-BERT (SBERT) based on patent claims. We leverage SBERTs efficiency in creating embedding distance measures to map p2p similarity in large sets of patent data. We deploy our framework for classification with a simple Nearest Neighbors (KNN) model that predicts Cooperative Patent Classification (CPC) of a patent based on the class assignment of the K patents with the highest p2p similarity. We thereby validate that the p2p similarity captures their technological features in terms of CPC overlap, and at the same demonstrate the usefulness of this approach for automatic patent classification based on text data. Furthermore, the presented classification framework is simple and the results easy to interpret and evaluate by end-users. In the out-of-sample model validation, we are able to perform a multi-label prediction of all assigned CPC classes on the subclass (663) level on 1,492,294 patents with an accuracy of 54% and F1 score > 66%, which suggests that our model outperforms the current state-of-the-art in text-based multi-label and multi-class patent classification. We furthermore discuss the applicability of the presented framework for semantic IP search, patent landscaping, and technology intelligence. We finally point towards a future research agenda for leveraging multi-source patent embeddings, their appropriateness across applications, as well as to improve and validate patent embeddings by creating domain-expert curated Semantic Textual Similarity (STS) benchmark datasets."
http://arxiv.org/abs/2103.12193v2,Understanding Factors that Influence Upskilling,2021-03-22 21:50:39+00:00,"['Eduardo Laguna-Muggenburg', 'Monica Bhole', 'Michael Meaney']",econ.GN,"We investigate the motivation and means through which individuals expand their skill-set by analyzing a survey of applicants from the Facebook Jobs product. Individuals who report being influenced by their networks or local economy are over 29% more likely to have a postsecondary degree, but peer effects still exist among those who do not acknowledge such influences. Users with postsecondary degrees are more likely to upskill in general, by continuing coursework or applying to higher-skill jobs, though the latter is more common among users across all education backgrounds. These findings indicate that policies aimed at connecting individuals with different educational backgrounds can encourage upskilling. Policies that encourage users to enroll in coursework may not be as effective among individuals with a high school degree or less. Instead, connecting such individuals to opportunities that value skills acquired outside of a formal education, and allow for on-the-job training, may be more effective."
http://arxiv.org/abs/2102.04658v1,View about consumption tax and grandchildren,2021-02-09 05:57:58+00:00,['Eiji Yamamura'],econ.GN,"In Japan, the increase in the consumption tax rate, a measure of balanced public finance, reduces the inequality of fiscal burden between the present and future generations. This study estimates the effect of grandchildren on an older person's view of consumption tax, using independently collected data. The results show that having grandchildren is positively associated with supporting an increase in consumption tax. Further, this association is observed strongly between granddaughters and grandparents. However, the association between grandsons and grandparents depends on the sub-sample. This implies that people of the old generation are likely to accept the tax burden to reduce the burden on their grandchildren, especially granddaughters. In other words, grandparents show intergenerational altruism."
http://arxiv.org/abs/2102.08994v1,Big Data meets Causal Survey Research: Understanding Nonresponse in the Recruitment of a Mixed-mode Online Panel,2021-02-17 19:37:53+00:00,"['Barbara Felderer', 'Jannis Kueck', 'Martin Spindler']",stat.ME,"Survey scientists increasingly face the problem of high-dimensionality in their research as digitization makes it much easier to construct high-dimensional (or ""big"") data sets through tools such as online surveys and mobile applications. Machine learning methods are able to handle such data, and they have been successfully applied to solve \emph{predictive} problems. However, in many situations, survey statisticians want to learn about \emph{causal} relationships to draw conclusions and be able to transfer the findings of one survey to another. Standard machine learning methods provide biased estimates of such relationships. We introduce into survey statistics the double machine learning approach, which gives approximately unbiased estimators of causal parameters, and show how it can be used to analyze survey nonresponse in a high-dimensional panel setting."
http://arxiv.org/abs/2102.04384v2,"Efficient, Fair, and Incentive-Compatible Healthcare Rationing",2021-02-08 17:45:52+00:00,"['Haris Aziz', 'Florian Brandl']",cs.GT,"Rationing of healthcare resources has emerged as an important issue, which has been discussed by medical experts, policy-makers, and the general public. We consider a rationing problem where medical units are to be allocated to patients. Each unit is reserved for one of several categories and each category has a priority ranking of the patients. We present an allocation rule that respects the priorities, complies with the eligibility requirements, allocates the largest feasible number of units, and does not incentivize agents to hide that they qualify through a category. The rule characterizes all possible allocations that satisfy the first three properties and is polynomial-time computable."
http://arxiv.org/abs/2102.07286v2,A Theory of Choice Bracketing under Risk,2021-02-15 00:41:34+00:00,['Mu Zhang'],econ.TH,"Aggregating risks from multiple sources can be complex and demanding, and decision makers usually adopt heuristics to simplify the evaluation process. This paper axiomatizes two closed related and yet different heuristics, narrow bracketing and correlation neglect, by relaxing the independence axiom in the expected utility theory. The flexibility of our framework allows for applications in various economic problems. First, our model can explain the experimental evidence of narrow bracketing over monetary gambles. Second, when one source represents background risk, we can accommodate Rabin (2000)'s critique and explain risk aversion over small gambles. Finally, when different sources represent consumptions in different periods, we unify three seemingly distinct models of time preferences and propose a novel model that simultaneously satisfies indifference to temporal resolution of uncertainty, separation of time and risk preferences, and recursivity in the domain of lotteries. As a direct application to macroeconomics and finance, we provide an alternative to Epstein and Zin (1989) which avoids the unreasonably high timing premium discussed in Epstein, Farhi, and Strzalecki (2014)."
http://arxiv.org/abs/2102.06586v1,Linear programming approach to nonparametric inference under shape restrictions: with an application to regression kink designs,2021-02-12 15:52:05+00:00,"['Harold D. Chiang', 'Kengo Kato', 'Yuya Sasaki', 'Takuya Ura']",econ.EM,"We develop a novel method of constructing confidence bands for nonparametric regression functions under shape constraints. This method can be implemented via a linear programming, and it is thus computationally appealing. We illustrate a usage of our proposed method with an application to the regression kink design (RKD). Econometric analyses based on the RKD often suffer from wide confidence intervals due to slow convergence rates of nonparametric derivative estimators. We demonstrate that economic models and structures motivate shape restrictions, which in turn contribute to shrinking the confidence interval for an analysis of the causal effects of unemployment insurance benefits on unemployment durations."
http://arxiv.org/abs/2103.00095v1,The Cost of Pollution in the Upper Atoyac River Basin: A Systematic Review,2021-02-27 01:07:05+00:00,"['Maria Eugenia Ibarraran', 'Romeo A. Saldana-Vazquez', 'Tamara Perez-Garcia']",econ.GN,"The Atoyac River is among the two most polluted in Mexico. Water quality in the Upper Atoyac River Basin (UARB) has been devastated by industrial and municipal wastewater, as well as from effluents from local dwellers, that go through little to no treatment, affecting health, production, ecosystems and property value. We did a systematic review and mapping of the costs that pollution imposes on different sectors and localities in the UARB, and initially found 358 studies, of which 17 were of our particular interest. We focus on estimating the cost of pollution through different valuation methods such as averted costs, hedonic pricing, and contingent valuation, and for that we only use 10 studies. Costs range from less than a million to over $16 million dollars a year, depending on the sector, with agriculture, industry and tourism yielding the highest costs. This exercise is the first of its kind in the UARB that maps costs for sectors and localities affected, and sheds light on the need of additional research to estimate the total cost of pollution throughout the basin. This information may help design further research needs in the region."
http://arxiv.org/abs/2102.13157v1,Does Bankruptcy Protection Affect Asset Prices? Evidence from changes in Homestead Exemptions,2021-02-25 20:32:45+00:00,"['Yildiray Yildirim', 'Albert Alex Zevelev']",econ.GN,"Does the ability to protect an asset from unsecured creditors affect its price? This paper identifies the impact of bankruptcy protection on house prices using 139 changes in homestead exemptions. Large increases in the homestead exemption raised house prices 3% before 2005. Smaller exemption increases, to adjust for inflation, did not affect house prices. The effect disappeared after BAPCPA, a 2005 federal law designed to prevent bankruptcy abuse. The effect was bigger in inelastic locations."
http://arxiv.org/abs/2102.13464v1,Granddaughter and voting for a female candidate,2021-02-24 23:22:12+00:00,['Eiji Yamamura'],econ.GN,"This study examines the influence of grandchildren's gender on grandparents' voting behavior using independently collected individual-level data. The survey was conducted immediately after the House of Councilors election in Japan. I observed that individuals with a granddaughter were more likely to vote for female candidates by around 10 % than those without. However, having a daughter did not affect the parents' voting behavior. Furthermore, having a son or a grandson did not influence grandparents' voting behavior. This implies that grandparents voted for their granddaughter's future benefit because granddaughters may be too young vote in a male-dominated and aging society."
http://arxiv.org/abs/2102.13482v2,Information Design in Multi-stage Games,2021-02-26 13:58:01+00:00,"['Miltiadis Makris', 'Ludovic Renou']",econ.TH,"This paper generalizes the concept of Bayes correlated equilibrium (Bergemann and Morris, 2016) to multi-stage games. We demonstrate the power of our characterization results by applying them to a number of illustrative examples and applications."
http://arxiv.org/abs/2102.12811v1,Matching with Trade-offs: Revealed Preferences over Competing Characteristics,2021-02-25 12:27:16+00:00,"['Alfred Galichon', 'Bernard Salanié']",econ.GN,"We investigate in this paper the theory and econometrics of optimal matchings with competing criteria. The surplus from a marriage match, for instance, may depend both on the incomes and on the educations of the partners, as well as on characteristics that the analyst does not observe. Even if the surplus is complementary in incomes, and complementary in educations, imperfect correlation between income and education at the individual level implies that the social optimum must trade off matching on incomes and matching on educations. Given a flexible specification of the surplus function, we characterize under mild assumptions the properties of the set of feasible matchings and of the socially optimal matching. Then we show how data on the covariation of the types of the partners in observed matches can be used to test that the observed matches are socially optimal for this specification, and to estimate the parameters that define social preferences over matches."
http://arxiv.org/abs/2103.10251v3,Optimal Targeting in Fundraising: A Causal Machine-Learning Approach,2021-03-10 23:06:35+00:00,"['Tobias Cagala', 'Ulrich Glogowsky', 'Johannes Rincke', 'Anthony Strittmatter']",econ.EM,"Ineffective fundraising lowers the resources charities can use to provide goods. We combine a field experiment and a causal machine-learning approach to increase a charity's fundraising effectiveness. The approach optimally targets a fundraising instrument to individuals whose expected donations exceed solicitation costs. Our results demonstrate that machine-learning-based optimal targeting allows the charity to substantially increase donations net of fundraising costs relative to uniform benchmarks in which either everybody or no one receives the gift. To that end, it (a) should direct its fundraising efforts to a subset of past donors and (b) never address individuals who were previously asked but never donated. Further, we show that the benefits of machine-learning-based optimal targeting even materialize when the charity only exploits publicly available geospatial information or applies the estimated optimal targeting rule to later fundraising campaigns conducted in similar samples. We conclude that charities not engaging in optimal targeting waste significant resources."
http://arxiv.org/abs/2102.10756v3,Equilibrium Price Formation with a Major Player and its Mean Field Limit,2021-02-22 03:43:34+00:00,"['Masaaki Fujii', 'Akihiko Takahashi']",q-fin.MF,"In this article, we consider the problem of equilibrium price formation in an incomplete securities market consisting of one major financial firm and a large number of minor firms. They carry out continuous trading via the securities exchange to minimize their cost while facing idiosyncratic and common noises as well as stochastic order flows from their individual clients. The equilibrium price process that balances demand and supply of the securities, including the functional form of the price impact for the major firm, is derived endogenously both in the market of finite population size and in the corresponding mean field limit."
http://arxiv.org/abs/2103.00711v1,Panel semiparametric quantile regression neural network for electricity consumption forecasting,2021-03-01 02:47:26+00:00,"['Xingcai Zhou', 'Jiangyan Wang']",stat.ML,"China has made great achievements in electric power industry during the long-term deepening of reform and opening up. However, the complex regional economic, social and natural conditions, electricity resources are not evenly distributed, which accounts for the electricity deficiency in some regions of China. It is desirable to develop a robust electricity forecasting model. Motivated by which, we propose a Panel Semiparametric Quantile Regression Neural Network (PSQRNN) by utilizing the artificial neural network and semiparametric quantile regression. The PSQRNN can explore a potential linear and nonlinear relationships among the variables, interpret the unobserved provincial heterogeneity, and maintain the interpretability of parametric models simultaneously. And the PSQRNN is trained by combining the penalized quantile regression with LASSO, ridge regression and backpropagation algorithm. To evaluate the prediction accuracy, an empirical analysis is conducted to analyze the provincial electricity consumption from 1999 to 2018 in China based on three scenarios. From which, one finds that the PSQRNN model performs better for electricity consumption forecasting by considering the economic and climatic factors. Finally, the provincial electricity consumptions of the next $5$ years (2019-2023) in China are reported by forecasting."
http://arxiv.org/abs/2103.01201v1,Can Machine Learning Catch the COVID-19 Recession?,2021-03-01 18:47:46+00:00,"['Philippe Goulet Coulombe', 'Massimiliano Marcellino', 'Dalibor Stevanovic']",econ.EM,"Based on evidence gathered from a newly built large macroeconomic data set for the UK, labeled UK-MD and comparable to similar datasets for the US and Canada, it seems the most promising avenue for forecasting during the pandemic is to allow for general forms of nonlinearity by using machine learning (ML) methods. But not all nonlinear ML methods are alike. For instance, some do not allow to extrapolate (like regular trees and forests) and some do (when complemented with linear dynamic components). This and other crucial aspects of ML-based forecasting in unprecedented times are studied in an extensive pseudo-out-of-sample exercise."
http://arxiv.org/abs/2102.08378v1,The economic dependency of the Bitcoin security,2021-02-16 12:36:21+00:00,"['Pavel Ciaian', ""d'Artis Kancs"", 'Miroslava Rajcaniova']",econ.GN,"We study to what extent the Bitcoin blockchain security permanently depends on the underlying distribution of cryptocurrency market outcomes. We use daily blockchain and Bitcoin data for 2014-2019 and employ the ARDL approach. We test three equilibrium hypotheses: (i) sensitivity of the Bitcoin blockchain to mining reward; (ii) security outcomes of the Bitcoin blockchain and the proof-of-work cost; and (iii) the speed of adjustment of the Bitcoin blockchain security to deviations from the equilibrium path. Our results suggest that the Bitcoin price and mining rewards are intrinsically linked to Bitcoin security outcomes. The Bitcoin blockchain security's dependency on mining costs is geographically differenced - it is more significant for the global mining leader China than for other world regions. After input or output price shocks, the Bitcoin blockchain security reverts to its equilibrium security level."
http://arxiv.org/abs/2103.03045v3,Factor-Based Imputation of Missing Values and Covariances in Panel Data of Large Dimensions,2021-03-04 14:07:47+00:00,"['Ercument Cahan', 'Jushan Bai', 'Serena Ng']",econ.EM,"Economists are blessed with a wealth of data for analysis, but more often than not, values in some entries of the data matrix are missing. Various methods have been proposed to handle missing observations in a few variables. We exploit the factor structure in panel data of large dimensions. Our \textsc{tall-project} algorithm first estimates the factors from a \textsc{tall} block in which data for all rows are observed, and projections of variable specific length are then used to estimate the factor loadings. A missing value is imputed as the estimated common component which we show is consistent and asymptotically normal without further iteration. Implications for using imputed data in factor augmented regressions are then discussed.
  To compensate for the downward bias in covariance matrices created by an omitted noise when the data point is not observed, we overlay the imputed data with re-sampled idiosyncratic residuals many times and use the average of the covariances to estimate the parameters of interest. Simulations show that the procedures have desirable finite sample properties."
http://arxiv.org/abs/2101.00281v1,Exploring the Impact of COVID-19 in the Sustainability of Airbnb Business Model,2021-01-01 17:57:53+00:00,"['Rim Krouk', 'Fernando Almeida']",econ.GN,"Society is undergoing many transformations and faces economic crises, environmental, social, and public health issues. At the same time, the Internet, mobile communications, cloud technologies, and social networks are growing rapidly and fostering the digitalization processes of business and society. It is in this context that the shared economy has assumed itself as a new social and economic system based on the sharing of resources and has allowed the emergence of innovative businesses like Airbnb. However, COVID-19 has challenged this business model in the face of restrictions imposed in the tourism sector. Its consequences are not exclusively short-term and may also call into question the sustainability of Airbnb. In this sense, this study aims to explore the sustainability of the Airbnb business model considering two theories which advocate that hosts can cover the short-term financial effects, while another defends a paradigm shift in the demand for long-term accommodations to ensure greater stability for hosts."
http://arxiv.org/abs/2104.06179v1,We Live in a Motorized Civilization: Robert Moses Replies to Robert Caro,2021-03-26 20:22:00+00:00,['Geoff Boeing'],econ.GN,"In 1974, Robert Caro published The Power Broker, a critical biography of Robert Moses's dictatorial tenure as the ""master builder"" of mid-century New York. Moses profoundly transformed New York's urban fabric and transportation system, producing the Brooklyn Battery Tunnel, the Verrazano Narrows Bridge, the Westside Highway, the Cross-Bronx Expressway, the Lincoln Center, the UN headquarters, Shea Stadium, Jones Beach State Park and many other projects. However, The Power Broker did lasting damage to his public image and today he remains one of the most controversial figures in city planning history. On August 26, 1974, Moses issued a turgid 23-page statement denouncing Caro's work as ""full of mistakes, unsupported charges, nasty baseless personalities, and random haymakers."" Moses's original typewritten statement survives today as a grainy photocopy in the New York City Parks Department archive. To better preserve and disseminate it, I have extracted and transcribed its text using optical character recognition and edited the result to correct errors. Here I compile my transcription of Moses's statement, alongside Caro's reply to it."
http://arxiv.org/abs/2103.13965v3,Putting a price on tenure,2021-03-25 16:45:34+00:00,['Thiago Marzagao'],econ.GN,"Government employees in Brazil are granted tenure after three years on the job. Firing a tenured government employee is all but impossible, so tenure is a big employee benefit. But exactly how big is it? In other words: how much money is tenure worth to a government employee in Brazil? No one has ever attempted to answer that question. I do that in this paper. I use a modified version of the Sharpe ratio to estimate what the risk-adjusted salaries of government workers should be. The difference between actual salary and risk-adjusted salary gives us an estimate of how much tenure is worth to each employee. I find that in the 2005-2019 period the monthly value of tenure was 3980 reais to the median federal government employee, 1971 reais to the median state government employee, and 500 reais to the median municipal government employee."
http://arxiv.org/abs/2103.14762v1,"Public transport users versus private vehicle users: differences about quality of service, satisfaction and attitudes toward public transport in Madrid (Spain)",2021-03-26 23:01:13+00:00,"['Juan de Oña', 'Esperanza Estévez', 'Rocio de Oña']",econ.GN,"This paper aims to further understand the main factors influencing the behavioural intentions (BI) of private vehicle users towards public transport to provide policymakers and public transport operators with the tools they need to attract more private vehicle users. As service quality, satisfaction and attitudes towards public transport are considered the main motivational forces behind the BI of public transport users, this research analyses 26 indicators frequently associated with these constructs for both public transport users and private vehicle users. Non-parametric tests and ordinal logit models have been applied to an online survey asked in Madrid's metropolitan area with a sample size of 1,025 respondents (525 regular public transport users and 500 regular private vehicle users). In order to achieve a comprehensive analysis and to deal with heterogeneity in perceptions, 338 models have been developed for the entire sample and for 12 users' segments. The results led to the identification of indicators with no significant differences between public transport and private vehicle users in any of the segments being considered (punctuality, information and low-income), as well as those that did show significant differences in all the segments (proximity, intermodality, save time and money, and lifestyle). The main differences between public transport and private vehicle users were found in the attitudes towards public transport and for certain user segments (residents in the city centre, males, young, with university qualification and with incomes above 2,700EUR/month). Findings from this study can be used to develop policies and recommendations for persuading more private vehicle users to use the public transport services."
http://arxiv.org/abs/2103.16118v2,The Formation of Global Free Trade Agreement,2021-03-30 07:10:02+00:00,"['Akira Okada', 'Yasuhiro Shirata']",econ.TH,"We investigate the formation of Free Trade Agreement (FTA) in a competing importers framework with $n$ countries. We show that (i) FTA formation causes a negative externality to non-participants, (ii) a non-participant is willing to join an FTA, and (iii) new participation may decrease the welfare of incumbent participants. A unique subgame perfect equilibrium of a sequential FTA formation game does not achieve global free trade under an open-access rule where a new applicant needs consent of members for accession, currently employed by many open regionalism agreements including APEC. We further show that global FTA is a unique subgame perfect equilibrium under an open-access rule without consent."
http://arxiv.org/abs/2104.00129v1,Productivity development in the construction industry and human capital: a literature review,2021-03-31 21:46:49+00:00,"['Matthias Bahr', 'Leif Laszig']",econ.GN,"Tis paper is a literature review focusing on human capital, skills of employees, demographic change, management, training and their impact on productivity growth. Intrafirm behaviour has been recognized as a potentially important driver for productivity. Results from surveys show that management practices have become more structured, in the sense of involving more data collection and analysis. Furthermore, a strong positive correlation between the measured management quality and firm performance can be observed. Studies suggest that there is a positive association between management score and productivity growth. The lack or low level of employees' skills and qualifications might be in different ways a possible explanation for the observed slowdown of productivity growth. The main reason for the decline in skilled labor is the demographic change. Construction sectors are increasingly affected by demographic developments. Labour reserves in construction are largely exhausted. Shortage of qualified workforce is impacting project cost, schedules and quality."
http://arxiv.org/abs/2104.00029v1,Models and numbers: Representing the world or imposing order?,2021-03-31 18:01:54+00:00,"['Matthias Kaiser', 'Tatjana Buklijas', 'Peter Gluckman']",cs.CY,"We argue for a foundational epistemic claim and a hypothesis about the production and uses of mathematical epidemiological models, exploring the consequences for our political and socio-economic lives. First, in order to make the best use of scientific models, we need to understand why models are not truly representational of our world, but are already pitched towards various uses. Second, we need to understand the implicit power relations in numbers and models in public policy, and, thus, the implications for good governance if numbers and models are used as the exclusive drivers of decision making."
http://arxiv.org/abs/2103.14626v1,Divide-and-Conquer: A Distributed Hierarchical Factor Approach to Modeling Large-Scale Time Series Data,2021-03-26 17:40:48+00:00,"['Zhaoxing Gao', 'Ruey S. Tsay']",stat.ME,"This paper proposes a hierarchical approximate-factor approach to analyzing high-dimensional, large-scale heterogeneous time series data using distributed computing. The new method employs a multiple-fold dimension reduction procedure using Principal Component Analysis (PCA) and shows great promises for modeling large-scale data that cannot be stored nor analyzed by a single machine. Each computer at the basic level performs a PCA to extract common factors among the time series assigned to it and transfers those factors to one and only one node of the second level. Each 2nd-level computer collects the common factors from its subordinates and performs another PCA to select the 2nd-level common factors. This process is repeated until the central server is reached, which collects common factors from its direct subordinates and performs a final PCA to select the global common factors. The noise terms of the 2nd-level approximate factor model are the unique common factors of the 1st-level clusters. We focus on the case of 2 levels in our theoretical derivations, but the idea can easily be generalized to any finite number of hierarchies. We discuss some clustering methods when the group memberships are unknown and introduce a new diffusion index approach to forecasting. We further extend the analysis to unit-root nonstationary time series. Asymptotic properties of the proposed method are derived for the diverging dimension of the data in each computing unit and the sample size $T$. We use both simulated data and real examples to assess the performance of the proposed method in finite samples, and compare our method with the commonly used ones in the literature concerning the forecastability of extracted factors."
http://arxiv.org/abs/2103.16879v1,Optimal class assignment problem: a case study at Gunma University,2021-03-31 08:02:38+00:00,"['Akifumi Kira', 'Kiyohito Nagano', 'Manabu Sugiyama', 'Naoyuki Kamiyama']",cs.GT,"In this study, we consider the real-world problem of assigning students to classes, where each student has a preference list, ranking a subset of classes in order of preference. Though we use existing approaches to include the daily class assignment of Gunma University, new concepts and adjustments are required to find improved results depending on real instances in the field. Thus, we propose minimax-rank constrained maximum-utility matchings and a compromise between maximum-utility matchings and fair matchings, where a matching is said to be fair if it lexicographically minimizes the number of students assigned to classes not included in their choices, the number of students assigned to their last choices, and so on. In addition, we also observe the potential inefficiency of the student proposing deferred acceptance mechanism with single tie-breaking, which a hot topic in the literature on the school choice problem."
http://arxiv.org/abs/2103.13297v1,Making it normal for new enrollments: Effect of institutional and pandemic influence on selecting an engineering institution under the COVID-19 pandemic situation,2021-03-23 17:35:29+00:00,"['Prashant Mahajan', 'Vaishali Patil']",econ.GN,"The COVID19 pandemic has forced Indian engineering institutions (EIs) to bring their previous half shut shades completely down. Fetching new admissions to EI campuses during the pandemic has become a now or never situation for EIs. During crisis situations, institutions have struggled to return to the normal track. The pandemic has drastically changed students behavior and family preferences due to mental stress and the emotional life attached to it. Consequently, it becomes a prerequisite, and emergencies need to examine the choice characteristics influencing the selection of EI during the COVID19 pandemic situation.
  The purpose of this study is to critically examine institutional influence and pandemic influence due to COVID19 that affects students choice about an engineering institution (EI) and consequently to explore relationships between institutional and pandemic influence. The findings of this quantitative research, conducted through a self-reported survey, have revealed that institutional and pandemic influence have governed EI choice under the COVID19 pandemic. Second, pandemic influence is positively affected by institutional influence. The study demonstrated that EIs will have to reposition themselves to normalize pandemic influence by tuning institutional characteristics that regulate situational influence and new enrollments. It can be yardstick for policy makers to attract new enrollments under pandemic situations."
http://arxiv.org/abs/2103.13259v1,Mostly electric assisted airplanes (MEAP) for regional aviation: A South Asian perspective,2021-03-24 15:25:29+00:00,['Vinamra Chaturvedi'],eess.SY,"Aircraft manufacturing relies on pre-order bookings. The configuration of the to be assembled aircraft is fixed by the design assisted market surveys. The sensitivity of the supply chain to the market conditions, makes, the relationship between the product (aircraft) and the associated service (aviation), precarious. Traditional model to mitigate this risk to profitability rely on increasing the scales of operations. However, the emergence of new standards of air quality monitoring and insistence on the implementation, demands additional corrective measures. In the quest for a solution, this research commentary establishes a link, between the airport taxes and the nature of the transporting unit. It warns, that merely, increasing the number of mid haulage range aircrafts (MHA) in the fleet, may not be enough, to overcome this challenge. In a two-pronged approach, the communication proposes, the use of mostly electric assisted air planes, and small sized airports as the key to solving this complex problem. As a side-note the appropriateness of South Asian region, as a test-bed for MEAP based aircrafts is also investigated. The success of this the idea can be potentially extended, to any other aviation friendly region of the world."
http://arxiv.org/abs/2101.03598v1,Using the Econometric Models for Identification of Risk Factors for Albanian SMEs (Case study: SMEs of Gjirokastra region),2021-01-10 18:39:40+00:00,"['Lorenc Kociu', 'Kledian Kodra']",econ.GN,"Using the econometric models, this paper addresses the ability of Albanian Small and Medium-sized Enterprises (SMEs) to identify the risks they face. To write this paper, we studied SMEs operating in the Gjirokastra region. First, qualitative data gathered through a questionnaire was used. Next, the 5-level Likert scale was used to measure it. Finally, the data was processed through statistical software SPSS version 21, using the binary logistic regression model, which reveals the probability of occurrence of an event when all independent variables are included. Logistic regression is an integral part of a category of statistical models, which are called General Linear Models. Logistic regression is used to analyze problems in which one or more independent variables interfere, which influences the dichotomous dependent variable. In such cases, the latter is seen as the random variable and is dependent on them. To evaluate whether Albanian SMEs can identify risks, we analyzed the factors that SMEs perceive as directly affecting the risks they face. At the end of the paper, we conclude that Albanian SMEs can identify risk"
http://arxiv.org/abs/2101.03625v1,The 'COVID' Crash of the 2020 U.S. Stock Market,2021-01-10 20:48:01+00:00,"['Min Shu', 'Ruiqiang Song', 'Wei Zhu']",q-fin.RM,"We employed the log-periodic power law singularity (LPPLS) methodology to systematically investigate the 2020 stock market crash in the U.S. equities sectors with different levels of total market capitalizations through four major U.S. stock market indexes, including the Wilshire 5000 Total Market index, the S&P 500 index, the S&P MidCap 400 index, and the Russell 2000 index, representing the stocks overall, the large capitalization stocks, the middle capitalization stocks and the small capitalization stocks, respectively. During the 2020 U.S. stock market crash, all four indexes lost more than a third of their values within five weeks, while both the middle capitalization stocks and the small capitalization stocks have suffered much greater losses than the large capitalization stocks and stocks overall. Our results indicate that the price trajectories of these four stock market indexes prior to the 2020 stock market crash have clearly featured the obvious LPPLS bubble pattern and were indeed in a positive bubble regime. Contrary to the popular belief that the COVID-19 led to the 2020 stock market crash, the 2020 U.S. stock market crash was endogenous, stemming from the increasingly systemic instability of the stock market itself. We also performed the complementary post-mortem analysis of the 2020 U.S. stock market crash. Our analyses indicate that the 2020 U.S. stock market crash originated from a bubble which began to form as early as September 2018; and the bubbles in stocks with different levels of total market capitalizations have significantly different starting time profiles. This study not only sheds new light on the making of the 2020 U.S. stock market crash but also creates a novel pipeline for future real-time crash detection and mechanism dissection of any financial market and/or economic index."
http://arxiv.org/abs/2102.00447v1,Social Mobility in India,2021-01-31 13:18:36+00:00,"['A. Singh', 'A. Forcina', 'K. Muniyoor']",econ.GN,"Rapid rise in income inequality in India is a serious concern. While the emphasis is on inclusive growth, it seems difficult to tackle the problem without looking at the intricacies of the problem. Social mobility is one such important tool which helps in reaching the cause of the problem and focuses on bringing long term equality in the country. The purpose of this study is to examine the role of social background and education attainment in generating occupation mobility in the country. By applying an extended version of the RC association model to 68th round (2011-12) of the Employment and Unemployment Survey by the National Sample Survey Office of India, we found that the role of education is not important in generating occupation mobility in India, while social background plays a critical role in determining one's occupation. This study successfully highlights the strong intergenerational occupation immobility in the country and also the need to focus on education. In this regard, further studies are needed to uncover other crucial factors limiting the growth of individuals in the country."
http://arxiv.org/abs/2102.00587v1,Controlling volatility of wind-solar power,2021-02-01 01:54:34+00:00,['Hans Lustfeld'],econ.GN,"The main advantage of wind and solar power plants is the power production free of CO2. Their main disadvantage is the volatility of the generated power. According to the estimates of H.-W. Sinn[1], suppressing this volatility requires pumped-storage plants with a huge capacity, several orders of magnitude larger than the present available capacity in Germany[2]. Sinn concluded that wind-solar power can be used only together with conventional power plants as backups. However, based on German power data[3] of 2019 we show that the required storage capacity can significantly be reduced, provided i) a surplus of wind-solar power plants is supplied, ii) smart meters are installed, iii) partly a different kind of wind turbines and solar panels are used in Germany. Our calculations suggest that all the electric energy, presently produced in Germany, can be obtained from wind-solar power alone. And our results let us predict that wind-solar power can be used to produce in addition the energy for transportation, warm water, space heating and in part for process heating, meaning an increase of the present electric energy production by a factor of about 5[1]. Of course, to put such a prediction on firm ground the present calculations have to be confirmed for a period of many years. And it should be kept in mind, that in any case a huge number of wind turbines and solar panels is required."
http://arxiv.org/abs/2101.06603v1,The Impact of Digital Marketing on Sausage Manufacturing Companies in the Altos of Jalisco,2021-01-17 06:31:48+00:00,['Guillermo Jose Navarro del Toro'],econ.GN,"One of the goals of any business, in addition to producing high-quality, community-accepted products, is to significantly increase sales. Unfortunately, there are regions where new marketing technologies that make it possible to reach a larger number of potential consumers, not only at the regional level, but also at the state and national level, are not yet used. This research, which included qualitative and quantitative methods, as well as interviews applied to owners, employees and clients of three sausage companies, seeks to measure the impact of digital marketing in the Altos of Jalisco, Mexico. Thus, in addition to inquiring about the degree of knowledge they have regarding information and communication technologies (ICT) to expand their markets to areas with higher population density, another goal is to know the opinion about their manufactured products, their quality and acceptance. It should not be forgotten that companies are moving to an increasingly connected world, which enables entrepreneurs to get their products to a greater number of consumers through the Internet and smart devices, such as cell phones, tablets and computers; and thus ensure the survival of the company and a longer stay in the market."
http://arxiv.org/abs/2102.01716v1,A survey of some recent applications of optimal transport methods to econometrics,2021-02-02 19:16:59+00:00,['Alfred Galichon'],econ.GN,This paper surveys recent applications of methods from the theory of optimal transport to econometric problems.
http://arxiv.org/abs/2101.07818v2,"Simultaneous supply and demand constraints in input-output networks: The case of Covid-19 in Germany, Italy, and Spain",2021-01-19 19:02:51+00:00,"['Anton Pichler', 'J. Doyne Farmer']",econ.GN,"Natural and anthropogenic disasters frequently affect both the supply and demand side of an economy. A striking recent example is the Covid-19 pandemic which has created severe disruptions to economic output in most countries. These direct shocks to supply and demand will propagate downstream and upstream through production networks. Given the exogenous shocks, we derive a lower bound on total shock propagation. We find that even in this best case scenario network effects substantially amplify the initial shocks. To obtain more realistic model predictions, we study the propagation of shocks bottom-up by imposing different rationing rules on industries if they are not able to satisfy incoming demand. Our results show that economic impacts depend strongly on the emergence of input bottlenecks, making the rationing assumption a key variable in predicting adverse economic impacts. We further establish that the magnitude of initial shocks and network density heavily influence model predictions."
http://arxiv.org/abs/2101.08008v2,Willingness to Pay and Attitudinal Preferences of Indian Consumers for Electric Vehicles,2021-01-20 07:43:06+00:00,"['Prateek Bansal', 'Rajeev Ranjan Kumar', 'Alok Raj', 'Subodh Dubey', 'Daniel J. Graham']",econ.GN,"Consumer preference elicitation is critical to devise effective policies for the diffusion of electric vehicles (EVs) in India. This study contributes to the EV demand literature in the Indian context by (a) analysing the EV attributes and attitudinal factors of Indian car buyers that determine consumers' preferences for EVs, (b) estimating Indian consumers' willingness to pay (WTP) to buy EVs with improved attributes, and c) quantifying how the reference dependence affects the WTP estimates. We adopt a hybrid choice modelling approach for the above analysis. The results indicate that accounting for reference dependence provides more realistic WTP estimates than the standard utility estimation approach. Our results suggest that Indian consumers are willing to pay an additional USD 10-34 in the purchase price to reduce the fast charging time by 1 minute, USD 7-40 to add a kilometre to the driving range of EVs at 200 kilometres, and USD 104-692 to save USD 1 per 100 kilometres in operating cost. These estimates and the effect of attitudes on the likelihood to adopt EVs provide insights about EV design, marketing strategies, and pro-EV policies (e.g., specialised lanes and reserved parking for EVs) to expedite the adoption of EVs in India."
http://arxiv.org/abs/2101.11036v1,Understanding the uneven spread of COVID-19 in the context of the global interconnected economy,2021-01-26 19:12:42+00:00,"['Dimitrios Tsiotas', 'Vassilis Tselios']",econ.GN,"Using network analysis, this paper develops a multidimensional methodological framework for understanding the uneven (cross-country) spread of COVID-19 in the context of the global interconnected economy. The globally interconnected system of tourism mobility is modeled as a complex network, where two main stages in the temporal spread of COVID-19 are revealed and defined by the cutting-point of the 44th day from Wuhan. The first stage describes the outbreak in Asia and North America, the second one in Europe, South America, and Africa, while the outbreak in Oceania is spread along both stages. The analysis shows that highly connected nodes in the global tourism network (GTN) are infected early by the pandemic, while nodes of lower connectivity are late infected. Moreover, countries with the same network centrality as China were early infected on average by COVID-19. The paper also finds that network interconnectedness, economic openness, and transport integration are key determinants in the early global spread of the pandemic, and it reveals that the spatio-temporal patterns of the worldwide spread of COVID-19 are more a matter of network interconnectivity than of spatial proximity."
http://arxiv.org/abs/2101.09886v2,The Two-Sided Market Network Analysis Based on Transfer Entropy & Labelr,2021-01-25 04:16:04+00:00,['Seung Bin Baik'],econ.GN,"This study more complex digital platforms in early stages in the two-sided market to produce powerful network effects. In this study, I use Transfer Entropy to look for super users who connect hominids in different networks to achieve higher network effects in the digital platform in the two-sided market, which has recently become more complex. And this study also aims to redefine the decision criteria of product managers by helping them define users with stronger network effects. With the development of technology, the structure of the industry is becoming more difficult to interpret and the complexity of business logic is increasing. This phenomenon is the biggest problem that makes it difficult for start-ups to challenge themselves. I hope this study will help product managers create new digital economic networks, enable them to make prioritized, data-driven decisions, and find users who can be the hub of the network even in small products."
http://arxiv.org/abs/2101.10548v1,"Exploring the Complicated Relationship Between Patents and Standards, With a Particular Focus on the Telecommunications Sector",2021-01-26 04:01:21+00:00,['Nikolaos Athanasios Anagnostopoulos'],econ.GN,"While patents and standards have been identified as essential driving components of innovation and market growth, the inclusion of a patent in a standard poses many difficulties. These difficulties arise from the contradicting natures of patents and standards, which makes their combination really challenging, but, also, from the opposing business and market strategies of different patent owners involved in the standardisation process. However, a varying set of policies has been adopted to address the issues occurring from the unavoidable inclusion of patents in standards concerning certain industry sectors with a constant high degree of innovation, such as telecommunications. As these policies have not always proven adequate enough, constant efforts are being made to improve and expand them. The intriguing and complicated relationship between patents and standards is finally examined through a review of the use cases of well-known standards of the telecommunications sector which include a growing set of essential patents."
http://arxiv.org/abs/2101.09357v1,Is the Capability approach a useful tool for decision aiding in public policy making?,2021-01-17 12:39:20+00:00,"['Nicolas Fayard', 'Chabane Mazri', 'Alexis Tsoukiàs']",econ.GN,"This paper aims at proposing a model representing individuals' welfare using Sen's capability approach (CA). It is the first step of an attempt to measure the negative impact caused by the damage at a Common on a given population's welfare, and widely speaking, a first step into modelling collective threat. The CA is a multidimensional representation of persons' well-beings which account for human diversity. It has received substantial attention from scholars from different disciplines such as philosophy, economics and social scientist. Nevertheless, there is no empirical work that really fits the theoretical framework. Our goal is to show that the capability approach can be very useful for decision aiding, especially if we fill the gap between the theory and the empirical work; thus we will propose a framework that is both usable and a close representation of what capability is."
http://arxiv.org/abs/2101.08487v1,Female teachers effect on male pupils' voting behavior and preference formation,2021-01-21 07:53:02+00:00,['Eiji Yamamura'],econ.GN,"This study examines the influence of learning in a female teacher homeroom class in elementary school on pupils' voting behavior later in life, using independently collected individual-level data. Further, we evaluate its effect on preference for women's participation in the workplace in adulthood. Our study found that having a female teacher in the first year of school makes individuals more likely to vote for female candidates, and to prefer policy for female labor participation in adulthood. However, the effect is only observed among males, and not female pupils. These findings offer new evidence for the female socialization hypothesis."
http://arxiv.org/abs/2101.08922v1,How does COVID-19 change insurance and vaccine demand? Evidence from short-panel data in Japan,2021-01-22 02:25:35+00:00,"['Eiji Yamamura', 'Yoshiro Tsutsui']",econ.GN,"In this study, we explored how the coronavirus disease (COVID-19) affected the demand for insurance and vaccines in Japan from mid-March to mid-April 2020. Through independent internet surveys, respondents were asked hypothetical questions concerning the demand for insurance and vaccines for protection against COVID-19. Using the collected short-panel data, after controlling for individual characteristics using the fixed effects model, the key findings, within the context of the pandemic, were as follows: (1) Contrary to extant studies, the demand for insurance by females was smaller than that by their male counterparts; (2) The gap in demand for insurance between genders increased as the pandemic prevailed; (3) The demand for a vaccine by females was higher than that for males; and (4) As COVID-19 spread throughout Japan, demand for insurance decreased, whereas the demand for a vaccine increased."
http://arxiv.org/abs/2101.08722v1,Mechanism Design for Cumulative Prospect Theoretic Agents: A General Framework and the Revelation Principle,2021-01-21 17:03:27+00:00,"['Soham R. Phade', 'Venkat Anantharam']",cs.GT,"This paper initiates a discussion of mechanism design when the participating agents exhibit preferences that deviate from expected utility theory (EUT). In particular, we consider mechanism design for systems where the agents are modeled as having cumulative prospect theory (CPT) preferences, which is a generalization of EUT preferences. We point out some of the key modifications needed in the theory of mechanism design that arise from agents having CPT preferences and some of the shortcomings of the classical mechanism design framework. In particular, we show that the revelation principle, which has traditionally played a fundamental role in mechanism design, does not continue to hold under CPT. We develop an appropriate framework that we call mediated mechanism design which allows us to recover the revelation principle for CPT agents. We conclude with some interesting directions for future work."
http://arxiv.org/abs/2103.12122v3,Moving from Linear to Conic Markets for Electricity,2021-03-22 18:26:33+00:00,"['Anubhav Ratha', 'Pierre Pinson', 'Hélène Le Cadre', 'Ana Virag', 'Jalal Kazempour']",econ.TH,"We propose a new forward electricity market framework that admits heterogeneous market participants with second-order cone strategy sets, who accurately express the nonlinearities in their costs and constraints through conic bids, and a network operator facing conic operational constraints. In contrast to the prevalent linear-programming-based electricity markets, we highlight how the inclusion of second-order cone constraints improves uncertainty-, asset- and network-awareness of the market, which is key to the successful transition towards an electricity system based on weather-dependent renewable energy sources. We analyze our general market-clearing proposal using conic duality theory to derive efficient spatially-differentiated prices for the multiple commodities, comprising of energy and flexibility services. Under the assumption of perfect competition, we prove the equivalence of the centrally-solved market-clearing optimization problem to a competitive spatial price equilibrium involving a set of rational and self-interested participants and a price setter. Finally, under common assumptions, we prove that moving towards conic markets does not incur the loss of desirable economic properties of markets, namely market efficiency, cost recovery and revenue adequacy. Our numerical studies focus on the specific use case of uncertainty-aware market design and demonstrate that the proposed conic market brings advantages over existing alternatives within the linear programming market framework."
http://arxiv.org/abs/2102.01291v7,Efficient Estimation for Staggered Rollout Designs,2021-02-02 04:04:18+00:00,"['Jonathan Roth', ""Pedro H. C. Sant'Anna""]",econ.EM,"We study estimation of causal effects in staggered rollout designs, i.e. settings where there is staggered treatment adoption and the timing of treatment is as-good-as randomly assigned. We derive the most efficient estimator in a class of estimators that nests several popular generalized difference-in-differences methods. A feasible plug-in version of the efficient estimator is asymptotically unbiased with efficiency (weakly) dominating that of existing approaches. We provide both $t$-based and permutation-test-based methods for inference. In an application to a training program for police officers, confidence intervals for the proposed estimator are as much as eight times shorter than for existing approaches."
http://arxiv.org/abs/2102.08809v5,Testing for Nonlinear Cointegration under Heteroskedasticity,2021-02-17 15:14:19+00:00,"['Christoph Hanck', 'Till Massing']",econ.EM,"This article discusses Shin (1994, Econometric Theory)-type tests for nonlinear cointegration in the presence of variance breaks. We build on cointegration test approaches under heteroskedasticity (Cavaliere and Taylor, 2006, Journal of Time Series Analysis) and nonlinearity, serial correlation, and endogeneity (Choi and Saikkonen, 2010, Econometric Theory) to propose a bootstrap test and prove its consistency. A Monte Carlo study shows the approach to have satisfactory finite-sample properties in a variety of scenarios. We provide an empirical application to the environmental Kuznets curves (EKC), finding that the cointegration test provides little evidence for the EKC hypothesis. Additionally, we examine a nonlinear relation between the US money demand and the interest rate, finding that our test does not reject the null of a smooth transition cointegrating relation"
http://arxiv.org/abs/2103.11047v1,"Risk, Agricultural Production, and Weather Index Insurance in Village India",2021-03-19 22:11:18+00:00,"['Jeffrey D. Michler', 'Frederi G. Viens', 'Gerald E. Shively']",econ.GN,"We investigate the sources of variability in agricultural production and their relative importance in the context of weather index insurance for smallholder farmers in India. Using parcel-level panel data, multilevel modeling, and Bayesian methods we measure how large a role seasonal variation in weather plays in explaining yield variance. Seasonal variation in weather accounts for 19-20 percent of total variance in crop yields. Motivated by this result, we derive pricing and payout schedules for actuarially fair index insurance. These calculations shed light on the low uptake rates of index insurance and provide direction for designing more suitable index insurance."
http://arxiv.org/abs/2102.02071v2,"Matching Function Equilibria with Partial Assignment: Existence, Uniqueness and Estimation",2021-02-03 14:02:39+00:00,"['Liang Chen', 'Eugene Choo', 'Alfred Galichon', 'Simon Weber']",econ.GN,"We argue that models coming from a variety of fields, such as matching models and discrete choice models among others, share a common structure that we call matching function equilibria with partial assignment. This structure includes an aggregate matching function and a system of nonlinear equations. We provide a proof of existence and uniqueness of an equilibrium and propose an efficient algorithm to compute it. For a subclass of matching models, we also develop a new parameter-free approach for constructing the counterfactual matching equilibrium. It has the advantage of not requiring parametric estimation when computing counterfactuals. We use our procedure to analyze the impact of the elimination of the Social Security Student Benefit Program in 1982 on the marriage market in the United States. We estimate several candidate models from our general class of matching functions and select the best fitting model using information based criterion."
http://arxiv.org/abs/2101.04771v2,Full-Information Estimation of Heterogeneous Agent Models Using Macro and Micro Data,2021-01-12 21:56:12+00:00,"['Laura Liu', 'Mikkel Plagborg-Møller']",econ.EM,"We develop a generally applicable full-information inference method for heterogeneous agent models, combining aggregate time series data and repeated cross sections of micro data. To handle unobserved aggregate state variables that affect cross-sectional distributions, we compute a numerically unbiased estimate of the model-implied likelihood function. Employing the likelihood estimate in a Markov Chain Monte Carlo algorithm, we obtain fully efficient and valid Bayesian inference. Evaluation of the micro part of the likelihood lends itself naturally to parallel computing. Numerical illustrations in models with heterogeneous households or firms demonstrate that the proposed full-information method substantially sharpens inference relative to using only macro data, and for some parameters micro data is essential for identification."
http://arxiv.org/abs/2103.04021v3,Asymptotic Theory for IV-Based Reinforcement Learning with Potential Endogeneity,2021-03-06 03:57:46+00:00,"['Jin Li', 'Ye Luo', 'Zigan Wang', 'Xiaowei Zhang']",stat.ML,"In the standard data analysis framework, data is collected (once and for all), and then data analysis is carried out. However, with the advancement of digital technology, decision-makers constantly analyze past data and generate new data through their decisions. We model this as a Markov decision process and show that the dynamic interaction between data generation and data analysis leads to a new type of bias -- reinforcement bias -- that exacerbates the endogeneity problem in standard data analysis. We propose a class of instrument variable (IV)-based reinforcement learning (RL) algorithms to correct for the bias and establish their theoretical properties by incorporating them into a stochastic approximation (SA) framework. Our analysis accommodates iterate-dependent Markovian structures and, therefore, can be used to study RL algorithms with policy improvement. We also provide formulas for inference on optimal policies of the IV-RL algorithms. These formulas highlight how intertemporal dependencies of the Markovian environment affect the inference."
http://arxiv.org/abs/2102.00618v5,Monotone additive statistics,2021-02-01 03:43:06+00:00,"['Xiaosheng Mu', 'Luciano Pomatto', 'Philipp Strack', 'Omer Tamuz']",econ.TH,"The expectation is an example of a descriptive statistic that is monotone with respect to stochastic dominance, and additive for sums of independent random variables. We provide a complete characterization of such statistics, and explore a number of applications to models of individual and group decision-making. These include a representation of stationary monotone time preferences, extending the work of Fishburn and Rubinstein (1982) to time lotteries. This extension offers a new perspective on risk attitudes toward time, as well as on the aggregation of multiple discount factors. We also offer a novel class of nonexpected utility preferences over gambles which satisfy invariance to background risk as well as betweenness, but are versatile enough to capture mixed risk attitudes."
http://arxiv.org/abs/2101.08559v3,"To VaR, or Not to VaR, That is the Question",2021-01-21 11:32:30+00:00,['Victor Olkhov'],econ.GN,"We consider economic obstacles that limit the reliability and accuracy of value-at-risk (VaR). Investors who manage large market transactions should take into account the impact of the randomness of large trade volumes on predictions of price probability and VaR assessments. We introduce market-based probabilities of price and return that depend on the randomness of market trade values and volumes. Contrary to them, the conventional frequency-based price probability describes the case of constant trade volumes. We derive the dependence of market-based price volatility on the volatilities and correlation of trade values and volumes. In the coming years, that will limit the accuracy of price probability predictions to Gaussian approximations, and even the forecasts of market-based price volatility will be inaccurate and highly uncertain."
http://arxiv.org/abs/2105.12915v5,Ordered Reference Dependent Choice,2021-05-27 02:22:02+00:00,['Xi Zhi Lim'],econ.TH,"This paper studies how violations of structural assumptions like expected utility and exponential discounting can be connected to basic rationality violations, even though these assumptions are typically regarded as independent building blocks in decision theory. A reference-dependent generalization of behavioral postulates captures preference shifts in various choice domains. When reference points are fixed, canonical models hold; otherwise, reference-dependent preference parameters (e.g., CARA coefficients, discount factors) give rise to ""non-standard"" behavior. The framework allows us to study risk, time, and social preferences collectively, where seemingly independent anomalies are interconnected through the lens of reference-dependent choice."
http://arxiv.org/abs/2105.01581v1,Reputational Bargaining with Ultimatum Opportunities,2021-05-04 15:44:48+00:00,"['Mehmet Ekmekci', 'Hanzhe Zhang']",econ.TH,"We study two-sided reputational bargaining with opportunities to issue an ultimatum -- threats to force dispute resolution. Each player is either a justified type, who never concedes and issues an ultimatum whenever an opportunity arrives, or an unjustified type, who can concede, wait, or bluff with an ultimatum. In equilibrium, the presence of ultimatum opportunities can harm or benefit a player by decelerating or accelerating reputation building. When only one player can issue an ultimatum, equilibrium play is unique. The hazard rate of dispute resolution is discontinuous and piecewise monotonic in time. As the probabilities of being justified vanish, agreement is immediate and efficient, and if the set of justifiable demands is rich, payoffs modify Abreu and Gul (2000), with the discount rate replaced by the ultimatum opportunity arrival rate if the former is smaller. When both players' ultimatum opportunities arrive sufficiently fast, there may exist multiple equilibria in which their reputations do not build up and negotiation lasts forever."
http://arxiv.org/abs/2105.12304v1,Multi-Dimensional Screening: Buyer-Optimal Learning and Informational Robustness,2021-05-26 02:31:57+00:00,"['Rahul Deb', 'Anne-Katrin Roesler']",econ.TH,"A monopolist seller of multiple goods screens a buyer whose type is initially unknown to both but drawn from a commonly known distribution. The buyer privately learns about his type via a signal. We derive the seller's optimal mechanism in two different information environments. We begin by deriving the buyer-optimal outcome. Here, an information designer first selects a signal, and then the seller chooses an optimal mechanism in response; the designer's objective is to maximize consumer surplus. Then, we derive the optimal informationally robust mechanism. In this case, the seller first chooses the mechanism, and then nature picks the signal that minimizes the seller's profits. We derive the relation between both problems and show that the optimal mechanism in both cases takes the form of pure bundling."
http://arxiv.org/abs/2105.02094v1,Sustainability of Collusion and Market Transparency in a Sequential Search Market: a Generalization,2021-05-05 14:49:10+00:00,"['Jacopo De Tullio', 'Giuseppe Puleio']",econ.TH,"The present work generalizes the analytical results of Petrikaite (2016) to a market where more than two firms interact. As a consequence, for a generic number of firms in the oligopoly model described by Janssen et al (2005), the relationship between the critical discount factor which sustains the monopoly collusive allocation and the share of perfectly informed buyers is non-monotonic, reaching a unique internal point of minimum. The first section locates the work within the proper economic framework. The second section hosts the analytical computations and the mathematical reasoning needed to derive the desired generalization, which mainly relies on the Leibniz rule for the differentiation under the integral sign and the Bounded Convergence Theorem."
http://arxiv.org/abs/2105.07565v1,Attention elasticities and invariant information costs,2021-05-17 01:24:32+00:00,['Dániel Csaba'],econ.GN,"We consider a generalization of rational inattention problems by measuring costs of information through the information radius (Sibson, 1969; Verdú, 2015) of statistical experiments. We introduce a notion of attention elasticity measuring the sensitivity of attention strategies with respect to changes in incentives. We show how the introduced class of cost functions controls attention elasticities while the Shannon model restricts attention elasticity to be unity. We explore further differences and similarities relative to the Shannon model in relation to invariance, posterior separability, consideration sets, and the ability to learn events with certainty. Lastly, we provide an efficient alternating minimization method -- analogous to the Blahut-Arimoto algorithm -- to obtain optimal attention strategies."
http://arxiv.org/abs/2104.07379v2,The Struggle with Inequality,2021-04-15 11:18:21+00:00,['Shin-Ichiro Inaba'],econ.GN,"This is an introductory textbook of the history of economics of inequality for undergraduates and genreral readers. It begins with Adam Smith's critique of Rousseau. The first and second chapters focus on Smith and Karl Marx, in the broad classical tradition of economics, where it is believed that there is an inseparable relationship between production and distribution, economic growth and inequality. Chapters 3 and 4 argue that despite the fact that the founders of the neoclassical school had shown an active interest in social issues, namely worker poverty, the issues of production and distribution became discussed separately among neoclassicals. Toward the end of the 20th century, however, there was a renewed awareness within economics of the problem of the relationship between production and distribution. The young Piketty's beginnings as an economist are set against this backdrop. Chapters 5 to 8 explain the circumstances of the restoration of classical concerns within the neoclassical framework. Then, in chapters 9 and 10, I discuss the fact that Thomas Piketty's seminal work is a new development in this ""inequality renaissance,"" and try to gain a perspective on future trends in the debate. Mathematical appendix presents simple models of growth and distribution."
http://arxiv.org/abs/2104.13159v1,Search and Competition with Flexible Investigations,2021-04-27 13:07:51+00:00,"['Vasudha Jain', 'Mark Whitmeyer']",econ.TH,"We modify the standard model of price competition with horizontally differentiated products, imperfect information, and search frictions by allowing consumers to flexibly acquire information about a product's match value during their visits. We characterize a consumer's optimal search and information acquisition protocol and analyze the pricing game between firms. Notably, we establish that in search markets there are fundamental differences between search frictions and information frictions, which affect market prices, profits, and consumer welfare in markedly different ways. Although higher search costs beget higher prices (and profits for firms), higher information acquisition costs lead to lower prices and may benefit consumers. We discuss implications of our findings for policies concerning disclosure rules and hidden fees."
http://arxiv.org/abs/2106.14820v2,Rational Pricing of Leveraged ETF Expense Ratios,2021-06-28 15:56:05+00:00,['Alex Garivaltis'],econ.TH,"This paper studies the general relationship between the gearing ratio of a Leveraged ETF and its corresponding expense ratio, viz., the investment management fees that are charged for the provision of this levered financial service. It must not be possible for an investor to combine two or more LETFs in such a way that his (continuously-rebalanced) LETF portfolio can match the gearing ratio of a given, professionally managed product and, at the same time, enjoy lower weighted-average expenses than the existing LETF. Given a finite set of LETFs that exist in the marketplace, I give necessary and sufficient conditions for these products to be undominated in the price-gearing plane. In a beautiful application of the duality theorem of linear programming, I prove a kind of two-fund theorem for LETFs: given a target gearing ratio for the investor, the cheapest way to achieve it is to combine (uniquely) the two nearest undominated LETF products that bracket it on the leverage axis. This also happens to be the implementation that has the lowest annual turnover. For the writer's enjoyment, we supply a second proof of the Main Theorem on LETFs that is based on Carathéodory's theorem in convex geometry. Thus, say, a triple-leveraged (""UltraPro"") exchange-traded product should never be mixed with cash, if the investor is able to trade in the underlying index. In terms of financial innovation, our two-fund theorem for LETFs implies that the introduction of new, undominated 2.5x products would increase the welfare of all investors whose preferred gearing ratios lie between 2x (""Ultra"") and 3x (""UltraPro""). Similarly for a 1.5x product."
http://arxiv.org/abs/2104.10657v9,Rationally Inattentive Echo Chambers,2021-04-21 17:32:04+00:00,"['Lin Hu', 'Anqi Li', 'Xu Tan']",econ.TH,"We study rationally inattentive echo chambers, where players allocate limited attention to biased primary sources and to other players as secondary sources to acquire information about an uncertain state. The resulting Poisson attention network stochastically transmits information from primary sources either directly or indirectly through others. We give conditions for echo-chamber equilibria, in which players restrict attention to their own-biased source and same-type peers. We characterize equilibrium attention networks, developing tools for comparative statics. Our results explain why modern information environments foster echo chambers, how small heterogeneity in attention capacities can magnify into large disparities in allocations, and why regulatory interventions such as altering user visibility on social media or mandating exposure to opposing views may backfire with unintended consequences."
http://arxiv.org/abs/2105.15175v1,An algebraic approach to revealed preferences,2021-05-31 17:34:06+00:00,"['Mikhail Freer', 'Cesar Martinelli']",econ.TH,"We propose and develop an algebraic approach to revealed preference. Our approach dispenses with non algebraic structure, such as topological assumptions. We provide algebraic axioms of revealed preference that subsume previous, classical revealed preference axioms, as well as generate new axioms for behavioral theories, and show that a data set is rationalizable if and only if it is consistent with an algebraic axiom."
http://arxiv.org/abs/2105.10965v1,Inference for multi-valued heterogeneous treatment effects when the number of treated units is small,2021-05-23 16:06:22+00:00,"['Marina Dias', 'Demian Pouzo']",econ.EM,"We propose a method for conducting asymptotically valid inference for treatment effects in a multi-valued treatment framework where the number of units in the treatment arms can be small and do not grow with the sample size. We accomplish this by casting the model as a semi-/non-parametric conditional quantile model and using known finite sample results about the law of the indicator function that defines the conditional quantile. Our framework allows for structural functions that are non-additively separable, with flexible functional forms and heteroskedasticy in the residuals, and it also encompasses commonly used designs like difference in difference. We study the finite sample behavior of our test in a Monte Carlo study and we also apply our results to assessing the effect of weather events on GDP growth."
http://arxiv.org/abs/2105.10099v2,Learning from zero: how to make consumption-saving decisions in a stochastic environment with an AI algorithm,2021-05-21 02:39:12+00:00,"['Rui', 'Shi']",econ.TH,"This exercise proposes a learning mechanism to model economic agent's decision-making process using an actor-critic structure in the literature of artificial intelligence. It is motivated by the psychology literature of learning through reinforcing good or bad decisions. In a model of an environment, to learn to make decisions, this AI agent needs to interact with its environment and make explorative actions. Each action in a given state brings a reward signal to the agent. These interactive experience is saved in the agent's memory, which is then used to update its subjective belief of the world. The agent's decision-making strategy is formed and adjusted based on this evolving subjective belief. This agent does not only take an action that it knows would bring a high reward, it also explores other possibilities. This is the process of taking explorative actions, and it ensures that the agent notices changes in its environment and adapt its subjective belief and decisions accordingly. Through a model of stochastic optimal growth, I illustrate that the economic agent under this proposed learning structure is adaptive to changes in an underlying stochastic process of the economy. AI agents can differ in their levels of exploration, which leads to different experience in the same environment. This reflects on to their different learning behaviours and welfare obtained. The chosen economic structure possesses the fundamental decision making problems of macroeconomic models, i.e., how to make consumption-saving decisions in a lifetime, and it can be generalised to other decision-making processes and economic models."
http://arxiv.org/abs/2104.13367v8,A model of multiple hypothesis testing,2021-04-27 17:53:49+00:00,"['Davide Viviano', 'Kaspar Wuthrich', 'Paul Niehaus']",econ.GN,"Multiple hypothesis testing practices vary widely, without consensus on which are appropriate when. This paper provides an economic foundation for these practices designed to capture leading examples, such as regulatory approval on the basis of clinical trials. In studies of multiple treatments or sub-populations, adjustments may be appropriate depending on scale economies in the research production function, with control of classical notions of compound errors emerging in some but not all cases. In studies with multiple outcomes, indexing is appropriate and adjustments to test levels may be appropriate if the intended audience is heterogeneous. Data on actual costs in the drug approval process suggest both that some adjustment is warranted in that setting and that standard procedures may be overly conservative."
http://arxiv.org/abs/2104.11300v1,The Crowd Classification Problem: Social Dynamics of Binary Choice Accuracy,2021-04-22 20:00:52+00:00,"['Joshua Becker', 'Douglas Guilbeault', 'Ned Smith']",econ.GN,"Decades of research suggest that information exchange in groups and organizations can reliably improve judgment accuracy in tasks such as financial forecasting, market research, and medical decision-making. However, we show that improving the accuracy of numeric estimates does not necessarily improve the accuracy of decisions. For binary choice judgments, also known as classification tasks--e.g. yes/no or build/buy decisions--social influence is most likely to grow the majority vote share, regardless of the accuracy of that opinion. As a result, initially inaccurate groups become increasingly inaccurate after information exchange even as they signal stronger support. We term this dynamic the ""crowd classification problem."" Using both a novel dataset as well as a reanalysis of three previous datasets, we study this process in two types of information exchange: (1) when people share votes only, and (2) when people form and exchange numeric estimates prior to voting. Surprisingly, when people exchange numeric estimates prior to voting, the binary choice vote can become less accurate even as the average numeric estimate becomes more accurate. Our findings recommend against voting as a form of decision-making when groups are optimizing for accuracy. For those cases where voting is required, we discuss strategies for managing communication to avoid the crowd classification problem. We close with a discussion of how our results contribute to a broader contingency theory of collective intelligence."
http://arxiv.org/abs/2104.12573v4,Robust decision-making under risk and ambiguity,2021-04-23 10:42:16+00:00,"['Maximilian Blesch', 'Philipp Eisenhauer']",econ.EM,"Economists often estimate economic models on data and use the point estimates as a stand-in for the truth when studying the model's implications for optimal decision-making. This practice ignores model ambiguity, exposes the decision problem to misspecification, and ultimately leads to post-decision disappointment. Using statistical decision theory, we develop a framework to explore, evaluate, and optimize robust decision rules that explicitly account for estimation uncertainty. We show how to operationalize our analysis by studying robust decisions in a stochastic dynamic investment model in which a decision-maker directly accounts for uncertainty in the model's transition dynamics."
http://arxiv.org/abs/2106.02861v1,Optimal Taxation of Assets,2021-06-05 10:27:19+00:00,"['Nicolaus Tideman', 'Thomas Mecherikunnel']",econ.GN,"The optimal taxation of assets requires attention to two concerns: 1) the elasticity of the supply of assets and 2) the impact of taxing assets on distributional objectives. The most efficient way to attend to these two concerns is to tax assets of different types separately, rather than having one tax on all assets. When assets are created by specialized effort rather than by saving, as with innovations, discoveries of mineral deposits and development of unregulated natural monopolies, it is interesting to consider a regime in which the government awards a prize for the creation of the asset and then collects the remaining value of the asset in taxes. Analytically, the prize is like a wage after taxes. In this perspective, prizes are awarded based on a variation on optimal taxation theory, while assets of different types are taxed in divergent ways, depending on their characteristics. Some categories of assets are abolished."
http://arxiv.org/abs/2104.09368v2,Deep Reinforcement Learning in a Monetary Model,2021-04-19 14:56:44+00:00,"['Mingli Chen', 'Andreas Joseph', 'Michael Kumhof', 'Xinlei Pan', 'Xuan Zhou']",econ.EM,"We propose using deep reinforcement learning to solve dynamic stochastic general equilibrium models. Agents are represented by deep artificial neural networks and learn to solve their dynamic optimisation problem by interacting with the model environment, of which they have no a priori knowledge. Deep reinforcement learning offers a flexible yet principled way to model bounded rationality within this general class of models. We apply our proposed approach to a classical model from the adaptive learning literature in macroeconomics which looks at the interaction of monetary and fiscal policy. We find that, contrary to adaptive learning, the artificially intelligent household can solve the model in all policy regimes."
http://arxiv.org/abs/2104.10365v5,Identification of Peer Effects with Miss-specified Peer Groups: Missing Data and Group Uncertainty,2021-04-21 06:09:26+00:00,"['Christiern Rose', 'Lizi Yu']",econ.EM,"We consider identification of peer effects under peer group miss-specification. Two leading cases are missing data and peer group uncertainty. Missing data can take the form of some individuals being entirely absent from the data. The researcher need not have any information on missing individuals and need not even know that they are missing. We show that peer effects are nevertheless identifiable under mild restrictions on the probabilities of observing individuals, and propose a GMM estimator to estimate the peer effects. In practice this means that the researcher need only have access to an individual level sample with group identifiers. Group uncertainty arises when the relevant peer group for the outcome under study is unknown. We show that peer effects are nevertheless identifiable if the candidate groups are nested within one another and propose a non-linear least squares estimator. We conduct a Monte-Carlo experiment to demonstrate our identification results and the performance of the proposed estimators, and apply our method to study peer effects in the career decisions of junior lawyers."
http://arxiv.org/abs/2104.02009v3,Identification and Estimation in Many-to-one Two-sided Matching without Transfers,2021-04-05 16:58:19+00:00,"['YingHua He', 'Shruti Sinha', 'Xiaoting Sun']",econ.EM,"In a setting of many-to-one two-sided matching with non-transferable utilities, e.g., college admissions, we study conditions under which preferences of both sides are identified with data on one single market. Regardless of whether the market is centralized or decentralized, assuming that the observed matching is stable, we show nonparametric identification of preferences of both sides under certain exclusion restrictions. To take our results to the data, we use Monte Carlo simulations to evaluate different estimators, including the ones that are directly constructed from the identification. We find that a parametric Bayesian approach with a Gibbs sampler works well in realistically sized problems. Finally, we illustrate our methodology in decentralized admissions to public and private schools in Chile and conduct a counterfactual analysis of an affirmative action policy."
http://arxiv.org/abs/2104.00473v4,Normalizations and misspecification in skill formation models,2021-04-01 14:02:50+00:00,['Joachim Freyberger'],econ.EM,"An important class of structural models studies the determinants of skill formation and the optimal timing of interventions. In this paper, I provide new identification results for these models and investigate the effects of seemingly innocuous scale and location restrictions on parameters of interest. To do so, I first characterize the identified set of all parameters without these additional restrictions and show that important policy-relevant parameters are point identified under weaker assumptions than commonly used in the literature. The implications of imposing standard scale and location restrictions depend on how the model is specified, but they generally impact the interpretation of parameters and may affect counterfactuals. Importantly, with the popular CES production function, commonly used scale restrictions fix identified parameters and lead to misspecification. Consequently, simply changing the units of measurements of observed variables might yield ineffective investment strategies and misleading policy recommendations. I show how existing estimators can easily be adapted to solve these issues. As a byproduct, this paper also presents a general and formal definition of when restrictions are truly normalizations."
http://arxiv.org/abs/2104.06904v1,Unprecedented decarbonization of China's power system in the post-COVID era,2021-04-14 14:54:32+00:00,"['Biqing Zhu', 'Rui Guo', 'Zhu Deng', 'Wenli Zhao', 'Piyu Ke', 'Xinyu Dou', 'Steven J. Davis', 'Philippe Ciais', 'Pierre Gentine', 'Zhu Liu']",physics.soc-ph,"In October of 2020, China announced that it aims to start reducing its carbon dioxide (CO2) emissions before 2030 and achieve carbon neutrality before 20601. The surprise announcement came in the midst of the COVID-19 pandemic which caused a transient drop in China's emissions in the first half of 2020. Here, we show an unprecedented de-carbonization of China's power system in late 2020: although China's power related carbon emissions were 0.5% higher in 2020 than 2019, the majority (92.9%) of the increased power demand was met by increases in low-carbon (renewables and nuclear) generation (increased by 9.3%), as compared to only 0.4% increase for fossil fuels. China's low-carbon generation in the country grew in the second half of 2020, supplying a record high of 36.7% (increased by 1.9% compared to 2019) of total electricity in 2020, when the fossil production dropped to a historical low of 63.3%. Combined, the carbon intensity of China's power sector decreased to an historical low of 519.9 tCO2/GWh in 2020. If the fast decarbonization and slowed down power demand growth from 2019 to 2020 were to continue, by 2030, over half (50.8%) of China's power demand could be provided by low carbon sources. Our results thus reveal that China made progress towards its carbon neutrality target during the pandemic, and suggest the potential for substantial further decarbonization in the next few years if the latest trends persist."
http://arxiv.org/abs/2104.12909v6,"Algorithm as Experiment: Machine Learning, Market Design, and Policy Eligibility Rules",2021-04-26 23:18:34+00:00,"['Yusuke Narita', 'Kohei Yata']",econ.EM,"Algorithms make a growing portion of policy and business decisions. We develop a treatment-effect estimator using algorithmic decisions as instruments for a class of stochastic and deterministic algorithms. Our estimator is consistent and asymptotically normal for well-defined causal effects. A special case of our setup is multidimensional regression discontinuity designs with complex boundaries. We apply our estimator to evaluate the Coronavirus Aid, Relief, and Economic Security Act, which allocated many billions of dollars worth of relief funding to hospitals via an algorithmic rule. The funding is shown to have little effect on COVID-19-related hospital activities. Naive estimates exhibit selection bias."
http://arxiv.org/abs/2105.05427v2,Random Double Auction: A Robust Bilateral Trading Mechanism,2021-05-12 04:46:28+00:00,['Wanchang Zhang'],econ.TH,"I construct a novel random double auction as a robust bilateral trading mechanism for a profit-maximizing intermediary who facilitates trade between a buyer and a seller. It works as follows. The intermediary publicly commits to charging a fixed commission fee and randomly drawing a spread from a uniform distribution. Then the buyer submits a bid price and the seller submits an ask price simultaneously. If the difference between the bid price and the ask price is greater than the realized spread, then the asset is transacted at the midpoint price, and each pays the intermediary half of the fixed commission fee. Otherwise, no trade takes place, and no one pays or receives anything. I show that the random double auction maximizes the worst-case expected profit across all dominant-strategy incentive compatible and ex-post individually rational mechanisms for the symmetric case. I also construct a robust trading mechanism with similar properties for the asymmetric case."
http://arxiv.org/abs/2106.02150v1,Interactive Communication in Bilateral Trade,2021-06-03 21:56:54+00:00,"['Jieming Mao', 'Renato Paes Leme', 'Kangning Wang']",cs.GT,"We define a model of interactive communication where two agents with private types can exchange information before a game is played. The model contains Bayesian persuasion as a special case of a one-round communication protocol. We define message complexity corresponding to the minimum number of interactive rounds necessary to achieve the best possible outcome. Our main result is that for bilateral trade, agents don't stop talking until they reach an efficient outcome: Either agents achieve an efficient allocation in finitely many rounds of communication; or the optimal communication protocol has infinite number of rounds. We show an important class of bilateral trade settings where efficient allocation is achievable with a small number of rounds of communication."
http://arxiv.org/abs/2106.11184v5,The decline of disruptive science and technology,2021-06-21 15:24:23+00:00,"['Michael Park', 'Erin Leahey', 'Russell Funk']",cs.SI,"Theories of scientific and technological change view discovery and invention as endogenous processes, wherein prior accumulated knowledge enables future progress by allowing researchers to, in Newton's words, ""stand on the shoulders of giants"". Recent decades have witnessed exponential growth in the volume of new scientific and technological knowledge, thereby creating conditions that should be ripe for major advances. Yet contrary to this view, studies suggest that progress is slowing in several major fields of science and technology. Here, we analyze these claims at scale across 6 decades, using data on 45 million papers and 3.5 million patents from 6 large-scale datasets. We find that papers and patents are increasingly less likely to break with the past in ways that push science and technology in new directions, a pattern that holds universally across fields. Subsequently, we link this decline in disruptiveness to a narrowing in the use of prior knowledge, allowing us to reconcile the patterns we observe with the ""shoulders of giants"" view. We find that the observed declines are unlikely to be driven by changes in the quality of published science, citation practices, or field-specific factors. Overall, our results suggest that slowing rates of disruption may reflect a fundamental shift in the nature of science and technology."
http://arxiv.org/abs/2105.10007v4,Fixed-k Tail Regression: New Evidence on Tax and Wealth Inequality from Forbes 400,2021-05-20 19:48:54+00:00,"['Ji Hyung Lee', 'Yuya Sasaki', 'Alexis Akira Toda', 'Yulong Wang']",econ.GN,"We develop a novel fixed-k tail regression method that accommodates the unique feature in the Forbes 400 data that observations are truncated from below at the 400th largest order statistic. Applying this method, we find that higher maximum marginal income tax rates induce higher wealth Pareto exponents. Setting the maximum tax rate to 30-40% (as in U.S. currently) leads to a Pareto exponent of 1.5-1.8, while counterfactually setting it to 80% (as suggested by Piketty, 2014) would lead to a Pareto exponent of 2.6. We present a simple economic model that explains these findings and discuss the welfare implications of taxation."
http://arxiv.org/abs/2106.04237v3,Testing Monotonicity of Mean Potential Outcomes in a Continuous Treatment with High-Dimensional Data,2021-06-08 10:33:09+00:00,"['Yu-Chin Hsu', 'Martin Huber', 'Ying-Ying Lee', 'Chu-An Liu']",econ.EM,"While most treatment evaluations focus on binary interventions, a growing literature also considers continuously distributed treatments. We propose a Cramér-von Mises-type test for testing whether the mean potential outcome given a specific treatment has a weakly monotonic relationship with the treatment dose under a weak unconfoundedness assumption. In a nonseparable structural model, applying our method amounts to testing monotonicity of the average structural function in the continuous treatment of interest. To flexibly control for a possibly high-dimensional set of covariates in our testing approach, we propose a double debiased machine learning estimator that accounts for covariates in a data-driven way. We show that the proposed test controls asymptotic size and is consistent against any fixed alternative. These theoretical findings are supported by the Monte-Carlo simulations. As an empirical illustration, we apply our test to the Job Corps study and reject a weakly negative relationship between the treatment (hours in academic and vocational training) and labor market performance among relatively low treatment values."
http://arxiv.org/abs/2105.14752v4,Regression-Adjusted Estimation of Quantile Treatment Effects under Covariate-Adaptive Randomizations,2021-05-31 07:33:31+00:00,"['Liang Jiang', 'Peter C. B. Phillips', 'Yubo Tao', 'Yichong Zhang']",econ.EM,"Datasets from field experiments with covariate-adaptive randomizations (CARs) usually contain extra covariates in addition to the strata indicators. We propose to incorporate these additional covariates via auxiliary regressions in the estimation and inference of unconditional quantile treatment effects (QTEs) under CARs. We establish the consistency and limit distribution of the regression-adjusted QTE estimator and prove that the use of multiplier bootstrap inference is non-conservative under CARs. The auxiliary regression may be estimated parametrically, nonparametrically, or via regularization when the data are high-dimensional. Even when the auxiliary regression is misspecified, the proposed bootstrap inferential procedure still achieves the nominal rejection probability in the limit under the null. When the auxiliary regression is correctly specified, the regression-adjusted estimator achieves the minimum asymptotic variance. We also discuss forms of adjustments that can improve the efficiency of the QTE estimators. The finite sample performance of the new estimation and inferential methods is studied in simulations and an empirical application to a well-known dataset concerned with expanding access to basic bank accounts on savings is reported."
http://arxiv.org/abs/2104.07761v1,Micro-Estimates of Wealth for all Low- and Middle-Income Countries,2021-04-15 20:37:46+00:00,"['Guanghua Chi', 'Han Fang', 'Sourav Chatterjee', 'Joshua E. Blumenstock']",econ.GN,"Many critical policy decisions, from strategic investments to the allocation of humanitarian aid, rely on data about the geographic distribution of wealth and poverty. Yet many poverty maps are out of date or exist only at very coarse levels of granularity. Here we develop the first micro-estimates of wealth and poverty that cover the populated surface of all 135 low and middle-income countries (LMICs) at 2.4km resolution. The estimates are built by applying machine learning algorithms to vast and heterogeneous data from satellites, mobile phone networks, topographic maps, as well as aggregated and de-identified connectivity data from Facebook. We train and calibrate the estimates using nationally-representative household survey data from 56 LMICs, then validate their accuracy using four independent sources of household survey data from 18 countries. We also provide confidence intervals for each micro-estimate to facilitate responsible downstream use. These estimates are provided free for public use in the hope that they enable targeted policy response to the COVID-19 pandemic, provide the foundation for new insights into the causes and consequences of economic development and growth, and promote responsible policymaking in support of the Sustainable Development Goals."
http://arxiv.org/abs/2104.00578v2,A Pricing Mechanism to Jointly Mitigate Market Power and Environmental Externalities in Electricity Markets,2021-04-01 16:00:22+00:00,"['Lamia Varawala', 'Mohammad Reza Hesamzadeh', 'György Dán', 'Derek Bunn', 'Juan Rosellón']",econ.TH,"The electricity industry has been one of the first to face technological changes motivated by sustainability concerns. Whilst efficiency aspects of market design have tended to focus upon market power concerns, the new policy challenges emphasise sustainability. We argue that market designs need to develop remedies for market conduct integrated with regard to environmental externalities. Accordingly, we develop an incentive-based market clearing mechanism using a power network representation with a distinctive feature of incomplete information regarding generation costs. The shortcomings of price caps to mitigate market power, in this context, are overcome with the proposed mechanism."
http://arxiv.org/abs/2105.03737v3,Difference-in-Differences Estimation with Spatial Spillovers,2021-05-08 16:43:02+00:00,['Kyle Butts'],econ.EM,"Empirical work often uses treatment assigned following geographic boundaries. When the effects of treatment cross over borders, classical difference-in-differences estimation produces biased estimates for the average treatment effect. In this paper, I introduce a potential outcomes framework to model spillover effects and decompose the estimate's bias in two parts: (1) the control group no longer identifies the counterfactual trend because their outcomes are affected by treatment and (2) changes in treated units' outcomes reflect the effect of their own treatment status and the effect from the treatment status of 'close' units. I propose conditions for non-parametric identification that can remove both sources of bias and semi-parametrically estimate the spillover effects themselves including in settings with staggered treatment timing. To highlight the importance of spillover effects, I revisit analyses of three place-based interventions."
http://arxiv.org/abs/2105.04900v2,Forecasting consumer confidence through semantic network analysis of online news,2021-05-11 09:41:25+00:00,"['A. Fronzetti Colladon', 'F. Grippa', 'B. Guardabascio', 'G. Costante', 'F. Ravazzolo']",econ.GN,"This research studies the impact of online news on social and economic consumer perceptions through semantic network analysis. Using over 1.8 million online articles on Italian media covering four years, we calculate the semantic importance of specific economic-related keywords to see if words appearing in the articles could anticipate consumers' judgments about the economic situation and the Consumer Confidence Index. We use an innovative approach to analyze big textual data, combining methods and tools of text mining and social network analysis. Results show a strong predictive power for the judgments about the current households and national situation. Our indicator offers a complementary approach to estimating consumer confidence, lessening the limitations of traditional survey-based methods."
http://arxiv.org/abs/2106.13347v1,"Relationship between Cultural Values, Sense of Community and Trust and the Effect of Trust in Workplace",2021-06-24 23:04:33+00:00,"['Nazli Mohammad', 'Yvonne Stedham']",econ.GN,"This paper provides a general overview of different perspectives and studies on trust, offers a definition of trust, and provides factors that play a substantial role in developing social trust, and shows from which perspectives it can be fostered. The results showed that trust is playing an important role in success for organizations involved in cross-national strategic partnerships. Trust can reduce transaction costs, promotes inter-organizational relationships, and improve subordinate relationships between managers."
http://arxiv.org/abs/2106.12705v1,Alternative Microfoundations for Strategic Classification,2021-06-24 00:30:58+00:00,"['Meena Jagadeesan', 'Celestine Mendler-Dünner', 'Moritz Hardt']",cs.LG,"When reasoning about strategic behavior in a machine learning context it is tempting to combine standard microfoundations of rational agents with the statistical decision theory underlying classification. In this work, we argue that a direct combination of these standard ingredients leads to brittle solution concepts of limited descriptive and prescriptive value. First, we show that rational agents with perfect information produce discontinuities in the aggregate response to a decision rule that we often do not observe empirically. Second, when any positive fraction of agents is not perfectly strategic, desirable stable points -- where the classifier is optimal for the data it entails -- cease to exist. Third, optimal decision rules under standard microfoundations maximize a measure of negative externality known as social burden within a broad class of possible assumptions about agent behavior.
  Recognizing these limitations we explore alternatives to standard microfoundations for binary classification. We start by describing a set of desiderata that help navigate the space of possible assumptions about how agents respond to a decision rule. In particular, we analyze a natural constraint on feature manipulations, and discuss properties that are sufficient to guarantee the robust existence of stable points. Building on these insights, we then propose the noisy response model. Inspired by smoothed analysis and empirical observations, noisy response incorporates imperfection in the agent responses, which we show mitigates the limitations of standard microfoundations. Our model retains analytical tractability, leads to more robust insights about stable points, and imposes a lower social burden at optimality."
http://arxiv.org/abs/2104.04601v1,The Effect of Sport in Online Dating: Evidence from Causal Machine Learning,2021-04-07 15:51:55+00:00,"['Daniel Boller', 'Michael Lechner', 'Gabriel Okasa']",econ.GN,"Online dating emerged as a key platform for human mating. Previous research focused on socio-demographic characteristics to explain human mating in online dating environments, neglecting the commonly recognized relevance of sport. This research investigates the effect of sport activity on human mating by exploiting a unique data set from an online dating platform. Thereby, we leverage recent advances in the causal machine learning literature to estimate the causal effect of sport frequency on the contact chances. We find that for male users, doing sport on a weekly basis increases the probability to receive a first message from a woman by 50%, relatively to not doing sport at all. For female users, we do not find evidence for such an effect. In addition, for male users the effect increases with higher income."
http://arxiv.org/abs/2104.04639v1,Vaccine allocation to blue-collar workers,2021-04-09 23:33:47+00:00,"['László Czaller', 'Gergő Tóth', 'Balázs Lengyel']",econ.GN,"Vaccination may be the solution to the pandemic-induced health crisis, but the allocation of vaccines is a complex task in which economic and social considerations can be important. The central problem is to use the limited number of vaccines in a country to reduce the risk of infection and mitigate economic uncertainty at the same time. In this paper, we propose a simple economic model for vaccine allocation across two types of workers: white-collars can work from home; while blue-collars must work on site. These worker types are complementary to each other, thus a negative shock to the supply of either one decreases the demand for the other that leads to unemployment. Using parameters of blue and white-collar labor supply, their infection risks, productivity losses at home office during lock-down, and available vaccines, we express the optimal share of vaccines allocated to blue-collars. The model points to the dominance of blue-collar vaccination, especially during waves when their relative infection risks increase and when the number of available vaccines is limited. Taking labor supply data from 28 European countries, we quantify blue-collar vaccine allocation that minimizes unemployment across levels of blue- and white-collar infection risks. The model favours blue-collar vaccination identically across European countries in case of vaccine scarcity. As more vaccines become available, economies that host large-shares of employees in home-office shall increasingly immunize them in case blue-collar infection risks can be kept down. Our results highlight that vaccination plans should include workers and rank them by type of occupation. We propose that prioritizing blue-collar workers during infection waves and early vaccination can also favour economy besides helping the most vulnerable who can transmit more infection."
http://arxiv.org/abs/2104.14002v2,Modeling Managerial Search Behavior based on Simon's Concept of Satisficing,2021-04-28 20:09:53+00:00,['Friederike Wall'],econ.GN,"Computational models of managerial search often build on backward-looking search based on hill-climbing algorithms. Regardless of its prevalence, there is some evidence that this family of algorithms does not universally represent managers' search behavior. Against this background, the paper proposes an alternative algorithm that captures key elements of Simon's concept of satisficing which received considerable support in behavioral experiments. The paper contrasts the satisficing-based algorithm to two variants of hill-climbing search in an agent-based model of a simple decision-making organization. The model builds on the framework of NK fitness landscapes which allows controlling for the complexity of the decision problem to be solved. The results suggest that the model's behavior may remarkably differ depending on whether satisficing or hill-climbing serves as an algorithmic representation for decision-makers' search. Moreover, with the satisficing algorithm, results indicate oscillating aspiration levels, even to the negative, and intense - and potentially destabilizing - search activities when intra-organizational complexity increases. Findings may shed some new light on prior computational models of decision-making in organizations and point to avenues for future research."
http://arxiv.org/abs/2104.14043v1,Where to Refuel: Modeling On-the-way Choice of Convenience Outlet,2021-04-28 23:02:24+00:00,"['Ari Pramono', 'Harmen Oppewal']",econ.GN,"This paper introduces on-the-way choice of retail outlet as a form of convenience shopping. It presents a model of on-the-way choice of retail outlet and applies the model in the context of fuel retailing to explore its implications for segmentation and spatial competition. The model is a latent class random utility choice model. An application to gas station choices observed in a medium-sized Asian city show the model to fit substantially better than existing models. The empirical results indicate consumers may adopt one of two decision strategies. When adopting an immediacy-oriented strategy they behave in accordance with the traditional gravity-based retail models and tend to choose the most spatially convenient outlet. When following a destination-oriented strategy they focus more on maintaining their overall trip efficiency and so will tend to visit outlets located closer to their main destination and are more susceptible to retail agglomeration effects. The paper demonstrates how the model can be used to inform segmentation and local competition analyses that account for variations in these strategies as well as variations in consumer type, origin and time of travel. Simulations of a duopoly setting further demonstrate the implications."
http://arxiv.org/abs/2104.09157v3,New axioms for top trading cycles,2021-04-19 09:36:12+00:00,"['Siwei Chen', 'Yajing Chen', 'Chia-Ling Hsu']",econ.TH,"School choice is of great importance both in theory and practice. This paper studies the (student-optimal) top trading cycles mechanism (TTCM) in an axiomatic way. We introduce two new axioms: MBG (mutual best group)-quota-rationality and MBG-robust efficiency. While stability implies MBG-quota-rationality, MBG-robust efficiency is weaker than robust efficiency, which is stronger than the combination of efficiency and group strategy-proofness. The TTCM is characterized by MBG-quota-rationality and MBG-robust efficiency. Our results construct a new basis to compare the TTCM with the other school choice mechanisms, especially the student-optimal stable mechanism under Ergin but not Kesten-acyclic priority structures."
http://arxiv.org/abs/2104.09210v1,A public micro pension programme in Brazil: Heterogeneity among states and setting up of benefit age adjustment,2021-04-19 11:11:57+00:00,"['Renata Gomes Alcoforado', 'Alfredo D. Egídio dos Reis']",econ.GN,"Brazil is the 5th largest country in the world, despite of having a ``High Human Development'' it is the 9th most unequal country. The existing Brazilian micro pension programme is one of the safety nets for poor people. To become eligible for this benefit, each person must have an income that is less than a quarter of the Brazilian minimum monthly wage and be either over 65 or considered disabled. That minimum income corresponds to approximately $2$ dollars per day. This paper analyses quantitatively some aspects of this programme in the Public Pension System of Brazil. We look for the impact of some particular economic variables on the number of people receiving the benefit, and seek if that impact significantly differs among the 27 Brazilian Federal Units. We search for heterogeneity. We perform regression and spatial cluster analysis for detection of geographical grouping. We use a database that includes the entire population that receives the benefit. Afterwards, we calculate the amount that the system spends with the beneficiaries, estimate values \textit{per capita} and the weight of each UF, searching for heterogeneity reflected on the amount spent \textit{per capita}. In this latter calculation we use a more comprehensive database, by individual, that includes all people that started receiving a benefit under the programme in the period from 2nd of January 2018 to 6th of April 2018. We compute the expected discounted benefit and confirm a high heterogeneity among UF's as well as gender. We propose achieving a more equitable system by introducing `age adjusting factors' to change the benefit age."
http://arxiv.org/abs/2104.08934v1,Costlier switching strengthens competition even without advertising,2021-04-18 19:02:30+00:00,['Sander Heinsalu'],econ.TH,"Consumers only discover at the first seller which product best fits their needs, then check its price online, then decide on buying. Switching sellers is costly. Equilibrium prices fall in the switching cost, eventually to the monopoly level, despite the exit of lower-value consumers when changing sellers becomes costlier. More expensive switching makes some buyers exit the market, leaving fewer inframarginal buyers to the sellers. Marginal buyers may change in either direction, so for a range of parameters, all firms cut prices."
http://arxiv.org/abs/2104.04703v1,Echo Chambers: Voter-to-Voter Communication and Political Competition,2021-04-10 07:47:44+00:00,['Monica Anna Giovanniello'],econ.TH,"I study how strategic communication among voters shapes both political outcomes and parties' advertising strategies in a model of informative campaign advertising. Two main results are derived. First, echo chambers arise endogenously. Surprisingly, a small ideological distance between voters is not sufficient to guarantee that a chamber is created, bias direction plays a crucial role. Second, when voters' network entails a significant waste of information, parties tailor their advertising to the opponent's supporters rather than to their own."
http://arxiv.org/abs/2104.04744v1,WTO GPA and Sustainable Procurement as Tools for Transitioning to a Circular Economy,2021-04-10 12:05:13+00:00,['Sareesh Rawat'],econ.GN,"We live in an age of consumption with an ever-increasing demand of already scarce resources and equally fast growing problems of waste generation and climate change. To tackle these difficult issues, we must learn from mother nature. Just like waste does not exist in nature, we must strive to create circular ecosystems where waste is minimized and energy is conserved. This paper focuses on how public procurement can help us transition to a more circular economy, while navigating international trade laws that govern it."
http://arxiv.org/abs/2104.07723v1,A robust specification test in linear panel data models,2021-04-15 19:12:36+00:00,"['Beste Hamiye Beyaztas', 'Soutir Bandyopadhyay', 'Abhijit Mandal']",stat.ME,"The presence of outlying observations may adversely affect statistical testing procedures that result in unstable test statistics and unreliable inferences depending on the distortion in parameter estimates. In spite of the fact that the adverse effects of outliers in panel data models, there are only a few robust testing procedures available for model specification. In this paper, a new weighted likelihood based robust specification test is proposed to determine the appropriate approach in panel data including individual-specific components. The proposed test has been shown to have the same asymptotic distribution as that of most commonly used Hausman's specification test under null hypothesis of random effects specification. The finite sample properties of the robust testing procedure are illustrated by means of Monte Carlo simulations and an economic-growth data from the member countries of the Organisation for Economic Co-operation and Development. Our records reveal that the robust specification test exhibit improved performance in terms of size and power of the test in the presence of contamination."
http://arxiv.org/abs/2105.05335v2,Robust Inference on Income Inequality: $t$-Statistic Based Approaches,2021-05-11 20:29:05+00:00,"['Rustam Ibragimov', 'Paul Kattuman', 'Anton Skrobotov']",econ.EM,"Empirical analyses on income and wealth inequality and those in other fields in economics and finance often face the difficulty that the data is heterogeneous, heavy-tailed or correlated in some unknown fashion. The paper focuses on applications of the recently developed \textit{t}-statistic based robust inference approaches in the analysis of inequality measures and their comparisons under the above problems. Following the approaches, in particular, a robust large sample test on equality of two parameters of interest (e.g., a test of equality of inequality measures in two regions or countries considered) is conducted as follows: The data in the two samples dealt with is partitioned into fixed numbers $q_1, q_2\ge 2$ (e.g., $q_1=q_2=2, 4, 8$) of groups, the parameters (inequality measures dealt with) are estimated for each group, and inference is based on a standard two-sample $t-$test with the resulting $q_1, q_2$ group estimators. Robust $t-$statistic approaches result in valid inference under general conditions that group estimators of parameters (e.g., inequality measures) considered are asymptotically independent, unbiased and Gaussian of possibly different variances, or weakly converge, at an arbitrary rate, to independent scale mixtures of normal random variables. These conditions are typically satisfied in empirical applications even under pronounced heavy-tailedness and heterogeneity and possible dependence in observations. The methods dealt with in the paper complement and compare favorably with other inference approaches available in the literature. The use of robust inference approaches is illustrated by an empirical analysis of income inequality measures and their comparisons across different regions in Russia."
http://arxiv.org/abs/2105.04718v1,Economic analysis of tidal stream turbine arrays: a review,2021-05-11 00:14:33+00:00,"['Zoe Goss', 'Daniel Coles', 'Matthew Piggott']",econ.GN,"This tidal stream energy industry has to date been comprised of small demonstrator projects made up of one to a four turbines. However, there are currently plans to expand to commercially sized projects with tens of turbines or more. As the industry moves to large-scale arrays for the first time, there has been a push to develop tools to optimise the array design and help bring down the costs. This review investigates different methods of modelling the economic performance of tidal-stream arrays, for use within these optimisation tools. The different cost reduction pathways are discussed from costs falling as the global installed capacity increases, due to greater experience, improved power curves through larger-diameter higher-rated turbines, to economic efficiencies that can be found by moving to large-scale arrays. A literature review is conducted to establish the most appropriate input values for use in economic models. This includes finding a best case, worst case and typical values for costs and other related parameters. The information collated in this review can provide a useful steering for the many optimisation tools that have been developed, especially when cost information is commercially sensitive and a realistic parameter range is difficult to obtain."
http://arxiv.org/abs/2104.04327v1,Projection Bias in Effort Choices,2021-04-09 12:21:00+00:00,['Marc Kaufmann'],econ.TH,"Working becomes harder as we grow tired or bored. I model individuals who underestimate these changes in marginal disutility -- as implied by ""projection bias"" -- when deciding whether or not to continue working. This bias causes people's plans to change: early in the day when they are rested, they plan to work more than late in the day when they are rested. Despite initially overestimating how much they will work, people facing a single task with decreasing returns to effort work optimally. However, when facing multiple tasks, they misprioritize urgent but unimportant over important but non-urgent tasks. And when they face a single task with all-or-nothing rewards (such as being promoted) they start, and repeatedly work on, some overly ambitious tasks that they later abandon. Each day they stop working once they have grown tired, which can lead to large daily welfare losses. Finally, when they have either increasing or decreasing productivity, people work less each day than previously planned. This moves people closer to optimal effort for decreasing, and further away from optimal effort for increasing productivity."
http://arxiv.org/abs/2104.06170v1,The Ruble Collapse in an Online Marketplace: Some Lessons for Market Designers,2021-04-13 13:22:00+00:00,['John Horton'],econ.GN,"The sharp devaluation of the ruble in 2014 increased the real returns to Russians from working in a global online labor marketplace, as con- tracts in this market are dollar-denominated. Russians clearly noticed the opportunity, with Russian hours-worked increasing substantially, primarily on the extensive margin -- incumbent Russians already active were fairly inelastic. Contrary to the predictions of bargaining models, there was little to no pass-through of the ruble price changes in to wages. There was also no evidence of a demand-side response, with buyers not posting more ""Russian friendly"" jobs, suggesting limited cross-side externalities. The key findings -- a high extensive margin elasticity but low intensive margin elasticity; little pass-through into wages; and little evidence of a cross-side externality -- have implications for market designers with respect to pricing and supply acquisition."
http://arxiv.org/abs/2105.02785v1,Stock Price Forecasting in Presence of Covid-19 Pandemic and Evaluating Performances of Machine Learning Models for Time-Series Forecasting,2021-05-04 14:55:57+00:00,"['Navid Mottaghi', 'Sara Farhangdoost']",q-fin.ST,"With the heightened volatility in stock prices during the Covid-19 pandemic, the need for price forecasting has become more critical. We investigated the forecast performance of four models including Long-Short Term Memory, XGBoost, Autoregression, and Last Value on stock prices of Facebook, Amazon, Tesla, Google, and Apple in COVID-19 pandemic time to understand the accuracy and predictability of the models in this highly volatile time region. To train the models, the data of all stocks are split into train and test datasets. The test dataset starts from January 2020 to April 2021 which covers the COVID-19 pandemic period. The results show that the Autoregression and Last value models have higher accuracy in predicting the stock prices because of the strong correlation between the previous day and the next day's price value. Additionally, the results suggest that the machine learning models (Long-Short Term Memory and XGBoost) are not performing as well as Autoregression models when the market experiences high volatility."
http://arxiv.org/abs/2104.01773v1,Spatial parking planning design with mixed conventional and autonomous vehicles,2021-04-05 04:59:27+00:00,"['Qida Su', 'David Z. W. Wang']",econ.GN,"Travellers in autonomous vehicles (AVs) need not to walk to the destination any more after parking like those in conventional human-driven vehicles (HVs). Instead, they can drop off directly at the destination and AVs can cruise for parking autonomously. It is a revolutionary change that such parking autonomy of AVs may increase the potential parking span substantially and affect the spatial parking equilibrium. Given this, from urban planners' perspective, it is of great necessity to reconsider the planning of parking supply along the city. To this end, this paper is the first to examine the spatial parking equilibrium considering the mix of AVs and HVs with parking cruising effect. It is found that the equilibrium solution of travellers' parking location choices can be biased due to the ignorance of cruising effects. On top of that, the optimal parking span of AVs at given parking supply should be no less than that at equilibrium. Besides, the optimal parking planning to minimize the total parking cost is also explored in a bi-level parking planning design problem (PPDP). While the optimal differentiated pricing allows the system to achieve optimal parking distribution, this study suggests that it is beneficial to encourage AVs to cruise further to park by reserving less than enough parking areas for AVs."
http://arxiv.org/abs/2105.00130v2,Integrating Hydrogen in Single-Price Electricity Systems: The Effects of Spatial Economic Signals,2021-05-01 00:36:44+00:00,"['Frederik vom Scheidt', 'Jingyi Qu', 'Philipp Staudt', 'Dharik S. Mallapragada', 'Christof Weinhardt']",econ.GN,"Hydrogen can contribute substantially to the reduction of carbon emissions in industry and transportation. However, the production of hydrogen through electrolysis creates interdependencies between hydrogen supply chains and electricity systems. Therefore, as governments worldwide are planning considerable financial subsidies and new regulation to promote hydrogen infrastructure investments in the next years, energy policy research is needed to guide such policies with holistic analyses. In this study, we link a electrolytic hydrogen supply chain model with an electricity system dispatch model, for a cross-sectoral case study of Germany in 2030. We find that hydrogen infrastructure investments and their effects on the electricity system are strongly influenced by electricity prices. Given current uniform prices, hydrogen production increases congestion costs in the electricity grid by 17%. In contrast, passing spatially resolved electricity price signals leads to electrolyzers being placed at low-cost grid nodes and further away from consumption centers. This causes lower end-use costs for hydrogen. Moreover, congestion management costs decrease substantially, by up to 20% compared to the benchmark case without hydrogen. These savings could be transferred into according subsidies for hydrogen production. Thus, our study demonstrates the benefits of differentiating economic signals for hydrogen production based on spatial criteria."
http://arxiv.org/abs/2105.00517v1,The Black Market for Beijing License Plates,2021-05-02 17:50:54+00:00,"['Øystein Daljord', 'Guillaume Pouliot', 'Junji Xiao', 'Mandy Hu']",econ.GN,"Black markets can reduce the effects of distortionary regulations by reallocating scarce resources toward consumers who value them most. The illegal nature of black markets, however, creates transaction costs that reduce the gains from trade. We take a partial identification approach to infer gains from trade and transaction costs in the black market for Beijing car license plates, which emerged following their recent rationing. We find that at least 11% of emitted license plates are illegally traded. The estimated transaction costs suggest severe market frictions: between 61% and 82% of the realized gains from trade are lost to transaction costs."
http://arxiv.org/abs/2105.00617v1,Selection and Behavioral Responses of Health Insurance Subsidies in the Long Run: Evidence from a Field Experiment in Ghana,2021-05-03 03:40:14+00:00,"['Patrick Opoku Asuming', 'Hyuncheol Bryant Kim', 'Armand Sim']",econ.GN,"We conduct a randomized experiment that varies one-time health insurance subsidy amounts (partial and full) in Ghana to study the impacts of subsidies on insurance enrollment and health care utilization. We find that both partial and full subsidies promote insurance enrollment in the long run, even after the subsidies expired. Although the long run enrollment rate and selective enrollment do not differ by subsidy level, long run health care utilization increased only for the partial subsidy group. We show that this can plausibly be explained by stronger learning-through-experience behavior in the partial than in the full subsidy group."
http://arxiv.org/abs/2105.00458v1,A model of inter-organizational network formation,2021-05-02 12:30:39+00:00,"['Shweta Gaonkar', 'Angelo Mele']",econ.EM,"How do inter-organizational networks emerge? Accounting for interdependence among ties while studying tie formation is one of the key challenges in this area of research. We address this challenge using an equilibrium framework where firms' decisions to form links with other firms are modeled as a strategic game. In this game, firms weigh the costs and benefits of establishing a relationship with other firms and form ties if their net payoffs are positive. We characterize the equilibrium networks as exponential random graphs (ERGM), and we estimate the firms' payoffs using a Bayesian approach. To demonstrate the usefulness of our approach, we apply the framework to a co-investment network of venture capital firms in the medical device industry. The equilibrium framework allows researchers to draw economic interpretation from parameter estimates of the ERGM Model. We learn that firms rely on their joint partners (transitivity) and prefer to form ties with firms similar to themselves (homophily). These results hold after controlling for the interdependence among ties. Another, critical advantage of a structural approach is that it allows us to simulate the effects of economic shocks or policy counterfactuals. We test two such policy shocks, namely, firm entry and regulatory change. We show how new firms' entry or a regulatory shock of minimum capital requirements increase the co-investment network's density and clustering."
http://arxiv.org/abs/2105.00216v1,Lecture Notes on Voting Theory,2021-05-01 11:19:44+00:00,['Davide Grossi'],cs.MA,These lecture notes have been developed for the course Computational Social Choice of the Artificial Intelligence MSc programme at the University of Groningen. They cover mathematical and algorithmic aspects of voting theory.
http://arxiv.org/abs/2104.11595v1,Does home advantage without crowd exist in American football?,2021-04-22 09:18:17+00:00,"['Dávid Zoltán Szabó', 'Diego Andrés Pérez']",econ.GN,"It is well-known that home team has an inherent advantage against visiting teams when playing team sports. One of the most obvious underlying reasons, the presence of supporting fans has mostly disappeared in major leagues with the emergence of COVID-19 pandemic. This paper investigates with the help of historical National Football League (NFL) data, how much effect spectators have on the game outcome. Our findings reveal that under no allowance of spectators the home teams' performance is substantially lower than under normal circumstances, even performing slightly worse than the visiting teams. On the other hand, when a limited amount of spectators are allowed to the game, the home teams' performance is no longer significantly different than what we observe with full stadiums. This suggests that from a psychological point of view the effect of crowd support is already induced by a fraction of regular fans."
http://arxiv.org/abs/2104.10205v2,On the relation between Preference Reversal and Strategy-Proofness,2021-04-20 18:50:28+00:00,"['K. P. S. Bhaskara Rao', 'Achille Basile', 'Surekha Rao']",econ.TH,"We analyze the relation between strategy-proofness and preference reversal in the case that agents may declare indifference. Interestingly, Berga and Moreno (2020), have recently derived preference reversal from group strategy-proofness of social choice functions on strict preferences domains if the range has no more than three elements. We extend this result and at the same time simplify it. Our analysis points out the role of individual strategy-proofness in deriving the preference reversal property, giving back to the latter its original individual nature (cfr. Eliaz, 2004).
  Moreover, we show that the difficulties Berga and Moreno highlighted relaxing the assumption on the cardinality of the range, disappear under a proper assumption on the domain. We introduce the concept of complete sets of preferences and show that individual strategy-proofness is sufficient to obtain the preference reversal property when the agents' feasible set of orderings is complete. This covers interesting cases like single peaked preferences, rich domains admitting regular social choice functions, and universal domains. The fact that we use individual rather than group strategy-proofness, allows to get immediately some of the known, and some new, equivalences between individual and group strategy-proofness. Finally, we show that group strategy-proofness is only really needed to obtain preference reversal if there are infinitely many voters."
http://arxiv.org/abs/2104.10281v1,Nonlinear Pricing with Misspecified and Arbitrary Perception of the Marginal Price,2021-04-20 23:22:42+00:00,['Diego Alejandro Murillo Taborda'],econ.GN,"In the context of nonlinear prices, the empirical evidence suggests that the consumers have cognitive biases represented in a limited understanding of nonlinear price structures, and they respond to some alternative perceptions of the marginal prices. In particular, consumers usually make choices based more on the average than the marginal prices, which can result in a suboptimal behavior. Taking the misspecification in the marginal price as exogenous, this document analyzes how is the optimal quadratic price scheme for a monopolist and the optimal quadratic price scheme that maximizes welfare when there is a continuum of representative consumers with an arbitrary perception of the marginal price. Under simple preferences and costs functional forms and very straightforward hypotheses, the results suggest that the bias in the marginal price doesn't affect the maximum welfare attainable with quadratic price schemes, and it has a negligible effect over the efficiency cost caused by the monopolist, so the misspecification in the marginal price is not relevant for welfare increasing policies. However, almost always the misspecification in the marginal price is beneficial for the monopolist. An interesting result is that under these functional forms, more misspecification in the marginal price is beneficial for both the consumers and the monopolist if the level of bias is low, so not always is socially optima to have educated consumers about the real marginal price. Finally, the document shows that the bias in the marginal price has a negative effect on reduction of aggregate consumption using two-tier increasing tariffs, which are commonly used for reduction of aggregate consumption."
http://arxiv.org/abs/2104.10528v1,Random perfect information games,2021-04-21 13:38:03+00:00,"['János Flesch', 'Arkadi Predtetchinski', 'Ville Suomala']",cs.GT,"The paper proposes a natural measure space of zero-sum perfect information games with upper semicontinuous payoffs. Each game is specified by the game tree, and by the assignment of the active player and of the capacity to each node of the tree. The payoff in a game is defined as the infimum of the capacity over the nodes that have been visited during the play. The active player, the number of children, and the capacity are drawn from a given joint distribution independently across the nodes. We characterize the cumulative distribution function of the value $v$ using the fixed points of the so-called value generating function. The characterization leads to a necessary and sufficient condition for the event $v \geq k$ to occur with positive probability. We also study probabilistic properties of the set of Player I's $k$-optimal strategies and the corresponding plays."
http://arxiv.org/abs/2104.12387v1,To What Extent do Labor Market Outcomes respond to UI Extensions?,2021-04-26 07:58:41+00:00,['Aiwei Huang'],econ.GN,"Unemployment benefits in the US were extended by up to 73 weeks during the Great Recession. Equilibrium labor market theory indicates that extensions of benefit duration impact not only search decisions by job seekers but also job vacancy creations by employers. Most of the literature focused on the former to show partial equilibrium effect that increment of unemployment benefits discourage job search and lead to a rise in unemployment. To study the total effect of UI benefit extensions on unemployment, I follow border county identification strategy, take advantage of quasi-differenced specification to control for changes in future benefit policies, apply interactive fixed effects model to deal with unobserved shocks so as to obtain unbiased and consistent estimation. I find that benefit extensions have a statistically significant positive effect on unemployment, which is consistent with the results of prevailing literature."
http://arxiv.org/abs/2104.12706v1,The Impact of Brazil on Global Grain Dynamics: A Study on Cross-Market Volatility Spillovers,2021-04-22 15:04:47+00:00,"['Felipe Avileis', 'Mindy Mallory']",econ.GN,"Brazil rose as a global powerhouse producer of soybeans and corn over the past 15 years has fundamentally changed global markets in these commodities. This is arguably due to the development of varieties of soybean and corn adapted to climates within Brazil, allowing farmers to double-crop corn after soybeans in the same year. Corn and soybean market participants increasingly look to Brazil for fundamental price information, and studies have shown that the two markets have become cointegrated. However little is known about how much volatility from each market spills over to the other. In this article we measure volatility spillover ratios between U.S. and Brazilian first crop corn, second crop corn, and soybeans. We find that linkages between the two countries increased after double cropping corn after soybeans expanded, volatility spillover magnitudes expanded, and the direction of volatility spillovers flipped from U.S. volatility spilling over to Brazil before double cropping, to Brazil spilling over to U.S. after double cropping."
http://arxiv.org/abs/2104.13425v1,State capacity and vulnerability to natural disasters,2021-04-27 18:52:13+00:00,['Richard S. J. Tol'],econ.GN,"Many empirical studies have shown that government quality is a key determinant of vulnerability to natural disasters. Protection against natural disasters can be a public good -- flood protection, for example -- or a natural monopoly -- early warning systems, for instance. Recovery from natural disasters is easier when the financial system is well-developed, particularly insurance services. This requires a strong legal and regulatory environment. This paper reviews the empirical literature to find that government quality and democracy reduce vulnerability to natural disasters while corruption of public officials increases vulnerability. The paper complements the literature by including tax revenue as an explanatory variable for vulnerability to natural disasters, and by modelling both the probability of natural disaster and the damage done. Countries with a larger public sector are better at preventing extreme events from doing harm. Countries that take more of their revenue in income taxes are better that reducing harm from natural disasters."
http://arxiv.org/abs/2104.03219v1,The Value of Excess Supply in Spatial Matching Markets,2021-04-07 16:15:54+00:00,"['Mohammad Akbarpour', 'Yeganeh Alimohammadi', 'Shengwu Li', 'Amin Saberi']",cs.DS,"We study dynamic matching in a spatial setting. Drivers are distributed at random on some interval. Riders arrive in some (possibly adversarial) order at randomly drawn points. The platform observes the location of the drivers, and can match newly arrived riders immediately, or can wait for more riders to arrive. Unmatched riders incur a waiting cost $c$ per period. The platform can match riders and drivers, irrevocably. The cost of matching a driver to a rider is equal to the distance between them. We quantify the value of slightly increasing supply. We prove that when there are $(1+ε)$ drivers per rider (for any $ε> 0$), the cost of matching returned by a simple greedy algorithm which pairs each arriving rider to the closest available driver is $O(\log^3(n))$, where $n$ is the number of riders. On the other hand, with equal number of drivers and riders, even the \emph{ex post} optimal matching does not have a cost less than $Θ(\sqrt{n})$. Our results shed light on the important role of (small) excess supply in spatial matching markets."
http://arxiv.org/abs/2104.02752v1,Multiscale Governance,2021-04-06 19:23:44+00:00,"['David Pastor-Escuredo', 'Philip Treleaven']",cs.CY,"Future societal systems will be characterized by heterogeneous human behaviors and also collective action. The interaction between local systems and global systems will be complex. Humandemics will propagate because of the pathways that connect the different systems and several invariant behaviors and patterns that have emerged globally. On the contrary, infodemics of misinformation can be a risk as it has occurred in the COVID-19 pandemic. The emerging fragility or robustness of the system will depend on how this complex network of systems is governed. Future societal systems will not be only multiscale in terms of the social dimension, but also in the temporality. Necessary and proper prevention and response systems based on complexity, ethic and multi-scale governance will be required. Real-time response systems are the basis for resilience to be the foundation of robust societies. A top-down approach led by Governmental organs for managing humandemics is not sufficient and may be only effective if policies are very restrictive and their efficacy depends not only in the measures implemented but also on the dynamics of the policies and the population perception and compliance. This top-down approach is even weaker if there is not national and international coordination. Coordinating top-down agencies with bottom-up constructs will be the design principle. Multi-scale governance integrates decision-making processes with signaling, sensing and leadership mechanisms to drive thriving societal systems with real-time sensitivity."
http://arxiv.org/abs/2106.11120v1,Mechanism Design for Efficient Nash Equilibrium in Oligopolistic Markets,2021-06-21 13:57:26+00:00,"['Kaiying Lin', 'Beibei Wang', 'Pengcheng You']",math.OC,"This paper investigates the efficiency loss in social cost caused by strategic bidding behavior of individual participants in a supply-demand balancing market, and proposes a mechanism to fully recover equilibrium social optimum via subsidization and taxation. We characterize the competition among supply-side firms to meet given inelastic demand, with linear supply function bidding and the proposed efficiency recovery mechanism. We show that the Nash equilibrium of such a game exists under mild conditions, and more importantly, it achieves the underlying efficient supply dispatch and the market clearing price that reflects the truthful system marginal production cost. Further, the mechanism can be tuned to guarantee self-sufficiency, i.e., taxes collected counterbalance subsidies needed. Extensive numerical case studies are run to validate the equilibrium analysis, and we employ individual net profit and a modified version of Lerner index as two metrics to evaluate the impact of the mechanism on market outcomes by varying its tuning parameter and firm heterogeneity."
http://arxiv.org/abs/2105.11077v1,Does Geopolitics Have an Impact on Energy Trade? Empirical Research on Emerging Countries,2021-05-24 03:12:23+00:00,"['Fen Li', 'Cunyi Yang', 'Zhenghui Li', 'Pierre Failler']",econ.GN,"The energy trade is an important pillar of each country's development, making up for the imbalance in the production and consumption of fossil fuels. Geopolitical risks affect the energy trade of various countries to a certain extent, but the causes of geopolitical risks are complex, and energy trade also involves many aspects, so the impact of geopolitics on energy trade is also complex. Based on the monthly data from 2000 to 2020 of 17 emerging economies, this paper employs the fixed-effect model and the regression-discontinuity (RD) model to verify the negative impact of geopolitics on energy trade first and then analyze the mechanism and heterogeneity of the impact. The following conclusions are drawn: First, geopolitics has a significant negative impact on the import and export of the energy trade, and the inhibition on the export is greater than that on the import. Second, the impact mechanism of geopolitics on the energy trade is reflected in the lagging effect and mediating effect on the imports and exports; that is, the negative impact of geopolitics on energy trade continued to be significant 10 months later. Coal and crude oil prices, as mediating variables, decreased to reduce the imports and exports, whereas natural gas prices showed an increase. Third, the impact of geopolitics on energy trade is heterogeneous in terms of national attribute characteristics and geo-event types."
http://arxiv.org/abs/2105.12044v1,"Climate, Agriculture and Food",2021-05-25 16:22:28+00:00,['Ariel Ortiz-Bobea'],econ.GN,Agriculture is arguably the most climate-sensitive sector of the economy. Growing concerns about anthropogenic climate change have increased research interest in assessing its potential impact on the sector and in identifying policies and adaptation strategies to help the sector cope with a changing climate. This chapter provides an overview of recent advancements in the analysis of climate change impacts and adaptation in agriculture with an emphasis on methods. The chapter provides an overview of recent research efforts addressing key conceptual and empirical challenges. The chapter also discusses practical matters about conducting research in this area and provides reproducible R code to perform common tasks of data preparation and model estimation in this literature. The chapter provides a hands-on introduction to new researchers in this area.
http://arxiv.org/abs/2105.11829v1,Perception of corruption influences entrepreneurship inside established companies,2021-05-25 10:54:37+00:00,"['F. Javier Sanchez-Vidal', 'Camino Ramon-Llorens']",econ.GN,"Based on the Global Entrepreneurship Monitor (GEM) surveys and conducting a panel data estimation to test our hypothesis, this paper examines whether corruption perceptions might sand or grease the wheels for entrepreneurship inside companies or intrapreneurship in a sample of 92 countries for the period 2012 to 2019. Our results find that the corruption perception sands the wheel for intrapreneurship. There is evidence of a quadratic relation, but this relation is only clear for the less developed countries, which sort of moderate the very negative effect of corruption for these countries. The results also confirm that corruption influences differently on intrapreneurship depending on the level of development of the country."
http://arxiv.org/abs/2106.06599v1,The value of travel speed,2021-06-11 20:19:44+00:00,['Cornelis Dirk van Goeverden'],econ.GN,"Travel speed is an intrinsic feature of transport, and enlarging the speed is considered as beneficial. The benefit of a speed increase is generally assessed as the value of the saved travel time. However, this approach conflicts with the observation that time spent on travelling is rather constant and might not be affected by speed changes. The paper aims to define the benefits of a speed increase and addresses two research questions. First, how will a speed increase in person transport work out, which factors are affected? Second, is the value of time a good proxy for the value of speed? Based on studies on time spending and research on the association between speed and land use, we argue that human wealth could be the main affected factor by speed changes, rather than time or access. Then the value of time is not a good proxy for the value of speed: the benefits of a wealth increase are negatively correlated with prosperity following the law of diminishing marginal utility, while the calculated benefits of saved travel time prove to be positively correlated. The inadequacy of the value of time is explained by some shortcomings with respect to the willingness to pay that is generally used for assessing the value of time: people do not predict correctly the personal benefits that will be gained from a decision, and they neglect the social impacts."
http://arxiv.org/abs/2106.06665v1,An Integration and Operation Framework of Geothermal Heat Pumps in Distribution Networks,2021-06-12 02:01:48+00:00,"['Lei Liang', 'Xuan Zhang', 'Hongbin Sun']",eess.SY,"The application of the energy-efficient thermal and energy management mechanism in Geothermal Heat Pumps (GHPs) is indispensable to reduce the overall energy consumption and carbon emission across the building sector. Besides, in the Virtual Power Plant (VPP) system, the demand response of clustered GHP systems can improve the operating flexibility of the power grid. This paper presents an integration and operation framework of GHPs in the distribution network, applying a layered communication and optimization method to coordinate multiple clustered GHPs in a community. In the proposed hierarchical operation scheme, the operator of regional GHPs collects the thermal zone information and the disturbance prediction of buildings in a short time granularity, predicts the energy demand, and transmits the information to an aggregator. Using a novel linearized optimal power flow model, the aggregator coordinates and aggregates load resources of GHP systems in the distribution network. In this way, GHP systems with thermal and energy management mechanisms can be applied to achieve the demand response in the VPP and offer more energy flexibility to the community."
http://arxiv.org/abs/2106.08047v1,Comparisons of Australian Mental Health Distributions,2021-06-15 11:07:51+00:00,"['David Gunawan', 'William Griffiths', 'Duangkamon Chotikapanich']",econ.EM,"Bayesian nonparametric estimates of Australian mental health distributions are obtained to assess how the mental health status of the population has changed over time and to compare the mental health status of female/male and indigenous/non-indigenous population subgroups. First- and second-order stochastic dominance are used to compare distributions, with results presented in terms of the posterior probability of dominance and the posterior probability of no dominance. Our results suggest mental health has deteriorated in recent years, that males mental health status is better than that of females, and non-indigenous health status is better than that of the indigenous population."
http://arxiv.org/abs/2106.08066v1,Re-examining the Philosophical Underpinnings of the Melting Pot vs. Multiculturalism in the Current Immigration Debate in the United States,2021-06-15 11:46:57+00:00,"['Daniel Woldeab', 'Robert Yawson', 'Irina Woldeab']",econ.GN,"Immigration to the United States is certainly not a new phenomenon, and it is therefore natural for immigration, culture and identity to be given due attention by the public and policy makers. However, current discussion of immigration, legal and illegal, and the philosophical underpinnings is lost in translation, not necessarily on ideological lines, but on political orientation. In this paper we reexamine the philosophical underpinnings of the melting pot versus multiculturalism as antecedents and precedents of current immigration debate and how the core issues are lost in translation. We take a brief look at immigrants and the economy to situate the current immigration debate. We then discuss the two philosophical approaches to immigration and how the understanding of the philosophical foundations can help streamline the current immigration debate."
http://arxiv.org/abs/2106.15698v1,Emotions in Macroeconomic News and their Impact on the European Bond Market,2021-06-15 10:15:18+00:00,"['Sergio Consoli', 'Luca Tiozzo Pezzoli', 'Elisa Tosetti']",econ.GN,"We show how emotions extracted from macroeconomic news can be used to explain and forecast future behaviour of sovereign bond yield spreads in Italy and Spain. We use a big, open-source, database known as Global Database of Events, Language and Tone to construct emotion indicators of bond market affective states. We find that negative emotions extracted from news improve the forecasting power of government yield spread models during distressed periods even after controlling for the number of negative words present in the text. In addition, stronger negative emotions, such as panic, reveal useful information for predicting changes in spread at the short-term horizon, while milder emotions, such as distress, are useful at longer time horizons. Emotions generated by the Italian political turmoil propagate to the Spanish news affecting this neighbourhood market."
http://arxiv.org/abs/2107.00106v1,"Choice of a Mentor: A Subjective Evaluation of Expectations, Experiences and Feedbacks",2021-06-30 21:18:37+00:00,['Kaibalyapati Mishra'],econ.GN,"Recent trends in academics show an increase in enrollment levels in higher education Predominantly in Doctoral programmes where individual scholars institutes and supervisors play the key roles The human factor at receiving end of academic excellence is the scholar having a supervisor at the facilitating end In this paper I try to establish the role of different factors and availability of information about them in forming the basic choice set in a scholars mind After studying three different groups of individuals who were subjected to substitutive choices we found that scholars prefer an approachable, moderately intervening and frequently interacting professor as their guide"
http://arxiv.org/abs/2106.10341v1,Scalable Econometrics on Big Data -- The Logistic Regression on Spark,2021-06-18 20:10:34+00:00,"['Aurélien Ouattara', 'Matthieu Bulté', 'Wan-Ju Lin', 'Philipp Scholl', 'Benedikt Veit', 'Christos Ziakas', 'Florian Felice', 'Julien Virlogeux', 'George Dikos']",stat.CO,"Extra-large datasets are becoming increasingly accessible, and computing tools designed to handle huge amount of data efficiently are democratizing rapidly. However, conventional statistical and econometric tools are still lacking fluency when dealing with such large datasets. This paper dives into econometrics on big datasets, specifically focusing on the logistic regression on Spark. We review the robustness of the functions available in Spark to fit logistic regression and introduce a package that we developed in PySpark which returns the statistical summary of the logistic regression, necessary for statistical inference."
http://arxiv.org/abs/2105.08475v1,AI and Shared Prosperity,2021-05-18 12:37:18+00:00,"['Katya Klinova', 'Anton Korinek']",cs.AI,"Future advances in AI that automate away human labor may have stark implications for labor markets and inequality. This paper proposes a framework to analyze the effects of specific types of AI systems on the labor market, based on how much labor demand they will create versus displace, while taking into account that productivity gains also make society wealthier and thereby contribute to additional labor demand. This analysis enables ethically-minded companies creating or deploying AI systems as well as researchers and policymakers to take into account the effects of their actions on labor markets and inequality, and therefore to steer progress in AI in a direction that advances shared prosperity and an inclusive economic future for all of humanity."
http://arxiv.org/abs/2106.05972v2,The separation of market and price in some free competitions and its related solution to the over-application problem in the job market,2021-06-10 02:44:50+00:00,['Vincent Zha'],econ.GN,"According to common understanding, in free completion of a private product, market and price, the two main factors in the competition that leads to economic efficiency, always exist together. This paper, however, points out the phenomenon that in some free competitions the two factors are separated hence causing inefficiency. For one type, the market exists whereas the price is absent, i.e. free, for a product. An example of this type is the job application market where the problem of over-application commonly exists, costing recruiters much time in finding desired candidates from massive applicants, resulting in inefficiency. To solve the problem, this paper proposes a solution that the recruiters charge submission fees to the applications to make the competition complete with both factors, hence enhancing the efficiency. For the other type, the price exists whereas the market is absent for a product. An example of this type is the real estate agent market, where the price of the agents exists but the market, i.e. the facility allowing the sellers' information to be efficiently discovered, is largely absent, also causing inefficiency. In summary, the contribution of this paper consists of two aspects: one is the discovery of the possible separation of the two factors in free competitions; the other is, thanks to the discovery, a solution to the over-application problem in the job market."
http://arxiv.org/abs/2106.11012v1,Doctors and Nurses Social Media Ads Reduced Holiday Travel and COVID-19 infections: A cluster randomized controlled trial in 13 States,2021-06-21 12:09:08+00:00,"['Emily Breza', 'Fatima Cody Stanford', 'Marcela Alsan', 'M. D. Ph. D.', 'Burak Alsan', 'Abhijit Banerjee', 'Arun G. Chandrasekhar', 'Sarah Eichmeyer', 'Traci Glushko', 'Paul Goldsmith-Pinkham', 'Kelly Holland', 'Emily Hoppe', 'Mohit Karnani', 'Sarah Liegl', 'Tristan Loisel', 'Lucy Ogbu-Nwobodo', 'Benjamin A. Olken Carlos Torres', 'Pierre-Luc Vautrey', 'Erica Warner', 'Susan Wootton', 'Esther Duflo']",econ.GN,"During the COVID-19 epidemic, many health professionals started using mass communication on social media to relay critical information and persuade individuals to adopt preventative health behaviors. Our group of clinicians and nurses developed and recorded short video messages to encourage viewers to stay home for the Thanksgiving and Christmas Holidays. We then conducted a two-stage clustered randomized controlled trial in 820 counties (covering 13 States) in the United States of a large-scale Facebook ad campaign disseminating these messages. In the first level of randomization, we randomly divided the counties into two groups: high intensity and low intensity. In the second level, we randomly assigned zip codes to either treatment or control such that 75% of zip codes in high intensity counties received the treatment, while 25% of zip codes in low intensity counties received the treatment. In each treated zip code, we sent the ad to as many Facebook subscribers as possible (11,954,109 users received at least one ad at Thanksgiving and 23,302,290 users received at least one ad at Christmas). The first primary outcome was aggregate holiday travel, measured using mobile phone location data, available at the county level: we find that average distance travelled in high-intensity counties decreased by -0.993 percentage points (95% CI -1.616, -0.371, p-value 0.002) the three days before each holiday. The second primary outcome was COVID-19 infection at the zip-code level: COVID-19 infections recorded in the two-week period starting five days post-holiday declined by 3.5 percent (adjusted 95% CI [-6.2 percent, -0.7 percent], p-value 0.013) in intervention zip codes compared to control zip codes."
http://arxiv.org/abs/2106.15518v1,An expert survey to assess the current status and future challenges of energy system analysis,2021-06-22 09:23:58+00:00,"['Fabian Scheller', 'Frauke Wiese', 'Jann Michael Weinand', 'Dominik Franjo Dominković', 'Russell McKenna']",physics.soc-ph,"Decision support systems like computer-aided energy system analysis (ESA) are considered one of the main pillars for developing sustainable and reliable energy transformation strategies. Although today's diverse tools can already support decision-makers in a variety of research questions, further developments are still necessary. Intending to identify opportunities and challenges in the field, we classify modelling capabilities (32), methodologies (15) implementation issues (15) and management issues (7) from an extensive literature review. Based on a quantitative expert survey of energy system modellers (N=61) mainly working with simulation and optimisation models, the status of development and the complexity of realisation of those modelling topics are assessed. While the rated items are considered to be more complex than actually represented, no significant outliers are determinable, showing that there is no consensus about particular aspects of ESA that are lacking development. Nevertheless, a classification of the items in terms of a specially defined modelling strategy matrix identifies capabilities like land-use planning patterns, equity and distributional effects and endogenous technological learning as ""low hanging fruits"" for enhancement, as well as a large number of complex topics that are already well implemented. The remaining ""tough nuts"" regarding modelling capabilities include non-energy sector and social behaviour interaction effects. In general, the optimisation and simulation models differ in their respective strengths, justifying the existence of both. While methods were generally rated as quite well developed, combinatorial optimisation approaches, as well as machine learning, are identified as important research methods to be developed further for ESA."
http://arxiv.org/abs/2106.15003v1,A Note on the Topology of the First Stage of 2SLS with Many Instruments,2021-06-28 22:22:52+00:00,['Guy Tchuente'],econ.EM,"The finite sample properties of estimators are usually understood or approximated using asymptotic theories. Two main asymptotic constructions have been used to characterize the presence of many instruments. The first assumes that the number of instruments increases with the sample size. I demonstrate that in this case, one of the key assumptions used in the asymptotic construction may imply that the number of ``effective"" instruments should be finite, resulting in an internal contradiction. The second asymptotic representation considers that the number of instrumental variables (IVs) may be finite, infinite, or even a continuum. The number does not change with the sample size. In this scenario, the regularized estimator obtained depends on the topology imposed on the set of instruments as well as on a regularization parameter. These restrictions may induce a bias or restrict the set of admissible instruments. However, the assumptions are internally coherent. The limitations of many IVs asymptotic assumptions provide support for finite sample distributional studies to better understand the behavior of many IV estimators."
http://arxiv.org/abs/2106.14659v1,Applications of Mechanism Design in Market-Based Demand-Side Management,2021-06-24 11:24:41+00:00,"['Khaled Abedrabboh', 'Luluwah Al-Fagih']",cs.GT,"The intermittent nature of renewable energy resources creates extra challenges in the operation and control of the electricity grid. Demand flexibility markets can help in dealing with these challenges by introducing incentives for customers to modify their demand. Market-based demand-side management (DSM) have garnered serious attention lately due to its promising capability of maintaining the balance between supply and demand, while also keeping customer satisfaction at its highest levels. Many researchers have proposed using concepts from mechanism design theory in their approaches to market-based DSM. In this work, we provide a review of the advances in market-based DSM using mechanism design. We provide a categorisation of the reviewed literature and evaluate the strengths and weaknesses of each design criteria. We also study the utility function formulations used in the reviewed literature and provide a critique of the proposed indirect mechanisms. We show that despite the extensiveness of the literature on this subject, there remains concerns and challenges that should be addressed for the realistic implementation of such DSM approaches. We draw conclusions from our review and discuss possible future research directions."
http://arxiv.org/abs/2105.13556v1,Blending Advertising with Organic Content in E-Commerce: A Virtual Bids Optimization Approach,2021-05-28 02:27:31+00:00,"['Carlos Carrion', 'Zenan Wang', 'Harikesh Nair', 'Xianghong Luo', 'Yulin Lei', 'Xiliang Lin', 'Wenlong Chen', 'Qiyu Hu', 'Changping Peng', 'Yongjun Bao', 'Weipeng Yan']",cs.LG,"In e-commerce platforms, sponsored and non-sponsored content are jointly displayed to users and both may interactively influence their engagement behavior. The former content helps advertisers achieve their marketing goals and provides a stream of ad revenue to the platform. The latter content contributes to users' engagement with the platform, which is key to its long-term health. A burning issue for e-commerce platform design is how to blend advertising with content in a way that respects these interactions and balances these multiple business objectives. This paper describes a system developed for this purpose in the context of blending personalized sponsored content with non-sponsored content on the product detail pages of JD.COM, an e-commerce company. This system has three key features: (1) Optimization of multiple competing business objectives through a new virtual bids approach and the expressiveness of the latent, implicit valuation of the platform for the multiple objectives via these virtual bids. (2) Modeling of users' click behavior as a function of their characteristics, the individual characteristics of each sponsored content and the influence exerted by other sponsored and non-sponsored content displayed alongside through a deep learning approach; (3) Consideration of externalities in the allocation of ads, thereby making it directly compatible with a Vickrey-Clarke-Groves (VCG) auction scheme for the computation of payments in the presence of these externalities. The system is currently deployed and serving all traffic through JD.COM's mobile application. Experiments demonstrating the performance and advantages of the system are presented."
http://arxiv.org/abs/2106.00460v1,Mobility and Economic Impact of COVID-19 Restrictions in Italy using Mobile Network Operator Data,2021-05-28 22:15:54+00:00,"['Michele Vespe', 'Umberto Minora', 'Stefano Maria Iacus', 'Spyridon Spyratos', 'Francesco Sermi', 'Matteo Fontana', 'Biagio Ciuffo', 'Panayotis Christidis']",econ.GN,"This work presents the analysis of the impact of restrictions on mobility in Italy, with a focus on the period from 6 November 2020 to 31 January 2021, when a three-tier system based on different levels of risk was adopted and applied at regional level to contrast the second wave of COVID-19. The impact is first evaluated on mobility using Mobile Network Operator anonymised and aggregate data shared in the framework of a Business-to-Government initiative with the European Commission. Mobility data, alongside additional information about electricity consuption, are then used to assess the impacts on an economic level of the three-tier system in different areas of the country."
http://arxiv.org/abs/2106.04218v1,Modeling Portfolios with Leptokurtic and Dependent Risk Factors,2021-06-08 09:57:46+00:00,"['Piero Quatto', 'Gianmarco Vacca', 'Maria Grazia Zoia']",econ.EM,"Recently, an approach to modeling portfolio distribution with risk factors distributed as Gram-Charlier (GC) expansions of the Gaussian law, has been conceived. GC expansions prove effective when dealing with moderately leptokurtic data. In order to cover the case of possibly severe leptokurtosis, the so-called GC-like expansions have been devised by reshaping parent leptokurtic distributions by means of orthogonal polynomials specific to them. In this paper, we focus on the hyperbolic-secant (HS) law as parent distribution whose GC-like expansions fit with kurtosis levels up to 19.4. A portfolio distribution has been obtained with risk factors modeled as GClike expansions of the HS law which duly account for excess kurtosis. Empirical evidence of the workings of the approach dealt with in the paper is included."
http://arxiv.org/abs/2106.04338v1,"Engines of Power: Electricity, AI, and General-Purpose Military Transformations",2021-06-08 13:55:19+00:00,"['Jeffrey Ding', 'Allan Dafoe']",econ.GN,"Major theories of military innovation focus on relatively narrow technological developments, such as nuclear weapons or aircraft carriers. Arguably the most profound military implications of technological change, however, come from more fundamental advances arising from general purpose technologies, such as the steam engine, electricity, and the computer. With few exceptions, political scientists have not theorized about GPTs. Drawing from the economics literature on GPTs, we distill several propositions on how and when GPTs affect military affairs. We call these effects general-purpose military transformations. In particular, we argue that the impacts of GMTs on military effectiveness are broad, delayed, and shaped by indirect productivity spillovers. Additionally, GMTs differentially advantage those militaries that can draw from a robust industrial base in the GPT. To illustrate the explanatory value of our theory, we conduct a case study of the military consequences of electricity, the prototypical GPT. Finally, we apply our findings to artificial intelligence, which will plausibly cause a profound general-purpose military transformation."
http://arxiv.org/abs/2106.03467v1,"The Intellectual Property Protection System of the Foreign Investment Law: Basic Structure, Motivation and Game Logic",2021-06-07 09:49:14+00:00,['Luo Ying'],econ.GN,"The intellectual property protection system constructed by China's Foreign Investment Law has opened a new phase of rule of law protection of intellectual property rights for foreign-invested enterprises, which is an important institutional support indispensable for optimizing the business environment under the rule of law.The development of the regime was influenced by the major concerns of investors' home countries, the ""innovation-driven development"" strategy, and the trend towards a high level of stringent protection of international intellectual property and investment rules.In addition, there is a latent game of interests between multiple subjects, which can be analyzed by constructing two standard formal game models according to legal game theory.The first game model aims to compare and analyze the gains and losses of China and India's IPR protection system for foreign-invested enterprises to attract foreign investment.The second game model is designed to analyze the benefits of China and foreign investors under their respective possible behaviors before and after the inclusion of IPR protection provisions in the Foreign Investment Law, with the optimal solution being a ""moderately cautious"" strategy for foreign investors and a ""strict enforcement"" strategy for China."
http://arxiv.org/abs/2105.14193v1,Characterization of the probability and information entropy of a process with an exponentially increasing sample space and its application to the Broad Money Supply,2021-05-14 14:31:54+00:00,['Laurence F Lacey'],q-fin.ST,"There is a random variable (X) with a determined outcome (i.e., X = x0), p(x0) = 1. Consider x0 to have a discrete uniform distribution over the integer interval [1, s], where the size of the sample space (s) = 1, in the initial state, such that p(x0) = 1. What is the probability of x0 and the associated information entropy (H), as s increases exponentially? If the sample space expansion occurs at an exponential rate (rate constant = lambda) with time (t) and applying time scaling, such that T = lambda x t, gives: p(x0|T)=exp(-T) and H(T)=T. The characterization has also been extended to include exponential expansion by means of simultaneous, independent processes, as well as the more general multi-exponential case. The methodology was applied to the expansion of the broad money supply of US$ over the period 2001-2019, as a real-world example. At any given time, the information entropy is related to the rate at which the sample space is expanding. In the context of the expansion of the broad money supply, the information entropy could be considered to be related to the ""velocity"" of the expansion of the money supply."
http://arxiv.org/abs/2105.14199v1,Impact of Public and Private Investments on Economic Growth of Developing Countries,2021-05-29 03:40:26+00:00,['Faruque Ahamed'],econ.GN,"This paper aims to study the impact of public and private investments on the economic growth of developing countries. The study uses the panel data of 39 developing countries covering the periods 1990-2019. The study was based on the neoclassical growth models or exogenous growth models state in which land, labor, capital accumulation, etc., and technology proved substantial for economic growth. The paper finds that public investment has a strong positive impact on economic growth than private investment. Gross capital formation, labor growth, and government final consumption expenditure were found significant in explaining the economic growth. Overall, both public and private investments are substantial for the economic growth and development of developing countries."
http://arxiv.org/abs/2105.07823v1,"Bank Density, Population Density, and Economic Deprivation Across the United States",2021-05-13 00:46:12+00:00,['Scott W. Hegerty'],econ.GN,"Recent research on the geographic locations of bank branches in the United States has identified thresholds below which a given area can be considered to be a ""banking desert."" Thus far, most analyses of the country as a whole have tended to focus on minimum distances from geographic areas to the nearest bank, while a recent density-based analysis focused only on the city of Chicago. As such, there is not yet a nationwide study of bank densities for the entire United States. This study calculates banks per square mile for U.S. Census tracts over ten different ranges of population density. One main finding is that bank density is sensitive to the measurement radius used (for example, density in urban areas can be calculated as the number of banks within two miles, while some rural areas require a 20-mile radius). This study then compiles a set of lower 5- and 10-percent thresholds that might be used to identify ""banking deserts"" in various urban, suburban, and rural areas; these largely conform to the findings of previous analyses. Finally, adjusting for population density using regression residuals, this paper examines whether an index of economic deprivation is significantly higher in the five percent of ""desert"" tracts than in the remaining 95 percent. The differences are largest -- and highly significant -- in the densest tracts in large urban areas."
http://arxiv.org/abs/2105.08048v2,Wealth rheology,2021-05-17 17:54:31+00:00,"['Zdzislaw Burda', 'Malgorzata J. Krawczyk', 'Krzysztof Malarz', 'Malgorzata Snarska']",econ.TH,"We study wealth rank correlations in a simple model of macro-economy. To quantify rank correlations between wealth rankings at different times, we use Kendall's $τ$ and Spearman's $ρ$, Goodman--Kruskal's $γ$, and the lists' overlap ratio. We show that the dynamics of wealth flow and the speed of reshuffling in the ranking list depend on parameters of the model controlling the wealth exchange rate and the wealth growth volatility. As an example of the rheology of wealth in real data, we analyze the lists of the richest people in Poland, Germany, the USA and the world."
http://arxiv.org/abs/2106.02512v1,"Interdependence of Growth, Structure, Size and Resource Consumption During an Economic Growth Cycle",2021-06-04 14:29:50+00:00,['Carey W. King'],econ.TH,"All economies require physical resource consumption to grow and maintain their structure. The modern economy is additionally characterized by private debt. The Human and Resources with MONEY (HARMONEY) economic growth model links these features using a stock and flow consistent framework in physical and monetary units. Via an updated version, we explore the interdependence of growth and three major structural metrics of an economy. First, we show that relative decoupling of gross domestic product (GDP) from resource consumption is an expected pattern that occurs because of physical limits to growth, not a response to avoid physical limits. While an increase in resource efficiency of operating capital does increase the level of relative decoupling, so does a change in pricing from one based on full costs to one based only on marginal costs that neglects depreciation and interest payments leading to higher debt ratios. Second, if assuming full labor bargaining power for wages, when a previously-growing economy reaches peak resource extraction and GDP, wages remain high but profits and debt decline to zero. By removing bargaining power, profits can remain positive at the expense of declining wages. Third, the distribution of intermediate transactions within the input-output table of the model follows the same temporal pattern as in the post-World War II U.S. economy. These results indicate that the HARMONEY framework enables realistic investigation of interdependent structural change and trade-offs between economic distribution, size, and resources consumption."
http://arxiv.org/abs/2106.02546v1,Testing the Goodwin Growth Cycles with Econophysics Approach in 2002-2019 Period in Turkey,2021-06-04 15:18:49+00:00,"['Kerim Eser Afşar', 'Mehmet Özyigit', 'Yusuf Yüksel', 'Ümit Akıncı']",econ.GN,"The Turkish economy between 2002-2019 period has been investigated within the econophysical approach. From the individual income data obtained from the Household Budget Survey, the Gompertz-Pareto distribution for each year and Goodwin cycle for the mentioned period have been obtained. For this period, in which thirteen elections were held under the single-party rule, it has been observed that the income distribution fits well with the Gompertz-Pareto distribution which shows the two-class structure of the Turkish economy. The variation of the threshold value $x_t$ (which separates these two classes) as well as Pareto coefficient have been obtained. Besides, Goodwin cycle has been observed within this period, centered at $(u,v)\cong (66.30,83.40)$ and a period of $T=18.30$ years. It has been concluded that these observations are consistent with the economic and social events experienced in the mentioned period."
http://arxiv.org/abs/2106.03263v1,Estimating the number of entities with vacancies using administrative and online data,2021-06-06 22:19:02+00:00,"['Maciej Beręsewicz', 'Herman Cherniaiev', 'Robert Pater']",stat.AP,"In this article we describe a study aimed at estimating job vacancy statistics, in particular the number of entities with at least one vacancy. To achieve this goal, we propose an alternative approach to the methodology exploiting survey data, which is based solely on data from administrative registers and online sources and relies on dual system estimation (DSE).
  As these sources do not cover the whole reference population and the number of units appearing in all datasets is small, we have developed a DSE approach for negatively dependent sources based on a recent work by Chatterjee and Bhuyan (2020). To achieve the main goal we conducted a thorough data cleaning procedure in order to remove out-of-scope units, identify entities from the target population, and link them by identifiers to minimize linkage errors. We verified the effectiveness and sensitivity of the proposed estimator in simulation studies.
  From a practical point of view, our results show that the current vacancy survey in Poland underestimates the number of entities with at least one vacancy by about 10-15%. The main reasons for this discrepancy are non-sampling errors due to non-response and under-reporting, which is identified by comparing survey data with administrative data."
http://arxiv.org/abs/2106.00830v1,Justice as a Social Bargain and Optimization Problem,2021-06-01 22:18:43+00:00,['Andreas Siemoneit'],econ.TH,"The question of ""Justice"" still divides social research and moral philosophy. Several Theories of Justice and conceptual approaches compete here, and distributive justice remains a major societal controversy. From an evolutionary point of view, fair and just exchange can be nothing but ""equivalent"", and this makes ""strict"" reciprocity (merit, equity) the foundational principle of justice, both theoretically and empirically. But besides being just, justice must be effective, efficient, and communicable. Moral reasoning is a communicative strategy for resolving conflict, enhancing status, and maintaining cooperation, thereby making justice rather a social bargain and an optimization problem. Social psychology (intuitions, rules of thumb, self-bindings) can inform us when and why the two auxiliary principles equality and need are more likely to succeed than merit would. Nevertheless, both equality and need are governed by reciprocal considerations, and self-bindings help to interpret altruism as ""very generalized reciprocity"". The Meritocratic Principle can be implemented, and its controversy avoided, by concentrating on ""non-merit"", i.e., institutionally draining the wellsprings of undeserved incomes (economic rents). Avoiding or taxing away economic rents is an effective implementation of justice in liberal democracies. This would enable market economies to bring economic achievement and income much more in line, thus becoming more just."
http://arxiv.org/abs/2105.03006v2,A Recursive Measure of Voting Power that Satisfies Reasonable Postulates,2021-05-06 23:46:15+00:00,"['Arash Abizadeh', 'Adrian Vetta']",econ.TH,"We design a recursive measure of voting power based on partial as well as full voting efficacy. Classical measures, by contrast, incorporate solely full efficacy. We motivate our design by representing voting games using a division lattice and via the notion of random walks in stochastic processes, and show the viability of our recursive measure by proving it satisfies a plethora of postulates that any reasonable voting measure should satisfy. These include the iso-invariance, dummy, dominance, donation, minimum-power bloc, and quarrel postulates."
http://arxiv.org/abs/2106.13644v3,Intergenerational risk sharing in a Defined Contribution pension system: analysis with Bayesian optimization,2021-06-25 13:55:10+00:00,"['An Chen', 'Motonobu Kanagawa', 'Fangyuan Zhang']",econ.GN,"We study a fully funded, collective defined-contribution (DC) pension system with multiple overlapping generations. We investigate whether the welfare of participants can be improved by intergenerational risk sharing (IRS) implemented with a realistic investment strategy (e.g., no borrowing) and without an outside entity (e.g., share holders) that helps finance the pension fund. To implement IRS, the pension system uses an automatic adjustment rule for the indexation of individual accounts, which adapts to the notional funding ratio of the pension system. The pension system has two parameters that determine the investment strategy and the strength of the adjustment rule, which are optimized by expected utility maximization using Bayesian optimization. The volatility of the retirement benefits and that of the funding ratio are analyzed, and it is shown that the trade-off between them can be controlled by the optimal adjustment parameter to attain IRS. Compared with the optimal individual DC benchmark using the life-cycle strategy, the studied pension system with IRS is shown to improve the welfare of risk-averse participants, when the financial market is volatile."
http://arxiv.org/abs/2104.14458v2,Nonparametric Difference-in-Differences in Repeated Cross-Sections with Continuous Treatments,2021-04-29 16:23:53+00:00,"[""Xavier D'Haultfoeuille"", 'Stefan Hoderlein', 'Yuya Sasaki']",econ.EM,"This paper studies the identification of causal effects of a continuous treatment using a new difference-in-difference strategy. Our approach allows for endogeneity of the treatment, and employs repeated cross-sections. It requires an exogenous change over time which affects the treatment in a heterogeneous way, stationarity of the distribution of unobservables and a rank invariance condition on the time trend. On the other hand, we do not impose any functional form restrictions or an additive time trend, and we are invariant to the scaling of the dependent variable. Under our conditions, the time trend can be identified using a control group, as in the binary difference-in-differences literature. In our scenario, however, this control group is defined by the data. We then identify average and quantile treatment effect parameters. We develop corresponding nonparametric estimators and study their asymptotic properties. Finally, we apply our results to the effect of disposable income on consumption."
http://arxiv.org/abs/2106.04850v1,Dynamic mechanism design: An elementary introduction,2021-06-09 07:16:04+00:00,['Kiho Yoon'],econ.TH,This paper introduces dynamic mechanism design in an elementary fashion. We first examine optimal dynamic mechanisms: We find necessary and sufficient conditions for perfect Bayesian incentive compatibility and formulate the optimal dynamic mechanism problem. We next examine efficient dynamic mechanisms: We establish the uniqueness of Groves mechanism and investigate budget balance of the dynamic pivot mechanism in some detail for a bilateral trading environment. This introduction reveals that many results and techniques of static mechanism design can be straightforwardly extended and adapted to the analysis of dynamic settings.
http://arxiv.org/abs/2106.02031v3,Change-Point Analysis of Time Series with Evolutionary Spectra,2021-06-03 17:56:24+00:00,"['Alessandro Casini', 'Pierre Perron']",math.ST,"This paper develops change-point methods for the spectrum of a locally stationary time series. We focus on series with a bounded spectral density that change smoothly under the null hypothesis but exhibits change-points or becomes less smooth under the alternative. We address two local problems. The first is the detection of discontinuities (or breaks) in the spectrum at unknown dates and frequencies. The second involves abrupt yet continuous changes in the spectrum over a short time period at an unknown frequency without signifying a break. Both problems can be cast into changes in the degree of smoothness of the spectral density over time. We consider estimation and minimax-optimal testing. We determine the optimal rate for the minimax distinguishable boundary, i.e., the minimum break magnitude such that we are able to uniformly control type I and type II errors. We propose a novel procedure for the estimation of the change-points based on a wild sequential top-down algorithm and show its consistency under shrinking shifts and possibly growing number of change-points. Our method can be used across many fields and a companion program is made available in popular software packages."
http://arxiv.org/abs/2104.12597v3,Valid Heteroskedasticity Robust Testing,2021-04-26 14:01:40+00:00,"['Benedikt M. Pötscher', 'David Preinerstorfer']",math.ST,"Tests based on heteroskedasticity robust standard errors are an important technique in econometric practice. Choosing the right critical value, however, is not simple at all: conventional critical values based on asymptotics often lead to severe size distortions; and so do existing adjustments including the bootstrap. To avoid these issues, we suggest to use smallest size-controlling critical values, the generic existence of which we prove in this article for the commonly used test statistics. Furthermore, sufficient and often also necessary conditions for their existence are given that are easy to check. Granted their existence, these critical values are the canonical choice: larger critical values result in unnecessary power loss, whereas smaller critical values lead to over-rejections under the null hypothesis, make spurious discoveries more likely, and thus are invalid. We suggest algorithms to numerically determine the proposed critical values and provide implementations in accompanying software. Finally, we numerically study the behavior of the proposed testing procedures, including their power properties."
http://arxiv.org/abs/2107.06544v3,Winners and losers of immigration,2021-07-14 08:28:43+00:00,"['Davide Fiaschi', 'Cristina Tealdi']",econ.GN,"We study the impact of low-skilled immigration in a general equilibrium search and matching model, with heterogeneous workers producing intermediate goods, which are used in the production of two final goods. In addition to complementarity/substitution between native and non-native workers, we explore how immigration affects the relative prices of final goods and wages. An application to Italy reveals a positive contribution of immigrants to GDP, public revenues, and the per capita provision of public goods. Employers and employees in the high-skilled-intensive market are winners, while losers are employers in the low-skilled-intensive market. The effects on low-skilled employees are instead inconclusive."
http://arxiv.org/abs/2107.01746v2,"Mobility decisions, economic dynamics and epidemic",2021-07-04 22:54:40+00:00,"['Giorgio Fabbri', 'Salvatore Federico', 'Davide Fiaschi', 'Fausto Gozzi']",econ.GN,"We propose a model, which nests a susceptible-infected-recovered-deceased (SIRD) epidemic model into a dynamic macroeconomic equilibrium framework with agents' mobility. The latter affect both their income and their probability of infecting and being infected. Strategic complementarities among individual mobility choices drive the evolution of aggregate economic activity, while infection externalities caused by individual mobility affect disease diffusion. The continuum of rational forward-looking agents coordinates on the Nash equilibrium of a discrete time, finite-state, infinite-horizon Mean Field Game. We prove the existence of an equilibrium and provide a recursive construction method for the search of an equilibrium(a), which also guides our numerical investigations. We calibrate the model by using Italian experience on COVID-19 epidemic and we discuss policy implications."
http://arxiv.org/abs/2107.06855v2,Comparing Intellectual property policy in the Global North and South -- A one-size-fits-all policy for economic prosperity?,2021-07-14 17:20:43+00:00,"['S Sidhartha Narayan', 'Malavika Ranjan', 'Madhumitha Raghuraman']",econ.TH,"This paper attempts to analyse policymaking in the field of Intellectual Property (IP) as an instrument of economic growth across the Global North and South. It begins by studying the links between economic growth and IP, followed by an understanding of Intellectual Property Rights (IPR) development in the US, a leading proponent of robust IPR protection internationally. The next section compares the IPR in the Global North and South and undertakes an analysis of the diverse factors that result in these differences. The paper uses the case study of the Indian Pharmaceutical Industry to understand how IPR may differentially affect economies and conclude that there may not yet be a one size fits all policy for the adoption of Intellectual Property Rights."
http://arxiv.org/abs/2108.12633v1,Uncertainty in Mechanism Design,2021-08-28 11:58:45+00:00,"['Giuseppe Lopomo', 'Luca Rigotti', 'Chris Shannon']",econ.TH,"This paper studies the design of mechanisms that are robust to misspecification. We introduce a novel notion of robustness that connects a variety of disparate approaches and study its implications in a wide class of mechanism design problems. This notion is quantifiable, allowing us to formalize and answer comparative statics questions relating the nature and degree of misspecification to sharp predictions regarding features of feasible mechanisms. This notion also has a behavioral foundation which reflects the perception of ambiguity, thus allowing the degree of misspecification to emerge endogenously. In a number of standard settings, robustness to arbitrarily small amounts of misspecification generates a discontinuity in the set of feasible mechanisms and uniquely selects simple, ex post incentive compatible mechanisms such as second-price auctions. Robustness also sheds light on the value of private information and the prevalence of full or virtual surplus extraction."
http://arxiv.org/abs/2108.11545v4,Identification of Peer Effects using Panel Data,2021-08-26 01:59:08+00:00,"['Marisa Miraldo', 'Carol Propper', 'Christiern Rose']",econ.EM,"We provide new identification results for panel data models with peer effects operating through unobserved individual heterogeneity. The results apply for general network structures governing peer interactions and allow for correlated effects. Identification hinges on a conditional mean restriction requiring exogenous mobility of individuals between groups over time. We apply our method to surgeon-hospital-year data to study take-up of keyhole surgery for cancer, finding a positive effect of the average individual heterogeneity of other surgeons practicing in the same hospital"
http://arxiv.org/abs/2109.11550v1,Absorptive capacities and economic growth in low and middle income economies,2021-09-23 17:49:51+00:00,['Muhammad Salar Khan'],econ.GN,"I extend the concept of absorptive capacity, used in the analysis of firms, to a framework applicable to the national level. First, employing confirmatory factor analyses on 47 variables, I build 13 composite factors crucial to measuring six national level capacities: technological capacity, financial capacity, human capacity, infrastructural capacity, public policy capacity, and social capacity. My data cover most low- and middle-income- economies (LMICs), eligible for the World Bank's International Development Association (IDA) support between 2005 and 2019. Second, I analyze the relationship between the estimated capacity factors and economic growth while controlling for some of the incoming flows from abroad and other confounders that might influence the relationship. Lastly, I conduct K-means cluster analysis and then analyze the results alongside regression estimates to glean patterns and classifications within the LMICs. Results indicate that enhancing infrastructure (ICT, energy, trade, and transport), financial (apparatus and environment), and public policy capacities is a prerequisite for attaining economic growth. Similarly, I find improving human capital with specialized skills positively impacts economic growth. Finally, by providing a ranking of which capacity is empirically more important for economic growth, I offer suggestions to governments with limited budgets to make wise investments. Likewise, my findings inform international policy and monetary bodies on how they could better channel their funding in LMICs to achieve sustainable development goals and boost shared prosperity."
http://arxiv.org/abs/2108.06282v4,Identification of Incomplete Preferences,2021-08-13 15:11:31+00:00,"['Luca Rigotti', 'Arie Beresteanu']",econ.EM,We provide a sharp identification region for discrete choice models where consumers' preferences are not necessarily complete even if only aggregate choice data is available. Behavior is modeled using an upper and a lower utility for each alternative so that non-comparability can arise. The identification region places intuitive bounds on the probability distribution of upper and lower utilities. We show that the existence of an instrumental variable can be used to reject the hypothesis that the preferences of all consumers are complete. We apply our methods to data from the 2018 mid-term elections in Ohio.
http://arxiv.org/abs/2108.08366v1,Risk Preferences in Time Lotteries,2021-08-18 19:46:55+00:00,"['Yonatan Berman', 'Mark Kirstein']",econ.TH,"An important but understudied question in economics is how people choose when facing uncertainty in the timing of events. Here we study preferences over time lotteries, in which the payment amount is certain but the payment time is uncertain. Expected discounted utility theory (EDUT) predicts decision makers to be risk-seeking over time lotteries. We explore a normative model of growth-optimality, in which decision makers maximise the long-term growth rate of their wealth. Revisiting experimental evidence on time lotteries, we find that growth-optimality accords better with the evidence than EDUT. We outline future experiments to scrutinise further the plausibility of growth-optimality."
http://arxiv.org/abs/2109.11917v2,The Boltzmann fair division for distributive justice,2021-09-24 12:17:04+00:00,"['Ji-Won Park', 'Jaeup U. Kim', 'Cheol-Min Ghim', 'Chae Un Kim']",econ.GN,"Fair division is a significant, long-standing problem and is closely related to social and economic justice. The conventional division methods such as cut-and-choose are hardly applicable to realworld problems because of their complexity and unrealistic assumptions about human behaviors. Here we propose a fair division method from a completely different perspective, using the Boltzmann distribution. The Boltzmann distribution adopted from the physical sciences gives the most probable and unbiased distribution derived from a goods-centric, rather than a player-centric, division process. The mathematical model of the Boltzmann fair division was developed for both homogeneous and heterogeneous division problems, and the players' key factors (contributions, needs, and preferences) could be successfully integrated. We show that the Boltzmann fair division is a well-balanced division method maximizing the players' total utility, and it could be easily finetuned and applicable to complex real-world problems such as income/wealth redistribution or international negotiations on fighting climate change."
http://arxiv.org/abs/2109.06995v2,Strategic Inventories in a Supply Chain with Downstream Cournot Duopoly,2021-09-14 22:21:29+00:00,"['Xiaowei Hu', 'Jaejin Jang', 'Nabeel Hamoud', 'Amirsaman Bajgiran']",econ.GN,"The inventories carried in a supply chain as a strategic tool to influence the competing firms are considered to be strategic inventories (SI). We present a two-period game-theoretic supply chain model, in which a singular manufacturer supplies products to a pair of identical Cournot duopolistic retailers. We show that the SI carried by the retailers under dynamic contract is Pareto-dominating for the manufacturer, retailers, consumers, the channel, and the society as well. We also find that the retailer's SI, however, can be eliminated when the manufacturer commits wholesale contract or inventory holding cost is too high. In comparing the cases with and without downstream competition, we also show that the downstream Cournot duopoly undermines the retailers in profits, but benefits all others."
http://arxiv.org/abs/2107.13737v3,Design-Robust Two-Way-Fixed-Effects Regression For Panel Data,2021-07-29 04:09:57+00:00,"['Dmitry Arkhangelsky', 'Guido W. Imbens', 'Lihua Lei', 'Xiaoman Luo']",econ.EM,"We propose a new estimator for average causal effects of a binary treatment with panel data in settings with general treatment patterns. Our approach augments the popular two-way-fixed-effects specification with unit-specific weights that arise from a model for the assignment mechanism. We show how to construct these weights in various settings, including the staggered adoption setting, where units opt into the treatment sequentially but permanently. The resulting estimator converges to an average (over units and time) treatment effect under the correct specification of the assignment model, even if the fixed effect model is misspecified. We show that our estimator is more robust than the conventional two-way estimator: it remains consistent if either the assignment mechanism or the two-way regression model is correctly specified. In addition, the proposed estimator performs better than the two-way-fixed-effect estimator if the outcome model and assignment mechanism are locally misspecified. This strong double robustness property underlines and quantifies the benefits of modeling the assignment process and motivates using our estimator in practice. We also discuss an extension of our estimator to handle dynamic treatment effects."
http://arxiv.org/abs/2108.11177v1,Influential News and Policy-making,2021-08-25 11:00:38+00:00,['Federico Vaccari'],econ.GN,"It is believed that interventions that change the media's costs of misreporting can increase the information provided by media outlets. This paper analyzes the validity of this claim and the welfare implications of those types of interventions that affect misreporting costs. I study a model of communication between an uninformed voter and a media outlet that knows the quality of two competing candidates. The alternatives available to the voter are endogenously championed by the two candidates. I show that higher costs may lead to more misreporting and persuasion, whereas low costs result in full revelation; interventions that increase misreporting costs never harm the voter, but those that do so slightly may be wasteful of public resources. I conclude that intuitions derived from the interaction between the media and voters, without incorporating the candidates' strategic responses to the media environment, do not capture properly the effects of these types of interventions."
http://arxiv.org/abs/2108.03709v3,Grade Inflation and Stunted Effort in a Curved Economics Course,2021-08-08 18:48:31+00:00,['Alex Garivaltis'],econ.TH,"To protect his teaching evaluations, an economics professor uses the following exam curve: if the class average falls below a known target, $m$, then all students will receive an equal number of free points so as to bring the mean up to $m$. If the average is above $m$ then there is no curve; curved grades above $100\%$ will never be truncated to $100\%$ in the gradebook. The $n$ students in the course all have Cobb-Douglas preferences over the grade-leisure plane; effort corresponds exactly to earned (uncurved) grades in a $1:1$ fashion. The elasticity of each student's utility with respect to his grade is his ability parameter, or relative preference for a high score. I find, classify, and give complete formulas for all the pure Nash equilibria of my own game, which my students have been playing for some eight semesters. The game is supermodular, featuring strategic complementarities, negative spillovers, and nonsmooth payoffs that generate non-convexities in the reaction correspondence. The $n+2$ types of equilibria are totally ordered with respect to effort and Pareto preference, and the lowest $n+1$ of these types are totally ordered in grade-leisure space. In addition to the no-curve (""try-hard"") and curved interior equilibria, we have the ""$k$-don't care"" equilibria, whereby the $k$ lowest-ability students are no-shows. As the class size becomes infinite in the curved interior equilibrium, all students increase their leisure time by a fixed percentage, i.e., $14\%$, in response to the disincentive, which amplifies any pre-existing ability differences. All students' grades inflate by this same (endogenous) factor, say, $1.14$ times what they would have been under the correct standard."
http://arxiv.org/abs/2107.04098v1,Centralized Matching with Incomplete Information,2021-07-08 20:32:22+00:00,"['Marcelo Ariel Fernandez', 'Kirill Rudov', 'Leeat Yariv']",econ.TH,"We study the impacts of incomplete information on centralized one-to-one matching markets. We focus on the commonly used Deferred Acceptance mechanism (Gale and Shapley, 1962). We show that many complete-information results are fragile to a small infusion of uncertainty about others' preferences."
http://arxiv.org/abs/2108.04885v2,Benefits of marriage as a search strategy,2021-08-10 19:24:38+00:00,['Davi B. Costa'],econ.TH,"We propose and investigate a model for mate searching and marriage in large societies based on a stochastic matching process and simple decision rules. Agents have preferences among themselves given by some probability distribution. They randomly search for better mates, forming new couples and breaking apart in the process. Marriage is implemented in the model by adding the decision of stopping searching for a better mate when the affinity between a couple is higher than a certain fixed amount. We show that the average utility in the system with marriage can be higher than in the system without it. Part of our results can be summarized in what sounds like a piece of advice: don't marry the first person you like and don't search for the love of your life, but get married if you like your partner more than a sigma above average. We also find that the average utility attained in our stochastic model is smaller than the one associated with a stable matching achieved using the Gale-Shapley algorithm. This can be taken as a formal argument in favor of a central planner (perhaps an app) with the information to coordinate the marriage market in order to set a stable matching. To roughly test the adequacy of our model to describe existent societies, we compare the evolution of the fraction of married couples in our model with real-world data and obtain good agreement. In the last section, we formulate the model in the limit of an infinite number of agents and find an analytical expression for the evolution of the system."
http://arxiv.org/abs/2109.10968v3,Ignorance is Bliss: A Game of Regret,2021-09-22 18:34:55+00:00,"['Claudia Cerrone', 'Francesco Feri', 'Philip R. Neary']",econ.TH,"An individual can only experience regret if she learns about an unchosen alternative. In many situations, learning about an unchosen alternative is possible only if someone else chose it. We develop a model where the ex-post information available to each regret averse individual depends both on their own choice and on the choices of others, as others can reveal ex-post information about what might have been. This implies that what appears to be a series of isolated single-person decision problems is in fact a rich multi-player behavioural game, the regret game, where the psychological payoffs that depend on ex-post information are interconnected. For an open set of parameters, the regret game is a coordination game with multiple equilibria, despite the fact that all individuals possess a uniquely optimal choice in isolation. We experimentally test this prediction and find support for it."
http://arxiv.org/abs/2107.11575v3,Peace through bribing,2021-07-24 10:12:10+00:00,"['Jingfeng Lu', 'Zongwei Lu', 'Christian Riis']",econ.TH,"We study a model in which before a conflict between two parties escalates into a war (in the form of an all-pay auction), a party can offer a take-it-or-leave-it bribe to the other for a peaceful settlement. In contrast to the received literature, we find that peace security is impossible in our model. We characterize the necessary and sufficient conditions for peace implementability. Furthermore, we find that separating equilibria do not exist and the number of (on-path) bribes in any non-peaceful equilibria is at most two. We also consider a requesting model and characterize the necessary and sufficient conditions for the existence of robust peaceful equilibria, all of which are sustained by the identical (on-path) request. Contrary to the bribing model, peace security is possible in the requesting model."
http://arxiv.org/abs/2108.07146v3,Dynamic Monopoly Pricing With Multiple Varieties: Trading Up,2021-08-16 15:22:25+00:00,"['Stefan Buehler', 'Nicolas Eschenbaum', 'Severin Lenhard']",econ.GN,"This paper studies dynamic monopoly pricing for a broad class of settings that allow for multiple durable, multiple rental, or a mix of varieties. We show that the driving force behind pricing dynamics is the existence of trading-up opportunities. If there are no trading-up opportunities in the static monopoly outcome, then pricing dynamics do not emerge in equilibrium. With trading-up opportunities, pricing dynamics arise until these opportunities are exhausted or the game ends. We characterize the lower bound for the emerging prices and profit and study the conditions under which pricing dynamics end in finite time."
http://arxiv.org/abs/2109.12568v1,Design and validation of an index to measure development in rural areas through stakeholder participation,2021-09-26 11:39:54+00:00,"['Abreu I.', 'Mesias F. J.', 'Ramajo', 'J']",econ.EM,"This paper proposes the development of an index to assess rural development based on a set of 25 demographic, economic, environmental, and social welfare indicators previously selected through a Delphi approach. Three widely accepted aggregation methods were then tested: a mixed arithmetic/geometric mean without weightings for each indicator; a weighted arithmetic mean using the weights previously generated by the Delphi panel and an aggregation through Principal Component Analysis. These three methodologies were later applied to 9 Portuguese NUTS III regions, and the results were presented to a group of experts in rural development who indicated which of the three forms of aggregation best measured the levels of rural development of the different territories. Finally, it was concluded that the unweighted arithmetic/geometric mean was the most accurate methodology for aggregating indicators to create a Rural Development Index."
http://arxiv.org/abs/2108.13506v1,Submission Fees in Risk-Taking Contests,2021-08-30 20:07:59+00:00,['Mark Whitmeyer'],econ.TH,"This paper investigates stochastic continuous time contests with a twist: the designer requires that contest participants incur some cost to submit their entries. When the designer wishes to maximize the (expected) performance of the top performer, a strictly positive submission fee is optimal. When the designer wishes to maximize total (expected) performance, either the highest submission fee or the lowest submission fee is optimal."
http://arxiv.org/abs/2107.01532v2,Decentralizing Centralized Matching Markets: Implications from Early Offers in University Admissions,2021-07-04 03:37:19+00:00,"['Julien Grenet', 'YingHua He', 'Dorothea Kübler']",econ.GN,"The matching literature often recommends market centralization under the assumption that agents know their own preferences and that their preferences are fixed. We find counterevidence to this assumption in a quasi-experiment. In Germany's university admissions, a clearinghouse implements the early stages of the Gale-Shapley algorithm in real time. We show that early offers made in this decentralized phase, although not more desirable, are accepted more often than later ones. These results, together with survey evidence and a theoretical model, are consistent with students' costly learning about universities. We propose a hybrid mechanism to combine the advantages of decentralization and centralization.
  Published at The Journal of Political Economy under a new title, ``Preference Discovery in University Admissions: The Case for Dynamic Multioffer Mechanisms,'' available at https://doi.org/10.1086/718983 (Open Access)."
http://arxiv.org/abs/2107.05263v2,A Lucas Critique Compliant SVAR model with Observation-driven Time-varying Parameters,2021-07-12 08:58:44+00:00,"['Giacomo Bormetti', 'Fulvio Corsi']",econ.EM,"We propose an observation-driven time-varying SVAR model where, in agreement with the Lucas Critique, structural shocks drive both the evolution of the macro variables and the dynamics of the VAR parameters. Contrary to existing approaches where parameters follow a stochastic process with random and exogenous shocks, our observation-driven specification allows the evolution of the parameters to be driven by realized past structural shocks, thus opening the possibility to gauge the impact of observed shocks and hypothetical policy interventions on the future evolution of the economic system."
http://arxiv.org/abs/2109.03061v4,Persuasion and Welfare,2021-09-07 12:51:58+00:00,"['Laura Doval', 'Alex Smolin']",econ.TH,"Information policies such as scores, ratings, and recommendations are increasingly shaping society's choices in high-stakes domains. We provide a framework to study the welfare implications of information policies on a population of heterogeneous individuals. We define and characterize the Bayes welfare set, consisting of the population's utility profiles that are feasible under some information policy. The Pareto frontier of this set can be recovered by a series of standard Bayesian persuasion problems, in which a utilitarian planner takes the role of the information designer. We provide necessary and sufficient conditions under which an information policy exists that Pareto dominates the no-information policy. We illustrate our results with applications to data leakage, price discrimination, and credit ratings."
http://arxiv.org/abs/2107.02780v6,"Causal Inference with Corrupted Data: Measurement Error, Missing Values, Discretization, and Differential Privacy",2021-07-06 17:42:49+00:00,"['Anish Agarwal', 'Rahul Singh']",econ.EM,"The US Census Bureau will deliberately corrupt data sets derived from the 2020 US Census, enhancing the privacy of respondents while potentially reducing the precision of economic analysis. To investigate whether this trade-off is inevitable, we formulate a semiparametric model of causal inference with high dimensional corrupted data. We propose a procedure for data cleaning, estimation, and inference with data cleaning-adjusted confidence intervals. We prove consistency and Gaussian approximation by finite sample arguments, with a rate of $n^{ 1/2}$ for semiparametric estimands that degrades gracefully for nonparametric estimands. Our key assumption is that the true covariates are approximately low rank, which we interpret as approximate repeated measurements and empirically validate. Our analysis provides nonasymptotic theoretical contributions to matrix completion, statistical learning, and semiparametric statistics. Calibrated simulations verify the coverage of our data cleaning adjusted confidence intervals and demonstrate the relevance of our results for Census-derived data."
http://arxiv.org/abs/2109.02730v3,Sorting with Teams,2021-09-06 20:49:37+00:00,"['Job Boerma', 'Aleh Tsyvinski', 'Alexander P. Zimin']",econ.GN,"We fully solve a sorting problem with heterogeneous firms and multiple heterogeneous workers whose skills are imperfect substitutes. We show that optimal sorting, which we call mixed and countermonotonic, is comprised of two regions. In the first region, mediocre firms sort with mediocre workers and coworkers such that the output losses are equal across all these teams (mixing). In the second region, a high skill worker sorts with low skill coworkers and a high productivity firm (countermonotonicity). We characterize the equilibrium wages and firm values. Quantitatively, our model can generate the dispersion of earnings within and across US firms."
http://arxiv.org/abs/2107.11869v3,Adaptive Estimation and Uniform Confidence Bands for Nonparametric Structural Functions and Elasticities,2021-07-25 18:46:33+00:00,"['Xiaohong Chen', 'Timothy Christensen', 'Sid Kankanala']",econ.EM,"We introduce two data-driven procedures for optimal estimation and inference in nonparametric models using instrumental variables. The first is a data-driven choice of sieve dimension for a popular class of sieve two-stage least squares estimators. When implemented with this choice, estimators of both the structural function $h_0$ and its derivatives (such as elasticities) converge at the fastest possible (i.e., minimax) rates in sup-norm. The second is for constructing uniform confidence bands (UCBs) for $h_0$ and its derivatives. Our UCBs guarantee coverage over a generic class of data-generating processes and contract at the minimax rate, possibly up to a logarithmic factor. As such, our UCBs are asymptotically more efficient than UCBs based on the usual approach of undersmoothing. As an application, we estimate the elasticity of the intensive margin of firm exports in a monopolistic competition model of international trade. Simulations illustrate the good performance of our procedures in empirically calibrated designs. Our results provide evidence against common parameterizations of the distribution of unobserved firm heterogeneity."
http://arxiv.org/abs/2107.08498v1,Decoupling Shrinkage and Selection for the Bayesian Quantile Regression,2021-07-18 17:22:33+00:00,"['David Kohns', 'Tibor Szendrei']",econ.EM,"This paper extends the idea of decoupling shrinkage and sparsity for continuous priors to Bayesian Quantile Regression (BQR). The procedure follows two steps: In the first step, we shrink the quantile regression posterior through state of the art continuous priors and in the second step, we sparsify the posterior through an efficient variant of the adaptive lasso, the signal adaptive variable selection (SAVS) algorithm. We propose a new variant of the SAVS which automates the choice of penalisation through quantile specific loss-functions that are valid in high dimensions. We show in large scale simulations that our selection procedure decreases bias irrespective of the true underlying degree of sparsity in the data, compared to the un-sparsified regression posterior. We apply our two-step approach to a high dimensional growth-at-risk (GaR) exercise. The prediction accuracy of the un-sparsified posterior is retained while yielding interpretable quantile specific variable selection results. Our procedure can be used to communicate to policymakers which variables drive downside risk to the macro economy."
http://arxiv.org/abs/2108.00723v6,Partial Identification and Inference for Conditional Distributions of Treatment Effects,2021-08-02 08:48:47+00:00,['Sungwon Lee'],econ.EM,"This paper considers identification and inference for the distribution of treatment effects conditional on observable covariates. Since the conditional distribution of treatment effects is not point identified without strong assumptions, we obtain bounds on the conditional distribution of treatment effects by using the Makarov bounds. We also consider the case where the treatment is endogenous and propose two stochastic dominance assumptions to tighten the bounds. We develop a nonparametric framework to estimate the bounds and establish the asymptotic theory that is uniformly valid over the support of treatment effects. An empirical example illustrates the usefulness of the methods."
http://arxiv.org/abs/2109.14204v4,Strategic formation of collaborative networks,2021-09-29 06:13:54+00:00,"['Philip Solimine', 'Luke Boosey']",econ.GN,"We examine behavior in an experimental collaboration game that incorporates endogenous network formation. The environment is modeled as a generalization of the voluntary contributions mechanism. By varying the information structure in a controlled laboratory experiment, we examine the underlying mechanisms of reciprocity that generate emergent patterns in linking and contribution decisions. Providing players more detailed information about the sharing behavior of others drastically increases efficiency, and positively affects a number of other key outcomes. To understand the driving causes of these changes in behavior we develop and estimate a structural model for actions and small network panels and identify how social preferences affect behavior. We find that the treatment reduces altruism but stimulates reciprocity, helping players coordinate to reach mutually beneficial outcomes. In a set of counterfactual simulations, we show that increasing trust in the community would encourage higher average contributions at the cost of mildly increased free-riding. Increasing overall reciprocity greatly increases collaborative behavior when there is limited information but can backfire in the treatment, suggesting that negative reciprocity and punishment can reduce efficiency. The largest returns would come from an intervention that drives players away from negative and toward positive reciprocity."
http://arxiv.org/abs/2107.02739v2,Shapes as Product Differentiation: Neural Network Embedding in the Analysis of Markets for Fonts,2021-07-06 17:12:27+00:00,"['Sukjin Han', 'Eric H. Schulman', 'Kristen Grauman', 'Santhosh Ramakrishnan']",econ.EM,"Many differentiated products have key attributes that are unstructured and thus high-dimensional (e.g., design, text). Instead of treating unstructured attributes as unobservables in economic models, quantifying them can be important to answer interesting economic questions. To propose an analytical framework for these types of products, this paper considers one of the simplest design products-fonts-and investigates merger and product differentiation using an original dataset from the world's largest online marketplace for fonts. We quantify font shapes by constructing embeddings from a deep convolutional neural network. Each embedding maps a font's shape onto a low-dimensional vector. In the resulting product space, designers are assumed to engage in Hotelling-type spatial competition. From the image embeddings, we construct two alternative measures that capture the degree of design differentiation. We then study the causal effects of a merger on the merging firm's creative decisions using the constructed measures in a synthetic control method. We find that the merger causes the merging firm to increase the visual variety of font design. Notably, such effects are not captured when using traditional measures for product offerings (e.g., specifications and the number of products) constructed from structured data."
http://arxiv.org/abs/2111.00987v1,Modelling the transition to a low-carbon energy supply,2021-09-25 12:37:05+00:00,['Alexander Kell'],econ.GN,"A transition to a low-carbon electricity supply is crucial to limit the impacts of climate change. Reducing carbon emissions could help prevent the world from reaching a tipping point, where runaway emissions are likely. Runaway emissions could lead to extremes in weather conditions around the world -- especially in problematic regions unable to cope with these conditions. However, the movement to a low-carbon energy supply can not happen instantaneously due to the existing fossil-fuel infrastructure and the requirement to maintain a reliable energy supply. Therefore, a low-carbon transition is required, however, the decisions various stakeholders should make over the coming decades to reduce these carbon emissions are not obvious. This is due to many long-term uncertainties, such as electricity, fuel and generation costs, human behaviour and the size of electricity demand. A well choreographed low-carbon transition is, therefore, required between all of the heterogenous actors in the system, as opposed to changing the behaviour of a single, centralised actor. The objective of this thesis is to create a novel, open-source agent-based model to better understand the manner in which the whole electricity market reacts to different factors using state-of-the-art machine learning and artificial intelligence methods. In contrast to other works, this thesis looks at both the long-term and short-term impact that different behaviours have on the electricity market by using these state-of-the-art methods."
http://arxiv.org/abs/2109.15288v1,Information Acquisition and Diffusion in Markets,2021-09-30 17:33:36+00:00,"['Atabek Atayev', 'Maarten Janssen']",econ.TH,"Consumers can acquire information through their own search efforts or through their social network. Information diffusion via word-of-mouth communication leads to some consumers free-riding on their ""friends"" and less information acquisition via active search. Free-riding also has an important positive effect, however, in that consumers that do not actively search themselves are more likely to be able to compare prices before purchase, imposing competitive pressure on firms. We show how market prices depend on the characteristics of the network and on search cost. For example, if the search cost becomes small, price dispersion disappears, while the price level converges to the monopoly level, implying that expected prices are decreasing for small enough search cost. More connected societies have lower market prices, while price dispersion remains even in fully connected societies."
http://arxiv.org/abs/2109.01822v1,Income inequality and mobility in geometric Brownian motion with stochastic resetting: theoretical results and empirical evidence of non-ergodicity,2021-09-04 09:07:07+00:00,"['Viktor Stojkoski', 'Petar Jolakoski', 'Arnab Pal', 'Trifce Sandev', 'Ljupco Kocarev', 'Ralf Metzler']",econ.GN,"We explore the role of non-ergodicity in the relationship between income inequality, the extent of concentration in the income distribution, and mobility, the feasibility of an individual to change their position in the income distribution. For this purpose, we explore the properties of an established model for income growth that includes ""resetting"" as a stabilising force which ensures stationary dynamics. We find that the dynamics of inequality is regime-dependent and may range from a strictly non-ergodic state where this phenomenon has an increasing trend, up to a stable regime where inequality is steady and the system efficiently mimics ergodic behaviour. Mobility measures, conversely, are always stable over time, but the stationary value is dependent on the regime, suggesting that economies become less mobile in non-ergodic regimes. By fitting the model to empirical data for the dynamics of income share of the top earners in the United States, we provide evidence that the income dynamics in this country is consistently in a regime in which non-ergodicity characterises inequality and immobility dynamics. Our results can serve as a simple rationale for the observed real world income dynamics and as such aid in addressing non-ergodicity in various empirical settings across the globe."
http://arxiv.org/abs/2109.04583v3,Fair Compensation,2021-09-09 23:11:43+00:00,['John E. Stovall'],econ.TH,"We introduce a novel framework that considers how a firm could fairly compensate its workers. A firm has a group of workers, each of whom has varying productivities over a set of tasks. After assigning workers to tasks, the firm must then decide how to distribute its output to the workers. We first consider three compensation rules and various fairness properties they may satisfy. We show that among efficient and symmetric rules: the Egalitarian rule is the only rule that does not decrease a worker's compensation when every worker becomes weakly more productive (Group Productivity Monotonicity); the Shapley Value rule is the only rule that, for any two workers, equalizes the impact one worker has on the other worker's compensation (Balanced Impact); and the Individual Contribution rule is the only rule that is invariant to the removal of workers and their assigned tasks (Consistency). We introduce other rules and axioms, and relate each rule to each axiom."
http://arxiv.org/abs/2109.08222v1,Short and Simple Confidence Intervals when the Directions of Some Effects are Known,2021-09-16 21:09:56+00:00,"['Philipp Ketz', 'Adam McCloskey']",econ.EM,"We provide adaptive confidence intervals on a parameter of interest in the presence of nuisance parameters when some of the nuisance parameters have known signs. The confidence intervals are adaptive in the sense that they tend to be short at and near the points where the nuisance parameters are equal to zero. We focus our results primarily on the practical problem of inference on a coefficient of interest in the linear regression model when it is unclear whether or not it is necessary to include a subset of control variables whose partial effects on the dependent variable have known directions (signs). Our confidence intervals are trivial to compute and can provide significant length reductions relative to standard confidence intervals in cases for which the control variables do not have large effects. At the same time, they entail minimal length increases at any parameter values. We prove that our confidence intervals are asymptotically valid uniformly over the parameter space and illustrate their length properties in an empirical application to a factorial design field experiment and a Monte Carlo study calibrated to the empirical application."
http://arxiv.org/abs/2107.02650v3,Gravity models of networks: integrating maximum-entropy and econometric approaches,2021-07-06 14:43:01+00:00,"['Marzio Di Vece', 'Diego Garlaschelli', 'Tiziano Squartini']",physics.soc-ph,"The World Trade Web (WTW) is the network of international trade relationships among world countries. Characterizing both the local link weights (observed trade volumes) and the global network structure (large-scale topology) of the WTW via a single model is still an open issue. While the traditional Gravity Model (GM) successfully replicates the observed trade volumes by employing macroeconomic properties such as GDP and geographic distance, it, unfortunately, predicts a fully connected network, thus returning a completely unrealistic topology of the WTW. To overcome this problem, two different classes of models have been introduced in econometrics and statistical physics. Econometric approaches interpret the traditional GM as the expected value of a probability distribution that can be chosen arbitrarily and tested against alternative distributions. Statistical physics approaches construct maximum-entropy probability distributions of (weighted) graphs from a chosen set of measurable structural constraints and test distributions resulting from different constraints. Here we compare and integrate the two approaches by considering a class of maximum-entropy models that can incorporate macroeconomic properties used in standard econometric models. We find that the integrated approach achieves a better performance than the purely econometric one. These results suggest that the maximum-entropy construction can serve as a viable econometric framework wherein extensive and intensive margins can be separately controlled for, by combining topological constraints and dyadic macroeconomic variables."
http://arxiv.org/abs/2109.04888v2,Auctioning with Strategically Reticent Bidders,2021-09-10 14:01:01+00:00,"['Jibang Wu', 'Ashwinkumar Badanidiyuru', 'Haifeng Xu']",cs.GT,"We propose and study a novel mechanism design setup where each bidder holds two kinds of private information: (1) type variable, which can be misreported; (2) information variable, which the bidder may want to conceal or partially reveal, but importantly, not to misreport. We refer to bidders with such behaviors as strategically reticent bidders. Among others, one direct motivation of our model is the ad auction in which many ad platforms today elicit from each bidder not only their private value per conversion but also their private information about Internet users (e.g., user activities on the advertiser's websites) in order to improve the platform's estimation of conversion rates.
  We show that in this new setup, it is still possible to design mechanisms that are both Incentive and Information Compatible (IIC). We develop two different black-box transformations, which convert any mechanism $\mathcal{M}$ for classic bidders to a mechanism $\bar{\mathcal{M}}$ for strategically reticent bidders, based on either outcome of expectation or expectation of outcome, respectively. We identify properties of the original mechanism $\mathcal{M}$ under which the transformation leads to IIC mechanisms $\bar{\mathcal{M}}$. Interestingly, as corollaries of these results, we show that running VCG with bidders' expected values maximizes welfare, whereas the mechanism using expected outcome of Myerson's auction maximizes revenue. Finally, we study how regulation on the auctioneer's usage of information can lead to more robust mechanisms."
http://arxiv.org/abs/2108.03623v2,Including the asymmetry of the Lorenz curve into measures of economic inequality,2021-08-08 12:35:19+00:00,['Mario Schlemmer'],econ.EM,The Gini index signals only the dispersion of the distribution and is not very sensitive to income differences at the tails of the distribution. The widely used index of inequality can be adjusted to also measure distributional asymmetry by attaching weights to the distances between the Lorenz curve and the 45-degree line. The measure is equivalent to the Gini if the distribution is symmetric. The alternative measure of inequality inherits good properties from the Gini but is more sensitive to changes in the extremes of the income distribution.
http://arxiv.org/abs/2107.02169v1,A Study of UK Household Wealth through Empirical Analysis and a Non-linear Kesten Process,2021-07-05 17:53:54+00:00,"['Samuel Forbes', 'Stefan Grosskinsky']",econ.TH,"We study the wealth distribution of UK households through a detailed analysis of data from wealth surveys and rich lists, and propose a non-linear Kesten process to model the dynamics of household wealth. The main features of our model are that we focus on wealth growth and disregard exchange, and that the rate of return on wealth is increasing with wealth. The linear case with wealth-independent return rate has been well studied, leading to a log-normal wealth distribution in the long time limit which is essentially independent of initial conditions. We find through theoretical analysis and simulations that the non-linearity in our model leads to more realistic power-law tails, and can explain an apparent two-tailed structure in the empirical wealth distribution of the UK and other countries. Other realistic features of our model include an increase in inequality over time, and a stronger dependence on initial conditions compared to linear models."
http://arxiv.org/abs/2108.02272v4,R&D Heterogeneity and Countercyclical Productivity Dispersion,2021-08-04 20:24:21+00:00,"['Shuowen Chen', 'Yang Ming']",econ.GN,"Why is the U.S. industry-level productivity dispersion countercyclical? Theoretically, we build a duopoly model in which heterogeneous R&D costs determine firms' optimal behaviors and the equilibrium technology gap after a negative profit shock. Quantitatively, we calibrate a parameterized model, simulate firms' post--shock responses and predict that productivity dispersion is due to the low-cost firm increasing R&D efforts and the high-cost firm doing the opposite. Empirically, we construct an index of negative profit shocks and provide two reduced-form tests for this mechanism."
http://arxiv.org/abs/2108.04622v2,Characterizing the Top Cycle via Strategyproofness,2021-08-10 12:03:56+00:00,"['Felix Brandt', 'Patrick Lederer']",econ.TH,"Gibbard and Satterthwaite have shown that the only single-valued social choice functions (SCFs) that satisfy non-imposition (i.e., the function's range coincides with its codomain) and strategyproofness (i.e., voters are never better off by misrepresenting their preferences) are dictatorships. In this paper, we consider set-valued social choice correspondences (SCCs) that are strategyproof according to Fishburn's preference extension and, in particular, the top cycle, an attractive SCC that returns the maximal elements of the transitive closure of the weak majority relation. Our main theorem implies that, under mild conditions, the top cycle is the only non-imposing strategyproof SCC whose outcome only depends on the quantified pairwise comparisons between alternatives. This result effectively turns the Gibbard-Satterthwaite impossibility into a complete characterization of the top cycle by moving from SCFs to SCCs. It is obtained as a corollary of a more general characterization of strategyproof SCCs."
http://arxiv.org/abs/2109.13177v3,Robust Equilibria in General Competing Mechanism Games,2021-09-27 16:41:45+00:00,['Seungjin Han'],econ.TH,"This paper proposes the notion of robust PBE in a general competing mechanism game of incomplete information where a mechanism allows its designer to send a message to himself at the same time agents send messages. It identifies the utility environments where the notion of robust PBE coincides with that of strongly robust PBE (Epstein and Peters (1999), Han (2007)) and with that of robust PBE respectively. If each agent's utility function is additively separable with respect to principals' actions, it is possible to provide the full characterization of equilibrium allocations under the notion of robust PBE and its variations, in terms of Bayesian incentive compatible (BIC) direct mechanisms, without reference to the set of arbitrary general mechanisms allowed in the game. However, in the standard competing mechanism agme, the adoption of robust PBE as the solution concept does not lead to the full characterization of equilibrium allocations in terms of BIC direct mechanisms even with agents' separable utility functions."
http://arxiv.org/abs/2109.02603v2,Semiparametric Estimation of Treatment Effects in Randomized Experiments,2021-09-06 17:01:03+00:00,"['Susan Athey', 'Peter J. Bickel', 'Aiyou Chen', 'Guido W. Imbens', 'Michael Pollmann']",stat.ME,"We develop new semiparametric methods for estimating treatment effects. We focus on settings where the outcome distributions may be thick tailed, where treatment effects may be small, where sample sizes are large and where assignment is completely random. This setting is of particular interest in recent online experimentation. We propose using parametric models for the treatment effects, leading to semiparametric models for the outcome distributions. We derive the semiparametric efficiency bound for the treatment effects for this setting, and propose efficient estimators. In the leading case with constant quantile treatment effects one of the proposed efficient estimators has an interesting interpretation as a weighted average of quantile treatment effects, with the weights proportional to minus the second derivative of the log of the density of the potential outcomes. Our analysis also suggests an extension of Huber's model and trimmed mean to include asymmetry."
http://arxiv.org/abs/2109.01992v1,Balanced House Allocation,2021-09-05 04:53:31+00:00,"['Xinghua Long', 'Rodrigo A. Velez']",econ.TH,"We introduce balancedness a fairness axiom in house allocation problems. It requires a mechanism to assign the top choice, the second top choice, and so on, on the same number of profiles for each agent. This axiom guarantees equal treatment of all agents at the stage in which the mechanism is announced when all preference profiles are equally likely. We show that, with an interesting exception for the three-agent case, Top Trading Cycles from individual endowments is the only mechanism that is balanced, efficient, and group strategy-proof."
http://arxiv.org/abs/2108.03715v1,A Theoretical Analysis of Logistic Regression and Bayesian Classifiers,2021-08-08 19:23:29+00:00,['Roman V. Kirin'],econ.EM,"This study aims to show the fundamental difference between logistic regression and Bayesian classifiers in the case of exponential and unexponential families of distributions, yielding the following findings. First, the logistic regression is a less general representation of a Bayesian classifier. Second, one should suppose distributions of classes for the correct specification of logistic regression equations. Third, in specific cases, there is no difference between predicted probabilities from correctly specified generative Bayesian classifier and discriminative logistic regression."
http://arxiv.org/abs/2108.03726v1,Improving Inference from Simple Instruments through Compliance Estimation,2021-08-08 20:18:34+00:00,"['Stephen Coussens', 'Jann Spiess']",econ.EM,"Instrumental variables (IV) regression is widely used to estimate causal treatment effects in settings where receipt of treatment is not fully random, but there exists an instrument that generates exogenous variation in treatment exposure. While IV can recover consistent treatment effect estimates, they are often noisy. Building upon earlier work in biostatistics (Joffe and Brensinger, 2003) and relating to an evolving literature in econometrics (including Abadie et al., 2019; Huntington-Klein, 2020; Borusyak and Hull, 2020), we study how to improve the efficiency of IV estimates by exploiting the predictable variation in the strength of the instrument. In the case where both the treatment and instrument are binary and the instrument is independent of baseline covariates, we study weighting each observation according to its estimated compliance (that is, its conditional probability of being affected by the instrument), which we motivate from a (constrained) solution of the first-stage prediction problem implicit to IV. The resulting estimator can leverage machine learning to estimate compliance as a function of baseline covariates. We derive the large-sample properties of a specific implementation of a weighted IV estimator in the potential outcomes and local average treatment effect (LATE) frameworks, and provide tools for inference that remain valid even when the weights are estimated nonparametrically. With both theoretical results and a simulation study, we demonstrate that compliance weighting meaningfully reduces the variance of IV estimates when first-stage heterogeneity is present, and that this improvement often outweighs any difference between the compliance-weighted and unweighted IV estimands. These results suggest that in a variety of applied settings, the precision of IV estimates can be substantially improved by incorporating compliance estimation."
http://arxiv.org/abs/2109.02452v1,"Keep it green, simple and socially fair: a choice experiment on prosumers' preferences for peer to peer electricity trading in the Netherlands",2021-09-06 13:29:09+00:00,"['Elena Georgarakis', 'Thomas Bauwens', 'Anne-Marie Pronk', 'Tarek AlSkaif']",econ.GN,"While the potential for peer-to-peer electricity trading, where households trade surplus electricity with peers in a local energy market, is rapidly growing, the drivers of participation in this trading scheme have been understudied so far. In particular, there is a dearth of research on the role of non-monetary incentives for trading surplus electricity, despite their potentially important role. This paper presents the first discrete choice experiment conducted with prosumers (i.e. proactive households actively managing their electricity production and consumption) in the Netherlands. Electricity trading preferences are analyzed regarding economic, environmental, social and technological parameters, based on survey data (N = 74). The dimensions most valued by prosumers are the environmental and, to a lesser extent, economic dimensions, highlighting the key motivating roles of environmental factors. Furthermore, a majority of prosumers stated they would provide surplus electricity for free or for non-monetary compensations, especially to energy-poor households. These observed trends were more pronounced among members of energy cooperatives. This suggests that peer-to-peer energy trading can advance a socially just energy transition. Regarding policy recommendations, these findings point to the need for communicating environmental and economic benefits when marketing P2P electricity trading platforms and for technical designs enabling effortless and customizable transactions"
http://arxiv.org/abs/2109.02512v1,Optimal Lockdown Strategy in a Pandemic: An Exploratory Analysis for Covid-19,2021-09-06 14:43:27+00:00,"['Gopal K. Basak', 'Chandramauli Chakraborty', 'Pranab Kumar Das']",math.DS,"The paper addresses the question of lives versus livelihood in an SIRD model augmented with a macroeconomic structure. The constraints on the availability of health facilities - both infrastructure and health workers determine the probability of receiving treatment which is found to be higher for the patients with severe infection than the patients with mild infection for the specific parametric configuration of the paper. Distinguishing between two types of direct intervention policy - hard lockdown and soft lockdown, the study derives alternative policy options available to the government. The study further indicates that the soft lockdown policy is optimal from a public policy perspective under the specific parametric configuration considered in this paper."
http://arxiv.org/abs/2108.01617v2,The macroeconomic cost of climate volatility,2021-07-21 15:22:11+00:00,"['Piergiorgio Alessandri', 'Haroon Mumtaz']",q-fin.GN,"We study the impact of climate volatility on economic growth exploiting data on 133 countries between 1960 and 2019. We show that the conditional (ex ante) volatility of annual temperatures increased steadily over time, rendering climate conditions less predictable across countries, with important implications for growth. Controlling for concomitant changes in temperatures, a +1 degree C increase in temperature volatility causes on average a 0.3 percent decline in GDP growth and a 0.7 percent increase in the volatility of GDP. Unlike changes in average temperatures, changes in temperature volatility affect both rich and poor countries."
http://arxiv.org/abs/2108.02755v1,The AI Economist: Optimal Economic Policy Design via Two-level Deep Reinforcement Learning,2021-08-05 17:42:35+00:00,"['Stephan Zheng', 'Alexander Trott', 'Sunil Srinivasa', 'David C. Parkes', 'Richard Socher']",cs.LG,"AI and reinforcement learning (RL) have improved many areas, but are not yet widely adopted in economic policy design, mechanism design, or economics at large. At the same time, current economic methodology is limited by a lack of counterfactual data, simplistic behavioral models, and limited opportunities to experiment with policies and evaluate behavioral responses. Here we show that machine-learning-based economic simulation is a powerful policy and mechanism design framework to overcome these limitations. The AI Economist is a two-level, deep RL framework that trains both agents and a social planner who co-adapt, providing a tractable solution to the highly unstable and novel two-level RL challenge. From a simple specification of an economy, we learn rational agent behaviors that adapt to learned planner policies and vice versa. We demonstrate the efficacy of the AI Economist on the problem of optimal taxation. In simple one-step economies, the AI Economist recovers the optimal tax policy of economic theory. In complex, dynamic economies, the AI Economist substantially improves both utilitarian social welfare and the trade-off between equality and productivity over baselines. It does so despite emergent tax-gaming strategies, while accounting for agent interactions and behavioral change more accurately than economic theory. These results demonstrate for the first time that two-level, deep RL can be used for understanding and as a complement to theory for economic design, unlocking a new computational learning-based approach to understanding economic policy."
http://arxiv.org/abs/2108.09265v2,Efficient Online Estimation of Causal Effects by Deciding What to Observe,2021-08-20 17:00:56+00:00,"['Shantanu Gupta', 'Zachary C. Lipton', 'David Childers']",cs.LG,"Researchers often face data fusion problems, where multiple data sources are available, each capturing a distinct subset of variables. While problem formulations typically take the data as given, in practice, data acquisition can be an ongoing process. In this paper, we aim to estimate any functional of a probabilistic model (e.g., a causal effect) as efficiently as possible, by deciding, at each time, which data source to query. We propose online moment selection (OMS), a framework in which structural assumptions are encoded as moment conditions. The optimal action at each step depends, in part, on the very moments that identify the functional of interest. Our algorithms balance exploration with choosing the best action as suggested by current estimates of the moments. We propose two selection strategies: (1) explore-then-commit (OMS-ETC) and (2) explore-then-greedy (OMS-ETG), proving that both achieve zero asymptotic regret as assessed by MSE. We instantiate our setup for average treatment effect estimation, where structural assumptions are given by a causal graph and data sources may include subsets of mediators, confounders, and instrumental variables."
http://arxiv.org/abs/2108.10109v1,Gender Differences in the Cost of Corrections in Group Work,2021-08-23 12:24:27+00:00,['Yuki Takahashi'],econ.GN,"Corrections among colleagues are an integral part of group work, but people may take corrections as personal criticism, especially corrections by women. I study whether people dislike collaborating with someone who corrects them and more so when that person is a woman. People, including those with high productivity, are less willing to collaborate with a person who has corrected them even if the correction improves group performance. Yet, people respond to corrections by women as negatively as by men. These findings suggest that although women do not face a higher hurdle, correcting colleagues is costly and reduces group efficiency."
http://arxiv.org/abs/2109.06847v1,Wrapping trust for interoperability. A study of wrapped tokens,2021-09-14 17:38:27+00:00,['Giulio Caldarelli'],econ.GN,"As known, blockchains are traditionally blind to the real world. This implies the reliance on third parties called oracles when extrinsic data is needed for smart contracts. However, reintroducing trust and single point of failure, oracles implementation is still controversial and debated. The blindness to the real world makes blockchains also unable to communicate with each other preventing any form of interoperability. An early approach to the interoperability issue is constituted by wrapped tokens, representing blockchain native tokens issued on a non-native blockchain. Similar to how oracles reintroduce trust, and single point of failure, the issuance of wrapped tokens involves third parties whose characteristics need to be considered when evaluating the advantages of crossing-chains. This paper provides an overview of the wrapped tokens and the main technologies implemented in their issuance. Advantages, as well as limitations, are also listed and discussed."
http://arxiv.org/abs/2107.02394v1,"Face masks, vaccination rates and low crowding drive the demand for the London Underground during the COVID-19 pandemic",2021-07-06 05:21:59+00:00,"['Prateek Bansal', 'Roselinde Kessels', 'Rico Krueger', 'Daniel J Graham']",stat.AP,"The COVID-19 pandemic has drastically impacted people's travel behaviour and out-of-home activity participation. While countermeasures are being eased with increasing vaccination rates, the demand for public transport remains uncertain. To investigate user preferences to travel by London Underground during the pandemic, we conducted a stated choice experiment among its pre-pandemic users (N=961). We analysed the collected data using multinomial and mixed logit models. Our analysis provides insights into the sensitivity of the demand for the London Underground with respect to travel attributes (crowding density and travel time), the epidemic situation (confirmed new COVID-19 cases), and interventions (vaccination rates and mandatory face masks). Mandatory face masks and higher vaccination rates are the top two drivers of travel demand for the London Underground during COVID-19. The positive impact of vaccination rates on the Underground demand increases with crowding density, and the positive effect of mandatory face masks decreases with travel time. Mixed logit reveals substantial preference heterogeneity. For instance, while the average effect of mandatory face masks is positive, preferences of around 20% of the pre-pandemic users to travel by the Underground are negatively affected. The estimated demand sensitivities are relevant for supply-demand management in transit systems and the calibration of advanced epidemiological models."
http://arxiv.org/abs/2107.01139v3,"The effects of incentives, social norms, and employees' values on work performance",2021-07-02 15:34:44+00:00,"['Michael Roos', 'Jessica Reale', 'Frederik Banning']",econ.TH,"This agent-based model contributes to a theory of corporate culture in which company performance and employees' behaviour result from the interaction between financial incentives, motivational factors and endogenous social norms. Employees' personal values are the main drivers of behaviour. They shape agents' decisions about how much of their working time to devote to individual tasks, cooperative, and shirking activities. The model incorporates two aspects of the management style, analysed both in isolation and combination: (i) monitoring efforts affecting intrinsic motivation, i.e. the firm is either trusting or controlling, and (ii) remuneration schemes affecting extrinsic motivation, i.e. individual or group rewards. The simulations show that financial incentives can (i) lead to inefficient levels of cooperation, and (ii) reinforce value-driven behaviours, amplified by emergent social norms. The company achieves the highest output with a flat wage and a trusting management. Employees that value self-direction highly are pivotal, since they are strongly (de-)motivated by the management style."
http://arxiv.org/abs/2107.08630v2,Data Sharing Markets,2021-07-19 06:00:34+00:00,"['Mohammad Rasouli', 'Michael I. Jordan']",econ.TH,"With the growing use of distributed machine learning techniques, there is a growing need for data markets that allows agents to share data with each other. Nevertheless data has unique features that separates it from other commodities including replicability, cost of sharing, and ability to distort. We study a setup where each agent can be both buyer and seller of data. For this setup, we consider two cases: bilateral data exchange (trading data with data) and unilateral data exchange (trading data with money). We model bilateral sharing as a network formation game and show the existence of strongly stable outcome under the top agents property by allowing limited complementarity. We propose ordered match algorithm which can find the stable outcome in O(N^2) (N is the number of agents). For the unilateral sharing, under the assumption of additive cost structure, we construct competitive prices that can implement any social welfare maximizing outcome. Finally for this setup when agents have private information, we propose mixed-VCG mechanism which uses zero cost data distortion of data sharing with its isolated impact to achieve budget balance while truthfully implementing socially optimal outcomes to the exact level of budget imbalance of standard VCG mechanisms. Mixed-VCG uses data distortions as data money for this purpose. We further relax zero cost data distortion assumption by proposing distorted-mixed-VCG. We also extend our model and results to data sharing via incremental inquiries and differential privacy costs."
http://arxiv.org/abs/2108.08999v1,Deep Sequence Modeling: Development and Applications in Asset Pricing,2021-08-20 04:40:55+00:00,"['Lin William Cong', 'Ke Tang', 'Jingyuan Wang', 'Yang Zhang']",cs.LG,"We predict asset returns and measure risk premia using a prominent technique from artificial intelligence -- deep sequence modeling. Because asset returns often exhibit sequential dependence that may not be effectively captured by conventional time series models, sequence modeling offers a promising path with its data-driven approach and superior performance. In this paper, we first overview the development of deep sequence models, introduce their applications in asset pricing, and discuss their advantages and limitations. We then perform a comparative analysis of these methods using data on U.S. equities. We demonstrate how sequence modeling benefits investors in general through incorporating complex historical path dependence, and that Long- and Short-term Memory (LSTM) based models tend to have the best out-of-sample performance."
http://arxiv.org/abs/2108.09083v1,A Theoretical Analysis of the Stationarity of an Unrestricted Autoregression Process,2021-08-20 09:31:34+00:00,['Varsha S. Kulkarni'],math.ST,"The higher dimensional autoregressive models would describe some of the econometric processes relatively generically if they incorporate the heterogeneity in dependence on times. This paper analyzes the stationarity of an autoregressive process of dimension $k>1$ having a sequence of coefficients $β$ multiplied by successively increasing powers of $0<δ<1$. The theorem gives the conditions of stationarity in simple relations between the coefficients and $k$ in terms of $δ$. Computationally, the evidence of stationarity depends on the parameters. The choice of $δ$ sets the bounds on $β$ and the number of time lags for prediction of the model."
http://arxiv.org/abs/2108.08229v1,Tilted Platforms: Rental Housing Technology and the Rise of Urban Big Data Oligopolies,2021-08-18 16:36:51+00:00,"['Geoff Boeing', 'Max Besbris', 'David Wachsmuth', 'Jake Wegmann']",econ.GN,"This article interprets emerging scholarship on rental housing platforms -- particularly the most well-known and used short- and long-term rental housing platforms - and considers how the technological processes connecting both short-term and long-term rentals to the platform economy are transforming cities. It discusses potential policy approaches to more equitably distribute benefits and mitigate harms. We argue that information technology is not value-neutral. While rental housing platforms may empower data analysts and certain market participants, the same cannot be said for all users or society at large. First, user-generated online data frequently reproduce the systematic biases found in traditional sources of housing information. Evidence is growing that the information broadcasting potential of rental housing platforms may increase rather than mitigate sociospatial inequality. Second, technology platforms curate and shape information according to their creators' own financial and political interests. The question of which data -- and people -- are hidden or marginalized on these platforms is just as important as the question of which data are available. Finally, important differences in benefits and drawbacks exist between short-term and long-term rental housing platforms, but are underexplored in the literature: this article unpacks these differences and proposes policy recommendations."
http://arxiv.org/abs/2109.06354v4,On the meaning of the Critical Cost Efficiency Index,2021-09-13 22:57:51+00:00,['Federico Echenique'],econ.TH,"This note provides a critical discussion of the \textit{Critical Cost-Efficiency Index} (CCEI) as used to assess deviations from utility-maximizing behavior. I argue that the CCEI is hard to interpret, and that it can disagree with other plausible measures of ""irrational"" behavior. The common interpretation of CCEI as wasted income is questionable. Moreover, I show that one agent may have more unstable preferences than another, but seem more rational according to the CCEI. This calls into question the (now common) use of CCEI as an ordinal and cardinal measure of degrees of rationality."
http://arxiv.org/abs/2109.04154v1,Variable Selection for Causal Inference via Outcome-Adaptive Random Forest,2021-09-09 10:29:26+00:00,['Daniel Jacob'],econ.EM,"Estimating a causal effect from observational data can be biased if we do not control for self-selection. This selection is based on confounding variables that affect the treatment assignment and the outcome. Propensity score methods aim to correct for confounding. However, not all covariates are confounders. We propose the outcome-adaptive random forest (OARF) that only includes desirable variables for estimating the propensity score to decrease bias and variance. Our approach works in high-dimensional datasets and if the outcome and propensity score model are non-linear and potentially complicated. The OARF excludes covariates that are not associated with the outcome, even in the presence of a large number of spurious variables. Simulation results suggest that the OARF produces unbiased estimates, has a smaller variance and is superior in variable selection compared to other approaches. The results from two empirical examples, the effect of right heart catheterization on mortality and the effect of maternal smoking during pregnancy on birth weight, show comparable treatment effects to previous findings but tighter confidence intervals and more plausible selected variables."
http://arxiv.org/abs/2109.10009v1,An AI-assisted Economic Model of Endogenous Mobility and Infectious Diseases: The Case of COVID-19 in the United States,2021-09-21 07:36:02+00:00,"['Lin William Cong', 'Ke Tang', 'Bing Wang', 'Jingyuan Wang']",econ.GN,"We build a deep-learning-based SEIR-AIM model integrating the classical Susceptible-Exposed-Infectious-Removed epidemiology model with forecast modules of infection, community mobility, and unemployment. Through linking Google's multi-dimensional mobility index to economic activities, public health status, and mitigation policies, our AI-assisted model captures the populace's endogenous response to economic incentives and health risks. In addition to being an effective predictive tool, our analyses reveal that the long-term effective reproduction number of COVID-19 equilibrates around one before mass vaccination using data from the United States. We identify a ""policy frontier"" and identify reopening schools and workplaces to be the most effective. We also quantify protestors' employment-value-equivalence of the Black Lives Matter movement and find that its public health impact to be negligible."
http://arxiv.org/abs/2107.14052v1,"The Role of Social Movements, Coalitions, and Workers in Resisting Harmful Artificial Intelligence and Contributing to the Development of Responsible AI",2021-07-11 18:51:29+00:00,['Susan von Struensee'],cs.CY,"There is mounting public concern over the influence that AI based systems has in our society. Coalitions in all sectors are acting worldwide to resist hamful applications of AI. From indigenous people addressing the lack of reliable data, to smart city stakeholders, to students protesting the academic relationships with sex trafficker and MIT donor Jeffery Epstein, the questionable ethics and values of those heavily investing in and profiting from AI are under global scrutiny. There are biased, wrongful, and disturbing assumptions embedded in AI algorithms that could get locked in without intervention. Our best human judgment is needed to contain AI's harmful impact. Perhaps one of the greatest contributions of AI will be to make us ultimately understand how important human wisdom truly is in life on earth."
http://arxiv.org/abs/2107.13866v1,Machine Learning and Factor-Based Portfolio Optimization,2021-07-29 09:58:37+00:00,"['Thomas Conlon', 'John Cotter', 'Iason Kynigakis']",q-fin.PM,"We examine machine learning and factor-based portfolio optimization. We find that factors based on autoencoder neural networks exhibit a weaker relationship with commonly used characteristic-sorted portfolios than popular dimensionality reduction techniques. Machine learning methods also lead to covariance and portfolio weight structures that diverge from simpler estimators. Minimum-variance portfolios using latent factors derived from autoencoders and sparse methods outperform simpler benchmarks in terms of risk minimization. These effects are amplified for investors with an increased sensitivity to risk-adjusted returns, during high volatility periods or when accounting for tail risk."
http://arxiv.org/abs/2107.13894v1,Inference in heavy-tailed non-stationary multivariate time series,2021-07-29 11:02:30+00:00,"['Matteo Barigozzi', 'Giuseppe Cavaliere', 'Lorenzo Trapani']",econ.EM,"We study inference on the common stochastic trends in a non-stationary, $N$-variate time series $y_{t}$, in the possible presence of heavy tails. We propose a novel methodology which does not require any knowledge or estimation of the tail index, or even knowledge as to whether certain moments (such as the variance) exist or not, and develop an estimator of the number of stochastic trends $m$ based on the eigenvalues of the sample second moment matrix of $y_{t}$. We study the rates of such eigenvalues, showing that the first $m$ ones diverge, as the sample size $T$ passes to infinity, at a rate faster by $O\left(T \right)$ than the remaining $N-m$ ones, irrespective of the tail index. We thus exploit this eigen-gap by constructing, for each eigenvalue, a test statistic which diverges to positive infinity or drifts to zero according to whether the relevant eigenvalue belongs to the set of the first $m$ eigenvalues or not. We then construct a randomised statistic based on this, using it as part of a sequential testing procedure, ensuring consistency of the resulting estimator of $m$. We also discuss an estimator of the common trends based on principal components and show that, up to a an invertible linear transformation, such estimator is consistent in the sense that the estimation error is of smaller order than the trend itself. Finally, we also consider the case in which we relax the standard assumption of \textit{i.i.d.} innovations, by allowing for heterogeneity of a very general form in the scale of the innovations. A Monte Carlo study shows that the proposed estimator for $m$ performs particularly well, even in samples of small size. We complete the paper by presenting four illustrative applications covering commodity prices, interest rates data, long run PPP and cryptocurrency markets."
http://arxiv.org/abs/2107.14350v2,Spousal Occupational Sorting and COVID-19 Incidence: Evidence from the United States,2021-07-29 22:10:42+00:00,['Egor Malkov'],econ.GN,"How do matching of spouses and the nature of work jointly shape the distribution of COVID-19 health risks? To address this question, I study the association between the incidence of COVID-19 and the degree of spousal sorting into occupations that differ by contact intensity at the workplace. The mechanism, that I explore, implies that the higher degree of positive spousal sorting mitigates intra-household contagion and this translates into a smaller number of individuals exposed to COVID-19 risk. Using the U.S. data at the state level, I argue that spousal sorting is an important factor for understanding the disparities in the prevalence of COVID-19 during the early stages of the pandemic. First, I document that it creates about two-thirds of the U.S. dual-earner couples that are exposed to higher COVID-19 health risk due to within-household transmission. Moreover, I uncover substantial heterogeneity in the degree of spousal sorting by state. Next, for the first week of April 2020, I estimate that a one standard deviation increase in the measure of spousal sorting is associated with a 30% reduction in the total number of cases per 100000 inhabitants and a 39.3% decline in the total number of deaths per 100000 inhabitants. Furthermore, I find substantial temporal heterogeneity as the coefficients decline in magnitude over time. My results speak to the importance of policies that allow mitigating intra-household contagion."
http://arxiv.org/abs/2108.13474v1,Fuzzy Conventions,2021-08-30 18:53:27+00:00,['Marcin Pęski'],econ.TH,We study binary coordination games with random utility played in networks. A typical equilibrium is fuzzy -- it has positive fractions of agents playing each action. The set of average behaviors that may arise in an equilibrium typically depends on the network. The largest set (in the set inclusion sense) is achieved by a network that consists of a large number of copies of a large complete graph. The smallest set (in the set inclusion sense) is achieved on a lattice-type network. It consists of a single outcome that corresponds to a novel version of risk dominance that is appropriate for games with random utility.
http://arxiv.org/abs/2109.09220v1,Unifying Design-based Inference: On Bounding and Estimating the Variance of any Linear Estimator in any Experimental Design,2021-09-19 20:44:33+00:00,['Joel A. Middleton'],stat.ME,"This paper provides a design-based framework for variance (bound) estimation in experimental analysis. Results are applicable to virtually any combination of experimental design, linear estimator (e.g., difference-in-means, OLS, WLS) and variance bound, allowing for unified treatment and a basis for systematic study and comparison of designs using matrix spectral analysis. A proposed variance estimator reproduces Eicker-Huber-White (aka. ""robust"", ""heteroskedastic consistent"", ""sandwich"", ""White"", ""Huber-White"", ""HC"", etc.) standard errors and ""cluster-robust"" standard errors as special cases. While past work has shown algebraic equivalences between design-based and the so-called ""robust"" standard errors under some designs, this paper motivates them for a wide array of design-estimator-bound triplets. In so doing, it provides a clearer and more general motivation for variance estimators."
http://arxiv.org/abs/2109.09238v1,A Data-Driven Convergence Bidding Strategy Based on Reverse Engineering of Market Participants' Performance: A Case of California ISO,2021-09-19 22:19:10+00:00,"['Ehsan Samani', 'Mahdi Kohansal', 'Hamed Mohsenian-Rad']",eess.SP,"Convergence bidding, a.k.a., virtual bidding, has been widely adopted in wholesale electricity markets in recent years. It provides opportunities for market participants to arbitrage on the difference between the day-ahead market locational marginal prices and the real-time market locational marginal prices. Given the fact that convergence bids (CBs) have a significant impact on the operation of electricity markets, it is important to understand how market participants strategically select their CBs in real-world. We address this open problem with focus on the electricity market that is operated by the California ISO. In this regard, we use the publicly available electricity market data to learn, characterize, and evaluate different types of convergence bidding strategies that are currently used by market participants. Our analysis includes developing a data-driven reverse engineering method that we apply to three years of real-world data. Our analysis involves feature selection and density-based data clustering. It results in identifying three main clusters of CB strategies in the California ISO market. Different characteristics and the performance of each cluster of strategies are analyzed. Interestingly, we unmask a common real-world strategy that does not match any of the existing strategic convergence bidding methods in the literature. Next, we build upon the lessons learned from the existing real-world strategies to propose a new CB strategy that can significantly outperform them. Our analysis includes developing a new strategy for convergence bidding. The new strategy has three steps: net profit maximization by capturing price spikes, dynamic node labeling, and strategy selection algorithm. We show through case studies that the annual net profit for the most lucrative market participants can increase by over 40% if the proposed convergence bidding strategy is used."
http://arxiv.org/abs/2109.08099v2,An Economic Analysis on the Potential and Steady Growth of China: a Practice Based on the Dualistic System Economics in China,2021-09-16 16:38:29+00:00,['Tianyong Zhou'],econ.GN,"The existing theorization of development economics and transition economics is probably inadequate and perhaps even flawed to accurately explain and analyze a dual economic system such as that in China. China is a country in the transition of dual structure and system. The reform of its economic system has brought off a long period of transformation. The allocation of factors is subjected to the dualistic regulation of planning or administration and market due to the dualistic system, and thus the signal distortion will be a commonly seen existence. From the perspective of balanced and safe growth, the institutional distortions of population birth, population flow, land transaction and housing supply, with the changing of export, may cause great influences on the production demand, which includes the iterative contraction of consumption, the increase of export competitive cost, the widening of urban-rural income gap, the transferring of residents' income and the crowding out of consumption. In view of the worldwide shift from a conservative model with more income than expenditure to the debt-based model with more expenditure than income and the need for loose monetary policy, we must explore a basic model that includes variables of debt and land assets that affecting money supply and price changes, especially in China, where the current debt ratio is high and is likely to rise continuously. Based on such a logical framework of dualistic system economics and its analysis method, a preliminary calculation system is formed through the establishment of models."
http://arxiv.org/abs/2109.08124v1,Education and Food Consumption Patterns: Quasi-Experimental Evidence from Indonesia,2021-09-16 17:25:13+00:00,"['Dr Mohammad Rafiqul Islam', 'Dr Nicholas Sim']",econ.GN,"How does food consumption improve educational outcomes is an important policy issue for developing countries. Applying the Indonesian Family Life Survey (IFLS) 2014, we estimate the returns of food consumption to education and investigate if more educated individuals tend to consume healthier bundles than less-educated individuals do. We implement the Expected Outcome Methodology, which is similar to Average Treatment on The Treated (ATT) conceptualized by Angrist and Pischke (2009). We find that education tends to tilt consumption towards healthier foods. Specifically, individuals with upper secondary or higher levels of education, on average, consume 31.5% more healthy foods than those with lower secondary education or lower levels of education. With respect to unhealthy food consumption, more highly-educated individuals, on average, consume 22.8% less unhealthy food than less-educated individuals. This suggests that education can increase the inequality in the consumption of healthy food bundles. Our study suggests that it is important to design policies to expand education for all for at least up to higher secondary level in the context of Indonesia. Our finding also speaks to the link between food-health gradient and human capital formation for a developing country such as Indonesia."
http://arxiv.org/abs/2109.09089v1,Constrained School Choice with Incomplete Information,2021-09-19 08:58:46+00:00,"['Hugo Gimbert', 'Claire Mathieu', 'Simon Mauras']",cs.GT,"School choice is the two-sided matching market where students (on one side) are to be matched with schools (on the other side) based on their mutual preferences. The classical algorithm to solve this problem is the celebrated deferred acceptance procedure, proposed by Gale and Shapley. After both sides have revealed their mutual preferences, the algorithm computes an optimal stable matching. Most often in practice, notably when the process is implemented by a national clearinghouse and thousands of schools enter the market, there is a quota on the number of applications that a student can submit: students have to perform a partial revelation of their preferences, based on partial information on the market. We model this situation by drawing each student type from a publicly known distribution and study Nash equilibria of the corresponding Bayesian game. We focus on symmetric equilibria, in which all students play the same strategy. We show existence of these equilibria in the general case, and provide two algorithms to compute such equilibria under additional assumptions, including the case where schools have identical preferences over students."
http://arxiv.org/abs/2107.03764v1,Limited intelligence and performance-based compensation: An agent-based model of the hidden action problem,2021-07-08 11:19:49+00:00,"['Patrick Reinwald', 'Stephan Leitner', 'Friederike Wall']",econ.GN,"Models of economic decision makers often include idealized assumptions, such as rationality, perfect foresight, and access to all relevant pieces of information. These assumptions often assure the models' internal validity, but, at the same time, might limit the models' power to explain empirical phenomena. This paper is particularly concerned with the model of the hidden action problem, which proposes an optimal performance-based sharing rule for situations in which a principal assigns a task to an agent, and the action taken to carry out this task is not observable by the principal. We follow the agentization approach and introduce an agent-based version of the hidden action problem, in which some of the idealized assumptions about the principal and the agent are relaxed so that they only have limited information access, are endowed with the ability to gain information, and store it in and retrieve it from their (limited) memory. We follow an evolutionary approach and analyze how the principal's and the agent's decisions affect the sharing rule, task performance, and their utility over time. The results indicate that the optimal sharing rule does not emerge. The principal's utility is relatively robust to variations in intelligence, while the agent's utility is highly sensitive to limitations in intelligence. The principal's behavior appears to be driven by opportunism, as she withholds a premium from the agent to assure the optimal utility for herself."
http://arxiv.org/abs/2107.00928v1,Partial Identification and Inference in Duration Models with Endogenous Censoring,2021-07-02 09:37:40+00:00,['Shosei Sakaguchi'],econ.EM,"This paper studies identification and inference in transformation models with endogenous censoring. Many kinds of duration models, such as the accelerated failure time model, proportional hazard model, and mixed proportional hazard model, can be viewed as transformation models. We allow the censoring of a duration outcome to be arbitrarily correlated with observed covariates and unobserved heterogeneity. We impose no parametric restrictions on either the transformation function or the distribution function of the unobserved heterogeneity. In this setting, we develop bounds on the regression parameters and the transformation function, which are characterized by conditional moment inequalities involving U-statistics. We provide inference methods for them by constructing an inference approach for conditional moment inequality models in which the sample analogs of moments are U-statistics. We apply the proposed inference methods to evaluate the effect of heart transplants on patients' survival time using data from the Stanford Heart Transplant Study."
http://arxiv.org/abs/2107.00410v1,Fiscal policy and inequality in a model with endogenous positional concerns,2021-07-01 12:36:17+00:00,"['Kirill Borissov', 'Nigar Hashimzade']",econ.TH,"We investigate the dynamics of wealth inequality in an economy where households have positional preferences, with the strength of the positional concern determined endogenously by inequality of wealth distribution in the society. We demonstrate that in the long run such an economy converges to a unique egalitarian steady-state equilibrium, with all households holding equal positive wealth, when the initial inequality is sufficiently low. Otherwise, the steady state is characterised by polarisation of households into rich, who own all the wealth, and poor, whose wealth is zero. A fiscal policy with government consumption funded by taxes on labour income and wealth can move the economy from any initial state towards an egalitarian equilibrium with a higher aggregate wealth."
http://arxiv.org/abs/2107.05163v1,"Recursive Utility with Investment Gains and Losses: Existence, Uniqueness, and Convergence",2021-07-12 02:00:42+00:00,"['Jing Guo', 'Xue Dong He']",q-fin.GN,"We consider a generalization of the recursive utility model by adding a new component that represents utility of investment gains and losses. We also study the utility process in this generalized model with constant elasticity of intertemporal substitution and relative risk aversion degree, and with infinite time horizon. In a specific, finite-state Markovian setting, we prove that the utility process uniquely exists when the agent derives nonnegative gain-loss utility, and that it can be non-existent or non-unique otherwise. Moreover, we prove that the utility process, when it uniquely exists, can be computed by starting from any initial guess and applying the recursive equation that defines the utility process repeatedly. We then consider a portfolio selection problem with gain-loss utility and solve it by proving that the corresponding dynamic programming equation has a unique solution. Finally, we extend certain previous results to the case in which the state space is infinite."
http://arxiv.org/abs/2107.05302v1,On Reward Sharing in Blockchain Mining Pools,2021-07-12 10:25:23+00:00,"['Burak Can', 'Jens Leth Hougaard', 'Mohsen Pourpouneh']",cs.GT,"This paper proposes a conceptual framework for the analysis of reward sharing schemes in mining pools, such as those associated with Bitcoin. The framework is centered around the reported shares in a pool instead of agents and results in two new fairness criteria, absolute and relative redistribution. These criteria impose that the addition of a share to the pool affects all previous shares in the same way, either in absolute amount or in relative ratio. We characterize two large classes of economically viable reward sharing schemes corresponding to each of these fairness criteria in turn. We further show that the intersection of these classes brings about a generalization of the well-known proportional scheme, which also leads to a new characterization of the proportional scheme as a corollary."
http://arxiv.org/abs/2107.05347v2,Seasonal and Secular Periodicities Identified in the Dynamics of US FDA Medical Devices (1976 2020) Portends Intrinsic Industrial Transformation and Independence of Certain Crises,2021-07-09 15:00:53+00:00,['Iraj Daizadeh'],econ.GN,"Background: The US Food and Drug Administration (FDA) regulates medical devices (MD), which are predicated on a concoction of economic and policy forces (e.g., supply/demand, crises, patents). Assuming that the number of FDA MD (Premarketing Notifications (PMN), Approvals (PMAs), and their sum) Applications behaves similarly to those of other econometrics, this work explores the hypothesis of the existence (and, if so, the length scale(s)) of economic cycles (periodicities). Methods: Beyond summary statistics, the monthly (May, 1976 to December, 2020) number of observed FDA MD Applications are investigated via an assortment of time series techniques (including: Discrete Wavelet Transform, Running Moving Average Filter (RMAF), Complete Ensemble Empirical Mode with Adaptive Noise decomposition (CEEMDAN), and Seasonal Trend Loess (STL) decomposition) to exhaustively search and characterize such periodicities. Results: The data were found to be non-normal, non-stationary (fractional order of integration < 1), non-linear, and strongly persistent (Hurst > 0.5). Importantly, periodicities exist and follow seasonal, 1 year short-term, 5-6 year (Juglar), and a single 24-year medium-term (Kuznets) period (when considering the total number of MD Applications). Economic crises (e.g., COVID-19) do not seem to affect the evolution of the periodicities. Conclusions: This work concludes that (1) PMA and PMN data may be viewed as a proxy measure of the MD industry; (2) periodicities exists in the data with time lengths associated with seasonal/1-year, Juglar and Kuznets affects; (4) these metrics do not seem affected by specific crises (such as COVID-19) (similarly with other econometrics used in periodicity assessments); (5) PMNs and PMAs evolve inversely and suggest a structural industrial transformation; (6) Total MDs are predicted to continue their decline into the mid-2020s prior to recovery."
http://arxiv.org/abs/2107.05527v1,"Collective intelligence and the blockchain: Technology, communities and social experiments",2021-07-12 15:56:21+00:00,['Andrea Baronchelli'],physics.soc-ph,"Blockchains are still perceived chiefly as a new technology. But each blockchain is also a community and a social experiment, built around social consensus. Here I discuss three examples showing how collective intelligence can help, threat or capitalize on blockchain-based ecosystems. They concern the immutability of smart contracts, code transparency and new forms of property. The examples show that more research, new norms and, eventually, laws are needed to manage the interaction between collective behaviour and the blockchain technology. Insights from researchers in collective intelligence can help society rise up to the challenge."
http://arxiv.org/abs/2107.07294v1,Exact inference from finite market data,2021-07-15 12:59:50+00:00,"['Felix Kübler', 'Raghav Malhotra', 'Herakles Polemarchakis']",econ.TH,"We develop conditions under which individual choices and Walrasian equilibrium prices and allocations can be exactly inferred from finite market data. First, we consider market data that consist of individual demands as prices and incomes change. Second, we show that finitely many observations of individual endowments and associated Walrasian equilibrium prices, and only prices, suffice to identify individual demands and, as a consequence, equilibrium comparative statics."
http://arxiv.org/abs/2108.06441v1,Logistics and trade flows in selected ECOWAS Countries: An empirical verification,2021-08-14 01:38:04+00:00,['Eriamiatoe Efosa Festus'],econ.GN,"This study investigates the role of logistics and its six components on trade flows in selected Economic Community of West Africa States (ECOWAS) countries. The impact of other macro-economic variables on trade flows was also investigated. Ten countries were selected in eight years period. We decomposed trade flows into import and export trade. The World Bank Logistics performance index was used as a measure of logistics performance. The LPI has six components, and the impact of these components on trade flows were also examined. The fixed-effect model was used to explain the cross-country result that was obtained. The results showed that logistics has no significant impact on both Import and export, thus logistics play no role on trade flows among the selected ECOWAS countries. The components of logistics except Timeliness of shipments in reaching the final destination ( CRC ),have no impact on trade flows. Income was found to be positively related to imports. Exchange rate, consumption and money supply, reserve and tariff have no significant impact on imports. Relative import price has an inverse and significant relationship with imports. GDP has a positive and significant impact on export trade. The study also found FDI, savings, exchange rate and labour to have insignificant impact on exports. Finally, we found that logistics is not a driver of trade among the selected ECOWAS countries. The study recommended the introduction of the single window system and improvement in border management in order to reduce the cost associated with Logistics and thereby enhance trade."
http://arxiv.org/abs/2108.07372v3,Density Sharpening: Principles and Applications to Discrete Data Analysis,2021-08-16 23:08:35+00:00,['Subhadeep Mukhopadhyay'],stat.ME,"This article introduces a general statistical modeling principle called ""Density Sharpening"" and applies it to the analysis of discrete count data. The underlying foundation is based on a new theory of nonparametric approximation and smoothing methods for discrete distributions which play a useful role in explaining and uniting a large class of applied statistical methods. The proposed modeling framework is illustrated using several real applications, from seismology to healthcare to physics."
http://arxiv.org/abs/2108.07163v1,Causal Impact Of European Union Emission Trading Scheme On Firm Behaviour And Economic Performance: A Study Of German Manufacturing Firms,2021-08-16 15:29:59+00:00,"['Nitish Gupta', 'Jay Shah', 'Satwik Gupta', 'Ruchir Kaul']",econ.GN,"In this paper, we estimate the causal impact (i.e. Average Treatment Effect, ATT) of the EU ETS on GHG emissions and firm competitiveness (primarily measured by employment, turnover, and exports levels) by combining a difference-in-differences approach with semi-parametric matching techniques and estimators an to investigate the effect of the EU ETS on the economic performance of these German manufacturing firms using a Stochastic Production Frontier model."
http://arxiv.org/abs/2108.04737v1,Weighted asymmetric least squares regression with fixed-effects,2021-08-10 14:59:04+00:00,"['Amadou Barry', 'Karim Oualkacha', 'Arthur Charpentier']",econ.EM,"The fixed-effects model estimates the regressor effects on the mean of the response, which is inadequate to summarize the variable relationships in the presence of heteroscedasticity. In this paper, we adapt the asymmetric least squares (expectile) regression to the fixed-effects model and propose a new model: expectile regression with fixed-effects $(\ERFE).$ The $\ERFE$ model applies the within transformation strategy to concentrate out the incidental parameter and estimates the regressor effects on the expectiles of the response distribution. The $\ERFE$ model captures the data heteroscedasticity and eliminates any bias resulting from the correlation between the regressors and the omitted factors. We derive the asymptotic properties of the $\ERFE$ estimators and suggest robust estimators of its covariance matrix. Our simulations show that the $\ERFE$ estimator is unbiased and outperforms its competitors. Our real data analysis shows its ability to capture data heteroscedasticity (see our R package, \url{github.com/AmBarry/erfe})."
http://arxiv.org/abs/2108.05488v2,Identifying poverty traps based on the network structure of economic output,2021-08-12 01:29:39+00:00,"['Vanessa Echeverri', 'Juan C. Duque', 'Daniel E. Restrepo']",econ.GN,"In this work, we explore the relationship between monetary poverty and production combining relatedness theory, graph theory, and regression analysis. We develop two measures at product level that capture short-run and long-run patterns of poverty, respectively. We use the network of related products (or product space) and both metrics to estimate the influence of the productive structure of a country in its current and future levels of poverty. We found that poverty is highly associated with poorly connected nodes in the PS, especially products based on natural resources. We perform a series of regressions with several controls (including human capital, institutions, income, and population) to show the robustness of our measures as predictors of poverty. Finally, by means of some illustrative examples, we show how our measures distinguishes between nuanced cases of countries with similar poverty and production and identify possibilities of improving their current poverty levels."
http://arxiv.org/abs/2107.12494v2,A Unifying Framework for Testing Shape Restrictions,2021-07-26 21:57:42+00:00,['Zheng Fang'],econ.EM,"This paper makes the following original contributions. First, we develop a unifying framework for testing shape restrictions based on the Wald principle. The test has asymptotic uniform size control and is uniformly consistent. Second, we examine the applicability and usefulness of some prominent shape enforcing operators in implementing our framework. In particular, in stark contrast to its use in point and interval estimation, the rearrangement operator is inapplicable due to a lack of convexity. The greatest convex minorization and the least concave majorization are shown to enjoy the analytic properties required to employ our framework. Third, we show that, despite that the projection operator may not be well-defined/behaved in general parameter spaces such as those defined by uniform norms, one may nonetheless employ a powerful distance-based test by applying our framework. Monte Carlo simulations confirm that our test works well. We further showcase the empirical relevance by investigating the relationship between weekly working hours and the annual wage growth in the high-end labor market."
http://arxiv.org/abs/2107.10980v1,Economic Recession Prediction Using Deep Neural Network,2021-07-21 22:55:14+00:00,"['Zihao Wang', 'Kun Li', 'Steve Q. Xia', 'Hongfu Liu']",econ.GN,We investigate the effectiveness of different machine learning methodologies in predicting economic cycles. We identify the deep learning methodology of Bi-LSTM with Autoencoder as the most accurate model to forecast the beginning and end of economic recessions in the U.S. We adopt commonly-available macro and market-condition features to compare the ability of different machine learning models to generate good predictions both in-sample and out-of-sample. The proposed model is flexible and dynamic when both predictive variables and model coefficients vary over time. It provided good out-of-sample predictions for the past two recessions and early warning about the COVID-19 recession.
http://arxiv.org/abs/2108.13671v2,Is happiness u-shaped in age everywhere? A methodological reconsideration for Europe,2021-08-31 08:23:53+00:00,['David Bartram'],econ.GN,"A recent contribution to research on age and well-being (Blanchflower 2021) found that the impact of age on happiness is ""u-shaped"" virtually everywhere: happiness declines towards middle age and subsequently rises, in almost all countries. This paper evaluates that finding for European countries, considering whether it is robust to alternative methodological approaches. The analysis here excludes control variables that are affected by age (noting that those variable are not themselves antecedents of age) and uses data from the entire adult age range (rather than using data only from respondents younger than 70). I also explore the relationship via models that do not impose a quadratic functional form. The paper shows that these alternate approaches do not lead us to perceive a u-shape ""everywhere"": u-shapes are evident for some countries, but for others the pattern is quite different."
http://arxiv.org/abs/2108.06093v3,A Unified Frequency Domain Cross-Validatory Approach to HAC Standard Error Estimation,2021-08-13 07:02:58+00:00,"['Zhihao Xu', 'Clifford M. Hurvich']",econ.EM,"A unified frequency domain cross-validation (FDCV) method is proposed to obtain a heteroskedasticity and autocorrelation consistent (HAC) standard error. This method enables model/tuning parameter selection across both parametric and nonparametric spectral estimators simultaneously. The candidate class for this approach consists of restricted maximum likelihood-based (REML) autoregressive spectral estimators and lag-weights estimators with the Parzen kernel. Additionally, an efficient technique for computing the REML estimators of autoregressive models is provided. Through simulations, the reliability of the FDCV method is demonstrated, comparing favorably with popular HAC estimators such as Andrews-Monahan and Newey-West."
http://arxiv.org/abs/2108.06587v2,Stable and extremely unequal,2021-08-14 16:58:41+00:00,"['Alfred Galichon', 'Octavia Ghelfi', 'Marc Henry']",econ.TH,"We highlight the tension between stability and equality in non transferable utility matching. We consider many to one matchings and refer to the two sides of the market as students and schools. The latter have aligned preferences, which in this context means that a school's utility is the sum of its students' utilities. We show that the unique stable allocation displays extreme inequality between matched pairs."
http://arxiv.org/abs/2109.00923v2,Auctions and Peer Prediction for Academic Peer Review,2021-08-27 23:47:15+00:00,"['Siddarth Srinivasan', 'Jamie Morgenstern']",econ.GN,"Peer reviewed publications are considered the gold standard in certifying and disseminating ideas that a research community considers valuable. However, we identify two major drawbacks of the current system: (1) the overwhelming demand for reviewers due to a large volume of submissions, and (2) the lack of incentives for reviewers to participate and expend the necessary effort to provide high-quality reviews. In this work, we adopt a mechanism-design approach to propose improvements to the peer review process, tying together the paper submission and review processes and simultaneously incentivizing high-quality submissions and reviews. In the submission stage, authors participate in a VCG auction for review slots by submitting their papers along with a bid that represents their expected value for having their paper reviewed. For the reviewing stage, we propose a novel peer prediction mechanism (H-DIPP) building on recent work in the information elicitation literature, which incentivizes participating reviewers to provide honest and effortful reviews. The revenue raised in the submission stage auction is used to pay reviewers based on the quality of their reviews in the reviewing stage."
http://arxiv.org/abs/2107.13380v2,Renewable Energy Targets and Unintended Storage Cycling: Implications for Energy Modeling,2021-07-28 14:11:24+00:00,"['Martin Kittel', 'Wolf-Peter Schill']",econ.GN,"To decarbonize the economy, many governments have set targets for the use of renewable energy sources. These are often formulated as relative shares of electricity demand or supply. Implementing respective constraints in energy models is a surprisingly delicate issue. They may cause a modeling artifact of excessive electricity storage use. We introduce this phenomenon as 'unintended storage cycling', which can be detected in case of simultaneous storage charging and discharging. In this paper, we provide an analytical representation of different approaches for implementing minimum renewable share constraints in models, and show how these may lead to unintended storage cycling. Using a parsimonious optimization model, we quantify related distortions of optimal dispatch and investment decisions as well as market prices, and identify important drivers of the phenomenon. Finally, we provide recommendations on how to avoid the distorting effects of unintended storage cycling in energy modeling."
http://arxiv.org/abs/2107.06141v2,Identification of Average Marginal Effects in Fixed Effects Dynamic Discrete Choice Models,2021-07-12 16:37:35+00:00,"['Victor Aguirregabiria', 'Jesus M. Carro']",econ.EM,"In nonlinear panel data models, fixed effects methods are often criticized because they cannot identify average marginal effects (AMEs) in short panels. The common argument is that identifying AMEs requires knowledge of the distribution of unobserved heterogeneity, but this distribution is not identified in a fixed effects model with a short panel. In this paper, we derive identification results that contradict this argument. In a panel data dynamic logit model, and for $T$ as small as three, we prove the point identification of different AMEs, including causal effects of changes in the lagged dependent variable or the last choice's duration. Our proofs are constructive and provide simple closed-form expressions for the AMEs in terms of probabilities of choice histories. We illustrate our results using Monte Carlo experiments and with an empirical application of a dynamic structural model of consumer brand choice with state dependence."
http://arxiv.org/abs/2109.08109v3,Standard Errors for Calibrated Parameters,2021-09-16 16:57:46+00:00,"['Matthew D. Cocci', 'Mikkel Plagborg-Møller']",econ.EM,"Calibration, the practice of choosing the parameters of a structural model to match certain empirical moments, can be viewed as minimum distance estimation. Existing standard error formulas for such estimators require a consistent estimate of the correlation structure of the empirical moments, which is often unavailable in practice. Instead, the variances of the individual empirical moments are usually readily estimable. Using only these variances, we derive conservative standard errors and confidence intervals for the structural parameters that are valid even under the worst-case correlation structure. In the over-identified case, we show that the moment weighting scheme that minimizes the worst-case estimator variance amounts to a moment selection problem with a simple solution. Finally, we develop tests of over-identifying or parameter restrictions. We apply our methods empirically to a model of menu cost pricing for multi-product firms and to a heterogeneous agent New Keynesian model."
http://arxiv.org/abs/2109.13648v4,Gaussian and Student's $t$ mixture vector autoregressive model with application to the effects of the Euro area monetary policy shock,2021-09-28 12:10:50+00:00,['Savi Virolainen'],econ.EM,"A new mixture vector autoregressive model based on Gaussian and Student's $t$ distributions is introduced. As its mixture components, our model incorporates conditionally homoskedastic linear Gaussian vector autoregressions and conditionally heteroskedastic linear Student's $t$ vector autoregressions. For a $p$th order model, the mixing weights depend on the full distribution of the preceding $p$ observations, which leads to attractive practical and theoretical properties such as ergodicity and full knowledge of the stationary distribution of $p+1$ consecutive observations. A structural version of the model with statistically identified shocks is also proposed. The empirical application studies the effects of the Euro area monetary policy shock. We fit a two-regime model to the data and find the effects, particularly on inflation, stronger in the regime that mainly prevails before the Financial crisis than in the regime that mainly dominates after it. The introduced methods are implemented in the accompanying R package gmvarkit."
http://arxiv.org/abs/2107.04946v4,Inference for the proportional odds cumulative logit model with monotonicity constraints for ordinal predictors and ordinal response,2021-07-11 02:37:05+00:00,"['Javier Espinosa-Brito', 'Christian Hennig']",econ.EM,"The proportional odds cumulative logit model (POCLM) is a standard regression model for an ordinal response. Ordinality of predictors can be incorporated by monotonicity constraints for the corresponding parameters. It is shown that estimators defined by optimization, such as maximum likelihood estimators, for an unconstrained model and for parameters in the interior set of the parameter space of a constrained model are asymptotically equivalent. This is used in order to derive asymptotic confidence regions and tests for the constrained model, involving simple modifications for finite samples. The finite sample coverage probability of the confidence regions is investigated by simulation. Tests concern the effect of individual variables, monotonicity, and a specified monotonicity direction. The methodology is applied on real data related to the assessment of school performance."
http://arxiv.org/abs/2109.11647v4,Treatment Effects in Market Equilibrium,2021-09-23 20:59:38+00:00,"['Evan Munro', 'Xu Kuang', 'Stefan Wager']",econ.EM,"Policy-relevant treatment effect estimation in a marketplace setting requires taking into account both the direct benefit of the treatment and any spillovers induced by changes to the market equilibrium. The standard way to address these challenges is to evaluate interventions via cluster-randomized experiments, where each cluster corresponds to an isolated market. This approach, however, cannot be used when we only have access to a single market (or a small number of markets). Here, we show how to identify and estimate policy-relevant treatment effects using a unit-level randomized trial run within a single large market. A standard Bernoulli-randomized trial allows consistent estimation of direct effects, and of treatment heterogeneity measures that can be used for welfare-improving targeting. Estimating spillovers - as well as providing confidence intervals for the direct effect - requires estimates of price elasticities, which we provide using an augmented experimental design. Our results rely on all spillovers being mediated via the (observed) prices of a finite number of traded goods, and the market power of any single unit decaying as the market gets large. We illustrate our results using a simulation calibrated to a conditional cash transfer experiment in the Philippines."
http://arxiv.org/abs/2109.11536v5,Persuasion with Ambiguous Receiver Preferences,2021-09-23 17:57:14+00:00,['Eitan Sapiro-Gheiler'],econ.TH,"I describe a Bayesian persuasion problem where Receiver has a private type representing a cutoff for choosing Sender's preferred action, and Sender has maxmin preferences over all Receiver type distributions with known mean and bounds. This problem can be represented as a zero-sum game where Sender chooses a distribution of posterior mean beliefs that is a mean-preserving contraction of the prior over states, and an adversarial Nature chooses a Receiver type distribution with the known mean; the player with the higher realization from their chosen distribution wins. I formalize the connection between maxmin persuasion and similar games used to model political spending, all-pay auctions, and competitive persuasion. In both a standard binary-state setting and a new continuous-state setting, Sender optimally linearizes the prior distribution over states to create a distribution of posterior means that is uniform on a known interval with an atom at the lower bound of its support."
http://arxiv.org/abs/2107.09235v4,Distributional Effects with Two-Sided Measurement Error: An Application to Intergenerational Income Mobility,2021-07-20 02:49:51+00:00,"['Brantly Callaway', 'Tong Li', 'Irina Murtazashvili', 'Emmanuel Tsyawo']",econ.EM,"This paper considers identification and estimation of distributional effect parameters that depend on the joint distribution of an outcome and another variable of interest (""treatment"") in a setting with ""two-sided"" measurement error -- that is, where both variables are possibly measured with error. Examples of these parameters in the context of intergenerational income mobility include transition matrices, rank-rank correlations, and the poverty rate of children as a function of their parents' income, among others. Building on recent work on quantile regression (QR) with measurement error in the outcome (particularly, Hausman, Liu, Luo, and Palmer (2021)), we show that, given (i) two linear QR models separately for the outcome and treatment conditional on other observed covariates and (ii) assumptions about the measurement error for each variable, one can recover the joint distribution of the outcome and the treatment. Besides these conditions, our approach does not require an instrument, repeated measurements, or distributional assumptions about the measurement error. Using recent data from the 1997 National Longitudinal Study of Youth, we find that accounting for measurement error notably reduces several estimates of intergenerational mobility parameters."
http://arxiv.org/abs/2110.00032v1,Truly Costly Search and Word-of-Mouth Communication,2021-09-30 18:03:12+00:00,['Atabek Atayev'],econ.TH,"In markets with search frictions, consumers can acquire information about goods either through costly search or from friends via word-of-mouth (WOM) communication. How do sellers' market power react to a very large increase in the number of consumers' friends with whom they engage in WOM? The answer to the question depends on whether consumers are freely endowed with price information. If acquiring price quotes is costly, equilibrium prices are dispersed and the expected price is higher than the marginal cost of production. This implies that firms retain market power even if price information is disseminated among a very large number of consumers due to technological progress, such as social networking websites."
http://arxiv.org/abs/2112.07155v2,Behavioral Foundations of Nested Stochastic Choice and Nested Logit,2021-12-14 04:30:14+00:00,"['Matthew Kovach', 'Gerelt Tserenjigmid']",econ.TH,"We provide the first behavioral characterization of nested logit, a foundational and widely applied discrete choice model, through the introduction of a non-parametric version of nested logit that we call Nested Stochastic Choice (NSC). NSC is characterized by a single axiom that weakens Independence of Irrelevant Alternatives based on revealed similarity to allow for the similarity effect. Nested logit is characterized by an additional menu-independence axiom. Our axiomatic characterization leads to a practical, data-driven algorithm that identifies the true nest structure from choice data. We also discuss limitations of generalizing nested logit by studying the testable implications of cross-nested logit."
http://arxiv.org/abs/2111.07295v1,Rational AI: A comparison of human and AI responses to triggers of economic irrationality in poker,2021-11-14 09:48:53+00:00,"['C. Grace Haaf', 'Devansh Singh', 'Cinny Lin', 'Scofield Zou']",econ.TH,"Humans exhibit irrational decision-making patterns in response to environmental triggers, such as experiencing an economic loss or gain. In this paper we investigate whether algorithms exhibit the same behavior by examining the observed decisions and latent risk and rationality parameters estimated by a random utility model with constant relative risk-aversion utility function. We use a dataset consisting of 10,000 hands of poker played by Pluribus, the first algorithm in the world to beat professional human players and find (1) Pluribus does shift its playing style in response to economic losses and gains, ceteris paribus; (2) Pluribus becomes more risk-averse and rational following a trigger but the humans become more risk-seeking and irrational; (3) the difference in playing styles between Pluribus and the humans on the dimensions of risk-aversion and rationality are particularly differentiable when both have experienced a trigger. This provides support that decision-making patterns could be used as ""behavioral signatures"" to identify human versus algorithmic decision-makers in unlabeled contexts."
http://arxiv.org/abs/2110.02755v5,Gambits: Theory and Evidence,2021-10-05 12:02:11+00:00,"['Shiva Maharaj', 'Nicholas Polson', 'Christian Turk']",econ.TH,"Gambits are central to human decision-making. Our goal is to provide a theory of Gambits. A Gambit is a combination of psychological and technical factors designed to disrupt predictable play. Chess provides an environment to study gambits and behavioral game theory. Our theory is based on the Bellman optimality path for sequential decision-making. This allows us to calculate the $Q$-values of a Gambit where material (usually a pawn) is sacrificed for dynamic play. On the empirical side, we study the effectiveness of a number of popular chess Gambits. This is a natural setting as chess Gambits require a sequential assessment of a set of moves (a.k.a. policy) after the Gambit has been accepted. Our analysis uses Stockfish 14.1 to calculate the optimal Bellman $Q$ values, which fundamentally measures if a position is winning or losing. To test whether Bellman's equation holds in play, we estimate the transition probabilities to the next board state via a database of expert human play. This then allows us to test whether the \emph{Gambiteer} is following the optimal path in his decision-making. Our methodology is applied to the popular Stafford and reverse Stafford (a.k.a. Boden-Kieretsky-Morphy) Gambit and other common ones including the Smith-Morra, Goring, Danish and Halloween Gambits. We build on research in human decision-making by proving an irrational skewness preference within agents in chess. We conclude with directions for future research."
http://arxiv.org/abs/2112.13649v1,Random Rank-Dependent Expected Utility,2021-12-27 13:18:42+00:00,"['Nail Kashaev', 'Victor Aguiar']",econ.TH,We present a novel characterization of random rank-dependent expected utility for finite datasets and finite prizes. The test lends itself to statistical testing using the tools in Kitamura and Stoye (2018).
http://arxiv.org/abs/2110.03432v3,"Noise, fake news, and tenacious Bayesians",2021-10-05 16:11:08+00:00,['Dorje C. Brody'],econ.TH,"A modelling framework, based on the theory of signal processing, for characterising the dynamics of systems driven by the unravelling of information is outlined, and is applied to describe the process of decision making. The model input of this approach is the specification of the flow of information. This enables the representation of (i) reliable information, (ii) noise, and (iii) disinformation, in a unified framework. Because the approach is designed to characterise the dynamics of the behaviour of people, it is possible to quantify the impact of information control, including those resulting from the dissemination of disinformation. It is shown that if a decision maker assigns an exceptionally high weight on one of the alternative realities, then under the Bayesian logic their perception hardly changes in time even if evidences presented indicate that this alternative corresponds to a false reality. Thus confirmation bias need not be incompatible with Bayesian updating. By observing the role played by noise in other areas of natural sciences, where noise is used to excite the system away from false attractors, a new approach to tackle the dark forces of fake news is proposed."
http://arxiv.org/abs/2110.10650v4,Attention Overload,2021-10-20 16:46:35+00:00,"['Matias D. Cattaneo', 'Paul Cheung', 'Xinwei Ma', 'Yusufcan Masatlioglu']",econ.TH,"We introduce an Attention Overload Model that captures the idea that alternatives compete for the decision maker's attention, and hence the attention that each alternative receives decreases as the choice problem becomes larger. Using this nonparametric restriction on the random attention formation, we show that a fruitful revealed preference theory can be developed and provide testable implications on the observed choice behavior that can be used to (point or partially) identify the decision maker's preference and attention frequency. We then enhance our attention overload model to accommodate heterogeneous preferences. Due to the nonparametric nature of our identifying assumption, we must discipline the amount of heterogeneity in the choice model: we propose the idea of List-based Attention Overload, where alternatives are presented to the decision makers as a list that correlates with both heterogeneous preferences and random attention. We show that preference and attention frequencies are (point or partially) identifiable under nonparametric assumptions on the list and attention formation mechanisms, even when the true underlying list is unknown to the researcher. Building on our identification results, for both preference and attention frequencies, we develop econometric methods for estimation and inference that are valid in settings with a large number of alternatives and choice problems, a distinctive feature of the economic environment we consider. We provide a software package in R implementing our empirical methods, and illustrate them in a simulation study."
http://arxiv.org/abs/2201.07975v1,Effect Structure and Thermodynamics Formulation of Demand-side Economics,2021-11-18 21:34:09+00:00,['Burin Gumjudpai'],physics.soc-ph,"We propose concept of equation of state (EoS) effect structure in form of diagrams and rules. This concept helps justifying EoS status of an empirical relation. We apply the concept to closed system of consumers and we are able to formulate its EoS. According to the new concept, EoS are classified into three classes. Manifold space of thermodynamics formulation of demand-side economics is identified. Formal analogies of thermodynamics and economics consumers' system are made. New quantities such as total wealth, generalized utility and generalized consumer surplus are defined. Microeconomics' concept of consumer surplus is criticized and replaced with generalized consumer surplus. Smith's law of demand is included in our new paradigm as a specific case resembling isothermal process. Absolute zero temperature state resembles the nirvana state in Buddhism philosophy. Econometric modelling of consumers' EoS is proposed at last."
http://arxiv.org/abs/2112.03626v7,Phase transitions in nonparametric regressions,2021-12-07 10:55:31+00:00,['Ying Zhu'],math.ST,"When the unknown regression function of a single variable is known to have derivatives up to the $(γ+1)$th order bounded in absolute values by a common constant everywhere or a.e. (i.e., $(γ+1)$th degree of smoothness), the minimax optimal rate of the mean integrated squared error (MISE) is stated as $\left(\frac{1}{n}\right)^{\frac{2γ+2}{2γ+3}}$ in the literature. This paper shows that: (i) if $n\leq\left(γ+1\right)^{2γ+3}$, the minimax optimal MISE rate is $\frac{\log n}{n\log(\log n)}$ and the optimal degree of smoothness to exploit is roughly $\max\left\{ \left\lfloor \frac{\log n}{2\log\left(\log n\right)}\right\rfloor ,\,1\right\} $; (ii) if $n>\left(γ+1\right)^{2γ+3}$, the minimax optimal MISE rate is $\left(\frac{1}{n}\right)^{\frac{2γ+2}{2γ+3}}$ and the optimal degree of smoothness to exploit is $γ+1$. The fundamental contribution of this paper is a set of metric entropy bounds we develop for smooth function classes. Some of our bounds are original, and some of them improve and/or generalize the ones in the literature (e.g., Kolmogorov and Tikhomirov, 1959). Our metric entropy bounds allow us to show phase transitions in the minimax optimal MISE rates associated with some commonly seen smoothness classes as well as non-standard smoothness classes, and can also be of independent interest outside the nonparametric regression problems."
http://arxiv.org/abs/2112.04637v1,"Efficient counterfactual estimation in semiparametric discrete choice models: a note on Chiong, Hsieh, and Shum (2017)",2021-12-09 00:49:56+00:00,['Grigory Franguridi'],econ.EM,"I suggest an enhancement of the procedure of Chiong, Hsieh, and Shum (2017) for calculating bounds on counterfactual demand in semiparametric discrete choice models. Their algorithm relies on a system of inequalities indexed by cycles of a large number $M$ of observed markets and hence seems to require computationally infeasible enumeration of all such cycles. I show that such enumeration is unnecessary because solving the ""fully efficient"" inequality system exploiting cycles of all possible lengths $K=1,\dots,M$ can be reduced to finding the length of the shortest path between every pair of vertices in a complete bidirected weighted graph on $M$ vertices. The latter problem can be solved using the Floyd--Warshall algorithm with computational complexity $O\left(M^3\right)$, which takes only seconds to run even for thousands of markets. Monte Carlo simulations illustrate the efficiency gain from using cycles of all lengths, which turns out to be positive, but small."
http://arxiv.org/abs/2112.04723v1,Covariate Balancing Sensitivity Analysis for Extrapolating Randomized Trials across Locations,2021-12-09 06:50:32+00:00,"['Xinkun Nie', 'Guido Imbens', 'Stefan Wager']",econ.EM,"The ability to generalize experimental results from randomized control trials (RCTs) across locations is crucial for informing policy decisions in targeted regions. Such generalization is often hindered by the lack of identifiability due to unmeasured effect modifiers that compromise direct transport of treatment effect estimates from one location to another. We build upon sensitivity analysis in observational studies and propose an optimization procedure that allows us to get bounds on the treatment effects in targeted regions. Furthermore, we construct more informative bounds by balancing on the moments of covariates. In simulation experiments, we show that the covariate balancing approach is promising in getting sharper identification intervals."
http://arxiv.org/abs/2112.06192v1,Housing Price Prediction Model Selection Based on Lorenz and Concentration Curves: Empirical Evidence from Tehran Housing Market,2021-12-12 09:44:28+00:00,['Mohammad Mirbagherijam'],econ.EM,"This study contributes a house price prediction model selection in Tehran City based on the area between Lorenz curve (LC) and concentration curve (CC) of the predicted price by using 206,556 observed transaction data over the period from March 21, 2018, to February 19, 2021. Several different methods such as generalized linear models (GLM) and recursive partitioning and regression trees (RPART), random forests (RF) regression models, and neural network (NN) models were examined house price prediction. We used 90% of all data samples which were chosen randomly to estimate the parameters of pricing models and 10% of remaining datasets to test the accuracy of prediction. Results showed that the area between the LC and CC curves (which are known as ABC criterion) of real and predicted prices in the test data sample of the random forest regression model was less than by other models under study. The comparison of the calculated ABC criteria leads us to conclude that the nonlinear regression models such as RF regression models give an accurate prediction of house prices in Tehran City."
http://arxiv.org/abs/2112.02877v1,"Cocoa pollination, biodiversity-friendly production, and the global market",2021-12-06 09:09:12+00:00,"['Thomas Cherico Wanger', 'Francis Dennig', 'Manuel Toledo-Hernández', 'Teja Tscharntke', 'Eric F. Lambin']",econ.GN,"Production of cocoa, the third largest trade commodity globally has experienced climate related yield stagnation since 2016, forcing farmers to expand production in forested habitats and to shift from nature friendly agroforestry systems to intensive monocultures. The goal for future large-scale cocoa production combines high yields with biodiversity friendly management into a climate adapted smart agroforestry system (SAS). As pollination limitation is a key driver of global production, we use data of more than 150,000 cocoa farms and results of hand pollination experiments to show that manually enhancing cocoa pollination (hereafter manual pollination) can produce SAS. Manual pollination can triple farm yields and double farmers annual profit in the major producer countries Ivory Coast, Ghana, and Indonesia, and can increase global cocoa supplies by up to 13%. We propose a win win scenario to mitigate negative long term price and socioeconomic effects, whereby manual pollination compensates only for yield losses resulting from climate and disease related decreases in production area and conversion of monocultures into agroforestry systems. Our results highlight that yields in biodiversity friendly and climate adapted SAS can be similar to yields currently only achieved in monocultures. Adoption of manual pollination could be achieved through wider implementation of ecocertification standards, carbon markets, and zero deforestation pledges."
http://arxiv.org/abs/2112.02893v1,Increased Electrification of Heating and Weather Risk in the Nordic Power System,2021-12-06 09:30:43+00:00,"['Ian M. Trotter', 'Torjus F. Bolkesjø', 'Eirik O. Jåstad', 'Jon Gustav Kirkerud']",econ.GN,"Weather is one of the main drivers of both the power demand and supply, especially in the Nordic region which is characterized by high heating needs and a high share of renewable energy. Furthermore, ambitious decarbonization plans may cause power to replace fossil fuels for heating in the Nordic region, at the same time as large wind power expansions are expected, resulting in even greater exposure to weather risk. In this study, we quantify the increase in weather risk resulting from replacing fossil fuels with power for heating in the Nordic region, at the same time as variable renewable generation expands. First, we calibrate statistical weather-driven power consumption models for each of the countries Norway, Sweden, Denmark, and Finland. Then, we modify the weather sensitivity of the models to simulate different levels of heating electrification, and use 300 simulated weather years to investigate how differing weather conditions impact power consumption at each electrification level. The results show that full replacement of fossil fuels by power for heating in 2040 leads to an increase in annual consumption of 155 TWh (30%) compared to a business-as-usual scenario during an average weather year, but a 178 TWh (34%) increase during a one-in-twenty weather year. However, the increase in the peak consumption is greater: around 50% for a normal weather year, and 70% for a one-in-twenty weather year. Furthermore, wind and solar generation contribute little during the consumption peaks. The increased weather sensitivity caused by heating electrification causes greater total load, but also causes a significant increase in inter-annual, seasonal, and intra-seasonal variations. We conclude that heating electrification must be accompanied by an increase in power system flexibility to ensure a stable and secure power supply."
http://arxiv.org/abs/2112.03075v1,Deep Quantile and Deep Composite Model Regression,2021-12-06 14:31:25+00:00,"['Tobias Fissler', 'Michael Merz', 'Mario V. Wüthrich']",stat.ME,"A main difficulty in actuarial claim size modeling is that there is no simple off-the-shelf distribution that simultaneously provides a good distributional model for the main body and the tail of the data. In particular, covariates may have different effects for small and for large claim sizes. To cope with this problem, we introduce a deep composite regression model whose splicing point is given in terms of a quantile of the conditional claim size distribution rather than a constant. To facilitate M-estimation for such models, we introduce and characterize the class of strictly consistent scoring functions for the triplet consisting a quantile, as well as the lower and upper expected shortfall beyond that quantile. In a second step, this elicitability result is applied to fit deep neural network regression models. We demonstrate the applicability of our approach and its superiority over classical approaches on a real accident insurance data set."
http://arxiv.org/abs/2112.07149v2,Factor Models with Sparse VAR Idiosyncratic Components,2021-12-14 04:10:59+00:00,"['Jonas Krampe', 'Luca Margaritella']",stat.ME,"We reconcile the two worlds of dense and sparse modeling by exploiting the positive aspects of both. We employ a factor model and assume {the dynamic of the factors is non-pervasive while} the idiosyncratic term follows a sparse vector autoregressive model (VAR) {which allows} for cross-sectional and time dependence. The estimation is articulated in two steps: first, the factors and their loadings are estimated via principal component analysis and second, the sparse VAR is estimated by regularized regression on the estimated idiosyncratic components. We prove the consistency of the proposed estimation approach as the time and cross-sectional dimension diverge. In the second step, the estimation error of the first step needs to be accounted for. Here, we do not follow the naive approach of simply plugging in the standard rates derived for the factor estimation. Instead, we derive a more refined expression of the error. This enables us to derive tighter rates. We discuss the implications of our model for forecasting, factor augmented regression, bootstrap of factor models, and time series dependence networks via semi-parametric estimation of the inverse of the spectral density matrix."
http://arxiv.org/abs/2112.05811v1,"On the Stability, Economic Efficiency and Incentive Compatibility of Electricity Market Dynamics",2021-12-10 20:04:44+00:00,"['Pengcheng You', 'Yan Jiang', 'Enoch Yeung', 'Dennice F. Gayme', 'Enrique Mallada']",math.OC,"This paper focuses on the operation of an electricity market that accounts for participants that bid at a sub-minute timescale. To that end, we model the market-clearing process as a dynamical system, called market dynamics, which is temporally coupled with the grid frequency dynamics and is thus required to guarantee system-wide stability while meeting the system operational constraints. We characterize participants as price-takers who rationally update their bids to maximize their utility in response to real-time schedules of prices and dispatch. For two common bidding mechanisms, based on quantity and price, we identify a notion of alignment between participants' behavior and planners' goals that leads to a saddle-based design of the market that guarantees convergence to a point meeting all operational constraints. We further explore cases where this alignment property does not hold and observe that misaligned participants' bidding can destabilize the closed-loop system. We thus design a regularized version of the market dynamics that recovers all the desirable stability and steady-state performance guarantees. Numerical tests validate our results on the IEEE 39-bus system."
http://arxiv.org/abs/2112.05876v1,The Past as a Stochastic Process,2021-12-11 00:15:59+00:00,"['David H. Wolpert', 'Michael H. Price', 'Stefani A. Crabtree', 'Timothy A. Kohler', 'Jurgen Jost', 'James Evans', 'Peter F. Stadler', 'Hajime Shimao', 'Manfred D. Laubichler']",stat.AP,"Historical processes manifest remarkable diversity. Nevertheless, scholars have long attempted to identify patterns and categorize historical actors and influences with some success. A stochastic process framework provides a structured approach for the analysis of large historical datasets that allows for detection of sometimes surprising patterns, identification of relevant causal actors both endogenous and exogenous to the process, and comparison between different historical cases. The combination of data, analytical tools and the organizing theoretical framework of stochastic processes complements traditional narrative approaches in history and archaeology."
http://arxiv.org/abs/2112.05950v1,Analysis of stability and bifurcation for two heterogeneous triopoly games with the isoelastic demand,2021-12-11 11:01:20+00:00,['Xiaoliang Li'],math.DS,"In this paper, we investigate two heterogeneous triopoly games where the demand function of the market is isoelastic. The local stability and the bifurcation of these games are systematically analyzed using the symbolic approach proposed by the author. The novelty of the present work is twofold. On one hand, the results of this paper are analytical, which are different from the existing results in the literature based on observations through numerical simulations. In particular, we rigorously prove the existence of double routes to chaos through the period-doubling bifurcation and through the Neimark-Sacker bifurcation. On the other hand, for the special case of the involved firms having identical marginal costs, we acquire the necessary and sufficient conditions of the local stability for both models. By further analyzing these conditions, it seems that that the presence of the local monopolistic approximation (LMA) mechanism might have a stabilizing effect for heterogeneous triopoly games with the isoelastic demand."
http://arxiv.org/abs/2112.08153v2,Taxes and Market Power: A Principal Components Approach,2021-12-15 14:21:27+00:00,"['Andrea Galeotti', 'Benjamin Golub', 'Sanjeev Goyal', 'Eduard Talamàs', 'Omer Tamuz']",econ.TH,"Suppliers of differentiated goods make simultaneous pricing decisions, which are strategically linked. Because of market power, the equilibrium is inefficient. We study how a policymaker should target a budget-balanced tax-and-subsidy policy to increase welfare. A key tool is a certain basis for the goods space, determined by the network of interactions among suppliers. It consists of eigenbundles -- orthogonal in the sense that a tax on any eigenbundle passes through only to its own price -- with pass-through coefficients determined by associated eigenvalues. Our basis permits a simple characterization of optimal interventions. A planner maximizing consumer surplus should tax eigenbundles with low pass-through and subsidize ones with high pass-through. The Pigouvian leverage of the system -- the gain in consumer surplus achievable by an optimal tax scheme -- depends only on the dispersion of the eigenvalues of the matrix of strategic interactions. We interpret these results in terms of the network structure of the market."
http://arxiv.org/abs/2112.14713v1,"Perspectives in Public and University Sector Co-operation in the Change of Higher Education Model in Hungary, in Light of China's Experience",2021-12-21 19:45:18+00:00,"['Attila Lajos Makai', 'Szabolcs Ramhap']",econ.GN,"The model shift of higher education in Hungary brought not only the deepening of university-industry relations and technology transfer processes, but also contribute the emerging role of universities in shaping regional innovation policy. This process provides a new framework for cooperation between actors in regional innovation ecosystems and raises the relationship between economic-governmental-academic systems to a new level. Active involvement of government, the predominance of state resources, and the strong innovation-organizing power of higher education institutions are similarities that characterize both the Hungarian and Chinese innovation systems. This paper attempts to gather Chinese good practices whose adaptation can contribute to successful public-university collaboration. In the light of the examined practices, the processes related to the university model shift implemented so far can be placed in a new context, which are presented through the example of the Széchenyi István University of Győr."
http://arxiv.org/abs/2112.15431v1,"Forecasting pandemic tax revenues in a small, open economy",2021-12-22 10:39:59+00:00,['Fabio Ashtar Telarico'],econ.GN,"Tax analysis and forecasting of revenues are of paramount importance to ensure fiscal policy's viability and sustainability. However, the measures taken to contain the spread of the recent pandemic pose an unprecedented challenge to established models and approaches. This paper proposes a model to forecast tax revenues in Bulgaria for the fiscal years 2020-2022 built in accordance with the International Monetary Fund's recommendations on a dataset covering the period between 1995 and 2019. The study further discusses the actual trustworthiness of official Bulgarian forecasts, contrasting those figures with the model previously estimated. This study's quantitative results both confirm the pandemic's assumed negative impact on tax revenues and prove that econometrics can be tweaked to produce consistent revenue forecasts even in the relatively-unexplored case of Bulgaria offering new insights to policymakers and advocates."
http://arxiv.org/abs/2112.13506v1,Estimation based on nearest neighbor matching: from density ratio to average treatment effect,2021-12-27 04:45:29+00:00,"['Zhexiao Lin', 'Peng Ding', 'Fang Han']",math.ST,"Nearest neighbor (NN) matching as a tool to align data sampled from different groups is both conceptually natural and practically well-used. In a landmark paper, Abadie and Imbens (2006) provided the first large-sample analysis of NN matching under, however, a crucial assumption that the number of NNs, $M$, is fixed. This manuscript reveals something new out of their study and shows that, once allowing $M$ to diverge with the sample size, an intrinsic statistic in their analysis actually constitutes a consistent estimator of the density ratio. Furthermore, through selecting a suitable $M$, this statistic can attain the minimax lower bound of estimation over a Lipschitz density function class. Consequently, with a diverging $M$, the NN matching provably yields a doubly robust estimator of the average treatment effect and is semiparametrically efficient if the density functions are sufficiently smooth and the outcome model is appropriately specified. It can thus be viewed as a precursor of double machine learning estimators."
http://arxiv.org/abs/2112.12179v5,Henderson--Chu model extended to two heterogeneous groups,2021-11-27 03:25:42+00:00,"['Oliver Chiriac', 'Jonathan Hall']",econ.GN,The goal of this paper is to revise the Henderson-Chu approach by dividing the commuters into two income-based groups: the `rich' and the `poor'. The rich are clearly more flexible in their arrival times and would rather have more schedule delay cost instead of travel delay. The poor are quite inflexible as they have to arrive at work at a specified time -- travel delay cost is preferred over schedule delay cost. We combined multiple models of peak-load bottleneck congestion with and without toll-pricing to generate a Pareto improvement in Lexus lanes.
http://arxiv.org/abs/2112.13842v1,Economics of Innovation and Perceptions of Renewed Education and Curriculum Design in Bangladesh,2021-12-23 23:14:00+00:00,"['Shifa Taslim Chowdhury', 'Mohammad Nur Nobi', 'Anm Moinul Islam']",econ.GN,"The creative Education system is one of the effective education systems in many countries like Finland, Denmark, and South Korea. Bangladesh Government has also launched the creative curriculum system in 2009 in both primary and secondary levels, where changes have been made in educational contents and exam question patterns. These changes in the previous curriculum aimed to avoid memorization and less creativity and increase the students' level of understanding and critical thinking. Though the Government has taken these steps, the quality of the educational system in Bangladesh is still deteriorating. Since the curriculum has been changed recently, this policy issue got massive attention of the people because the problem of a substandard education system has arisen. Many students have poor performances in examinations, including entrance hall exams in universities and board examinations. This deteriorating situation is mostly for leakage of question paper, inadequate equipment and materials, and insufficient training. As a result, the existing education system has failed to provide the standard level of education. This research will discuss and find why this creative educational system is getting impacted by these factors. It will be qualitative research. A systematic questionnaire will interview different school teachers, parents, experts, and students."
http://arxiv.org/abs/2112.13849v3,The Long-Run Impact of Electoral Violence on Health and Human Capital in Kenya,2021-12-27 12:23:04+00:00,['Roxana Gutiérrez-Romero'],econ.GN,"This paper examines the long-term effects of prenatal, childhood, and teen exposure to electoral violence on health and human capital. Furthermore, it investigates whether these effects are passed down to future generations. We exploit the temporal and spatial variation of electoral violence in Kenya between 1992 and 2013 in conjunction with a nationally representative survey to identify people exposed to such violence. Using coarsened matching, we find that exposure to electoral violence between prenatal and the age of sixteen reduces adult height. Previous research has demonstrated that protracted, large-scale armed conflicts can pass down stunting effects to descendants. In line with these studies, we find that the low-scale but recurrent electoral violence in Kenya has affected the height-for-age of children whose parents were exposed to such violence during their growing years. Only boys exhibit this intergenerational effect, possibly due to their increased susceptibility to malnutrition and stunting in Sub-Saharan Africa. In contrast to previous research on large-scale conflicts, childhood exposure to electoral violence has no long-term effect on educational attainment or household consumption per capita. Most electoral violence in Kenya has occurred during school breaks, which may have mitigated its long-term effects on human capital and earning capacity."
http://arxiv.org/abs/2110.01741v1,Beware the Gini Index! A New Inequality Measure,2021-10-04 23:10:35+00:00,['Sabiou Inoua'],stat.ME,"The Gini index underestimates inequality for heavy-tailed distributions: for example, a Pareto distribution with exponent 1.5 (which has infinite variance) has the same Gini index as any exponential distribution (a mere 0.5). This is because the Gini index is relatively robust to extreme observations: while a statistic's robustness to extremes is desirable for data potentially distorted by outliers, it is misleading for heavy-tailed distributions, which inherently exhibit extremes. We propose an alternative inequality index: the variance normalized by the second moment. This ratio is more stable (hence more reliable) for large samples from an infinite-variance distribution than the Gini index paradoxically. Moreover, the new index satisfies the normative axioms of inequality measurement; notably, it is decomposable into inequality within and between subgroups, unlike the Gini index."
http://arxiv.org/abs/2110.01873v1,A New Multivariate Predictive Model for Stock Returns,2021-10-05 08:23:14+00:00,['Jianying Xie'],econ.EM,"One of the most important studies in finance is to find out whether stock returns could be predicted. This research aims to create a new multivariate model, which includes dividend yield, earnings-to-price ratio, book-to-market ratio as well as consumption-wealth ratio as explanatory variables, for future stock returns predictions. The new multivariate model will be assessed for its forecasting performance using empirical analysis. The empirical analysis is performed on S&P500 quarterly data from Quarter 1, 1952 to Quarter 4, 2019 as well as S&P500 monthly data from Month 12, 1920 to Month 12, 2019. Results have shown this new multivariate model has predictability for future stock returns. When compared to other benchmark models, the new multivariate model performs the best in terms of the Root Mean Squared Error (RMSE) most of the time."
http://arxiv.org/abs/2110.13303v1,Negotiating Networks in Oligopoly Markets for Price-Sensitive Products,2021-10-25 22:29:48+00:00,"['Naman Shukla', 'Kartik Yellepeddi']",cs.LG,"We present a novel framework to learn functions that estimate decisions of sellers and buyers simultaneously in an oligopoly market for a price-sensitive product. In this setting, the aim of the seller network is to come up with a price for a given context such that the expected revenue is maximized by considering the buyer's satisfaction as well. On the other hand, the aim of the buyer network is to assign probability of purchase to the offered price to mimic the real world buyers' responses while also showing price sensitivity through its action. In other words, rejecting the unnecessarily high priced products. Similar to generative adversarial networks, this framework corresponds to a minimax two-player game. In our experiments with simulated and real-world transaction data, we compared our framework with the baseline model and demonstrated its potential through proposed evaluation metrics."
http://arxiv.org/abs/2110.13317v1,Exposure of occupations to technologies of the fourth industrial revolution,2021-10-25 23:23:56+00:00,"['Benjamin Meindl', 'Morgan R. Frank', 'Joana Mendonça']",cs.CY,"The fourth industrial revolution (4IR) is likely to have a substantial impact on the economy. Companies need to build up capabilities to implement new technologies, and automation may make some occupations obsolete. However, where, when, and how the change will happen remain to be determined. Robust empirical indicators of technological progress linked to occupations can help to illuminate this change. With this aim, we provide such an indicator based on patent data. Using natural language processing, we calculate patent exposure scores for more than 900 occupations, which represent the technological progress related to them. To provide a lens on the impact of the 4IR, we differentiate between traditional and 4IR patent exposure. Our method differs from previous approaches in that it both accounts for the diversity of task-level patent exposures within an occupation and reflects work activities more accurately. We find that exposure to 4IR patents differs from traditional patent exposure. Manual tasks, and accordingly occupations such as construction and production, are exposed mainly to traditional (non-4IR) patents but have low exposure to 4IR patents. The analysis suggests that 4IR technologies may have a negative impact on job growth; this impact appears 10 to 20 years after patent filing. Further, we compared the 4IR exposure to other automation and AI exposure scores. Whereas many measures refer to theoretical automation potential, our patent-based indicator reflects actual technology diffusion. Our work not only allows analyses of the impact of 4IR technologies as a whole, but also provides exposure scores for more than 300 technology fields, such as AI and smart office technologies. Finally, the work provides a general mapping of patents to tasks and occupations, which enables future researchers to construct individual exposure measures."
http://arxiv.org/abs/2110.05891v1,Group network effects in price competition,2021-10-12 11:05:29+00:00,"['Renato Soeiro', 'Alberto Pinto']",econ.TH,"The partition of society into groups, polarization, and social networks are part of most conversations today. How do they influence price competition? We discuss Bertrand duopoly equilibria with demand subject to network effects. Contrary to models where network effects depend on one aggregate variable (demand for each choice), partitioning the dependence into groups creates a wealth of pure price equilibria with profit for both price setters, even if positive network effects are the dominant element of the game. If there is some asymmetry in how groups interact, two groups are sufficient. If network effects are based on undirected and unweighted graphs, at least five groups are required but, without other differentiation, outcomes are symmetric."
http://arxiv.org/abs/2110.15263v1,Coresets for Time Series Clustering,2021-10-28 16:21:13+00:00,"['Lingxiao Huang', 'K. Sudhir', 'Nisheeth K. Vishnoi']",cs.LG,"We study the problem of constructing coresets for clustering problems with time series data. This problem has gained importance across many fields including biology, medicine, and economics due to the proliferation of sensors facilitating real-time measurement and rapid drop in storage costs. In particular, we consider the setting where the time series data on $N$ entities is generated from a Gaussian mixture model with autocorrelations over $k$ clusters in $\mathbb{R}^d$. Our main contribution is an algorithm to construct coresets for the maximum likelihood objective for this mixture model. Our algorithm is efficient, and under a mild boundedness assumption on the covariance matrices of the underlying Gaussians, the size of the coreset is independent of the number of entities $N$ and the number of observations for each entity, and depends only polynomially on $k$, $d$ and $1/\varepsilon$, where $\varepsilon$ is the error parameter. We empirically assess the performance of our coreset with synthetic data."
http://arxiv.org/abs/2110.00952v1,Information Elicitation Meets Clustering,2021-10-03 08:47:55+00:00,['Yuqing Kong'],cs.GT,"In the setting where we want to aggregate people's subjective evaluations, plurality vote may be meaningless when a large amount of low-effort people always report ""good"" regardless of the true quality. ""Surprisingly popular"" method, picking the most surprising answer compared to the prior, handle this issue to some extent. However, it is still not fully robust to people's strategies. Here in the setting where a large number of people are asked to answer a small number of multi-choice questions (multi-task, large group), we propose an information aggregation method that is robust to people's strategies. Interestingly, this method can be seen as a rotated ""surprisingly popular"". It is based on a new clustering method, Determinant MaxImization (DMI)-clustering, and a key conceptual idea that information elicitation without ground-truth can be seen as a clustering problem. Of independent interest, DMI-clustering is a general clustering method that aims to maximize the volume of the simplex consisting of each cluster's mean multiplying the product of the cluster sizes. We show that DMI-clustering is invariant to any non-degenerate affine transformation for all data points. When the data point's dimension is a constant, DMI-clustering can be solved in polynomial time. In general, we present a simple heuristic for DMI-clustering which is very similar to Lloyd's algorithm for k-means. Additionally, we also apply the clustering idea in the single-task setting and use the spectral method to propose a new aggregation method that utilizes the second-moment information elicited from the crowds."
http://arxiv.org/abs/2111.02633v1,Network analysis regarding international trade network,2021-11-04 05:17:18+00:00,"['Xiufeng Yan', 'Qi Tang']",econ.GN,"We study the effect of globalization of world economy between 1980 and 2010 by using network analysis technics on trade and GDP data of 71 countries in the world. We draw results distinguishing relatively developing and relatively developed countries during this period of time and point out the standing out economies among the BRICS countries during the years of globalization: within our context of study, China and Russia are the countries that already exhibit developed economy characters, India is next in line but have some unusual features, while Brazil and South Africa still have erratic behaviors"
http://arxiv.org/abs/2111.01762v1,AIRCC-Clim: a user-friendly tool for generating regional probabilistic climate change scenarios and risk measures,2021-10-30 11:09:03+00:00,"['Francisco Estrada', 'Oscar Calderón-Bustamante', 'Wouter Botzen', 'Julián A. Velasco', 'Richard S. J. Tol']",physics.ao-ph,"Complex physical models are the most advanced tools available for producing realistic simulations of the climate system. However, such levels of realism imply high computational cost and restrictions on their use for policymaking and risk assessment. Two central characteristics of climate change are uncertainty and that it is a dynamic problem in which international actions can significantly alter climate projections and information needs, including partial and full compliance of global climate goals. Here we present AIRCC-Clim, a simple climate model emulator that produces regional probabilistic climate change projections of monthly and annual temperature and precipitation, as well as risk measures, based both on standard and user-defined emissions scenarios for six greenhouse gases. AIRCC-Clim emulates 37 atmosphere-ocean coupled general circulation models with low computational and technical requirements for the user. This standalone, user-friendly software is designed for a variety of applications including impact assessments, climate policy evaluation and integrated assessment modelling."
http://arxiv.org/abs/2111.02300v1,Autoregressive conditional duration modelling of high frequency data,2021-11-03 15:32:38+00:00,['Xiufeng Yan'],econ.EM,"This paper explores the duration dynamics modelling under the Autoregressive Conditional Durations (ACD) framework (Engle and Russell 1998). I test different distributions assumptions for the durations. The empirical results suggest unconditional durations approach the Gamma distributions. Moreover, compared with exponential distributions and Weibull distributions, the ACD model with Gamma distributed innovations provide the best fit of SPY durations."
http://arxiv.org/abs/2111.02328v1,A Linear Model for Distributed Flexibility Markets and DLMPs: A Comparison with the SOCP Formulation,2021-11-03 16:22:20+00:00,"['Anibal Sanjab', 'Yuting Mou', 'Ana Virag', 'Kris Kessels']",cs.GT,"This paper examines the performance trade-offs between an introduced linear flexibility market model for congestion management and a benchmark second-order cone programming (SOCP) formulation. The linear market model incorporates voltage magnitudes and reactive powers, while providing a simpler formulation than the SOCP model, which enables its practical implementation. The paper provides a structured comparison of the two formulations relying on developed deterministic and statistical Monte Carlo case analyses using two distribution test systems (the Matpower 69-bus and 141-bus systems). The case analyses show that with the increasing spread of offered flexibility throughout the system, the linear formulation increasingly preserves the reliability of the computed system variables as compared to the SOCP formulation, while more lenient imposed voltage limits can improve the approximation of prices and power flows at the expense of a less accurate computation of voltage magnitudes."
http://arxiv.org/abs/2111.02376v1,Multiplicative Component GARCH Model of Intraday Volatility,2021-11-03 17:44:23+00:00,['Xiufeng Yan'],econ.EM,"This paper proposes a multiplicative component intraday volatility model. The intraday conditional volatility is expressed as the product of intraday periodic component, intraday stochastic volatility component and daily conditional volatility component. I extend the multiplicative component intraday volatility model of Engle (2012) and Andersen and Bollerslev (1998) by incorporating the durations between consecutive transactions. The model can be applied to both regularly and irregularly spaced returns. I also provide a nonparametric estimation technique of the intraday volatility periodicity. The empirical results suggest the model can successfully capture the interdependency of intraday returns."
http://arxiv.org/abs/2110.05579v1,Fixed $T$ Estimation of Linear Panel Data Models with Interactive Fixed Effects,2021-10-11 19:42:14+00:00,['Ayden Higgins'],econ.EM,"This paper studies the estimation of linear panel data models with interactive fixed effects, where one dimension of the panel, typically time, may be fixed. To this end, a novel transformation is introduced that reduces the model to a lower dimension, and, in doing so, relieves the model of incidental parameters in the cross-section. The central result of this paper demonstrates that transforming the model and then applying the principal component (PC) estimator of \cite{bai_panel_2009} delivers $\sqrt{n}$ consistent estimates of regression slope coefficients with $T$ fixed. Moreover, these estimates are shown to be asymptotically unbiased in the presence of cross-sectional dependence, serial dependence, and with the inclusion of dynamic regressors, in stark contrast to the usual case. The large $n$, large $T$ properties of this approach are also studied, where many of these results carry over to the case in which $n$ is growing sufficiently fast relative to $T$. Transforming the model also proves to be useful beyond estimation, a point illustrated by showing that with $T$ fixed, the eigenvalue ratio test of \cite{horenstein} provides a consistent test for the number of factors when applied to the transformed model."
http://arxiv.org/abs/2110.10192v1,Difference-in-Differences with Geocoded Microdata,2021-10-19 18:20:22+00:00,['Kyle Butts'],econ.EM,"This paper formalizes a common approach for estimating effects of treatment at a specific location using geocoded microdata. This estimator compares units immediately next to treatment (an inner-ring) to units just slightly further away (an outer-ring). I introduce intuitive assumptions needed to identify the average treatment effect among the affected units and illustrates pitfalls that occur when these assumptions fail. Since one of these assumptions requires knowledge of exactly how far treatment effects are experienced, I propose a new method that relaxes this assumption and allows for nonparametric estimation using partitioning-based least squares developed in Cattaneo et. al. (2019). Since treatment effects typically decay/change over distance, this estimator improves analysis by estimating a treatment effect curve as a function of distance from treatment. This is contrast to the traditional method which, at best, identifies the average effect of treatment. To illustrate the advantages of this method, I show that Linden and Rockoff (2008) under estimate the effects of increased crime risk on home values closest to the treatment and overestimate how far the effects extend by selecting a treatment ring that is too wide."
http://arxiv.org/abs/2110.10230v1,Optimally Targeting Interventions in Networks during a Pandemic: Theory and Evidence from the Networks of Nursing Homes in the United States,2021-10-19 19:56:50+00:00,"['Roland Pongou', 'Guy Tchuente', 'Jean-Baptiste Tondji']",econ.TH,"This study develops an economic model for a social planner who prioritizes health over short-term wealth accumulation during a pandemic. Agents are connected through a weighted undirected network of contacts, and the planner's objective is to determine the policy that contains the spread of infection below a tolerable incidence level, and that maximizes the present discounted value of real income, in that order of priority. The optimal unique policy depends both on the configuration of the contact network and the tolerable infection incidence. Comparative statics analyses are conducted: (i) they reveal the tradeoff between the economic cost of the pandemic and the infection incidence allowed; and (ii) they suggest a correlation between different measures of network centrality and individual lockdown probability with the correlation increasing with the tolerable infection incidence level. Using unique data on the networks of nursing and long-term homes in the U.S., we calibrate our model at the state level and estimate the tolerable COVID-19 infection incidence level. We find that laissez-faire (more tolerance to the virus spread) pandemic policy is associated with an increased number of deaths in nursing homes and higher state GDP growth. In terms of the death count, laissez-faire is more harmful to nursing homes than more peripheral in the networks, those located in deprived counties, and those who work for a profit. We also find that U.S. states with a Republican governor have a higher level of tolerable incidence, but policies tend to converge with high death count."
http://arxiv.org/abs/2111.00450v1,"On Time-Varying VAR Models: Estimation, Testing and Impulse Response Analysis",2021-10-31 10:00:19+00:00,"['Yayi Yan', 'Jiti Gao', 'Bin Peng']",econ.EM,"Vector autoregressive (VAR) models are widely used in practical studies, e.g., forecasting, modelling policy transmission mechanism, and measuring connection of economic agents. To better capture the dynamics, this paper introduces a new class of time-varying VAR models in which the coefficients and covariance matrix of the error innovations are allowed to change smoothly over time. Accordingly, we establish a set of theories, including the impulse responses analyses subject to both of the short-run timing and the long-run restrictions, an information criterion to select the optimal lag, and a Wald-type test to determine the constant coefficients. Simulation studies are conducted to evaluate the theoretical findings. Finally, we demonstrate the empirical relevance and usefulness of the proposed methods through an application to the transmission mechanism of U.S. monetary policy."
http://arxiv.org/abs/2111.09846v3,The Curious Case of the 2021 Minneapolis Ward 2 City Council Election,2021-11-18 18:14:16+00:00,"['David McCune', 'Lori McCune']",econ.GN,"In this article we explain why the November 2021 election for the Ward 2 city council seat in Minneapolis, MN, may be the mathematically most interesting ranked choice election in US history."
http://arxiv.org/abs/2111.04483v2,Revisiting the Properties of Money,2021-11-08 13:19:29+00:00,"['Isaiah Hull', 'Or Sattath']",econ.GN,"The properties of money commonly referenced in the economics literature were originally identified by Jevons (1876) and Menger (1892) in the late 1800s and were intended to describe physical currencies, such as commodity money, metallic coins, and paper bills. In the digital era, many non-physical currencies have either entered circulation or are under development, including demand deposits, cryptocurrencies, stablecoins, central bank digital currencies (CBDCs), in-game currencies, and quantum money. These forms of money have novel properties that have not been studied extensively within the economics literature, but may be important determinants of the monetary equilibrium that emerges in the forthcoming era of heightened currency competition. This paper makes the first exhaustive attempt to identify and define the properties of all physical and digital forms of money. It reviews both the economics and computer science literatures and categorizes properties within an expanded version of the original functions-and-properties framework of money that includes societal and regulatory objectives."
http://arxiv.org/abs/2111.04165v1,On the Limits of Design: What Are the Conceptual Constraints on Designing Artificial Intelligence for Social Good?,2021-11-07 19:56:13+00:00,['Jakob Mokander'],econ.GN,"Artificial intelligence AI can bring substantial benefits to society by helping to reduce costs, increase efficiency and enable new solutions to complex problems. Using Floridi's notion of how to design the 'infosphere' as a starting point, in this chapter I consider the question: what are the limits of design, i.e. what are the conceptual constraints on designing AI for social good? The main argument of this chapter is that while design is a useful conceptual tool to shape technologies and societies, collective efforts towards designing future societies are constrained by both internal and external factors. Internal constraints on design are discussed by evoking Hardin's thought experiment regarding 'the Tragedy of the Commons'. Further, Hayek's classical distinction between 'cosmos' and 'taxis' is used to demarcate external constraints on design. Finally, five design principles are presented which are aimed at helping policymakers manage the internal and external constraints on design. A successful approach to designing future societies needs to account for the emergent properties of complex systems by allowing space for serendipity and socio-technological coevolution."
http://arxiv.org/abs/2111.07465v2,Decoding Causality by Fictitious VAR Modeling,2021-11-14 22:43:02+00:00,['Xingwei Hu'],stat.ML,"In modeling multivariate time series for either forecast or policy analysis, it would be beneficial to have figured out the cause-effect relations within the data. Regression analysis, however, is generally for correlation relation, and very few researches have focused on variance analysis for causality discovery. We first set up an equilibrium for the cause-effect relations using a fictitious vector autoregressive model. In the equilibrium, long-run relations are identified from noise, and spurious ones are negligibly close to zero. The solution, called causality distribution, measures the relative strength causing the movement of all series or specific affected ones. If a group of exogenous data affects the others but not vice versa, then, in theory, the causality distribution for other variables is necessarily zero. The hypothesis test of zero causality is the rule to decide a variable is endogenous or not. Our new approach has high accuracy in identifying the true cause-effect relations among the data in the simulation studies. We also apply the approach to estimating the causal factors' contribution to climate change."
http://arxiv.org/abs/2112.01435v1,RIF Regression via Sensitivity Curves,2021-12-02 17:24:43+00:00,"['Javier Alejo', 'Gabriel Montes-Rojas', 'Walter Sosa-Escudero']",econ.EM,"This paper proposes an empirical method to implement the recentered influence function (RIF) regression of Firpo, Fortin and Lemieux (2009), a relevant method to study the effect of covariates on many statistics beyond the mean. In empirically relevant situations where the influence function is not available or difficult to compute, we suggest to use the \emph{sensitivity curve} (Tukey, 1977) as a feasible alternative. This may be computationally cumbersome when the sample size is large. The relevance of the proposed strategy derives from the fact that, under general conditions, the sensitivity curve converges in probability to the influence function. In order to save computational time we propose to use a cubic splines non-parametric method for a random subsample and then to interpolate to the rest of the cases where it was not computed. Monte Carlo simulations show good finite sample properties. We illustrate the proposed estimator with an application to the polarization index of Duclos, Esteban and Ray (2004)."
http://arxiv.org/abs/2112.01639v2,Patient-Centered Appraisal of Race-Free Clinical Risk Assessment,2021-12-02 23:37:07+00:00,['Charles F. Manski'],econ.EM,"Until recently, there has been a consensus that clinicians should condition patient risk assessments on all observed patient covariates with predictive power. The broad idea is that knowing more about patients enables more accurate predictions of their health risks and, hence, better clinical decisions. This consensus has recently unraveled with respect to a specific covariate, namely race. There have been increasing calls for race-free risk assessment, arguing that using race to predict patient outcomes contributes to racial disparities and inequities in health care. Writers calling for race-free risk assessment have not studied how it would affect the quality of clinical decisions. Considering the matter from the patient-centered perspective of medical economics yields a disturbing conclusion: Race-free risk assessment would harm patients of all races."
http://arxiv.org/abs/2110.08425v2,Exact Bias Correction for Linear Adjustment of Randomized Controlled Trials,2021-10-16 00:48:20+00:00,"['Haoge Chang', 'Joel Middleton', 'P. M. Aronow']",stat.ME,"In an influential critique of empirical practice, Freedman (2008) showed that the linear regression estimator was biased for the analysis of randomized controlled trials under the randomization model. Under Freedman's assumptions, we derive exact closed-form bias corrections for the linear regression estimator with and without treatment-by-covariate interactions. We show that the limiting distribution of the bias corrected estimator is identical to the uncorrected estimator, implying that the asymptotic gains from adjustment can be attained without introducing any risk of bias. Taken together with results from Lin (2013), our results show that Freedman's theoretical arguments against the use of regression adjustment can be completely resolved with minor modifications to practice."
http://arxiv.org/abs/2110.08723v1,Gender identity and relative income within household: Evidence from China,2021-10-17 04:52:44+00:00,"['Han Dongcheng', 'Kong Fanbo', 'Wang Zixun']",econ.GN,"How does women's obedience to traditional gender roles affect their labour outcomes? To investigate on this question, we employ discontinuity tests and fixed effect regressions with time lag to measure how married women in China diminish their labour outcomes so as to maintain the bread-winning status of their husbands. In the first half of this research, our discontinuity test exhibits a missing mass of married women who just out-earn their husbands, which is interpreted as an evidence showing that these females diminish their earnings under the influence of gender norms. In the second half, we use fixed effect regressions with time lag to assess the change of a female's future labour outcomes if she currently earns more than her husband. Our results suggest that women's future labour participation decisions (whether they still join the workforce) are unaffected, but their yearly incomes and weekly working hours will be reduced in the future. Lastly, heterogeneous studies are conducted, showing that low-income and less educated married women are more susceptible to the influence of gender norms."
http://arxiv.org/abs/2110.08807v2,Estimating returns to special education: combining machine learning and text analysis to address confounding,2021-10-17 12:25:35+00:00,['Aurélien Sallin'],econ.GN,"Leveraging unique insights into the special education placement process through written individual psychological records, I present results from the first ever study to examine short- and long-term returns to special education programs with causal machine learning and computational text analysis methods. I find that special education programs in inclusive settings have positive returns in terms of academic performance as well as labor-market integration. Moreover, I uncover a positive effect of inclusive special education programs in comparison to segregated programs. This effect is heterogenous: segregation has least negative effects for students with emotional or behavioral problems, and for nonnative students with special needs. Finally, I deliver optimal program placement rules that would maximize aggregated school performance and labor market integration for students with special needs at lower program costs. These placement rules would reallocate most students with special needs from segregation to inclusion."
http://arxiv.org/abs/2110.04088v1,Risk aversion in flexible electricity markets,2021-10-08 12:32:46+00:00,"['Thomas Möbius', 'Iegor Riepin', 'Felix Müsgens', 'Adriaan H. van der Weijde']",econ.GN,"Flexibility options, such as demand response, energy storage and interconnection, have the potential to reduce variation in electricity prices between different future scenarios, therefore reducing investment risk. Moreover, investment in flexibility options can lower the need for generation capacity. However, there are complex interactions between different flexibility options. In this paper, we investigate the interactions between flexibility and investment risk in electricity markets. We employ a large-scale stochastic transmission and generation expansion model of the European electricity system. Using this model, we first investigate the effect of risk aversion on the investment decisions. We find that the interplay of parameters leads to (i) more investment in a less emission-intensive energy system if planners are risk averse (hedging against CO2 price uncertainty) and (ii) constant total installed capacity, regardless of the level of risk aversion (planners do not hedge against demand and RES deployment uncertainties). Second, we investigate the individual effects of three flexibility elements on optimal investment levels under different levels of risk aversion: demand response, investment in additional interconnection capacity and investment in additional energy storage. We find that that flexible technologies have a higher value for risk-averse decision-makers, although the effects are nonlinear. Finally, we investigate the interactions between the flexibility elements. We find that risk-averse decision-makers show a strong preference for transmission grid expansion once flexibility is available at low cost levels."
http://arxiv.org/abs/2111.11211v1,Inequality in the use frequency of patent technology codes,2021-11-22 13:53:37+00:00,"['José Alejandro Mendoza', 'Faustino Prieto', 'José María Sarabia']",econ.GN,"Technology codes are assigned to each patent for classification purposes and to identify the components of its novelty. Not all the technology codes are used with the same frequency - if we study the use frequency of codes in a year, we can find predominant technologies used in many patents and technology codes not so frequent as part of a patent. In this paper, we measure that inequality in the use frequency of patent technology codes. First, we analyze the total inequality in that use frequency considering the patent applications filed under the Patent Co-operation Treaty at international phase, with the European Patent Office as designated office, in the period 1977-2018, on a yearly basis. Then, we analyze the decomposition of that inequality by grouping the technology codes by productive economic activities. We show that total inequality had an initial period of growth followed by a phase of relative stabilization, and that it tends to be persistently high. We also show that total inequality was mainly driven by inequality within productive economic activities, with a low contribution of the between-activities component."
http://arxiv.org/abs/2110.13966v2,Fisheries Management in Congested Waters: A Game-Theoretic Assessment of the East China Sea,2021-10-26 19:06:07+00:00,['Michael Macgregor Perry'],econ.GN,"Fisheries in the East China Sea (ECS) face multiple concerning trends. Aside from depleted stocks caused by overfishing, illegal encroachments by fishermen from one nation into another's legal waters are a common occurrence. This behavior presumably could be stopped via strong monitoring, controls, and surveillance (MCS), but MCS is routinely rated below standards for nations bordering the ECS. This paper generalizes the ECS to a model of a congested maritime environment, defined as an environment where multiple nations can fish in the same waters with equivalent operating costs, and uses game-theoretic analysis to explain why the observed behavior persists in the ECS. The paper finds that nations in congested environments are incentivized to issue excessive quotas, which in turn tacitly encourages illegal fishing and extracts illegal rent from another's legal waters. This behavior couldn't persist in the face of strong MCS measures, and states are thus likewise incentivized to use poor MCS. A bargaining problem is analyzed to complement the noncooperative game, and a key finding is the nation with lower nonoperating costs has great leverage during the bargain."
http://arxiv.org/abs/2110.02474v3,Can an AI agent hit a moving target?,2021-10-06 03:16:54+00:00,"['Rui', 'Shi']",econ.TH,"I model the belief formation and decision making processes of economic agents during a monetary policy regime change (an acceleration in the money supply) with a deep reinforcement learning algorithm in the AI literature. I show that when the money supply accelerates, the learning agents only adjust their actions, which include consumption and demand for real balance, after gathering learning experience for many periods. This delayed adjustments leads to low returns during transition periods. Once they start adjusting to the new environment, their welfare improves. Their changes in beliefs and actions lead to temporary inflation volatility. I also show that, 1. the AI agents who explores their environment more adapt to the policy regime change quicker, which leads to welfare improvements and less inflation volatility, and 2. the AI agents who have experienced a structural change adjust their beliefs and behaviours quicker than an inexperienced learning agent."
http://arxiv.org/abs/2110.11245v3,Evolutionary Foundation for Heterogeneity in Risk Aversion,2021-10-21 16:15:17+00:00,"['Yuval Heller', 'Ilan Nehama']",econ.TH,"We examine the evolutionary basis for risk aversion with respect to aggregate risk. We study populations in which agents face choices between alternatives with different levels of aggregate risk. We show that the choices that maximize the long-run growth rate are induced by a heterogeneous population in which the least and most risk-averse agents are indifferent between facing an aggregate risk and obtaining its linear and harmonic mean for sure, respectively. Moreover, approximately optimal behavior can be induced by a simple distribution according to which all agents have constant relative risk aversion, and the coefficient of relative risk aversion is uniformly distributed between zero and two."
http://arxiv.org/abs/2111.08631v3,Spillovers of US Interest Rates: Monetary Policy & Information Effects,2021-11-16 17:21:56+00:00,['Santiago Camara'],econ.GN,"This paper quantifies the international spillovers of US monetary policy by exploiting the high-frequency movement of multiple financial assets around FOMC announcements. I use the identification strategy introduced by Jarocinski & Karadi (2022) to identify two FOMC shocks: a pure US monetary policy and an information disclosure shock. These two FOMC shocks have intuitive and very different international spillovers. On the one hand, a US tightening caused by a pure US monetary policy shock leads to an economic recession, an exchange rate depreciation and tighter financial conditions. On the other hand, a tightening of US monetary policy caused by the FOMC disclosing positive information about the state of the US economy leads to an economic expansion, an exchange rate appreciation and looser financial conditions. Ignoring the disclosure of information by the FOMC biases the impact of a US monetary policy tightening and may explain recent atypical findings."
http://arxiv.org/abs/2111.07451v2,On Risk and Time Pressure: When to Think and When to Do,2021-11-14 21:20:31+00:00,"['Christoph Carnehl', 'Johannes Schneider']",econ.TH,"We study the tradeoff between fundamental risk and time. A time-constrained agent has to solve a problem. She dynamically allocates effort between implementing a risky initial idea and exploring alternatives. Discovering an alternative implies progress that has to be converted to a solution. As time runs out, the chances of converting it in time shrink. We show that the agent may return to the initial idea after having left it in the past to explore alternatives. Our model helps explain so-called false starts. To finish fast, the agent delays exploring alternatives reducing the overall success probability."
http://arxiv.org/abs/2111.08601v2,Optimal index insurance and basis risk decomposition: an application to Kenya,2021-11-16 16:34:47+00:00,"['Matthieu Stigler', 'David Lobell']",econ.GN,"Index insurance is a promising tool to reduce the risk faced by farmers, but high basis risk, which arises from imperfect correlation between the index and individual farm yields, has limited its adoption to date. Basis risk arises from two fundamental sources: the intrinsic heterogeneity within an insurance zone (zonal risk), and the lack of predictive accuracy of the index (design risk). Whereas previous work has focused almost exclusively on design risk, a theoretical and empirical understanding of the role of zonal risk is still lacking.
  Here we investigate the relative roles of zonal and design risk, using the case of maize yields in Kenya. Our first contribution is to derive a formal decomposition of basis risk, providing a simple upper bound on the insurable basis risk that any index can reach within a given zone. Our second contribution is to provide the first large-scale empirical analysis of the extent of zonal versus design risk. To do so, we use satellite estimates of yields at 10m resolution across Kenya, and investigate the effect of using smaller zones versus using different indices. Our results show a strong local heterogeneity in yields, underscoring the challenge of implementing index insurance in smallholder systems, and the potential benefits of low-cost yield measurement approaches that can enable more local definitions of insurance zones."
http://arxiv.org/abs/2110.06763v4,Efficient Estimation in NPIV Models: A Comparison of Various Neural Networks-Based Estimators,2021-10-13 15:00:33+00:00,"['Jiafeng Chen', 'Xiaohong Chen', 'Elie Tamer']",econ.EM,"Artificial Neural Networks (ANNs) can be viewed as nonlinear sieves that can approximate complex functions of high dimensional variables more effectively than linear sieves. We investigate the performance of various ANNs in nonparametric instrumental variables (NPIV) models of moderately high dimensional covariates that are relevant to empirical economics. We present two efficient procedures for estimation and inference on a weighted average derivative (WAD): an orthogonalized plug-in with optimally-weighted sieve minimum distance (OP-OSMD) procedure and a sieve efficient score (ES) procedure. Both estimators for WAD use ANN sieves to approximate the unknown NPIV function and are root-n asymptotically normal and first-order equivalent. We provide a detailed practitioner's recipe for implementing both efficient procedures. We compare their finite-sample performances in various simulation designs that involve smooth NPIV function of up to 13 continuous covariates, different nonlinearities and covariate correlations. Some Monte Carlo findings include: 1) tuning and optimization are more delicate in ANN estimation; 2) given proper tuning, both ANN estimators with various architectures can perform well; 3) easier to tune ANN OP-OSMD estimators than ANN ES estimators; 4) stable inferences are more difficult to achieve with ANN (than spline) estimators; 5) there are gaps between current implementations and approximation theories. Finally, we apply ANN NPIV to estimate average partial derivatives in two empirical demand examples with multivariate covariates."
http://arxiv.org/abs/2110.05611v2,Heat and Economic Preferences,2021-10-05 06:21:23+00:00,"['Michelle Escobar Carias', 'David Johnston', 'Rachel Knott', 'Rohan Sweeney']",econ.GN,"The empirical evidence suggests that key accumulation decisions and risky choices associated with economic development depend, at least in part, on economic preferences such as willingness to take risk and patience. This paper studies whether temperature could be one of the potential channels that influences such economic preferences. Using data from the Indonesia Family Life Survey and NASAs Modern Era Retrospective Analysis for Research and Applications data we exploit quasi exogenous variations in outdoor temperatures caused by the random allocation of survey dates. This approach allows us to estimate the effects of temperature on elicited measures of risk aversion, rational choice violations, and impatience. We then explore three possible mechanisms behind this relationship, cognition, sleep, and mood. Our findings show that higher temperatures lead to significantly increased rational choice violations and impatience, but do not significantly increase risk aversion. These effects are mainly driven by night time temperatures on the day prior to the survey and less so by temperatures on the day of the survey. This impact is quasi linear and increasing when midnight outdoor temperatures are above 22C. The evidence shows that night time temperatures significantly deplete cognitive functioning, mathematical skills in particular. Based on these findings we posit that heat induced night time disturbances cause stress on critical parts of the brain, which then manifest in significantly lower cognitive functions that are critical for individuals to perform economically rational decision making."
http://arxiv.org/abs/2110.13847v2,Coupling the Gini and Angles to Evaluate Economic Dispersion,2021-10-26 16:44:13+00:00,['Mario Schlemmer'],econ.EM,"Classical measures of inequality use the mean as the benchmark of economic dispersion. They are not sensitive to inequality at the left tail of the distribution, where it would matter most. This paper presents a new inequality measurement tool that gives more weight to inequality at the lower end of the distribution, it is based on the comparison of all value pairs and synthesizes the dispersion of the whole distribution. The differences that sum to the Gini coefficient are scaled by angular differences between observations. The resulting index possesses a set of desirable properties, including normalization, scale invariance, population invariance, transfer sensitivity, and weak decomposability."
http://arxiv.org/abs/2111.08157v2,Optimal Stratification of Survey Experiments,2021-11-16 00:42:28+00:00,['Max Cytrynbaum'],econ.EM,"This paper studies a two-stage model of experimentation, where the researcher first samples representative units from an eligible pool, then assigns each sampled unit to treatment or control. To implement balanced sampling and assignment, we introduce a new family of finely stratified designs that generalize matched pairs randomization to propensities p(x) not equal to 1/2. We show that two-stage stratification nonparametrically dampens the variance of treatment effect estimation. We formulate and solve the optimal stratification problem with heterogeneous costs and fixed budget, providing simple heuristics for the optimal design. In settings with pilot data, we show that implementing a consistent estimate of this design is also efficient, minimizing asymptotic variance subject to the budget constraint. We also provide new asymptotically exact inference methods, allowing experimenters to fully exploit the efficiency gains from both stratified sampling and assignment. An application to nine papers recently published in top economics journals demonstrates the value of our methods."
http://arxiv.org/abs/2110.04924v4,High-dimensional Inference for Dynamic Treatment Effects,2021-10-10 23:05:29+00:00,"['Jelena Bradic', 'Weijie Ji', 'Yuqian Zhang']",stat.ME,"Estimating dynamic treatment effects is a crucial endeavor in causal inference, particularly when confronted with high-dimensional confounders. Doubly robust (DR) approaches have emerged as promising tools for estimating treatment effects due to their flexibility. However, we showcase that the traditional DR approaches that only focus on the DR representation of the expected outcomes may fall short of delivering optimal results. In this paper, we propose a novel DR representation for intermediate conditional outcome models that leads to superior robustness guarantees. The proposed method achieves consistency even with high-dimensional confounders, as long as at least one nuisance function is appropriately parametrized for each exposure time and treatment path. Our results represent a significant step forward as they provide new robustness guarantees. The key to achieving these results is our new DR representation, which offers superior inferential performance while requiring weaker assumptions. Lastly, we confirm our findings in practice through simulations and a real data application."
http://arxiv.org/abs/2110.01615v1,Geography of Science: Competitiveness and Inequality,2021-10-03 11:56:16+00:00,"['Aurelio Patelli', 'Lorenzo Napolitano', 'Giulio Cimini', 'Andrea Gabrielli']",cs.DL,"Using ideas and tools of complexity science we design a holistic measure of \textit{Scientific Fitness}, encompassing the scientific knowledge, capabilities and competitiveness of a research system. We characterize the temporal dynamics of Scientific Fitness and R\&D expenditures at the geographical scale of nations, highlighting patterns of similar research systems, and showing how developing nations (China in particular) are quickly catching up the developed ones. Down-scaling the aggregation level of the analysis, we find that even developed nations show a considerable level of inequality in the Scientific Fitness of their internal regions. Further, we assess comparatively how the competitiveness of each geographic region is distributed over the spectrum of research sectors. Overall, the Scientific Fitness represents the first high quality estimation of the scientific strength of nations and regions, opening new policy-making applications for better allocating resources, filling inequality gaps and ultimately promoting innovation."
http://arxiv.org/abs/2110.11651v1,Free Riding in Networks,2021-10-22 08:28:10+00:00,"['Markus Kinateder', 'Luca Paolo Merlino']",econ.TH,"Players allocate their budget to links, a local public good and a private good. A player links to free ride on others' public good provision. We derive sufficient conditions for the existence of a Nash equilibrium. In equilibrium, large contributors link to each other, while others link to them. Poorer players can be larger contributors if linking costs are sufficiently high. In large societies, free riding reduces inequality only in networks in which it is initially low; otherwise, richer players benefit more, as they can afford more links. Finally, we study the policy implications, deriving income redistribution that increases welfare and personalized prices that implement the efficient solution."
http://arxiv.org/abs/2110.03146v3,Solving Multistage Stochastic Linear Programming via Regularized Linear Decision Rules: An Application to Hydrothermal Dispatch Planning,2021-10-07 02:36:14+00:00,"['Felipe Nazare', 'Alexandre Street']",math.OC,"The solution of multistage stochastic linear problems (MSLP) represents a challenge for many application areas. Long-term hydrothermal dispatch planning (LHDP) materializes this challenge in a real-world problem that affects electricity markets, economies, and natural resources worldwide. No closed-form solutions are available for MSLP and the definition of non-anticipative policies with high-quality out-of-sample performance is crucial. Linear decision rules (LDR) provide an interesting simulation-based framework for finding high-quality policies for MSLP through two-stage stochastic models. In practical applications, however, the number of parameters to be estimated when using an LDR may be close to or higher than the number of scenarios of the sample average approximation problem, thereby generating an in-sample overfit and poor performances in out-of-sample simulations. In this paper, we propose a novel regularized LDR to solve MSLP based on the AdaLASSO (adaptive least absolute shrinkage and selection operator). The goal is to use the parsimony principle, as largely studied in high-dimensional linear regression models, to obtain better out-of-sample performance for LDR applied to MSLP. Computational experiments show that the overfit threat is non-negligible when using classical non-regularized LDR to solve the LHDP, one of the most studied MSLP with relevant applications. Our analysis highlights the following benefits of the proposed framework in comparison to the non-regularized benchmark: 1) significant reductions in the number of non-zero coefficients (model parsimony), 2) substantial cost reductions in out-of-sample evaluations, and 3) improved spot-price profiles."
http://arxiv.org/abs/2110.06285v3,Partial Identification of Marginal Treatment Effects with discrete instruments and misreported treatment,2021-10-12 19:16:59+00:00,['Santiago Acerenza'],econ.EM,This paper provides partial identification results for the marginal treatment effect ($MTE$) when the binary treatment variable is potentially misreported and the instrumental variable is discrete. Identification results are derived under different sets of nonparametric assumptions. The identification results are illustrated in identifying the marginal treatment effects of food stamps on health.
http://arxiv.org/abs/2111.08654v2,Exploration of the Parameter Space in Macroeconomic Agent-Based Models,2021-11-16 17:39:56+00:00,"['Karl Naumann-Woleske', 'Max Sina Knicker', 'Michael Benzaquen', 'Jean-Philippe Bouchaud']",econ.GN,"Agent-Based Models (ABM) are computational scenario-generators, which can be used to predict the possible future outcomes of the complex system they represent. To better understand the robustness of these predictions, it is necessary to understand the full scope of the possible phenomena the model can generate. Most often, due to high-dimensional parameter spaces, this is a computationally expensive task. Inspired by ideas coming from systems biology, we show that for multiple macroeconomic models, including an agent-based model and several Dynamic Stochastic General Equilibrium (DSGE) models, there are only a few stiff parameter combinations that have strong effects, while the other sloppy directions are irrelevant. This suggest an algorithm that efficiently explores the space of parameters by primarily moving along the stiff directions. We apply our algorithm to a medium-sized agent-based model, and show that it recovers all possible dynamics of the unemployment rate. The application of this method to Agent-based Models may lead to a more thorough and robust understanding of their features, and provide enhanced parameter sensitivity analyses. Several promising paths for future research are discussed."
http://arxiv.org/abs/2110.10781v6,Marital Stability With Committed Couples: A Revealed Preference Analysis,2021-10-20 21:13:51+00:00,"['Mikhail Freer', 'Khushboo Surana']",econ.GN,"We present a revealed preference characterization of marital stability where some couples are committed. A couple is committed if they can divorce only with mutual consent. We provide theoretical insights into the potential of the characterization for identifying intrahousehold consumption patterns. We demonstrate that without price variation for private goods among potential couples, intrahousehold resource allocations can only be identified for non-committed couples. We conduct simulations using Dutch household data to support our theoretical findings. Our results show that with price variation, the empirical implications of marital stability allow for the identification of household consumption allocations for both committed and non-committed couples."
http://arxiv.org/abs/2110.03443v3,Unpacking the Black Box: Regulating Algorithmic Decisions,2021-10-05 23:20:25+00:00,"['Laura Blattner', 'Scott Nelson', 'Jann Spiess']",econ.GN,"What should regulators of complex algorithms regulate? We propose a model of oversight over 'black-box' algorithms used in high-stakes applications such as lending, medical testing, or hiring. In our model, a regulator is limited in how much she can learn about a black-box model deployed by an agent with misaligned preferences. The regulator faces two choices: first, whether to allow for the use of complex algorithms; and second, which key properties of algorithms to regulate. We show that limiting agents to algorithms that are simple enough to be fully transparent is inefficient as long as the misalignment is limited and complex algorithms have sufficiently better performance than simple ones. Allowing for complex algorithms can improve welfare, but the gains depend on how the regulator regulates them. Regulation that focuses on the overall average behavior of algorithms, for example based on standard explainer tools, will generally be inefficient. Targeted regulation that focuses on the source of incentive misalignment, e.g., excess false positives or racial disparities, can provide second-best solutions. We provide empirical support for our theoretical findings using an application in consumer lending, where we document that complex models regulated based on context-specific explanation tools outperform simple, fully transparent models. This gain from complex models represents a Pareto improvement across our empirical applications that is preferred both by the lender and from the perspective of the financial regulator."
http://arxiv.org/abs/2111.14590v2,The Fixed-b Limiting Distribution and the ERP of HAR Tests Under Nonstationarity,2021-11-29 15:23:59+00:00,['Alessandro Casini'],econ.EM,"We show that the nonstandard limiting distribution of HAR test statistics under fixed-b asymptotics is not pivotal (even after studentization) when the data are nonstationarity. It takes the form of a complicated function of Gaussian processes and depends on the integrated local long-run variance and on on the second moments of the relevant series (e.g., of the regressors and errors for the case of the linear regression model). Hence, existing fixed-b inference methods based on stationarity are not theoretically valid in general. The nuisance parameters entering the fixed-b limiting distribution can be consistently estimated under small-b asymptotics but only with nonparametric rate of convergence. Hence, We show that the error in rejection probability (ERP) is an order of magnitude larger than that under stationarity and is also larger than that of HAR tests based on HAC estimators under conventional asymptotics. These theoretical results reconcile with recent finite-sample evidence in Casini (2021) and Casini, Deng and Perron (2021) who showing that fixed-b HAR tests can perform poorly when the data are nonstationary. They can be conservative under the null hypothesis and have non-monotonic power under the alternative hypothesis irrespective of how large the sample size is."
http://arxiv.org/abs/2112.11563v2,Cultural Diversity and Its Impact on Governance,2021-12-15 23:36:56+00:00,"['Tomáš Evan', 'Vladimír Holý']",econ.GN,"Hofstede's six cultural dimensions make it possible to measure the culture of countries but are criticized for assuming the homogeneity of each country. In this paper, we propose two measures based on Hofstede's cultural dimensions which take into account the heterogeneous structure of citizens with respect to their countries of origin. Using these improved measures, we study the influence of heterogeneous culture and cultural diversity on the quality of institutions measured by the six worldwide governance indicators. We use a linear regression model allowing for dependence in spatial and temporal dimensions as well as high correlation between the governance indicators. Our results show that the effect of cultural diversity improves some of the governance indicators while worsening others depending on the individual Hofstede cultural dimension."
http://arxiv.org/abs/2112.14356v5,Private Private Information,2021-12-29 01:30:39+00:00,"['Kevin He', 'Fedor Sandomirskiy', 'Omer Tamuz']",econ.TH,"Private signals model noisy information about an unknown state. Although these signals are called ""private,"" they may still carry information about each other. Our paper introduces the concept of private private signals, which contain information about the state but not about other signals. To achieve privacy, signal quality may need to be sacrificed. We study the informativeness of private private signals and characterize those that are optimal in the sense that they cannot be made more informative without violating privacy. We discuss implications for privacy in recommendation systems, information design, causal inference, and mechanism design."
http://arxiv.org/abs/2111.06224v1,Occupational Income Inequality of Thailand: A Case Study of Exploratory Data Analysis beyond Gini Coefficient,2021-11-05 10:01:19+00:00,"['Wanetha Sudswong', 'Anon Plangprasopchok', 'Chainarong Amornbunchornvej']",econ.GN,"Income inequality is an important issue that has to be solved in order to make progress in our society. The study of income inequality is well received through the Gini coefficient, which is used to measure degrees of inequality in general. While this method is effective in several aspects, the Gini coefficient alone inevitably overlooks minority subpopulations (e.g. occupations) which results in missing undetected patterns of inequality in minority.
  In this study, the surveys of incomes and occupations from more than 12 millions households across Thailand have been analyzed by using both Gini coefficient and network densities of income domination networks to get insight regarding the degrees of general and occupational income inequality issues. The results show that, in agricultural provinces, there are less issues in both types of inequality (low Gini coefficients and network densities), while some non-agricultural provinces face an issue of occupational income inequality (high network densities) without any symptom of general income inequality (low Gini coefficients). Moreover, the results also illustrate the gaps of income inequality using estimation statistics, which not only support whether income inequality exists, but that we are also able to tell the magnitudes of income gaps among occupations. These results cannot be obtained via Gini coefficients alone. This work serves as a use case of analyzing income inequality from both general population and subpopulations perspectives that can be utilized in studies of other countries."
http://arxiv.org/abs/2111.06365v1,"It's not always about the money, sometimes it's about sending a message: Evidence of Informational Content in Monetary Policy Announcements",2021-11-11 18:25:00+00:00,"['Yong Cai', 'Santiago Camara', 'Nicholas Capel']",econ.GN,This paper introduces a transparent framework to identify the informational content of FOMC announcements. We do so by modelling the expectations of the FOMC and private sector agents using state of the art computational linguistic tools on both FOMC statements and New York Times articles. We identify the informational content of FOMC announcements as the projection of high frequency movements in financial assets onto differences in expectations. Our recovered series is intuitively reasonable and shows that information disclosure has a significant impact on the yields of short-term government bonds.
http://arxiv.org/abs/2111.06371v1,Can you always reap what you sow? Network and functional data analysis of VC investments in health-tech companies,2021-11-10 00:16:40+00:00,"['Christian Esposito', 'Marco Gortan', 'Lorenzo Testa', 'Francesca Chiaromonte', 'Giorgio Fagiolo', 'Andrea Mina', 'Giulio Rossetti']",cs.SI,"""Success"" of firms in venture capital markets is hard to define, and its determinants are still poorly understood. We build a bipartite network of investors and firms in the healthcare sector, describing its structure and its communities. Then, we characterize ""success"" introducing progressively more refined definitions, and we find a positive association between such definitions and the centrality of a company. In particular, we are able to cluster funding trajectories of firms into two groups capturing different ""success"" regimes and to link the probability of belonging to one or the other to their network features (in particular their centrality and the one of their investors). We further investigate this positive association by introducing scalar as well as functional ""success"" outcomes, confirming our findings and their robustness."
http://arxiv.org/abs/2111.09111v1,Forecasting Crude Oil Price Using Event Extraction,2021-11-14 08:48:43+00:00,"['Jiangwei Liu', 'Xiaohong Huang']",cs.LG,"Research on crude oil price forecasting has attracted tremendous attention from scholars and policymakers due to its significant effect on the global economy. Besides supply and demand, crude oil prices are largely influenced by various factors, such as economic development, financial markets, conflicts, wars, and political events. Most previous research treats crude oil price forecasting as a time series or econometric variable prediction problem. Although recently there have been researches considering the effects of real-time news events, most of these works mainly use raw news headlines or topic models to extract text features without profoundly exploring the event information. In this study, a novel crude oil price forecasting framework, AGESL, is proposed to deal with this problem. In our approach, an open domain event extraction algorithm is utilized to extract underlying related events, and a text sentiment analysis algorithm is used to extract sentiment from massive news. Then a deep neural network integrating the news event features, sentimental features, and historical price features is built to predict future crude oil prices. Empirical experiments are performed on West Texas Intermediate (WTI) crude oil price data, and the results show that our approach obtains superior performance compared with several benchmark methods."
http://arxiv.org/abs/2111.11875v1,Functional Model of Residential Consumption Elasticity under Dynamic Tariffs,2021-11-22 15:29:50+00:00,"['Kamalanathan Ganesan', 'João Tomé Saraiva', 'Ricardo J. Bessa']",eess.SY,"One of the major barriers for the retailers is to understand the consumption elasticity they can expect from their contracted demand response (DR) clients. The current trend of DR products provided by retailers are not consumer-specific, which poses additional barriers for the active engagement of consumers in these programs. The elasticity of consumers demand behavior varies from individual to individual. The utility will benefit from knowing more accurately how changes in its prices will modify the consumption pattern of its clients. This work proposes a functional model for the consumption elasticity of the DR contracted consumers. The model aims to determine the load adjustment the DR consumers can provide to the retailers or utilities for different price levels. The proposed model uses a Bayesian probabilistic approach to identify the actual load adjustment an individual contracted client can provide for different price levels it can experience. The developed framework provides the retailers or utilities with a tool to obtain crucial information on how an individual consumer will respond to different price levels. This approach is able to quantify the likelihood with which the consumer reacts to a DR signal and identify the actual load adjustment an individual contracted DR client provides for different price levels they can experience. This information can be used to maximize the control and reliability of the services the retailer or utility can offer to the System Operators."
http://arxiv.org/abs/2112.01237v1,Designing a Framework for Digital KYC Processes Built on Blockchain-Based Self-Sovereign Identity,2021-11-11 11:05:06+00:00,"['Vincent Schlatt', 'Johannes Sedlmeir', 'Simon Feulner', 'Nils Urbach']",cs.CY,"Know your customer (KYC) processes place a great burden on banks, because they are costly, inefficient, and inconvenient for customers. While blockchain technology is often mentioned as a potential solution, it is not clear how to use the technology's advantages without violating data protection regulations and customer privacy. We demonstrate how blockchain-based self-sovereign identity (SSI) can solve the challenges of KYC. We follow a rigorous design science research approach to create a framework that utilizes SSI in the KYC process, deriving nascent design principles that theorize on blockchain's role for SSI."
http://arxiv.org/abs/2112.01377v2,Structural Sieves,2021-12-01 16:37:02+00:00,['Konrad Menzel'],econ.EM,"This paper explores the use of deep neural networks for semiparametric estimation of economic models of maximizing behavior in production or discrete choice. We argue that certain deep networks are particularly well suited as a nonparametric sieve to approximate regression functions that result from nonlinear latent variable models of continuous or discrete optimization. Multi-stage models of this type will typically generate rich interaction effects between regressors (""inputs"") in the regression function so that there may be no plausible separability restrictions on the ""reduced-form"" mapping form inputs to outputs to alleviate the curse of dimensionality. Rather, economic shape, sparsity, or separability restrictions either at a global level or intermediate stages are usually stated in terms of the latent variable model. We show that restrictions of this kind are imposed in a more straightforward manner if a sufficiently flexible version of the latent variable model is in fact used to approximate the unknown regression function."
http://arxiv.org/abs/2110.02735v3,Optimal pricing for electricity retailers based on data-driven consumers' price-response,2021-10-04 10:06:52+00:00,"['Román Pérez-Santalla', 'Miguel Carrión', 'Carlos Ruiz']",math.OC,"In the present work we tackle the problem of finding the optimal price tariff to be set by a risk-averse electric retailer participating in the pool and whose customers are price-sensitive. We assume that the retailer has access to a sufficiently large smart-meter dataset from which it can statistically characterize the relationship between the tariff price and the demand load of its clients. Three different models are analyzed to predict the aggregated load as a function of the electricity prices and other parameters, as humidity or temperature. More specifically, we train linear regression (predictive) models to forecast the resulting demand load as a function of the retail price. Then we will insert this model in a quadratic optimization problem which evaluates the optimal price to be offered. This optimization problem accounts for different sources of uncertainty including consumer's response, pool prices and renewable source availability, and relies on a stochastic and risk-averse formulation. In particular, one important contribution of this work is to base the scenario generation and reduction procedure on the statistical properties of the resulting predictive model. This allows us to properly quantify (data-driven) not only the expected value but the level of uncertainty associated with the main problem parameters. Moreover, we consider both standard forward based contracts and the recently introduced power purchase agreement contracts as risk-hedging tools for the retailer. The results are promising as profits are found for the retailer with highly competitive prices and some possible improvements are shown if richer datasets could be available in the future. A realistic case study and multiple sensitivity analyses have been performed to characterize the risk-aversion behavior of the retailer considering price-sensitive consumers."
http://arxiv.org/abs/2111.03035v1,Structural Breaks in Interactive Effects Panels and the Stock Market Reaction to COVID-19,2021-11-04 17:37:29+00:00,"['Yiannis Karavias', 'Paresh Narayan', 'Joakim Westerlund']",econ.EM,"Dealing with structural breaks is an important step in most, if not all, empirical economic research. This is particularly true in panel data comprised of many cross-sectional units, such as individuals, firms or countries, which are all affected by major events. The COVID-19 pandemic has affected most sectors of the global economy, and there is by now plenty of evidence to support this. The impact on stock markets is, however, still unclear. The fact that most markets seem to have partly recovered while the pandemic is still ongoing suggests that the relationship between stock returns and COVID-19 has been subject to structural change. It is therefore important to know if a structural break has occurred and, if it has, to infer the date of the break. In the present paper we take this last observation as a source of motivation to develop a new break detection toolbox that is applicable to different sized panels, easy to implement and robust to general forms of unobserved heterogeneity. The toolbox, which is the first of its kind, includes a test for structural change, a break date estimator, and a break date confidence interval. Application to a panel covering 61 countries from January 3 to September 25, 2020, leads to the detection of a structural break that is dated to the first week of April. The effect of COVID-19 is negative before the break and zero thereafter, implying that while markets did react, the reaction was short-lived. A possible explanation for this is the quantitative easing programs announced by central banks all over the world in the second half of March."
http://arxiv.org/abs/2111.02872v1,Feasibility trade-offs in decarbonisation of power sector with high coal dependence: A case of Korea,2021-10-25 23:03:18+00:00,"['Minwoo Hyun', 'Aleh Cherp', 'Jessica Jewell', 'Yeong Jae Kim', 'Jiyong Eom']",physics.soc-ph,Decarbonisation of the power sector requires feasible strategies for rapid phase-out of fossil fuels and expansion of low-carbon sources. This study develops and uses a model with an explicit account of power plant stocks to explore plausible decarbonization scenarios of the power sector in the Republic of Korea through 2050 and 2060. The results show that achieving zero emissions from the power sector by the mid-century requires either ambitious expansion of renewables backed by gas-fired generation equipped with carbon capture and storage or significant expansion of nuclear power. The first strategy implies replicating and maintaining for decades maximum growth rates of solar power achieved in leading countries and becoming an early and ambitious adopter of the CCS technology. The alternative expansion of nuclear power has historical precedents in Korea and other countries but may not be acceptable in the current political and regulatory environment.
http://arxiv.org/abs/2110.12013v2,The Absence of Attrition in a War of Attrition under Complete Information,2021-10-22 18:55:39+00:00,"['George Georgiadis', 'Youngsoo Kim', 'H. Dharma Kwon']",math.OC,"We consider a two-player game of war of attrition under complete information. It is well-known that this class of games admits equilibria in pure, as well as mixed strategies, and much of the literature has focused on the latter. We show that if the players' payoffs whilst in ""war"" vary stochastically and their exit payoffs are heterogeneous, then the game admits Markov Perfect equilibria in pure strategies only. This is true irrespective of the degree of randomness and heterogeneity, thus highlighting the fragility of mixed-strategy equilibria to a natural perturbation of the canonical model. In contrast, when the players' flow payoffs are deterministic or their exit payoffs are homogeneous, the game admits equilibria in pure and mixed strategies."
http://arxiv.org/abs/2110.15750v1,Process Design and Economics of Production of p-Aminophenol,2021-10-29 12:54:33+00:00,"['Chinmay Ghoroi', 'Jay Shah', 'Devanshu Thakar', 'Sakshi Baheti']",econ.GN,"Para-Aminophenol is one of the key chemicals required for the synthesis of Paracetamol, an analgesic and antipyretic drug. Data shows a large fraction of India's demand for Para-Aminophenol being met through imports from China. The uncertainty in the India-China relations would affect the supply and price of this ""Key Starting Material."" This report is a detailed business plan for setting up a plant and producing Para-Aminophenol in India at a competitive price. The plant is simulated in AspenPlus V8 and different Material Balances and Energy Balances calculations are carried out. The plant produces 22.7 kmols Para-Aminophenol per hour with a purity of 99.9%. Along with the simulation, economic analysis is carried out for this plant to determine the financial parameters like Payback Period and Return on Investment."
http://arxiv.org/abs/2110.15229v1,"Moral Hazard, Dynamic Incentives, and Ambiguous Perceptions",2021-10-28 15:42:13+00:00,['Martin Dumav'],econ.GN,"This paper considers dynamic moral hazard settings, in which the consequences of the agent's actions are not precisely understood. In a new continuous-time moral hazard model with drift ambiguity, the agent's unobservable action translates to drift set that describe the evolution of output. The agent and the principal have imprecise information about the technology, and both seek robust performance from a contract in relation to their respective worst-case scenarios. We show that the optimal long-term contract aligns the parties' pessimistic expectations and broadly features compressing of the high-powered incentives. Methodologically, we provide a tractable way to formulate and characterize optimal long-run contracts with drift ambiguity. Substantively, our results provide some insights into the formal link between robustness and simplicity of dynamic contracts, in particular high-powered incentives become less effective in the presence of ambiguity."
http://arxiv.org/abs/2111.11630v1,"Aggregation of Models, Choices, Beliefs, and Preferences",2021-11-23 03:26:42+00:00,"['Hamed Hamze Bajgiran', 'Houman Owhadi']",econ.TH,"A natural notion of rationality/consistency for aggregating models is that, for all (possibly aggregated) models $A$ and $B$, if the output of model $A$ is $f(A)$ and if the output model $B$ is $f(B)$, then the output of the model obtained by aggregating $A$ and $B$ must be a weighted average of $f(A)$ and $f(B)$. Similarly, a natural notion of rationality for aggregating preferences of ensembles of experts is that, for all (possibly aggregated) experts $A$ and $B$, and all possible choices $x$ and $y$, if both $A$ and $B$ prefer $x$ over $y$, then the expert obtained by aggregating $A$ and $B$ must also prefer $x$ over $y$. Rational aggregation is an important element of uncertainty quantification, and it lies behind many seemingly different results in economic theory: spanning social choice, belief formation, and individual decision making. Three examples of rational aggregation rules are as follows. (1) Give each individual model (expert) a weight (a score) and use weighted averaging to aggregate individual or finite ensembles of models (experts). (2) Order/rank individual model (expert) and let the aggregation of a finite ensemble of individual models (experts) be the highest-ranked individual model (expert) in that ensemble. (3) Give each individual model (expert) a weight, introduce a weak order/ranking over the set of models/experts, aggregate $A$ and $B$ as the weighted average of the highest-ranked models (experts) in $A$ or $B$. Note that (1) and (2) are particular cases of (3). In this paper, we show that all rational aggregation rules are of the form (3). This result unifies aggregation procedures across different economic environments. Following the main representation, we show applications and extensions of our representation in various separated economics topics such as belief formation, choice theory, and social welfare economics."
http://arxiv.org/abs/2110.14346v1,A Scalable Inference Method For Large Dynamic Economic Systems,2021-10-27 10:52:17+00:00,"['Pratha Khandelwal', 'Philip Nadler', 'Rossella Arcucci', 'William Knottenbelt', 'Yi-Ke Guo']",econ.EM,"The nature of available economic data has changed fundamentally in the last decade due to the economy's digitisation. With the prevalence of often black box data-driven machine learning methods, there is a necessity to develop interpretable machine learning methods that can conduct econometric inference, helping policymakers leverage the new nature of economic data. We therefore present a novel Variational Bayesian Inference approach to incorporate a time-varying parameter auto-regressive model which is scalable for big data. Our model is applied to a large blockchain dataset containing prices, transactions of individual actors, analyzing transactional flows and price movements on a very granular level. The model is extendable to any dataset which can be modelled as a dynamical system. We further improve the simple state-space modelling by introducing non-linearities in the forward model with the help of machine learning architectures."
http://arxiv.org/abs/2111.13228v1,Securities Lending Haircuts and Indemnification Pricing,2021-11-25 19:15:37+00:00,['Wujiang Lou'],q-fin.MF,"Securities borrowing and lending are critical to proper functioning of securities markets. To alleviate securities owners' exposure to borrower default risk, overcollateralization and indemnification are provided by the borrower and the lending agent respectively. Haircuts as the level of overcollateralization and the cost of indemnification are naturally interrelated: the higher haircut is, the lower cost shall become. This article presents a method of quantifying their relationship. Borrower dependent haircuts satisfying the lender's credit risk appetite are computed for US Treasuries and main equities by applying a repo haircut model to bilateral securities lending transactions. Indemnification is designed to fulfill a triple-A risk appetite when the transaction haircut fails to deliver. The cost of indemnification consists of a risk charge, a capital charge, and a funding charge, each corresponding to the expected loss, the economic capital, and the redundant fund needed to arrive at the triple-A haircut."
http://arxiv.org/abs/2111.14613v2,Do price reductions attract customers in urban public transport? A synthetic control approach,2021-11-25 07:43:26+00:00,"['Hannes Wallimann', 'Kevin Blättler', 'Widar von Arx']",econ.GN,"In this paper, we assess the demand effects of lower public transport fares in Geneva, an urban area in Switzerland. Considering a unique sample based on transport companies' annual reports, we find that, when reducing the costs of annual season tickets, day tickets and hourly tickets (by up to 29%, 6% and 20%, respectively), demand increases by, on average, over five years, about 10.6%. To the best of our knowledge, we are the first to show how the synthetic control method (Abadie and Gardeazabal, 2003, Abadie, Diamond, and Hainmueller, 2010) can be used to assess such (for policy-makers) important price reduction effects in urban public transport. Furthermore, we propose an aggregate metric that inherits changes in public transport supply (e.g., frequency increases) to assess these demand effects, namely passenger trips per vehicle kilometre. This metric helps us to isolate the impact of price reductions by ensuring that companies' frequency increases do not affect estimators of interest. In addition, we show how to investigate the robustness of results in similar settings. Using a recent statistical method and a different study design, i.e., not blocking off supply changes as an alternate explanation of the effect, leads us to a lower bound of the effect, amounting to an increase of 3.7%. Finally, as far as we know, it is the first causal estimate of price reduction on urban public transport initiated by direct democracy."
http://arxiv.org/abs/2111.14620v1,An Investigation of the Impact of COVID-19 Non-Pharmaceutical Interventions and Economic Support Policies on Foreign Exchange Markets with Explainable AI Techniques,2021-11-02 07:02:28+00:00,"['Siyuan Liu', 'Mehmet Orcun Yalcin', 'Hsuan Fu', 'Xiuyi Fan']",econ.GN,"Since the onset of the the COVID-19 pandemic, many countries across the world have implemented various non-pharmaceutical interventions (NPIs) to contain the spread of virus, as well as economic support policies (ESPs) to save their economies. The pandemic and the associated NPIs have triggered unprecedented waves of economic shocks to the financial markets, including the foreign exchange (FX) markets. Although there are some studies exploring the impact of the NPIs and ESPs on FX markets, the relative impact of individual NPIs or ESPs has not been studied in a combined framework. In this work, we investigate the relative impact of NPIs and ESPs with Explainable AI (XAI) techniques. Experiments over exchange rate data of G10 currencies during the period from January 1, 2020 to January 13, 2021 suggest strong impacts on exchange rate markets by all measures of the strict lockdown, such as stay at home requirements, workplace closing, international travel control, and restrictions on internal movement. Yet, the impact of individual NPI and ESP can vary across different currencies. To the best of our knowledge, this is the first work that uses XAI techniques to study the relative impact of NPIs and ESPs on the FX market. The derived insights can guide governments and policymakers to make informed decisions when facing with the ongoing pandemic and a similar situation in the near future."
http://arxiv.org/abs/2111.14635v1,A Resolution of St. Petersburg Paradox,2021-11-24 13:59:32+00:00,['V. I. Yukalov'],math.OC,"The St. Petersburg paradox is the oldest paradox in decision theory and has played a pivotal role in the introduction of increasing concave utility functions embodying risk aversion and decreasing marginal utility of gains. All attempts to resolve it have considered some variants of the original set-up, but the original paradox has remained unresolved, while the proposed variants have introduced new complications and problems. Here a rigorous mathematical resolution of the St. Petersburg paradox is suggested based on a probabilistic approach to decision theory."
http://arxiv.org/abs/2110.11582v1,An Economy of Neural Networks: Learning from Heterogeneous Experiences,2021-10-22 04:21:51+00:00,['Artem Kuriksha'],econ.GN,"This paper proposes a new way to model behavioral agents in dynamic macro-financial environments. Agents are described as neural networks and learn policies from idiosyncratic past experiences. I investigate the feedback between irrationality and past outcomes in an economy with heterogeneous shocks similar to Aiyagari (1994). In the model, the rational expectations assumption is seriously violated because learning of a decision rule for savings is unstable. Agents who fall into learning traps save either excessively or save nothing, which provides a candidate explanation for several empirical puzzles about wealth distribution. Neural network agents have a higher average MPC and exhibit excess sensitivity of consumption. Learning can negatively affect intergenerational mobility."
http://arxiv.org/abs/2110.10480v1,Bi-integrative analysis of two-dimensional heterogeneous panel data model,2021-10-20 10:40:54+00:00,"['Wei Wang', 'Xiaodong Yan', 'Yanyan Ren', 'Zhijie Xiao']",econ.EM,"Heterogeneous panel data models that allow the coefficients to vary across individuals and/or change over time have received increasingly more attention in statistics and econometrics. This paper proposes a two-dimensional heterogeneous panel regression model that incorporate a group structure of individual heterogeneous effects with cohort formation for their time-variations, which allows common coefficients between nonadjacent time points. A bi-integrative procedure that detects the information regarding group and cohort patterns simultaneously via a doubly penalized least square with concave fused penalties is introduced. We use an alternating direction method of multipliers (ADMM) algorithm that automatically bi-integrates the two-dimensional heterogeneous panel data model pertaining to a common one. Consistency and asymptotic normality for the proposed estimators are developed. We show that the resulting estimators exhibit oracle properties, i.e., the proposed estimator is asymptotically equivalent to the oracle estimator obtained using the known group and cohort structures. Furthermore, the simulation studies provide supportive evidence that the proposed method has good finite sample performance. A real data empirical application has been provided to highlight the proposed method."
http://arxiv.org/abs/2110.03070v2,Robust Generalized Method of Moments: A Finite Sample Viewpoint,2021-10-06 21:06:22+00:00,"['Dhruv Rohatgi', 'Vasilis Syrgkanis']",stat.ML,"For many inference problems in statistics and econometrics, the unknown parameter is identified by a set of moment conditions. A generic method of solving moment conditions is the Generalized Method of Moments (GMM). However, classical GMM estimation is potentially very sensitive to outliers. Robustified GMM estimators have been developed in the past, but suffer from several drawbacks: computational intractability, poor dimension-dependence, and no quantitative recovery guarantees in the presence of a constant fraction of outliers. In this work, we develop the first computationally efficient GMM estimator (under intuitive assumptions) that can tolerate a constant $ε$ fraction of adversarially corrupted samples, and that has an $\ell_2$ recovery guarantee of $O(\sqrtε)$. To achieve this, we draw upon and extend a recent line of work on algorithmic robust statistics for related but simpler problems such as mean estimation, linear regression and stochastic optimization. As two examples of the generality of our algorithm, we show how our estimation algorithm and assumptions apply to instrumental variables linear and logistic regression. Moreover, we experimentally validate that our estimator outperforms classical IV regression and two-stage Huber regression on synthetic and semi-synthetic datasets with corruption."
http://arxiv.org/abs/2110.08563v1,Auction design with ambiguity: Optimality of the first-price and all-pay auctions,2021-10-16 12:45:59+00:00,"['Sosung Baik', 'Sung-Ha Hwang']",econ.TH,"We study the optimal auction design problem when bidders' preferences follow the maxmin expected utility model. We suppose that each bidder's set of priors consists of beliefs close to the seller's belief, where ""closeness"" is defined by a divergence. For a given allocation rule, we identify a class of optimal transfer candidates, named the win-lose dependent transfers, with the following property: each type of bidder's transfer conditional on winning or losing is independent of the competitor's type report. Our result reduces the infinite-dimensional optimal transfer problem to a two-dimensional optimization problem. By solving the reduced problem, we find that: (i) among efficient mechanisms with no premiums for losers, the first-price auction is optimal; and, (ii) among efficient winner-favored mechanisms where each bidder pays smaller amounts when she wins than loses: the all-pay auction is optimal. Under a simplifying assumption, these two auctions remain optimal under the endogenous allocation rule."
http://arxiv.org/abs/2110.07224v1,Choice probabilities and correlations in closed-form route choice models: specifications and drawbacks,2021-10-14 08:41:34+00:00,"['Fiore Tinessa', 'Vittorio Marzano', 'Andrea Papola']",econ.EM,"This paper investigates the performance, in terms of choice probabilities and correlations, of existing and new specifications of closed-form route choice models with flexible correlation patterns, namely the Link Nested Logit (LNL), the Paired Combinatorial Logit (PCL) and the more recent Combination of Nested Logit (CoNL) models. Following a consolidated track in the literature, choice probabilities and correlations of the Multinomial Probit (MNP) model by (Daganzo and Sheffi, 1977) are taken as target. Laboratory experiments on small/medium-size networks are illustrated, also leveraging a procedure for practical calculation of correlations of any GEV models, proposed by (Marzano 2014). Results show that models with inherent limitations in the coverage of the domain of feasible correlations yield unsatisfactory performance, whilst the specifications of the CoNL proposed in the paper appear the best in fitting both MNP correlations and probabilities. Performance of the models are appreciably ameliorated by introducing lower bounds to the nesting parameters. Overall, the paper provides guidance for the practical application of tested models."
http://arxiv.org/abs/2111.04267v1,Exponential GARCH-Ito Volatility Models,2021-11-08 04:20:26+00:00,['Donggyu Kim'],econ.EM,"This paper introduces a novel Ito diffusion process to model high-frequency financial data, which can accommodate low-frequency volatility dynamics by embedding the discrete-time non-linear exponential GARCH structure with log-integrated volatility in a continuous instantaneous volatility process. The key feature of the proposed model is that, unlike existing GARCH-Ito models, the instantaneous volatility process has a non-linear structure, which ensures that the log-integrated volatilities have the realized GARCH structure. We call this the exponential realized GARCH-Ito (ERGI) model. Given the auto-regressive structure of the log-integrated volatility, we propose a quasi-likelihood estimation procedure for parameter estimation and establish its asymptotic properties. We conduct a simulation study to check the finite sample performance of the proposed model and an empirical study with 50 assets among the S\&P 500 compositions. The numerical studies show the advantages of the new proposed model."
http://arxiv.org/abs/2111.07225v1,Large Order-Invariant Bayesian VARs with Stochastic Volatility,2021-11-14 02:52:28+00:00,"['Joshua C. C. Chan', 'Gary Koop', 'Xuewen Yu']",econ.EM,"Many popular specifications for Vector Autoregressions (VARs) with multivariate stochastic volatility are not invariant to the way the variables are ordered due to the use of a Cholesky decomposition for the error covariance matrix. We show that the order invariance problem in existing approaches is likely to become more serious in large VARs. We propose the use of a specification which avoids the use of this Cholesky decomposition. We show that the presence of multivariate stochastic volatility allows for identification of the proposed model and prove that it is invariant to ordering. We develop a Markov Chain Monte Carlo algorithm which allows for Bayesian estimation and prediction. In exercises involving artificial and real macroeconomic data, we demonstrate that the choice of variable ordering can have non-negligible effects on empirical results. In a macroeconomic forecasting exercise involving VARs with 20 variables we find that our order-invariant approach leads to the best forecasts and that some choices of variable ordering can lead to poor forecasts using a conventional, non-order invariant, approach."
http://arxiv.org/abs/2111.15255v1,Double Fuzzy Probabilistic Interval Linguistic Term Set and a Dynamic Fuzzy Decision Making Model based on Markov Process with tts Application in Multiple Criteria Group Decision Making,2021-11-30 10:17:08+00:00,['Zongmin Liu'],eess.SY,"The probabilistic linguistic term has been proposed to deal with probability distributions in provided linguistic evaluations. However, because it has some fundamental defects, it is often difficult for decision-makers to get reasonable information of linguistic evaluations for group decision making. In addition, weight information plays a significant role in dynamic information fusion and decision making process. However, there are few research methods to determine the dynamic attribute weight with time. In this paper, I propose the concept of double fuzzy probability interval linguistic term set (DFPILTS). Firstly, fuzzy semantic integration, DFPILTS definition, its preference relationship, some basic algorithms and aggregation operators are defined. Then, a fuzzy linguistic Markov matrix with its network is developed. Then, a weight determination method based on distance measure and information entropy to reducing the inconsistency of DFPILPR and obtain collective priority vector based on group consensus is developed. Finally, an aggregation-based approach is developed, and an optimal investment case from a financial risk is used to illustrate the application of DFPILTS and decision method in multi-criteria decision making."
http://arxiv.org/abs/2201.13000v1,A General Description of Growth Trends,2022-01-31 05:05:08+00:00,['Moshe Elitzur'],econ.EM,"Time series that display periodicity can be described with a Fourier expansion. In a similar vein, a recently developed formalism enables description of growth patterns with the optimal number of parameters (Elitzur et al, 2020). The method has been applied to the growth of national GDP, population and the COVID-19 pandemic; in all cases the deviations of long-term growth patterns from pure exponential required no more than two additional parameters, mostly only one. Here I utilize the new framework to develop a unified formulation for all functions that describe growth deceleration, wherein the growth rate decreases with time. The result offers the prospects for a new general tool for trend removal in time-series analysis."
http://arxiv.org/abs/2202.04796v5,The Transfer Performance of Economic Models,2022-02-10 02:13:50+00:00,"['Isaiah Andrews', 'Drew Fudenberg', 'Lihua Lei', 'Annie Liang', 'Chaofeng Wu']",econ.TH,"Economists often estimate models using data from a particular domain, e.g. estimating risk preferences in a particular subject pool or for a specific class of lotteries. Whether a model's predictions extrapolate well across domains depends on whether the estimated model has captured generalizable structure. We provide a tractable formulation for this ""out-of-domain"" prediction problem and define the transfer error of a model based on how well it performs on data from a new domain. We derive finite-sample forecast intervals that are guaranteed to cover realized transfer errors with a user-selected probability when domains are iid, and use these intervals to compare the transferability of economic models and black box algorithms for predicting certainty equivalents. We find that in this application, the black box algorithms we consider outperform standard economic models when estimated and tested on data from the same domain, but the economic models generalize across domains better than the black-box algorithms do."
http://arxiv.org/abs/2202.07517v2,An Equilibrium Model of the First-Price Auction with Strategic Uncertainty: Theory and Empirics,2022-02-15 15:45:37+00:00,['Bernhard Kasberger'],econ.TH,"In many first-price auctions, bidders face considerable strategic uncertainty: They cannot perfectly anticipate the other bidders' bidding behavior. We propose a model in which bidders do not know the entire distribution of opponent bids but only the expected (winning) bid and lower and upper bounds on the opponent bids. We characterize the optimal bidding strategies and prove the existence of equilibrium beliefs. Finally, we apply the model to estimate the cost distribution in highway procurement auctions and find good performance out-of-sample."
http://arxiv.org/abs/2201.08995v1,"Fuel consumption elasticities, rebound effect and feebate effectiveness in the Indian and Chinese new car markets",2022-01-22 08:41:02+00:00,"['Prateek Bansal', 'Rubal Dua']",econ.GN,"China and India, the world's two most populous developing economies, are also among the world's largest automotive markets and carbon emitters. To reduce carbon emissions from the passenger car sector, both countries have considered various policy levers affecting fuel prices, car prices and fuel economy. This study estimates the responsiveness of new car buyers in China and India to such policy levers and drivers including income. Furthermore, we estimate the potential for rebound effect and the effectiveness of a feebate policy. To accomplish this, we developed a joint discrete-continuous model of car choice and usage based on revealed preference survey data from approximately 8000 new car buyers from India and China who purchased cars in 2016-17. Conditional on buying a new car, the fuel consumption in both markets is found to be relatively unresponsive to fuel price and income, with magnitudes of elasticity estimates ranging from 0.12 to 0.15. For both markets, the mean segment-level direct elasticities of fuel consumption relative to car price and fuel economy range from 0.57 to 0.65. The rebound effect on fuel savings due to cost-free fuel economy improvement is found to be 17.1% for India and 18.8% for China. A revenue-neutral feebate policy, with average rebates and fees of up to around 15% of the retail price, resulted in fuel savings of around 0.7% for both markets. While the feebate policy's rebound effect is low - 7.3% for India and 1.6% for China - it does not appear to be an effective fuel conservation policy."
http://arxiv.org/abs/2201.07170v1,What is the mission of innovation?,2022-01-14 18:27:31+00:00,['Julian D. Cortes'],cs.SI,"Governments and organizations recognize the need to revisit a mission-driven innovation amidst national and organizational innovation policy formulations. Notwithstanding a fertile research agenda on mission statements (hereafter mission(s)), several lines of inquiry remain open, such as crossnational and multisectorial studies and an examination of research knowledge intensive institutions. In this article, we identify similarities and differences in the content of missions from government, private, higher education, and health research knowledge intensive institutions in a sample of over 1,900 institutions from 89 countries through the deployment of sentiment analysis, readability, and lexical diversity; semantic networks; and a similarity computation between document corpus. We found that missions of research knowledge intensive institutions are challenging to read texts with lower lexical diversity that favors positive rather than negative words. In stark contrast to this, the non-profit sector is consonant in multiple dimensions in its use of Corporate Social Responsibility jargon. The lexical appearance of research in the missions varies according to mission sectorial context, and each sector has a cluster specific focus. Utilizing the mission as a strategic planning tool in higher income regions might serve to explain corpora similarities shared by sectors and continents."
http://arxiv.org/abs/2201.03092v1,Uncovering the Source of Machine Bias,2022-01-09 21:05:24+00:00,"['Xiyang Hu', 'Yan Huang', 'Beibei Li', 'Tian Lu']",cs.LG,"We develop a structural econometric model to capture the decision dynamics of human evaluators on an online micro-lending platform, and estimate the model parameters using a real-world dataset. We find two types of biases in gender, preference-based bias and belief-based bias, are present in human evaluators' decisions. Both types of biases are in favor of female applicants. Through counterfactual simulations, we quantify the effect of gender bias on loan granting outcomes and the welfare of the company and the borrowers. Our results imply that both the existence of the preference-based bias and that of the belief-based bias reduce the company's profits. When the preference-based bias is removed, the company earns more profits. When the belief-based bias is removed, the company's profits also increase. Both increases result from raising the approval probability for borrowers, especially male borrowers, who eventually pay back loans. For borrowers, the elimination of either bias decreases the gender gap of the true positive rates in the credit risk evaluation. We also train machine learning algorithms on both the real-world data and the data from the counterfactual simulations. We compare the decisions made by those algorithms to see how evaluators' biases are inherited by the algorithms and reflected in machine-based decisions. We find that machine learning algorithms can mitigate both the preference-based bias and the belief-based bias."
http://arxiv.org/abs/2201.04266v10,Safe Equilibrium,2022-01-12 01:45:51+00:00,['Sam Ganzfried'],cs.GT,"The standard game-theoretic solution concept, Nash equilibrium, assumes that all players behave rationally. If we follow a Nash equilibrium and opponents are irrational (or follow strategies from a different Nash equilibrium), then we may obtain an extremely low payoff. On the other hand, a maximin strategy assumes that all opposing agents are playing to minimize our payoff (even if it is not in their best interest), and ensures the maximal possible worst-case payoff, but results in exceedingly conservative play. We propose a new solution concept called safe equilibrium that models opponents as behaving rationally with a specified probability and behaving potentially arbitrarily with the remaining probability. We prove that a safe equilibrium exists in all strategic-form games (for all possible values of the rationality parameters), and prove that its computation is PPAD-hard. We present exact algorithms for computing a safe equilibrium in both 2 and $n$-player games, as well as scalable approximation algorithms."
http://arxiv.org/abs/2201.06169v3,On Well-posedness and Minimax Optimal Rates of Nonparametric Q-function Estimation in Off-policy Evaluation,2022-01-17 01:09:38+00:00,"['Xiaohong Chen', 'Zhengling Qi']",math.ST,"We study the off-policy evaluation (OPE) problem in an infinite-horizon Markov decision process with continuous states and actions. We recast the $Q$-function estimation into a special form of the nonparametric instrumental variables (NPIV) estimation problem. We first show that under one mild condition the NPIV formulation of $Q$-function estimation is well-posed in the sense of $L^2$-measure of ill-posedness with respect to the data generating distribution, bypassing a strong assumption on the discount factor $γ$ imposed in the recent literature for obtaining the $L^2$ convergence rates of various $Q$-function estimators. Thanks to this new well-posed property, we derive the first minimax lower bounds for the convergence rates of nonparametric estimation of $Q$-function and its derivatives in both sup-norm and $L^2$-norm, which are shown to be the same as those for the classical nonparametric regression (Stone, 1982). We then propose a sieve two-stage least squares estimator and establish its rate-optimality in both norms under some mild conditions. Our general results on the well-posedness and the minimax lower bounds are of independent interest to study not only other nonparametric estimators for $Q$-function but also efficient estimation on the value of any target policy in off-policy settings."
http://arxiv.org/abs/2201.02272v2,Surveying 5G Techno-Economic Research to Inform the Evaluation of 6G Wireless Technologies,2022-01-06 23:09:58+00:00,"['Edward J. Oughton', 'William Lehr']",eess.SP,"Techno-economic assessment is a fundamental technique engineers use for evaluating new communications technologies. However, despite the techno-economics of the fifth cellular generation (5G) being an active research area, it is surprising there are few comprehensive evaluations of this growing literature. With mobile network operators deploying 5G across their networks, it is therefore an opportune time to appraise current accomplishments and review the state-of-the-art. Such insight can inform the flurry of 6G research papers currently underway and help engineers in their mission to provide affordable high-capacity, low-latency broadband connectivity, globally. The survey discusses emerging trends from the 5G techno-economic literature and makes five key recommendations for the design and standardization of Next Generation 6G wireless technologies."
http://arxiv.org/abs/2201.02760v1,Bibliometric analysis of the scientific production found in Scopus and Web of Science about business administration,2022-01-08 04:28:12+00:00,"['Félix Lirio-Loli', 'William Dextre-Martínez']",econ.GN,"Introduction: This study analyzes the scientific production in business administration in scientific articles based on modeling partial least squares structural equations (Partial Least Squares Structural Equation Modeling PLS-SEM) in the 2011-2020 period. Methodology: The study is exploratory - descriptive and has three phases: a) Selection of keywords and search criteria; (b) Search and refinement of information; c) information analysis. A method of bibliometric review of the specific literature has been used based on the analysis of predefined indicators and completed with a qualitative content synthesis. Results: A total of 167 publications were analyzed, making correlations from the year, search criteria, authors, impact factor by quartile, and by citation variables. More outstanding scientific production comes from Scopus under the search criteria ((pls AND sem) OR ""partial least squares"") AND (business OR management), being the figure of 4,870 scientific articles, while Web of Science accumulates 3,946 articles Conclusion: There has been a progressive growth in scientific articles with the PLS-SEM technique from 2011 to 2020. Scopus, compared to WoS, presents a more significant number of scientific productions with this statistical approach. The authors who register scientific articles demonstrate a high H index; in addition, there is an important number of scientific articles with a PLS-SEM approach in universities in Malaysia that could be related to the expansion of higher education in that country, as well as in Singapore, Taiwan, and Indonesia. Finally, business administration, accounting, and economics are outstanding scientific production."
http://arxiv.org/abs/2201.02793v1,The component-wise egalitarian Myerson value for Network Games,2022-01-08 08:54:40+00:00,"['Surajit Borkotokey', 'Sujata Goala', 'Niharika Kakoty', 'Parishmita Boruah']",econ.TH,"We introduce the component-wise egalitarian Myerson value for network games. This new value being a convex combination of the Myerson value and the component-wise equal division rule is a player-based allocation rule. In network games under the cooperative framework, the Myerson value is an extreme example of marginalism, while the equal division rule signifies egalitarianism. In the proposed component-wise egalitarian Myerson value, a convexity parameter combines these two attributes and determines the degree of solidarity to the players. Here, by solidarity, we mean the mutual support or compensation among the players in a network. We provide three axiomatic characterizations of the value. Further, we propose an implementation mechanism for the component-wise egalitarian Myerson value under subgame perfect Nash equilibrium."
http://arxiv.org/abs/2201.02919v4,Economic Integration and Agglomeration of Multinational Production with Transfer Pricing,2022-01-09 03:55:09+00:00,"['Hayato Kato', 'Hirofumi Okoshi']",econ.GN,"Do low corporate taxes always favor multinational production over economic integration? We propose a two-country model in which multinationals choose the locations of production plants and foreign distribution affiliates and shift profits between them through transfer prices. With high trade costs, plants are concentrated in the low-tax country; surprisingly, this pattern reverses with low trade costs. Indeed, economic integration has a non-monotonic impact: falling trade costs first decrease and then increase the plant share in the high-tax country, which we empirically confirm. Moreover, allowing for transfer pricing makes tax competition tougher and international coordination on transfer-pricing regulation can be beneficial."
http://arxiv.org/abs/2201.02804v1,A study on bribery networks with a focus on harassment bribery and ways to control corruption,2022-01-08 10:46:50+00:00,['Chanchal Pramanik'],econ.TH,The paper focuses on the bribery network emphasizing harassment bribery. A bribery network ends with the police officer whose utility from the bribe is positive and the approving officer in the network. The persistent nature of corruption is due to colluding behavior of the bribery networks. The probability of detection of bribery incidents will help in improving controlling corruption in society. The asymmetric form of punishment and award equivalent to the amount of punishment to the network can enhance the probability of detection of harassment bribery $(p_{h})$ and thus increasing the probability of detection of overall bribery $(p_{h} \in p)$.
http://arxiv.org/abs/2202.02462v3,Sequential Veto Bargaining with Incomplete Information,2022-02-05 01:50:03+00:00,"['S. Nageeb Ali', 'Navin Kartik', 'Andreas Kleiner']",econ.TH,"We study sequential bargaining between a proposer and a veto player. Both have single-peaked preferences, but the proposer is uncertain about the veto player's ideal point. The proposer cannot commit to future proposals. When players are patient, there can be equilibria with Coasian dynamics: the veto player's private information can largely nullify proposer's bargaining power. Our main result, however, is that under some conditions there are also equilibria in which the proposer obtains the high payoff that he would with commitment power. The driving force is that the veto player's single-peaked preferences give the proposer an option to ""leapfrog"", i.e., to secure agreement from only low-surplus types early on to credibly extract surplus from high types later. Methodologically, we exploit the connection between sequential bargaining and static mechanism design."
http://arxiv.org/abs/2202.03110v1,Predicting Default Probabilities for Stress Tests: A Comparison of Models,2022-02-07 12:45:03+00:00,['Martin Guth'],econ.EM,"Since the Great Financial Crisis (GFC), the use of stress tests as a tool for assessing the resilience of financial institutions to adverse financial and economic developments has increased significantly. One key part in such exercises is the translation of macroeconomic variables into default probabilities for credit risk by using macrofinancial linkage models. A key requirement for such models is that they should be able to properly detect signals from a wide array of macroeconomic variables in combination with a mostly short data sample. The aim of this paper is to compare a great number of different regression models to find the best performing credit risk model. We set up an estimation framework that allows us to systematically estimate and evaluate a large set of models within the same environment. Our results indicate that there are indeed better performing models than the current state-of-the-art model. Moreover, our comparison sheds light on other potential credit risk models, specifically highlighting the advantages of machine learning models and forecast combinations."
http://arxiv.org/abs/2202.03332v1,Forecasting Environmental Data: An example to ground-level ozone concentration surfaces,2022-02-07 16:22:33+00:00,"['Alexander Gleim', 'Nazarii Salish']",stat.ME,"Environmental problems are receiving increasing attention in socio-economic and health studies. This in turn fosters advances in recording and data collection of many related real-life processes. Available tools for data processing are often found too restrictive as they do not account for the rich nature of such data sets. In this paper, we propose a new statistical perspective on forecasting spatial environmental data collected sequentially over time. We treat this data set as a surface (functional) time series with a possibly complicated geographical domain. By employing novel techniques from functional data analysis we develop a new forecasting methodology. Our approach consists of two steps. In the first step, time series of surfaces are reconstructed from measurements sampled over some spatial domain using a finite element spline smoother. In the second step, we adapt the dynamic functional factor model to forecast a surface time series. The advantage of this approach is that we can account for and explore simultaneously spatial as well as temporal dependencies in the data. A forecasting study of ground-level ozone concentration over the geographical domain of Germany demonstrates the practical value of this new perspective, where we compare our approach with standard functional benchmark models."
http://arxiv.org/abs/2202.02988v1,Detecting Structural Breaks in Foreign Exchange Markets by using the group LASSO technique,2022-02-07 08:04:01+00:00,['Mikio Ito'],econ.EM,This article proposes an estimation method to detect breakpoints for linear time series models with their parameters that jump scarcely. Its basic idea owes the group LASSO (group least absolute shrinkage and selection operator). The method practically provides estimates of such time-varying parameters of the models. An example shows that our method can detect each structural breakpoint's date and magnitude.
http://arxiv.org/abs/2202.04811v1,Creating an institutional ecosystem for cash transfer programming: Lessons from post-disaster governance in Indonesia,2022-02-10 03:10:55+00:00,"['Jonatan A. Lassa', 'Gisela Emanuela Nappoe', 'Susilo Budhi Sulistyo']",econ.GN,"Humanitarian and disaster management actors have increasingly adopted cash transfer to reduce the sufferings and vulnerability of the survivors. Case transfers have also been used as a critical instrument in the current COVID-19 pandemic. Unfortunately, academic work on humanitarian and disaster-cash transfer related issues remains limited. This article explores how NGOs and governments implement humanitarian cash transfer in a post-disaster setting using an exploratory research strategy. It asks What are institutional constraints and opportunities faced by humanitarian emergency responders in ensuring an effective humanitarian cash transfer and how humanitarian actors address such institutional conditions. We introduced a new conceptual framework, namely humanitarian and disaster management ecosystem for cash transfer. This framework allows non-governmental actors to restore complex relations between the state, disaster survivors or citizen, local market economy and civil society. Mixed methods and multistage research strategy were used to collect and analyze primary and secondary data. The findings suggest that implementing cash transfers in the context of post tsunamigenic earthquakes and liquefaction hazards, NGOs must co-create an ecosystem of response that not only aimed at restoring peoples access to cash and basic needs but first they must restore relations between the states and their citizen while linking the at-risk communities with the private sectors to jump-starting local livelihoods and market economy."
http://arxiv.org/abs/2202.04245v2,Regulatory Instruments for Fair Personalized Pricing,2022-02-09 03:07:08+00:00,"['Renzhe Xu', 'Xingxuan Zhang', 'Peng Cui', 'Bo Li', 'Zheyan Shen', 'Jiazheng Xu']",cs.CY,"Personalized pricing is a business strategy to charge different prices to individual consumers based on their characteristics and behaviors. It has become common practice in many industries nowadays due to the availability of a growing amount of high granular consumer data. The discriminatory nature of personalized pricing has triggered heated debates among policymakers and academics on how to design regulation policies to balance market efficiency and equity. In this paper, we propose two sound policy instruments, i.e., capping the range of the personalized prices or their ratios. We investigate the optimal pricing strategy of a profit-maximizing monopoly under both regulatory constraints and the impact of imposing them on consumer surplus, producer surplus, and social welfare. We theoretically prove that both proposed constraints can help balance consumer surplus and producer surplus at the expense of total surplus for common demand distributions, such as uniform, logistic, and exponential distributions. Experiments on both simulation and real-world datasets demonstrate the correctness of these theoretical results. Our findings and insights shed light on regulatory policy design for the increasingly monopolized business in the digital era."
http://arxiv.org/abs/2202.04573v5,On the Uniqueness and Stability of the Equilibrium Price in Quasi-Linear Economies,2022-02-09 17:13:43+00:00,['Yuhki Hosoya'],econ.TH,"In this paper, we show that if every consumer in an economy has a quasi-linear utility function, then the normalized equilibrium price is unique, and is locally stable with respect to the tâtonnement process. Our study can be seen as that extends the results in partial equilibrium theory to economies with more than two dimensional consumption space. Moreover, we discuss the surplus analysis in such economies."
http://arxiv.org/abs/2203.01233v2,"Optimal Defaults, Limited Enforcement and the Regulation of Contracts",2022-03-02 16:56:49+00:00,"['Zoë Hitzig', 'Benjamin Niswonger']",econ.TH,"We study how governments promote social welfare through the design of contracting environments. We model the regulation of contracting as default delegation: the government chooses a delegation set of contract terms it is willing to enforce, and influences the default terms that serve as outside options in parties' negotiations. Our analysis shows that limiting the delegation set principally mitigates externalities, while default terms primarily achieve distributional objectives. Applying our model to the regulation of labor contracts, we derive comparative statics on the optimal default delegation policy. As equity concerns or externalities increase, in-kind support for workers increases (e.g. through benefits requirements and public health insurance). Meanwhile, when worker bargaining power decreases away from parity, support for workers increases in cash (e.g. through cash transfers and minimum wage laws)."
http://arxiv.org/abs/2203.00070v2,Decisions over Sequences,2022-02-28 20:16:24+00:00,"['Bhavook Bhardwaj', 'Siddharth Chatterjee']",econ.TH,"This paper introduces a class of objects called decision rules that map infinite sequences of alternatives to a decision space. These objects can be used to model situations where a decision maker encounters alternatives in a sequence such as receiving recommendations. Within the class of decision rules, we study natural subclasses: stopping and uniform stopping rules. Our main result establishes the equivalence of these two subclasses of decision rules. Next, we introduce the notion of computability of decision rules using Turing machines and show that computable rules can be implemented using a simpler computational device: a finite automaton. We further show that computability of choice rules -- an important subclass of decision rules -- is implied by their continuity with respect to a natural topology. Finally, we introduce some natural heuristics in this framework and provide their behavioral characterization."
http://arxiv.org/abs/2203.01068v1,Satellite Image and Machine Learning based Knowledge Extraction in the Poverty and Welfare Domain,2022-03-02 12:38:20+00:00,"['Ola Hall', 'Mattias Ohlsson', 'Thortseinn Rögnvaldsson']",cs.CY,"Recent advances in artificial intelligence and machine learning have created a step change in how to measure human development indicators, in particular asset based poverty. The combination of satellite imagery and machine learning has the capability to estimate poverty at a level similar to what is achieved with workhorse methods such as face-to-face interviews and household surveys. An increasingly important issue beyond static estimations is whether this technology can contribute to scientific discovery and consequently new knowledge in the poverty and welfare domain. A foundation for achieving scientific insights is domain knowledge, which in turn translates into explainability and scientific consistency. We review the literature focusing on three core elements relevant in this context: transparency, interpretability, and explainability and investigate how they relates to the poverty, machine learning and satellite imagery nexus. Our review of the field shows that the status of the three core elements of explainable machine learning (transparency, interpretability and domain knowledge) is varied and does not completely fulfill the requirements set up for scientific insights and discoveries. We argue that explainability is essential to support wider dissemination and acceptance of this research, and explainability means more than just interpretability."
http://arxiv.org/abs/2203.00618v1,The resilience of the multirelational structure of geopolitical treaties is critically linked to past colonial world order and offshore fiscal havens,2022-03-01 16:59:36+00:00,"['Pier Luigi Sacco', 'Alex Arenas', 'Manlio De Domenico']",econ.GN,"The governance of the political and economic world order builds on a complex architecture of international treaties at various geographical scales. In a historical phase of high institutional turbulence, assessing the stability of such architecture with respect to the unilateral defection of single countries and to the breakdown of single treaties is important. We carry out this analysis on the whole global architecture and find that the countries with the highest disruption potential are not major world powers (with the exception of Germany and France) but mostly medium-small and micro-countries. Political stability is highly dependent on many former colonial overseas territories that are today part of the global network of fiscal havens, as well as on emerging economies, mostly from South-East Asia. Economic stability depends on medium sized European and African countries. However, single global treaties have surprisingly less disruptive potential, with the major exception of the WTO. These apparently counter-intuitive results highlight the importance to a nonlinear approach to international relations where the complex multilayered architecture of global governance is analyzed using advanced network science techniques. Our results suggest that the potential fragility of the world order seem to be much more directly related to global inequality and fiscal injustice than it is commonly believed, and that the legacy of the colonial world order is still very strong in the current international relations scenario. In particular, vested interests related to tax avoidance seem to have a structural role in the political architecture of global governance"
http://arxiv.org/abs/2203.14904v1,Why Do Banks Find Business Process Compliance So Challenging? An Australian Case Study,2022-03-24 23:35:46+00:00,"['Nigel Adams', 'Adriano Augusto', 'Michael Davern', 'Marcello La Rosa']",econ.GN,"Banks play an intrinsic role in any modern economy, recycling capital from savers to borrowers. They are heavily regulated and there have been a significant number of well publicized compliance failings in recent years. This is despite Business Process Compliance (BPC) being both a well researched domain in academia and one where significant progress has been made. This study seeks to determine why Australian banks find BPC so challenging. We interviewed 22 senior managers from a range of functions within the four major Australian banks to identify the key challenges. Not every process in every bank is facing the same issues, but in processes where a bank is particularly challenged to meet its compliance requirements, the same themes emerge. The compliance requirement load they bear is excessive, dynamic and complex. Fulfilling these requirements relies on impenetrable spaghetti processes, and the case for sustainable change remains elusive, locking banks into a fail-fix cycle that increases the underlying complexity. This paper proposes a conceptual framework that identifies and aggregates the challenges, and a circuit-breaker approach as an ""off ramp"" to the fail-fix cycle."
http://arxiv.org/abs/2203.04878v1,The management of scientific and technological infrastructures: the case of the Mexican National Laboratories,2022-03-01 23:53:00+00:00,"['L. Munguía', 'J. C. Escalante', 'E. Robles Belmont']",physics.soc-ph,"The effectiveness of research units is assessed on the basis of their performance in relation to scientific, technological and innovation production, the quality of their results and their contribution to the solution of scientific and social problems. This paper examines the management practices employed in some Mexican National Laboratories in order to identify those that could explain their effectiveness in meeting their objectives. The results of other works that propose common elements among laboratories with outstanding performance are used and verified directly in the field. Considering the inherent complexity of each field of knowledge and the socio-spatial characteristics in which the laboratories operate, we report which management practices are relevant for their effectiveness, how they contribute to their consolidation as fundamental scientific and technological infrastructures, how these can be translated into indicators that support the evaluation of their performance, and still pending."
http://arxiv.org/abs/2203.12431v1,Bounds for Bias-Adjusted Treatment Effect in Linear Econometric Models,2022-03-23 14:11:44+00:00,['Deepankar Basu'],econ.EM,"In linear econometric models with proportional selection on unobservables, omitted variable bias in estimated treatment effects are real roots of a cubic equation involving estimated parameters from a short and intermediate regression. The roots of the cubic are functions of $δ$, the degree of selection on unobservables, and $R_{max}$, the R-squared in a hypothetical long regression that includes the unobservable confounder and all observable controls. In this paper I propose and implement a novel algorithm to compute roots of the cubic equation over relevant regions of the $δ$-$R_{max}$ plane and use the roots to construct bounding sets for the true treatment effect. The algorithm is based on two well-known mathematical results: (a) the discriminant of the cubic equation can be used to demarcate regions of unique real roots from regions of three real roots, and (b) a small change in the coefficients of a polynomial equation will lead to small change in its roots because the latter are continuous functions of the former. I illustrate my method by applying it to the analysis of maternal behavior on child outcomes."
http://arxiv.org/abs/2203.12402v1,Performance evaluation of volatility estimation methods for Exabel,2022-03-23 13:26:36+00:00,"['Øyvind Grotmol', 'Martin Jullum', 'Kjersti Aas', 'Michael Scheuerer']",stat.AP,"Quantifying both historic and future volatility is key in portfolio risk management. This note presents and compares estimation strategies for volatility estimation in an estimation universe consisting on 28 629 unique companies from February 2010 to April 2021, with 858 different portfolios. The estimation methods are compared in terms of how they rank the volatility of the different subsets of portfolios. The overall best performing approach estimates volatility from direct entity returns using a GARCH model for variance estimation."
http://arxiv.org/abs/2203.11872v1,Performance of long short-term memory artificial neural networks in nowcasting during the COVID-19 crisis,2022-03-22 16:48:41+00:00,['Daniel Hopp'],stat.ML,"The COVID-19 pandemic has demonstrated the increasing need of policymakers for timely estimates of macroeconomic variables. A prior UNCTAD research paper examined the suitability of long short-term memory artificial neural networks (LSTM) for performing economic nowcasting of this nature. Here, the LSTM's performance during the COVID-19 pandemic is compared and contrasted with that of the dynamic factor model (DFM), a commonly used methodology in the field. Three separate variables, global merchandise export values and volumes and global services exports, were nowcast with actual data vintages and performance evaluated for the second, third, and fourth quarters of 2020 and the first and second quarters of 2021. In terms of both mean absolute error and root mean square error, the LSTM obtained better performance in two-thirds of variable/quarter combinations, as well as displayed more gradual forecast evolutions with more consistent narratives and smaller revisions. Additionally, a methodology to introduce interpretability to LSTMs is introduced and made available in the accompanying nowcast_lstm Python library, which is now also available in R, MATLAB, and Julia."
http://arxiv.org/abs/2201.13267v1,Micro-level Reserving for General Insurance Claims using a Long Short-Term Memory Network,2022-01-27 02:49:42+00:00,"['Ihsan Chaoubi', 'Camille Besse', 'Hélène Cossette', 'Marie-Pier Côté']",cs.LG,"Detailed information about individual claims are completely ignored when insurance claims data are aggregated and structured in development triangles for loss reserving. In the hope of extracting predictive power from the individual claims characteristics, researchers have recently proposed to move away from these macro-level methods in favor of micro-level loss reserving approaches. We introduce a discrete-time individual reserving framework incorporating granular information in a deep learning approach named Long Short-Term Memory (LSTM) neural network. At each time period, the network has two tasks: first, classifying whether there is a payment or a recovery, and second, predicting the corresponding non-zero amount, if any. We illustrate the estimation procedure on a simulated and a real general insurance dataset. We compare our approach with the chain-ladder aggregate method using the predictive outstanding loss estimates and their actual values. Based on a generalized Pareto model for excess payments over a threshold, we adjust the LSTM reserve prediction to account for extreme payments."
http://arxiv.org/abs/2202.01080v3,Understanding European Integration with Bipartite Networks of Comparative Advantage,2022-02-02 15:15:20+00:00,"['Riccardo Di Clemente', 'Balázs Lengyel', 'Lars F. Andersson', 'Rikard Eriksson']",econ.GN,"Core objectives of European common market integration are convergence and economic growth, but these are hampered by redundancy, and value chain asymmetries. The challenge is how to harmonize labor division to reach global competitiveness, meanwhile bridging productivity differences across the EU. We develop a bipartite network approach to trace pairwise co-specialization, by applying the Revealed Comparative Advantage method, within and between EU15 and Central and Eastern European (CEE). This approach assesses redundancies and division of labor in the EU at the level of industries and countries. We find significant co-specialization among CEE countries but a diverging specialization between EU15 and CEE. Productivity increases in those CEE industries that have co-specialized with other CEE countries after EU accession, while co-specialization across CEE and EU15 countries is less related to productivity growth. These results show that a division of sectoral specialization can lead to productivity convergence between EU15 and CEE countries."
http://arxiv.org/abs/2202.01661v2,Selection in the Presence of Implicit Bias: The Advantage of Intersectional Constraints,2022-02-03 16:21:50+00:00,"['Anay Mehrotra', 'Bary S. R. Pradelski', 'Nisheeth K. Vishnoi']",cs.CY,"In selection processes such as hiring, promotion, and college admissions, implicit bias toward socially-salient attributes such as race, gender, or sexual orientation of candidates is known to produce persistent inequality and reduce aggregate utility for the decision maker. Interventions such as the Rooney Rule and its generalizations, which require the decision maker to select at least a specified number of individuals from each affected group, have been proposed to mitigate the adverse effects of implicit bias in selection. Recent works have established that such lower-bound constraints can be very effective in improving aggregate utility in the case when each individual belongs to at most one affected group. However, in several settings, individuals may belong to multiple affected groups and, consequently, face more extreme implicit bias due to this intersectionality. We consider independently drawn utilities and show that, in the intersectional case, the aforementioned non-intersectional constraints can only recover part of the total utility achievable in the absence of implicit bias. On the other hand, we show that if one includes appropriate lower-bound constraints on the intersections, almost all the utility achievable in the absence of implicit bias can be recovered. Thus, intersectional constraints can offer a significant advantage over a reductionist dimension-by-dimension non-intersectional approach to reducing inequality."
http://arxiv.org/abs/2203.03032v1,Weighted-average quantile regression,2022-03-06 19:06:53+00:00,"['Denis Chetverikov', 'Yukun Liu', 'Aleh Tsyvinski']",econ.EM,"In this paper, we introduce the weighted-average quantile regression framework, $\int_0^1 q_{Y|X}(u)ψ(u)du = X'β$, where $Y$ is a dependent variable, $X$ is a vector of covariates, $q_{Y|X}$ is the quantile function of the conditional distribution of $Y$ given $X$, $ψ$ is a weighting function, and $β$ is a vector of parameters. We argue that this framework is of interest in many applied settings and develop an estimator of the vector of parameters $β$. We show that our estimator is $\sqrt T$-consistent and asymptotically normal with mean zero and easily estimable covariance matrix, where $T$ is the size of available sample. We demonstrate the usefulness of our estimator by applying it in two empirical settings. In the first setting, we focus on financial data and study the factor structures of the expected shortfalls of the industry portfolios. In the second setting, we focus on wage data and study inequality and social welfare dependence on commonly used individual characteristics."
http://arxiv.org/abs/2203.03044v3,Speculation in Procurement Auctions,2022-03-06 20:35:54+00:00,['Shanglyu Deng'],econ.TH,"A speculator can take advantage of a procurement auction by acquiring items for sale before the auction. The accumulated market power can then be exercised in the auction and may lead to a large enough gain to cover the acquisition costs. I show that speculation always generates a positive expected profit in second-price auctions but could be unprofitable in first-price auctions. In the case where speculation is profitable in first-price auctions, it is more profitable in second-price auctions. This comparison in profitability is driven by different competition patterns in the two auction mechanisms. In terms of welfare, speculation causes private value destruction and harms efficiency. Sellers benefit from the acquisition offer made by the speculator. Therefore, speculation comes at the expense of the auctioneer."
http://arxiv.org/abs/2203.04001v1,Cooperation and punishment mechanisms in uncertain and dynamic networks,2022-03-08 10:56:26+00:00,"['Edoardo Gallo', 'Yohanes E. Riyanto', 'Nilanjan Roy', 'Tat-How Teh']",econ.GN,"This paper examines experimentally how reputational uncertainty and the rate of change of the social environment determine cooperation. Reputational uncertainty significantly decreases cooperation, while a fast-changing social environment only causes a second-order qualitative increase in cooperation. At the individual level, reputational uncertainty induces more leniency and forgiveness in imposing network punishment through the link proposal and removal processes, inhibiting the formation of cooperative clusters. However, this effect is significant only in the fast-changing environment and not in the slow-changing environment. A substitution pattern between network punishment and action punishment (retaliatory defection) explains this discrepancy across the two social environments."
http://arxiv.org/abs/2201.10961v1,The Impact of COVID-19 Pandemic on Ridesourcing Services Differed Between Small Towns and Large Cities,2022-01-26 14:25:06+00:00,"['Nael Alsaleh', 'Bilal Farooq']",econ.GN,"The COVID-19 pandemic has significantly influenced all modes of transportation. However, it is still unclear how the pandemic affected the demand for ridesourcing services and whether these effects varied between small towns and large cities. We analyzed over 220 million ride requests in the City of Chicago (population: 2.7 million), Illinois, and 52 thousand in the Town of Innisfil (population: 37 thousand), Ontario, to investigate the impact of the COVID-19 pandemic on the ridesourcing demand in the two locations. Overall, the pandemic resulted in fewer trips in areas with higher proportions of seniors and more trips to parks and green spaces. Ridesourcing demand was adversely affected by the stringency index and COVID-19-related variables, and positively affected by vaccination rates. However, compared to Innisfil, ridesourcing services in Chicago experienced higher reductions in demand, were more affected by the number of hospitalizations and deaths, were less impacted by vaccination rates, and had lower recovery rates."
http://arxiv.org/abs/2202.12452v1,The outcome of the restabilization process in matching markets,2022-02-25 01:30:28+00:00,['Millán Guerra Beatriz Alejandra'],econ.TH,"For a many-to-one matching model, we study the matchings obtained through the restabilization of stable matchings that had been disrupted by a change in the population. We include a simple representation of the stable matching obtained in terms of the initial stable matching (i.e., before being disrupted by changes in the population) and the firm-optimal stable matching. (We used Lattice Theory to characterize the outcome of the restabilization process.) We also describe the connection between the original stable matching and the one obtained after the restabilization process in the new market."
http://arxiv.org/abs/2202.12495v2,Fast variational Bayes methods for multinomial probit models,2022-02-25 04:45:42+00:00,"['Rubén Loaiza-Maya', 'Didier Nibbering']",econ.EM,"The multinomial probit model is often used to analyze choice behaviour. However, estimation with existing Markov chain Monte Carlo (MCMC) methods is computationally costly, which limits its applicability to large choice data sets. This paper proposes a variational Bayes method that is accurate and fast, even when a large number of choice alternatives and observations are considered. Variational methods usually require an analytical expression for the unnormalized posterior density and an adequate choice of variational family. Both are challenging to specify in a multinomial probit, which has a posterior that requires identifying restrictions and is augmented with a large set of latent utilities. We employ a spherical transformation on the covariance matrix of the latent utilities to construct an unnormalized augmented posterior that identifies the parameters, and use the conditional posterior of the latent utilities as part of the variational family. The proposed method is faster than MCMC, and can be made scalable to both a large number of choice alternatives and a large number of observations. The accuracy and scalability of our method is illustrated in numerical experiments and real purchase data with one million observations."
http://arxiv.org/abs/2202.12511v2,A general characterization of optimal tie-breaker designs,2022-02-25 06:13:09+00:00,"['Harrison H. Li', 'Art B. Owen']",stat.ME,"Tie-breaker designs trade off a statistical design objective with short-term gain from preferentially assigning a binary treatment to those with high values of a running variable $x$. The design objective is any continuous function of the expected information matrix in a two-line regression model, and short-term gain is expressed as the covariance between the running variable and the treatment indicator. We investigate how to specify design functions indicating treatment probabilities as a function of $x$ to optimize these competing objectives, under external constraints on the number of subjects receiving treatment. Our results include sharp existence and uniqueness guarantees, while accommodating the ethically appealing requirement that treatment probabilities are non-decreasing in $x$. Under such a constraint, there always exists an optimal design function that is constant below and above a single discontinuity. When the running variable distribution is not symmetric or the fraction of subjects receiving the treatment is not $1/2$, our optimal designs improve upon a $D$-optimality objective without sacrificing short-term gain, compared to the three level tie-breaker designs of Owen and Varian (2020) that fix treatment probabilities at $0$, $1/2$, and $1$. We illustrate our optimal designs with data from Head Start, an early childhood government intervention program."
http://arxiv.org/abs/2201.13004v5,Improving Estimation Efficiency via Regression-Adjustment in Covariate-Adaptive Randomizations with Imperfect Compliance,2022-01-31 05:37:28+00:00,"['Liang Jiang', 'Oliver B. Linton', 'Haihan Tang', 'Yichong Zhang']",econ.EM,"We investigate how to improve efficiency using regression adjustments with covariates in covariate-adaptive randomizations (CARs) with imperfect subject compliance. Our regression-adjusted estimators, which are based on the doubly robust moment for local average treatment effects, are consistent and asymptotically normal even with heterogeneous probability of assignment and misspecified regression adjustments. We propose an optimal but potentially misspecified linear adjustment and its further improvement via a nonlinear adjustment, both of which lead to more efficient estimators than the one without adjustments. We also provide conditions for nonparametric and regularized adjustments to achieve the semiparametric efficiency bound under CARs."
http://arxiv.org/abs/2202.12078v1,Confidence Intervals of Treatment Effects in Panel Data Models with Interactive Fixed Effects,2022-02-24 13:02:59+00:00,"['Xingyu Li', 'Yan Shen', 'Qiankun Zhou']",econ.EM,"We consider the construction of confidence intervals for treatment effects estimated using panel models with interactive fixed effects. We first use the factor-based matrix completion technique proposed by Bai and Ng (2021) to estimate the treatment effects, and then use bootstrap method to construct confidence intervals of the treatment effects for treated units at each post-treatment period. Our construction of confidence intervals requires neither specific distributional assumptions on the error terms nor large number of post-treatment periods. We also establish the validity of the proposed bootstrap procedure that these confidence intervals have asymptotically correct coverage probabilities. Simulation studies show that these confidence intervals have satisfactory finite sample performances, and empirical applications using classical datasets yield treatment effect estimates of similar magnitudes and reliable confidence intervals."
http://arxiv.org/abs/2202.11043v1,Differentially Private Estimation of Heterogeneous Causal Effects,2022-02-22 17:21:18+00:00,"['Fengshi Niu', 'Harsha Nori', 'Brian Quistorff', 'Rich Caruana', 'Donald Ngwe', 'Aadharsh Kannan']",stat.ML,"Estimating heterogeneous treatment effects in domains such as healthcare or social science often involves sensitive data where protecting privacy is important. We introduce a general meta-algorithm for estimating conditional average treatment effects (CATE) with differential privacy (DP) guarantees. Our meta-algorithm can work with simple, single-stage CATE estimators such as S-learner and more complex multi-stage estimators such as DR and R-learner. We perform a tight privacy analysis by taking advantage of sample splitting in our meta-algorithm and the parallel composition property of differential privacy. In this paper, we implement our approach using DP-EBMs as the base learner. DP-EBMs are interpretable, high-accuracy models with privacy guarantees, which allow us to directly observe the impact of DP noise on the learned causal model. Our experiments show that multi-stage CATE estimators incur larger accuracy loss than single-stage CATE or ATE estimators and that most of the accuracy loss from differential privacy is due to an increase in variance, not biased estimates of treatment effects."
http://arxiv.org/abs/2202.10347v1,The geographic proximity effect on domestic cross-sector vis-a-vis intra-sector research collaborations,2022-02-18 15:58:33+00:00,"['Giovanni Abramo', 'Francesca Apponi', ""Ciriaco Andrea D'Angelo""]",econ.GN,"Geographic proximity is acknowledged to be a key factor in research collaborations. Specifically, it can work as a possible substitute for institutional proximity. The present study investigates the relevance of the ""proximity"" effect for different types of national research collaborations. We apply a bibliometric approach based on the Italian 2010-2017 scientific production indexed in the Web of Science. On such dataset, we apply statistical tools for analyzing if and to what extent geographical distance between co-authors in the byline of a publication varies across collaboration types, scientific disciplines, and along time. Results can inform policies aimed at effectively stimulating cross-sector collaborations, and also bear direct practical implications for research performance assessments."
http://arxiv.org/abs/2202.10122v2,HCMD-zero: Learning Value Aligned Mechanisms from Data,2022-02-21 11:13:53+00:00,"['Jan Balaguer', 'Raphael Koster', 'Ari Weinstein', 'Lucy Campbell-Gillingham', 'Christopher Summerfield', 'Matthew Botvinick', 'Andrea Tacchetti']",cs.MA,"Artificial learning agents are mediating a larger and larger number of interactions among humans, firms, and organizations, and the intersection between mechanism design and machine learning has been heavily investigated in recent years. However, mechanism design methods often make strong assumptions on how participants behave (e.g. rationality), on the kind of knowledge designers have access to a priori (e.g. access to strong baseline mechanisms), or on what the goal of the mechanism should be (e.g. total welfare). Here we introduce HCMD-zero, a general purpose method to construct mechanisms making none of these three assumptions. HCMD-zero learns to mediate interactions among participants and adjusts the mechanism parameters to make itself more likely to be preferred by participants. It does so by remaining engaged in an electoral contest with copies of itself, thereby accessing direct feedback from participants. We test our method on a stylized resource allocation game that highlights the tension between productivity, equality and the temptation to free ride. HCMD-zero produces a mechanism that is preferred by human participants over a strong baseline, it does so automatically, without requiring prior knowledge, and using human behavioral trajectories sparingly and effectively. Our analysis shows HCMD-zero consistently makes the mechanism policy more and more likely to be preferred by human participants over the course of training, and that it results in a mechanism with an interpretable and intuitive policy."
http://arxiv.org/abs/2201.00776v1,"Observability, Dominance, and Induction in Learning Models",2022-01-03 17:45:51+00:00,"['Daniel Clark', 'Drew Fudenberg', 'Kevin He']",econ.TH,"Learning models do not in general imply that weakly dominated strategies are irrelevant or justify the related concept of ""forward induction,"" because rational agents may use dominated strategies as experiments to learn how opponents play, and may not have enough data to rule out a strategy that opponents never use. Learning models also do not support the idea that the selected equilibria should only depend on a game's normal form, even though two games with the same normal form present players with the same decision problems given fixed beliefs about how others play. However, playing the extensive form of a game is equivalent to playing the normal form augmented with the appropriate terminal node partitions so that two games are information equivalent, i.e., the players receive the same feedback about others' strategies."
http://arxiv.org/abs/2201.00161v1,On income inequality and population size,2022-01-01 10:37:34+00:00,"['Thitithep Sitthiyot', 'Kanyarat Holasut']",econ.GN,"The pursuit of having an appropriate level of income inequality should be viewed as one of the biggest challenges facing academic scholars as well as policy makers. Unfortunately, research on this issue is currently lacking. This study is the first to introduce the theoretical concept of targeted level of income inequality for a given size of population. By employing the World Bank's data on population size and Gini coefficient from sixty-nine countries in 2012, this study finds that the relationship between Gini coefficient and natural logarithm of population size is nonlinear in the form of a second-degree polynomial function. The estimated results using regression analysis show that the majority of countries in the sample have Gini coefficients either too high or too low compared to their appropriate values. These findings could be used as a guideline for policy makers before designing and implementing public policies in order to achieve the targeted level of income inequality."
http://arxiv.org/abs/2201.00578v1,'Moving On' -- Investigating Inventors' Ethnic Origins Using Supervised Learning,2022-01-03 10:47:47+00:00,['Matthias Niggli'],econ.GN,"Patent data provides rich information about technical inventions, but does not disclose the ethnic origin of inventors. In this paper, I use supervised learning techniques to infer this information. To do so, I construct a dataset of 95'202 labeled names and train an artificial recurrent neural network with long-short-term memory (LSTM) to predict ethnic origins based on names. The trained network achieves an overall performance of 91% across 17 ethnic origins. I use this model to classify and investigate the ethnic origins of 2.68 million inventors and provide novel descriptive evidence regarding their ethnic origin composition over time and across countries and technological fields. The global ethnic origin composition has become more diverse over the last decades, which was mostly due to a relative increase of Asian origin inventors. Furthermore, the prevalence of foreign-origin inventors is especially high in the USA, but has also increased in other high-income economies. This increase was mainly driven by an inflow of non-western inventors into emerging high-technology fields for the USA, but not for other high-income countries."
http://arxiv.org/abs/2201.00274v1,COVID Lessons: Was there any way to reduce the negative effect of COVID-19 on the United States economy?,2022-01-02 02:25:34+00:00,['Mohammadreza Mahmoudi'],econ.GN,"This paper aims to study the economic impact of COVID-19. To do that, in the first step, I showed that the adjusted SEQIER model, which is a generalization form of SEIR model, is a good fit to the real COVID-induced daily death data in a way that it could capture the nonlinearities of the data very well. Then, I used this model with extra parameters to evaluate the economic effect of COVID-19 through job market. The results show that there was a simple strategy that US government could implemented in order to reduce the negative effect of COVID-19. Because of that the answer to the paper's title is yes. If lockdown policies consider the heterogenous characteristics of population and impose more restrictions on old people and control the interactions between them and the rest of population the devastating impact of COVID-19 on people lives and US economy reduced dramatically. Specifically, based on this paper's results, this strategy could reduce the death rate and GDP loss of the United States 0.03 percent and 2 percent respectively. By comparing these results with actual data which show death rate and GDP loss 0.1 percent and 3.5 percent respectively, we could figure out that death rate reduction is 0.07 percent which means for the same percent of GDP loss executing optimal targeted policy could save 2/3 lives. Approximately, 378,000 persons dead because of COVID-19 during 2020, hence reducing death rate to 0.03 percent means saving around 280,000 lives, which is huge."
http://arxiv.org/abs/2201.01398v1,Influence of trip distance and population density on intra-city mobility patterns in Tokyo during COVID-19 pandemic,2022-01-05 00:53:06+00:00,"['Kazufumi Tsuboi', 'Naoya Fujiwara', 'Ryo Itoh']",econ.GN,"This study investigates the influence of infection cases of COVID-19 and two non-compulsory lockdowns on human mobility within the Tokyo metropolitan area. Using the data of hourly staying population in each 500m$\times$500m cell and their city-level residency, we show that long-distance trips or trips to crowded places decrease significantly when infection cases increase. The same result holds for the two lockdowns, although the second lockdown was less effective. Hence, Japanese non-compulsory lockdowns influence mobility in a similar way to the increase in infection cases. This means that they are accepted as alarm triggers for people who are at risk of contracting COVID-19."
http://arxiv.org/abs/2201.01160v1,The financial value of the within-government political network: Evidence from Chinese municipal corporate bonds,2022-01-04 14:50:42+00:00,"['Jaehyuk Choi', 'Lei Lu', 'Heungju Park', 'Sungbin Sohn']",q-fin.GN,"This paper examines the effect of the political network of Chinese municipal leaders on the pricing of municipal corporate bonds. Using municipal leaders' working experience to measure the political network, we find that this network reduces the bond issuance yield spreads by improving the credit ratings of the issuer, the local government financing vehicle. The relationship between political networks and issuance yield spreads is strengthened in areas where financial markets and legal systems are less developed."
http://arxiv.org/abs/2201.11441v1,Human-centered mechanism design with Democratic AI,2022-01-27 10:56:33+00:00,"['Raphael Koster', 'Jan Balaguer', 'Andrea Tacchetti', 'Ari Weinstein', 'Tina Zhu', 'Oliver Hauser', 'Duncan Williams', 'Lucy Campbell-Gillingham', 'Phoebe Thacker', 'Matthew Botvinick', 'Christopher Summerfield']",cs.AI,"Building artificial intelligence (AI) that aligns with human values is an unsolved problem. Here, we developed a human-in-the-loop research pipeline called Democratic AI, in which reinforcement learning is used to design a social mechanism that humans prefer by majority. A large group of humans played an online investment game that involved deciding whether to keep a monetary endowment or to share it with others for collective benefit. Shared revenue was returned to players under two different redistribution mechanisms, one designed by the AI and the other by humans. The AI discovered a mechanism that redressed initial wealth imbalance, sanctioned free riders, and successfully won the majority vote. By optimizing for human preferences, Democratic AI may be a promising method for value-aligned policy innovation."
http://arxiv.org/abs/2202.08366v2,Preference Learning in School Choice Problems,2022-02-16 22:48:21+00:00,['SangMok Lee'],econ.TH,"In school choice, students make decisions based on their expectations of particular schools' suitability, and the decision to gather information about schools is influenced by the acceptance odds determined by the mechanism in place. We study a school choice model where students can obtain information about their preferences by incurring a cost. We demonstrate greater homogeneity in rank-order reports and reduced information acquisition under the Deferred-Acceptance (DA) mechanism, resulting in an increased reliance on random tie-breaking and ultimately inefficient outcomes. Thus, it is critical for the DA mechanism to have easy access to school information in order to maintain its efficiency."
http://arxiv.org/abs/2202.08415v2,Continuity Postulates and Solvability Axioms in Economic Theory and in Mathematical Psychology: A Consolidation of the Theory of Individual Choice,2022-02-17 02:33:12+00:00,"['Aniruddha Ghosh', 'M. Ali Khan', 'Metin Uyanik']",econ.TH,"This paper presents four theorems that connect continuity postulates in mathematical economics to solvability axioms in mathematical psychology, and ranks them under alternative supplementary assumptions. Theorem 1 connects notions of continuity (full, separate, Wold, weak Wold, Archimedean, mixture) with those of solvability (restricted, unrestricted) under the completeness and transitivity of a binary relation. Theorem 2 uses the primitive notion of a separately-continuous function to answer the question when an analogous property on a relation is fully continuous. Theorem 3 provides a portmanteau theorem on the equivalence between restricted solvability and various notions of continuity under weak monotonicity. Finally, Theorem 4 presents a variant of Theorem 3 that follows Theorem 1 in dispensing with the dimensionality requirement and in providing partial equivalences between solvability and continuity notions. These theorems are motivated for their potential use in representation theorems."
http://arxiv.org/abs/2202.08822v1,Objectives of platform research: A co-citation and systematic literature review analysis,2022-02-17 18:45:28+00:00,"['Fabian Schueler', 'Dimitri Petrik']",econ.GN,"Business economics research on digital platforms often overlooks existing knowledge from other fields of research leading to conceptual ambiguity and inconsistent findings. To reduce these restrictions and foster the utilization of the extensive body of literature, we apply a mixed methods design to summarize the key findings of scientific platform research. Our bibliometric analysis identifies 14 platform-related research fields. Conducting a systematic qualitative content analysis, we identify three primary research objectives related to platform ecosystems: (1) general literature defining and unifying research on platforms; (2) exploitation of platform and ecosystem strategies; (3) improvement of platforms and ecosystems. Finally, we discuss the identified insights from a business economics perspective and present promising future research directions that could enhance business economics and management research on digital platforms and platform ecosystems."
http://arxiv.org/abs/2202.09391v1,Counterfactual Analysis of the Impact of the IMF Program on Child Poverty in the Global-South Region using Causal-Graphical Normalizing Flows,2022-02-17 12:18:14+00:00,"['Sourabh Balgi', 'Jose M. Peña', 'Adel Daoud']",cs.AI,"This work demonstrates the application of a particular branch of causal inference and deep learning models: \emph{causal-Graphical Normalizing Flows (c-GNFs)}. In a recent contribution, scholars showed that normalizing flows carry certain properties, making them particularly suitable for causal and counterfactual analysis. However, c-GNFs have only been tested in a simulated data setting and no contribution to date have evaluated the application of c-GNFs on large-scale real-world data. Focusing on the \emph{AI for social good}, our study provides a counterfactual analysis of the impact of the International Monetary Fund (IMF) program on child poverty using c-GNFs. The analysis relies on a large-scale real-world observational data: 1,941,734 children under the age of 18, cared for by 567,344 families residing in the 67 countries from the Global-South. While the primary objective of the IMF is to support governments in achieving economic stability, our results find that an IMF program reduces child poverty as a positive side-effect by about 1.2$\pm$0.24 degree (`0' equals no poverty and `7' is maximum poverty). Thus, our article shows how c-GNFs further the use of deep learning and causal inference in AI for social good. It shows how learning algorithms can be used for addressing the untapped potential for a significant social impact through counterfactual inference at population level (ACE), sub-population level (CACE), and individual level (ICE). In contrast to most works that model ACE or CACE but not ICE, c-GNFs enable personalization using \emph{`The First Law of Causal Inference'}."
http://arxiv.org/abs/2201.12618v1,The effect of the pandemic on complex socio-economic systems: community detection induced by communicability,2022-01-29 17:04:27+00:00,"['Gian Paolo Clemente', 'Rosanna Grassi', 'Giorgio Rizzini']",econ.GN,"The increasing complexity of interrelated systems has made the use of multiplex networks an important tool for explaining the nature of relations between elements in the system. In this paper, we aim at investigating various aspects of countries' behaviour during the coronavirus pandemic period. By means of a multiplex network we consider simultaneously stringency index values, COVID-19 infections and international trade data, in order to detect clusters of countries that showed a similar reaction to the pandemic. We propose a new methodological approach based on the Estrada communicability for identifying communities on a multiplex network, based on a two-step optimization. At first, we determine the optimal inter-layer intensity between levels by minimizing a distance function. Hence, the optimal inter-layer intensity is used to detect communities on each layer. Our findings show that the community detection on this multiplex network has greater information power than classical methods for single-layer networks. Our approach better reveals clusters on each layer with respect to the application of the same approach on each single-layer. Moreover, detected groups in the multiplex case benefit of a higher cohesion, leading to identifying on each layer a lower number of communities with respect to the ones obtained in the single-layer cases."
http://arxiv.org/abs/2201.12619v1,Negotiation problem,2022-01-29 17:06:45+00:00,"['Izat B. Baybusinov', 'Enrico Maria Fenoaltea', 'Yi-Cheng Zhang']",physics.soc-ph,"We propose and solve a negotiation model of multiple players facing many alternative solutions. The model can be generalized to many relevant circumstances where stakeholders' interests partially overlap and partially oppose. We also show that the model can be mapped into the well-known directed percolation and directed polymers problems. Moreover, many statistical mechanics tools, such as the Replica method, can be fruitfully employed. Studying our negotiation model can enlighten the links between social-economic phenomena and traditional statistical mechanics and help to develop new perspectives and tools in the fertile interdisciplinary field."
http://arxiv.org/abs/2202.06245v3,Reduced-Form Allocations with Complementarity: A 2-Person Case,2022-02-13 07:52:06+00:00,['Xu Lang'],econ.TH,"We investigate the implementation of reduced-form allocation probabilities in a two-person bargaining problem without side payments, where the agents have to select one alternative from a finite set of social alternatives. We provide a necessary and sufficient condition for the implementability. We find that the implementability condition in bargaining has some new feature compared to Border's theorem. Our results have applications in compromise problems and package exchange problems where the agents barter indivisible objects and the agents value the objects as complements."
http://arxiv.org/abs/2202.05743v1,Inflation and income inequality: Does the level of income inequality matter?,2022-02-11 16:32:22+00:00,"['Edmond Berisha', 'Ram Sewak Dubey', 'Orkideh Gharehgozli']",econ.GN,"In the recent times of global Covid pandemic, the Federal Reserve has raised the concerns of upsurges in prices. Given the complexity of interaction between inflation and inequality, we examine whether the impact of inflation on inequality differs among distinct levels of income inequality across the US states. Results reveal that there is a negative contemporaneous effect of inflation on the inequality which becomes stronger with higher levels of income inequality. However, over a one year period, we find higher inflation rate to further increase income inequality only when income inequality is initially relatively low."
http://arxiv.org/abs/2202.05789v5,A constraint on the dynamics of wealth concentration,2022-02-11 17:35:09+00:00,['Valerio Astuti'],econ.TH,"In the context of a large class of stochastic processes used to describe the dynamics of wealth growth, we prove a set of inequalities establishing necessary and sufficient conditions in order to avoid infinite wealth concentration. These inequalities generalize results previously found only in the context of particular models, or with more restrictive sets of hypotheses. In particular, we emphasize the role of the additive component of growth - usually representing labor incomes - in limiting the growth of inequality. Our main result is a proof that in an economy with random wealth growth, with returns non-negatively correlated with wealth, an average labor income growing at least proportionally to the average wealth is necessary to avoid a runaway concentration. One of the main advantages of this result with respect to the standard economics literature is the independence from the concept of an equilibrium wealth distribution, which does not always exist in random growth models. We analyze in this light three toy models, widely studied in the economics and econophysics literature."
http://arxiv.org/abs/2202.05885v1,Equilibrium Defaultable Corporate Debt and Investment,2022-02-11 19:56:54+00:00,"['Hong Chen', 'Murray Zed Frank']",q-fin.GN,"In dynamic capital structure models with an investor break-even condition, the firm's Bellman equation may not generate a contraction mapping, so the standard existence and uniqueness conditions do not apply. First, we provide an example showing the problem in a classical trade-off model. The firm can issue one-period defaultable debt, invest in capital and pay a dividend. If the firm cannot meet the required debt payment, it is liquidated. Second, we show how to use a dual to the original problem and a change of measure, such that existence and uniqueness can be proved. In the unique Markov-perfect equilibrium, firm decisions reflect state-dependent capital and debt targets. Our approach may be useful for other dynamic firm models that have an investor break-even condition."
http://arxiv.org/abs/2202.05186v5,Fair allocation of a multiset of indivisible items,2022-02-10 17:40:58+00:00,"['Pranay Gorantla', 'Kunal Marwaha', 'Santhoshini Velusamy']",cs.GT,"We study the problem of fairly allocating a multiset $M$ of $m$ indivisible items among $n$ agents with additive valuations. Specifically, we introduce a parameter $t$ for the number of distinct types of items and study fair allocations of multisets that contain only items of these $t$ types, under two standard notions of fairness:
  1. Envy-freeness (EF): For arbitrary $n$, $t$, we show that a complete EF allocation exists when at least one agent has a unique valuation and the number of items of each type exceeds a particular finite threshold. We give explicit upper and lower bounds on this threshold in some special cases.
  2. Envy-freeness up to any good (EFX): For arbitrary $n$, $m$, and for $t\le 2$, we show that a complete EFX allocation always exists. We give two different proofs of this result. One proof is constructive and runs in polynomial time; the other is geometrically inspired."
http://arxiv.org/abs/2202.05946v5,Artificial Intelligence and Spontaneous Collusion,2022-02-12 00:50:15+00:00,"['Martino Banchio', 'Giacomo Mantegazza']",econ.TH,"We develop a tractable model for studying strategic interactions between learning algorithms. We uncover a mechanism responsible for the emergence of algorithmic collusion. We observe that algorithms periodically coordinate on actions that are more profitable than static Nash equilibria. This novel collusive channel relies on an endogenous statistical linkage in the algorithms' estimates which we call spontaneous coupling. The model's parameters predict whether the statistical linkage will appear, and what market structures facilitate algorithmic collusion. We show that spontaneous coupling can sustain collusion in prices and market shares, complementing experimental findings in the literature. Finally, we apply our results to design algorithmic markets."
http://arxiv.org/abs/2202.05984v3,scpi: Uncertainty Quantification for Synthetic Control Methods,2022-02-12 04:54:39+00:00,"['Matias D. Cattaneo', 'Yingjie Feng', 'Filippo Palomba', 'Rocio Titiunik']",stat.ME,"The synthetic control method offers a way to quantify the effect of an intervention using weighted averages of untreated units to approximate the counterfactual outcome that the treated unit(s) would have experienced in the absence of the intervention. This method is useful for program evaluation and causal inference in observational studies. We introduce the software package scpi for prediction and inference using synthetic controls, implemented in Python, R, and Stata. For point estimation or prediction of treatment effects, the package offers an array of (possibly penalized) approaches leveraging the latest optimization methods. For uncertainty quantification, the package offers the prediction interval methods introduced by Cattaneo, Feng and Titiunik (2021) and Cattaneo, Feng, Palomba and Titiunik (2022). The paper includes numerical illustrations and a comparison with other synthetic control software."
http://arxiv.org/abs/2202.05229v1,How rare are the properties of binary relations?,2022-02-10 18:36:38+00:00,"['Ram Sewak Dubey', 'Giorgio Laguzzi']",econ.TH,"Knoblauch (2014) and Knoblauch (2015) investigate the relative size of the collection of binary relations with desirable features as compared to the set of all binary relations using symmetric difference metric (Cantor) topology and Hausdorff metric topology. We consider Ellentuck and doughnut topologies to further this line of investigation. We report the differences among the size of the useful binary relations in Cantor, Ellentuck and doughnut topologies. It turns out that the doughnut topology admits binary relations with more general properties in contrast to the other two. We further prove that among the induced Cantor and Ellentuck topologies, the latter captures the relative size of partial orders among the collection of all quasi-orders. Finally we show that the class of ethical binary relations is small in Ellentuck (and therefore in Cantor) topology but is not small in doughnut topology. In essence, the Ellentuck topology fares better compared to Cantor topology in capturing the relative size of collections of binary relations."
http://arxiv.org/abs/2202.05339v2,Closure operators: Complexity and applications to classification and decision-making,2022-02-10 21:39:42+00:00,"['Hamed Hamze Bajgiran', 'Federico Echenique']",econ.TH,"We study the complexity of closure operators, with applications to machine learning and decision theory. In machine learning, closure operators emerge naturally in data classification and clustering. In decision theory, they can model equivalence of choice menus, and therefore situations with a preference for flexibility. Our contribution is to formulate a notion of complexity of closure operators, which translate into the complexity of a classifier in ML, or of a utility function in decision theory."
http://arxiv.org/abs/2202.07300v2,Choosing an algorithmic fairness metric for an online marketplace: Detecting and quantifying algorithmic bias on LinkedIn,2022-02-15 10:33:30+00:00,"['YinYin Yu', 'Guillaume Saint-Jacques']",econ.GN,"In this paper, we derive an algorithmic fairness metric from the fairness notion of equal opportunity for equally qualified candidates for recommendation algorithms commonly used by two-sided marketplaces. We borrow from the economic literature on discrimination to arrive at a test for detecting bias that is solely attributable to the algorithm, as opposed to other sources such as societal inequality or human bias on the part of platform users. We use the proposed method to measure and quantify algorithmic bias with respect to gender of two algorithms used by LinkedIn, a popular online platform used by job seekers and employers. Moreover, we introduce a framework and the rationale for distinguishing algorithmic bias from human bias, both of which can potentially exist on a two-sided platform where algorithms make recommendations to human users. Finally, we discuss the shortcomings of a few other common algorithmic fairness metrics and why they do not capture the fairness notion of equal opportunity for equally qualified candidates."
http://arxiv.org/abs/2203.07663v1,Gender differences of the effect of vaccination on perceptions of COVID-19 and mental health in Japan,2022-03-15 05:57:01+00:00,"['Eiji Yamamura', 'Youki Kosaka', 'Yoshiro Tsutsui', 'Fumio Ohtake']",econ.GN,"Vaccination has been promoted to mitigate the spread of the coronavirus disease 2019 (COVID-19). Vaccination is expected to reduce the probability of and alleviate the seriousness of COVID-19 infection. Accordingly, this might significantly change an individuals subjective well-being and mental health. However, it is unknown how vaccinated people perceive the effectiveness of COVID-19 and how their subjective well-being and mental health change after vaccination. We thus observed the same individuals on a monthly basis from March 2020 to September 2021 in all parts of Japan. Then, large sample panel data (N=54,007) were independently constructed. Using the data, we compared the individuals perceptions of COVID-19, subjective well-being, and mental health before and after vaccination. Furthermore, we compared the effect of vaccination on the perceptions of COVID-19 and mental health for females and males. We used the fixed-effects model to control for individual time-invariant characteristics. The major findings were as follows: First, the vaccinated people perceived the probability of getting infected and the seriousness of COVID-19 to be lower than before vaccination. This was observed not only when we used the whole sample, but also when we used sub-samples. Second, using the whole sample, subjective well-being and mental health improved. The same results were also observed using the sub-sample of females, whereas the improvements were not observed using a sub-sample of males."
http://arxiv.org/abs/2203.09118v1,Time and the Value of Data,2022-03-17 06:53:46+00:00,"['Ehsan Valavi', 'Joel Hestness', 'Newsha Ardalani', 'Marco Iansiti']",cs.LG,"Managers often believe that collecting more data will continually improve the accuracy of their machine learning models. However, we argue in this paper that when data lose relevance over time, it may be optimal to collect a limited amount of recent data instead of keeping around an infinite supply of older (less relevant) data. In addition, we argue that increasing the stock of data by including older datasets may, in fact, damage the model's accuracy. Expectedly, the model's accuracy improves by increasing the flow of data (defined as data collection rate); however, it requires other tradeoffs in terms of refreshing or retraining machine learning models more frequently.
  Using these results, we investigate how the business value created by machine learning models scales with data and when the stock of data establishes a sustainable competitive advantage. We argue that data's time-dependency weakens the barrier to entry that the stock of data creates. As a result, a competing firm equipped with a limited (yet sufficient) amount of recent data can develop more accurate models. This result, coupled with the fact that older datasets may deteriorate models' accuracy, suggests that created business value doesn't scale with the stock of available data unless the firm offloads less relevant data from its data repository. Consequently, a firm's growth policy should incorporate a balance between the stock of historical data and the flow of new data.
  We complement our theoretical results with an experiment. In the experiment, we empirically measure the loss in the accuracy of a next word prediction model trained on datasets from various time periods. Our empirical measurements confirm the economic significance of the value decline over time. For example, 100MB of text data, after seven years, becomes as valuable as 50MB of current data for the next word prediction task."
http://arxiv.org/abs/2203.09128v1,"Time Dependency, Data Flow, and Competitive Advantage",2022-03-17 07:09:30+00:00,"['Ehsan Valavi', 'Joel Hestness', 'Marco Iansiti', 'Newsha Ardalani', 'Feng Zhu', 'Karim R. Lakhani']",cs.LG,"Data is fundamental to machine learning-based products and services and is considered strategic due to its externalities for businesses, governments, non-profits, and more generally for society. It is renowned that the value of organizations (businesses, government agencies and programs, and even industries) scales with the volume of available data. What is often less appreciated is that the data value in making useful organizational predictions will range widely and is prominently a function of data characteristics and underlying algorithms.
  In this research, our goal is to study how the value of data changes over time and how this change varies across contexts and business areas (e.g. next word prediction in the context of history, sports, politics). We focus on data from Reddit.com and compare the value's time-dependency across various Reddit topics (Subreddits). We make this comparison by measuring the rate at which user-generated text data loses its relevance to the algorithmic prediction of conversations. We show that different subreddits have different rates of relevance decline over time.
  Relating the text topics to various business areas of interest, we argue that competing in a business area in which data value decays rapidly alters strategies to acquire competitive advantage. When data value decays rapidly, access to a continuous flow of data will be more valuable than access to a fixed stock of data. In this kind of setting, improving user engagement and increasing user-base help creating and maintaining a competitive advantage."
http://arxiv.org/abs/2203.09157v1,Dynamic groups in complex task environments: To change or not to change a winning team?,2022-03-17 08:26:00+00:00,"['Darío Blanco-Fernández', 'Stephan Leitner', 'Alexandra Rausch']",econ.GN,"Organisations rely upon group formation to solve complex tasks, and groups often adapt to the demands of the task they face by changing their composition periodically. Previous research comes to ambiguous results regarding the effects of group adaptation on task performance. This paper aims to understand the impact of group adaptation, defined as a process of periodically changing a group's composition, on complex task performance and considers the moderating role of individual learning and task complexity in this relationship. We base our analyses on an agent-based model of adaptive groups in a complex task environment based on the NK-framework. The results indicate that reorganising well-performing groups might be beneficial, but only if individual learning is restricted. However, there are also cases in which group adaptation might unfold adverse effects. We provide extensive analyses that shed additional light on and, thereby, help explain the ambiguous results of previous research."
http://arxiv.org/abs/2203.09972v1,Cournot duopoly games with isoelastic demands and diseconomies of scale,2022-03-18 14:07:47+00:00,['Xiaoliang Li'],econ.TH,"In this discussion draft, we investigate five different models of duopoly games, where the market is assumed to have an isoelastic demand function. Moreover, quadratic cost functions reflecting decreasing returns to scale are considered. The games in this draft are formulated with systems of two nonlinear difference equations. Existing equilibria and their local stability are analyzed by symbolic computations. In the model where a gradiently adjusting player and a rational (or a boundedly rational) player compete with each other, diseconomies of scale are proved to have an effect of stability enhancement, which is consistent with the similar results found by Fisher for homogeneous oligopolies with linear demand functions."
http://arxiv.org/abs/2203.16405v1,"Labour by Design: Contributions of David Card, Joshua Angrist, and Guido Imbens",2022-03-30 15:37:05+00:00,"['Peter Hull', 'Michal Kolesár', 'Christopher Walters']",econ.GN,"The 2021 Sveriges Riksbank Prize in Economic Sciences in Memory of Alfred Nobel was awarded to David Card ""for his empirical contributions to labour economics"" and to Joshua Angrist and Guido Imbens ""for their methodological contributions to the analysis of causal relationships."" We survey these contributions of the three laureates, and discuss how their empirical and methodological insights transformed the modern practice of applied microeconomics. By emphasizing research design and formalizing the causal content of different econometric procedures, the laureates shed new light on key questions in labour economics and advanced a robust toolkit for empirical analyses across many fields."
http://arxiv.org/abs/2203.06279v1,Synthetic Controls in Action,2022-03-11 23:07:34+00:00,"['Alberto Abadie', 'Jaume Vives-i-Bastida']",stat.ME,"In this article we propose a set of simple principles to guide empirical practice in synthetic control studies. The proposed principles follow from formal properties of synthetic control estimators, and pertain to the nature, implications, and prevention of over-fitting biases within a synthetic control framework, to the interpretability of the results, and to the availability of validation exercises. We discuss and visually demonstrate the relevance of the proposed principles under a variety of data configurations."
http://arxiv.org/abs/2203.04418v1,A class of dissimilarity semimetrics for preference relations,2022-03-08 21:48:43+00:00,"['Hiroki Nishimura', 'Efe A. Ok']",math.CO,"We propose a class of semimetrics for preference relations any one of which is an alternative to the classical Kemeny-Snell-Bogart metric. (We take a fairly general viewpoint about what constitutes a preference relation, allowing for any acyclic order to act as one.) These semimetrics are based solely on the implications of preferences for choice behavior, and thus appear more suitable in economic contexts and choice experiments. In our main result, we obtain a fairly simple axiomatic characterization for the class we propose. The apparently most important member of this class (at least in the case of finite alternative spaces), which we dub the top-difference semimetric, is characterized separately. We also obtain alternative formulae for it, and relative to this metric, compute the diameter of the space of complete preferences, as well as the best transitive extension of a given acyclic preference relation. Finally, we prove that our preference metric spaces cannot be isometically embedded in a Euclidean space."
http://arxiv.org/abs/2203.10595v4,On the Fragility of the Basis on the Hamilton-Jacobi-Bellman Equation in Economic Dynamics,2022-03-20 16:41:07+00:00,['Yuhki Hosoya'],econ.TH,"In this paper, we provide an example of the optimal growth model in which there exist infinitely many solutions to the Hamilton-Jacobi-Bellman equation but the value function does not satisfy this equation. We consider the cause of this phenomenon, and find that the lack of a solution to the original problem is crucial. We show that under several conditions, there exists a solution to the original problem if and only if the value function solves the Hamilton-Jacobi-Bellman equation. Moreover, in this case, the value function is the unique nondecreasing concave solution to the Hamilton-Jacobi-Bellman equation. We also show that without our conditions, this uniqueness result does not hold."
http://arxiv.org/abs/2203.08933v1,The Digital Divide in Canada and the Role of LEO Satellites in Bridging the Gap,2022-03-16 20:39:49+00:00,"['Tuheen Ahmmed', 'Afsoon Alidadi', 'Zichao Zhang', 'Aizaz U. Chaudhry', 'Halim Yanikomeroglu']",eess.SP,"Overcoming the digital divide in rural and remote areas has always been a big challenge for Canada with its huge geographical area. In 2016, the Canadian Radio-television and Telecommunications Commission announced broadband Internet as a basic service available for all Canadians. However, approximately one million Canadians still do not have access to broadband services as of 2020. The COVID-19 pandemic has made the situation more challenging, as social, economic, and educational activities have increasingly been transferred online. The condition is more unfavorable for Indigenous communities. A key challenge in deploying rural and remote broadband Internet is to plan and implement high-capacity backbones, which are now available only in denser urban areas. For any Internet provider, it is almost impossible to make a viable business proposal in these areas. For example, the vast land of the Northwest Territories, Yukon, and Nunavuts diverse geographical features present obstacles for broadband infrastructure. In this paper, we investigate the digital divide in Canada with a focus on rural and remote areas. In so doing, we highlight two potential solutions using low Earth orbit (LEO) constellations to deliver broadband Internet in rural and remote areas to address the access inequality and the digital divide. The first solution involves integrating LEO constellations as a backbone for the existing 4G/5G telecommunications network. This solution uses satellites in a LEO constellation to provide a backhaul network connecting the 4G/5G access network to its core network. The 3rd Generation Partnership Project already specifies how to integrate LEO satellite networks into the 4G/5G network, and the Canadian satellite operator Telesat has already showcased this solution with one terrestrial operator, TIM Brasil, in their 4G network."
http://arxiv.org/abs/2203.12842v5,Financial statements of companies in Norway,2022-03-24 04:10:02+00:00,['Ranik Raaen Wahlstrøm'],econ.GN,This document details a dataset that contains all unconsolidated annual financial statements of the universe of Norwegian private and public limited liability companies. It also includes all financial statements of other company types reported to the Norwegian authorities.
http://arxiv.org/abs/2202.10378v3,Selling to a principal and a budget-constrained agent,2022-02-21 17:15:10+00:00,"['Debasis Mishra', 'Kolagani Paramahamsa']",econ.TH,"We analyze a model of selling a single object to a principal-agent pair who want to acquire the object for a firm. The principal and the agent have different assessments of the object's value to the firm. The agent is budget-constrained while the principal is not. The agent participates in the mechanism, but she can (strategically) delegate decision-making to the principal. We derive the revenue-maximizing mechanism in a two-dimensional type space (values of the agent and the principal). We show that below a threshold budget, a mechanism involving two posted prices and three outcomes (one of which involves randomization) is the optimal mechanism for the seller. Otherwise, a single posted price mechanism is optimal."
http://arxiv.org/abs/2203.03497v5,Inference in Linear Dyadic Data Models with Network Spillovers,2022-03-07 16:18:15+00:00,"['Nathan Canen', 'Ko Sugiura']",econ.EM,"When using dyadic data (i.e., data indexed by pairs of units), researchers typically assume a linear model, estimate it using Ordinary Least Squares and conduct inference using ``dyadic-robust"" variance estimators. The latter assumes that dyads are uncorrelated if they do not share a common unit (e.g., if the same individual is not present in both pairs of data). We show that this assumption does not hold in many empirical applications because indirect links may exist due to network connections, generating correlated outcomes. Hence, ``dyadic-robust'' estimators can be biased in such situations. We develop a consistent variance estimator for such contexts by leveraging results in network statistics. Our estimator has good finite sample properties in simulations, while allowing for decay in spillover effects. We illustrate our message with an application to politicians' voting behavior when they are seating neighbors in the European Parliament."
http://arxiv.org/abs/2201.11051v2,Toward a More Populous Online Platform: The Economic Impacts of Compensated Reviews,2022-01-26 16:45:02+00:00,"['Peng Li', 'Arim Park', 'Soohyun Cho', 'Yao Zhao']",econ.GN,"Many companies nowadays offer compensation to online reviews (called compensated reviews), expecting to increase the volume of their non-compensated reviews and overall rating. Does this strategy work? On what subjects or topics does this strategy work the best? These questions have still not been answered in the literature but draw substantial interest from the industry. In this paper, we study the effect of compensated reviews on non-compensated reviews by utilizing online reviews on 1,240 auto shipping companies over a ten-year period from a transportation website. Because some online reviews have missing information on their compensation status, we first develop a classification algorithm to differentiate compensated reviews from non-compensated reviews by leveraging a machine learning-based identification process, drawing upon the unique features of the compensated reviews. From the classification results, we empirically investigate the effects of compensated reviews on non-compensated. Our results indicate that the number of compensated reviews does indeed increase the number of non-compensated reviews. In addition, the ratings of compensated reviews positively affect the ratings of non-compensated reviews. Moreover, if the compensated reviews feature the topic or subject of a car shipping function, the positive effect of compensated reviews on non-compensated ones is the strongest. Besides methodological contributions in text classification and empirical modeling, our study provides empirical evidence on how to prove the effectiveness of compensated online reviews in terms of improving the platform's overall online reviews and ratings. Also, it suggests a guideline for utilizing compensated reviews to their full strength, that is, with regard to featuring certain topics or subjects in these reviews to achieve the best outcome."
http://arxiv.org/abs/2202.04146v2,A Neural Phillips Curve and a Deep Output Gap,2022-02-08 21:10:09+00:00,['Philippe Goulet Coulombe'],econ.EM,"Many problems plague empirical Phillips curves (PCs). Among them is the hurdle that the two key components, inflation expectations and the output gap, are both unobserved. Traditional remedies include proxying for the absentees or extracting them via assumptions-heavy filtering procedures. I propose an alternative route: a Hemisphere Neural Network (HNN) whose architecture yields a final layer where components can be interpreted as latent states within a Neural PC. There are benefits. First, HNN conducts the supervised estimation of nonlinearities that arise when translating a high-dimensional set of observed regressors into latent states. Second, forecasts are economically interpretable. Among other findings, the contribution of real activity to inflation appears understated in traditional PCs. In contrast, HNN captures the 2021 upswing in inflation and attributes it to a large positive output gap starting from late 2020. The unique path of HNN's gap comes from dispensing with unemployment and GDP in favor of an amalgam of nonlinearly processed alternative tightness indicators."
http://arxiv.org/abs/2202.02787v1,Stable cooperation emerges in stochastic multiplicative growth,2022-02-06 14:51:58+00:00,"['Lorenzo Fant', 'Onofrio Mazzarisi', 'Emanuele Panizon', 'Jacopo Grilli']",q-bio.PE,"Understanding the evolutionary stability of cooperation is a central problem in biology, sociology, and economics. There exist only a few known mechanisms that guarantee the existence of cooperation and its robustness to cheating. Here, we introduce a new mechanism for the emergence of cooperation in the presence of fluctuations. We consider agents whose wealth change stochastically in a multiplicative fashion. Each agent can share part of her wealth as public good, which is equally distributed among all the agents. We show that, when agents operate with long time-horizons, cooperation produce an advantage at the individual level, as it effectively screens agents from the deleterious effect of environmental fluctuations."
http://arxiv.org/abs/2203.04080v3,On Robust Inference in Time Series Regression,2022-03-08 13:49:10+00:00,"['Richard T. Baillie', 'Francis X. Diebold', 'George Kapetanios', 'Kun Ho Kim', 'Aaron Mora']",econ.EM,"Least squares regression with heteroskedasticity consistent standard errors (""OLS-HC regression"") has proved very useful in cross section environments. However, several major difficulties, which are generally overlooked, must be confronted when transferring the HC technology to time series environments via heteroskedasticity and autocorrelation consistent standard errors (""OLS-HAC regression""). First, in plausible time-series environments, OLS parameter estimates can be inconsistent, so that OLS-HAC inference fails even asymptotically. Second, most economic time series have autocorrelation, which renders OLS parameter estimates inefficient. Third, autocorrelation similarly renders conditional predictions based on OLS parameter estimates inefficient. Finally, the structure of popular HAC covariance matrix estimators is ill-suited for capturing the autoregressive autocorrelation typically present in economic time series, which produces large size distortions and reduced power in HAC-based hypothesis testing, in all but the largest samples. We show that all four problems are largely avoided by the use of a simple and easily-implemented dynamic regression procedure, which we call DURBIN. We demonstrate the advantages of DURBIN with detailed simulations covering a range of practical issues."
http://arxiv.org/abs/2201.01813v4,"Reputation, Learning and Project Choice in Frictional Economies",2022-01-05 20:50:26+00:00,['Farzad Pourbabaee'],econ.TH,"I introduce a dynamic model of learning and random meetings between a long-lived agent with unknown ability and heterogeneous projects with observable qualities. The outcomes of the agent's matches with the projects determine her posterior belief about her ability (i.e., her reputation). In a self-type learning framework with endogenous outside option, I find the optimal project selection strategy of the agent, that determines what types of projects the agent with a certain level of reputation will accept. Sections of the optimal matching set become increasing intervals, with different cutoffs across different types of the projects. Increasing the meeting rate has asymmetric effects on the sections of the matching sets: it unambiguously expands the section for the high type projects, while on some regions, it initially expands and then shrinks the section of the low type projects."
http://arxiv.org/abs/2203.08050v5,Pairwise Valid Instruments,2022-03-15 16:44:20+00:00,"['Zhenting Sun', 'Kaspar Wüthrich']",econ.EM,"Finding valid instruments is difficult. We propose Validity Set Instrumental Variable (VSIV) estimation, a method for estimating local average treatment effects (LATEs) in heterogeneous causal effect models when the instruments are partially invalid. We consider settings with pairwise valid instruments, that is, instruments that are valid for a subset of instrument value pairs. VSIV estimation exploits testable implications of instrument validity to remove invalid pairs and provides estimates of the LATEs for all remaining pairs, which can be aggregated into a single parameter of interest using researcher-specified weights. We show that the proposed VSIV estimators are asymptotically normal under weak conditions and remove or reduce the asymptotic bias relative to standard LATE estimators (that is, LATE estimators that do not use testable implications to remove invalid variation). We evaluate the finite sample properties of VSIV estimation in application-based simulations and apply our method to estimate the returns to college education using parental education as an instrument."
http://arxiv.org/abs/2202.10030v5,Multivariate Tie-breaker Designs,2022-02-21 07:53:09+00:00,"['Tim P. Morrison', 'Art B. Owen']",stat.ME,"In a tie-breaker design (TBD), subjects with high values of a running variable are given some (usually desirable) treatment, subjects with low values are not, and subjects in the middle are randomized. TBDs are intermediate between regression discontinuity designs (RDDs) and randomized controlled trials (RCTs). TBDs allow a tradeoff between the resource allocation efficiency of an RDD and the statistical efficiency of an RCT. We study a model where the expected response is one multivariate regression for treated subjects and another for control subjects. We propose a prospective D-optimality, analogous to Bayesian optimal design, to understand design tradeoffs without reference to a specific data set. For given covariates, we show how to use convex optimization to choose treatment probabilities that optimize this criterion. We can incorporate a variety of constraints motivated by economic and ethical considerations. In our model, D-optimality for the treatment effect coincides with D-optimality for the whole regression, and, without constraints, an RCT is globally optimal. We show that a monotonicity constraint favoring more deserving subjects induces sparsity in the number of distinct treatment probabilities. We apply the convex optimization solution to a semi-synthetic example involving triage data from the MIMIC-IV-ED database."
http://arxiv.org/abs/2202.12292v2,Bridging Level-K to Nash Equilibrium,2022-02-24 18:49:19+00:00,"['Dan Levin', 'Luyao Zhang']",econ.GN,"We introduce NLK, a model that connects the Nash equilibrium (NE) and Level-K. It allows a player in a game to believe that her opponent may be either less or as sophisticated as, she is, a view supported in psychology. We apply NLK to data from five published papers on static, dynamic, and auction games. NLK provides different predictions than those of the NE and Level-K; moreover, a simple version of NLK explains the experimental data better in many cases, with the same or lower number of parameters. We discuss extensions to games with more than two players and heterogeneous beliefs."
http://arxiv.org/abs/2202.10121v2,A Dutch-Book Trap for Misspecification,2022-02-21 11:13:22+00:00,"['Emiliano Catonini', 'Giacomo Lanzani']",econ.TH,"We provide Dutch-book arguments against two forms of misspecified Bayesian learning. An agent progressively learns about a state and is offered a bet after every new discovery. We say the agent is Dutch-booked when they are willing to accept all bets, but their payoff is negative under each state either ex-post, or in expectation given the objective conditional probabilities of the discoveries (i.e., the correct data-generating process, DGP). Respectively, an agent cannot be Dutch-booked if and only if they update their beliefs with Bayes rule either from the previous belief, even using misspecified likelihood functions, or from one lexicographic prior, using the correct data-generating process. Under a large population interpretation of the DGP, this means that a population can suffer aggregate losses under all states when different individuals update their beliefs from different (lexicographic) priors, or using misspecified likelihoods. Thus, the Dutch-book argument offers a general characterization of the perils of misspecification."
http://arxiv.org/abs/2202.12062v4,Semiparametric Estimation of Dynamic Binary Choice Panel Data Models,2022-02-24 12:39:15+00:00,"['Fu Ouyang', 'Thomas Tao Yang']",econ.EM,"We propose a new approach to the semiparametric analysis of panel data binary choice models with fixed effects and dynamics (lagged dependent variables). The model we consider has the same random utility framework as in Honore and Kyriazidou (2000). We demonstrate that, with additional serial dependence conditions on the process of deterministic utility and tail restrictions on the error distribution, the (point) identification of the model can proceed in two steps, and only requires matching the value of an index function of explanatory variables over time, as opposed to that of each explanatory variable. Our identification approach motivates an easily implementable, two-step maximum score (2SMS) procedure -- producing estimators whose rates of convergence, in contrast to Honore and Kyriazidou's (2000) methods, are independent of the model dimension. We then derive the asymptotic properties of the 2SMS procedure and propose bootstrap-based distributional approximations for inference. Monte Carlo evidence indicates that our procedure performs adequately in finite samples."
http://arxiv.org/abs/2202.03960v4,Continuous permanent unobserved heterogeneity in dynamic discrete choice models,2022-02-08 16:12:19+00:00,['Jackson Bunting'],econ.EM,"In dynamic discrete choice (DDC) analysis, it is common to use mixture models to control for unobserved heterogeneity. However, consistent estimation typically requires both restrictions on the support of unobserved heterogeneity and a high-level injectivity condition that is difficult to verify. This paper provides primitive conditions for point identification of a broad class of DDC models with multivariate continuous permanent unobserved heterogeneity. The results apply to both finite- and infinite-horizon DDC models, do not require a full support assumption, nor a long panel, and place no parametric restriction on the distribution of unobserved heterogeneity. In addition, I propose a seminonparametric estimator that is computationally attractive and can be implemented using familiar parametric methods."
http://arxiv.org/abs/2203.16272v4,The infinite information gap between mathematical and physical representations,2022-03-30 13:05:10+00:00,"['Pedro Hack', 'Daniel A. Braun', 'Sebastian Gottwald']",math.CO,"Partial orders have been used to model several experimental setups, going from classical thermodynamics and general relativity to the quantum realm with its resource theories. In order to study such experimental setups, one typically characterizes them via a (numerical) representation, that is, a set of real-valued functions. In the context of resource theory, it is customary to use \textbf{mathematical} representations, i.e. a set of \textbf{measurement outcomes} which characterize the achievable transitions within the experimental setup. However, in line with the minimum energy and maximum entropy principles in classical mechanics and thermodynamics, respectively, one would expect an optimization interpretation for a representation to be called \textbf{physical}. More specifically, a physical representation could consist of a set of competing \textbf{optimization principles} such that a transition happens provided they are all optimized by it. Somewhat surprisingly, we show that this distinction can result in an \textbf{infinite information gap}, with some partial orders having mathematical representations that involve a finite amount of information and requiring infinite information to build a physical representation. We connect this phenomenon with well-known resource-theoretic scenarios like majorization, and develop notions of partial order dimension that run in parallel to the representations that we consider. Our results improve on the classification of preordered spaces in terms of real-valued functions."
http://arxiv.org/abs/2201.10115v2,Effects of Privacy-Inducing Noise on Welfare and Influence of Referendum Systems,2022-01-25 06:22:57+00:00,"['Suat Evren', 'Praneeth Vepakomma']",cs.GT,"Social choice functions help aggregate individual preferences while differentially private mechanisms provide formal privacy guarantees to release answers of queries operating on sensitive data. However, preserving differential privacy requires introducing noise to the system, and therefore may lead to undesired byproducts. Does an increase in the level of differential privacy for releasing the outputs of social choice functions increase or decrease the level of influence and welfare, and at what rate? In this paper, we mainly address this question in more precise terms in a referendum setting with two candidates when the celebrated randomized response mechanism is used. We show that there is an inversely-proportional relation between welfare and privacy, and also influence and privacy."
http://arxiv.org/abs/2201.07903v1,Identification of Direct Socio-Geographical Price Discrimination: An Empirical Study on iPhones,2022-01-19 23:01:17+00:00,['Davidson Cheng'],econ.EM,"Price discrimination is a practice where firms utilize varying sensitivities to prices among consumers to increase profits. The welfare effects of price discrimination are not agreed on among economists, but identification of such actions may contribute to our standing of firms' pricing behaviors. In this letter, I use econometric tools to analyze whether Apple Inc, one of the largest companies in the globe, is practicing price discrimination on the basis of socio-economical and geographical factors. My results indicate that iPhones are significantly (p $<$ 0.01) more expensive in markets where competitions are weak or where Apple has a strong market presence. Furthermore, iPhone prices are likely to increase (p $<$ 0.01) in developing countries/regions or markets with high income inequality."
http://arxiv.org/abs/2201.09064v1,An Intergenerational Issue: The Equity Issues due to Public-Private Partnerships. The Critical Aspect of the Social Discount Rate Choice for Future Generations,2022-01-22 14:44:06+00:00,"['Abeer Al Yaqoobi', 'Marcel Ausloos']",econ.GN,"This paper investigates the impact of Social Discount Rate (SDR) choice on intergenerational equity issues caused by Public-Private Partnerships (PPPs) projects. Indeed, more PPPs mean more debt being accumulated for future generations leading to a fiscal deficit crisis. The paper draws on how the SDR level taken today distributes societies on the Social Welfare Function (SWF). This is done by answering two sub-questions: (i) What is the risk of PPPs debts being off-balance sheet? (ii) How do public policies, based on the envisaged SDR, position society within different ethical perspectives? The answers are obtained from a discussion of the different SDRs (applied in the UK for examples) according to the merits of the pertinent ethical theories, namely libertarian, egalitarian, utilitarian and Rawlsian. We find that public policymakers can manipulate the SDR to make PPPs looking like a better option than the traditional financing form. However, this antagonises the Value for Money principle. We also point out that public policy is not harmonised with ethical theories. We find that at present (in the UK), the SDR is somewhere between weighted utilitarian and Rawlsian societies in the trade-off curve. Alas, our study finds no evidence that the (UK) government is using a sophisticated system to keep pace with the accumulated off-balance sheet debts. Thus, the exact prediction of the final state is hardly made because of the uncertainty factor. We conclude that our study hopefully provides a good analytical framework for policymakers in order to draw on the merits of ethical theories before initiating public policies like PPPs."
http://arxiv.org/abs/2201.09182v1,Consolidating Marginalism and Egalitarianism: A New Value for Transferable Utility Games,2022-01-23 05:16:41+00:00,"['D. Choudhury', 'S. Borkotokey', 'Rajnish Kumar', 'Sudipta Sarangi']",econ.TH,"In cooperative games with transferable utilities, the Shapley value is an extreme case of marginalism while the Equal Division rule is an extreme case of egalitarianism. The Shapley value does not assign anything to the non-productive players and the Equal Division rule does not concern itself to the relative efficiency of the players in generating a resource. However, in real life situations neither of them is a good fit for the fair distribution of resources as the society is neither devoid of solidarity nor it can be indifferent to rewarding the relatively more productive players. Thus a trade-off between these two extreme cases has caught attention from many researchers. In this paper, we obtain a new value for cooperative games with transferable utilities that adopts egalitarianism in smaller coalitions on one hand and on the other hand takes care of the players' marginal productivity in sufficiently large coalitions. Our value is identical with the Shapley value on one extreme and the Equal Division rule on the other extreme. We provide four characterizations of the value using variants of standard axioms in the literature. We have also developed a strategic implementation mechanism of our value in sub-game perfect Nash equilibrium."
http://arxiv.org/abs/2201.04880v1,"Exit, Voice and Political Change: Evidence from Swedish Mass Migration to the United States; A Comment",2022-01-13 10:48:57+00:00,['Per Pettersson-Lidbom'],econ.GN,"In this comment, I revisit the question raised in Karadja and Prawitz (2019) concerning a causal relationship between mass emigration and long-run political outcomes. I discuss a number of potential problems with their instrumental variable analysis. First, there are at least three reasons why their instrument violates the exclusion restriction: (i) failing to control for internal migration, (ii) insufficient control for confounders correlated with their instrument, and (iii) emigration measured with a nonclassical measurement error. Second, I also discuss two problems with the statistical inference, both of which indicate that the instrument does not fulfill the relevance condition, i.e., the instrument is not sufficiently correlated with the endogenous variable emigration. Correcting for any of these problems reveals that there is no relationship between emigration and political outcomes."
http://arxiv.org/abs/2201.05959v1,Master Equation for Discrete-Time Stackelberg Mean Field Games with single leader,2022-01-16 03:43:48+00:00,"['Deepanshu Vasal', 'Randall Berry']",eess.SY,"In this paper, we consider a discrete-time Stackelberg mean field game with a leader and an infinite number of followers. The leader and the followers each observe types privately that evolve as conditionally independent controlled Markov processes. The leader commits to a dynamic policy and the followers best respond to that policy and each other. Knowing that the followers would play a mean field game based on her policy, the leader chooses a policy that maximizes her reward. We refer to the resulting outcome as a Stackelberg mean field equilibrium (SMFE). In this paper, we provide a master equation of this game that allows one to compute all SMFE. Based on our framework, we consider two numerical examples. First, we consider an epidemic model where the followers get infected based on the mean field population. The leader chooses subsidies for a vaccine to maximize social welfare and minimize vaccination costs. In the second example, we consider a technology adoption game where the followers decide to adopt a technology or a product and the leader decides the cost of one product that maximizes his returns, which are proportional to the people adopting that technology"
http://arxiv.org/abs/2201.06072v2,Dynamics of Bitcoin mining,2022-01-16 15:30:48+00:00,['Nemo Semret'],econ.GN,"What happens to mining when the Bitcoin price changes, when there are mining supply shocks, the price of energy changes, or hardware technology evolves? We give precise answers based on the technical forces and incentives in the system. We then build on these dynamics to consider value: what is the cost and purpose of mining, and is it worth it? Does it use too much energy, is it bad for the environment? Finally we extend our analysis to the long term: is mining economically feasible forever? What will the global hash rate be in 40 years? How is mining impacted by the limits of computation and energy? Is it physically sustainable in the long run? From first principles, we derive a fundamental scale-invariant feasibility constraint, which enables us to analyze the interlocking dynamics, find key invariants, and answer these questions mathematically."
http://arxiv.org/abs/2201.06020v2,Referral Hiring and Social Network Structure,2022-01-16 11:11:54+00:00,['Yoshitaka Ogisu'],econ.TH,"It is well known that differences in the average number of friends among social groups can cause inequality in the average wage and/or unemployment rate. However, the impact of social network structure on inequality is not evident. In this paper, we show that not only the average number of friends but also the heterogeneity of degree distribution can affect inter-group inequality. A worker group with a scale-free network tends to be disadvantaged in the labor market compared to a group with an Erdős-Rényi network structure. This feature becomes strengthened as the skewness of the degree distribution increases in scale-free networks. We show that the government's policy of discouraging referral hiring worsens social welfare and can exacerbate inequality."
http://arxiv.org/abs/2206.04605v6,A Two-Ball Ellsberg Paradox: An Experiment,2022-06-09 16:50:38+00:00,"['Brian Jabarian', 'Simon Lazarus']",econ.TH,"We conduct an incentivized experiment on a nationally representative US sample \\ (N=708) to test whether people prefer to avoid ambiguity even when it means choosing dominated options. In contrast to the literature, we find that 55\% of subjects prefer a risky act to an ambiguous act that always provides a larger probability of winning. Our experimental design shows that such a preference is not mainly due to a lack of understanding. We conclude that subjects avoid ambiguity \textit{per se} rather than avoiding ambiguity because it may yield a worse outcome. Such behavior cannot be reconciled with existing models of ambiguity aversion in a straightforward manner."
http://arxiv.org/abs/2205.04573v1,Robust Data-Driven Decisions Under Model Uncertainty,2022-05-09 21:36:20+00:00,['Xiaoyu Cheng'],econ.TH,"When sample data are governed by an unknown sequence of independent but possibly non-identical distributions, the data-generating process (DGP) in general cannot be perfectly identified from the data. For making decisions facing such uncertainty, this paper presents a novel approach by studying how the data can best be used to robustly improve decisions. That is, no matter which DGP governs the uncertainty, one can make a better decision than without using the data. I show that common inference methods, e.g., maximum likelihood and Bayesian updating cannot achieve this goal. To address, I develop new updating rules that lead to robustly better decisions either asymptotically almost surely or in finite sample with a pre-specified probability. Especially, they are easy to implement as are given by simple extensions of the standard statistical procedures in the case where the possible DGPs are all independent and identically distributed. Finally, I show that the new updating rules also lead to more intuitive conclusions in existing economic models such as asset pricing under ambiguity."
http://arxiv.org/abs/2204.05105v1,Describing Sen's Transitivity Condition in Inequalities and Equations,2022-04-08 02:00:26+00:00,['Fujun Hou'],econ.TH,"In social choice theory, Sen's value restriction condition is a sufficiency condition restricted to individuals' ordinal preferences so as to obtain a transitive social preference under the majority decision rule. In this article, Sen's transitivity condition is described by use of inequality and equation. First, for a triple of alternatives, an individual's preference is represented by a preference map, whose entries are sets containing the ranking position or positions derived from the individual's preference over that triple of those alternatives. Second, by using the union operation of sets and the cardinality concept, Sen's transitivity condition is described by inequalities. Finally, by using the membership function of sets, Sen's transitivity condition is further described by equations."
http://arxiv.org/abs/2205.01882v4,Approximating Choice Data by Discrete Choice Models,2022-05-04 04:07:16+00:00,"['Haoge Chang', 'Yusuke Narita', 'Kota Saito']",econ.TH,"We obtain a necessary and sufficient condition under which random-coefficient discrete choice models, such as mixed-logit models, are rich enough to approximate any nonparametric random utility models arbitrarily well across choice sets. The condition turns out to be the affine-independence of the set of characteristic vectors. When the condition fails, resulting in some random utility models that cannot be closely approximated, we identify preferences and substitution patterns that are challenging to approximate accurately. We also propose algorithms to quantify the magnitude of approximation errors."
http://arxiv.org/abs/2205.14186v2,The Effect of Increased Access to IVF on Women's Careers,2022-05-21 23:25:04+00:00,['Lingxi Chen'],econ.GN,"Motherhood is the main contributor to gender gaps in the labor market. IVF is a method of assisted reproduction that can delay fertility, which results in decreased motherhood income penalty. In this research, I estimate the effects of expanded access to in vitro fertilization (IVF) arising from state insurance mandates. I use a difference-in-differences model to estimate the effect of increased IVF accessibility for delaying childbirth and decreasing the motherhood income penalty. Using the fertility supplement dataset from the Current Population Survey (CPS), I estimate how outcomes change in states when they implement their mandates compared to how outcomes change in states that are not changing their policies. The results indicate that IVF mandates increase the probability of motherhood by 38 by 3.1 percentage points (p<0.01). However, the results provide no evidence that IVF insurance mandates impact women's earnings."
http://arxiv.org/abs/2206.14321v1,Reducing US Biofuels Requirements Mitigates Short-term Impacts of Global Population and Income Growth on Agricultural Environmental Outcomes,2022-06-28 23:12:43+00:00,"['David R. Johnson', 'Nathan B. Geldner', 'Jing Liu', 'Uris Lantz Baldos', 'Thomas Hertel']",econ.GN,"Biobased energy, particularly corn starch-based ethanol and other liquid renewable fuels, are a major element of federal and state energy policies in the United States. These policies are motivated by energy security and climate change mitigation objectives, but corn ethanol does not substantially reduce greenhouse gas emissions when compared to petroleum-based fuels. Corn production also imposes substantial negative externalities (e.g., nitrogen leaching, higher food prices, water scarcity, and indirect land use change). In this paper, we utilize a partial equilibrium model of corn-soy production and trade to analyze the potential of reduced US demand for corn as a biobased energy feedstock to mitigate increases in nitrogen leaching, crop production and land use associated with growing global populations and income from 2020 to 2050. We estimate that a 23% demand reduction would sustain land use and nitrogen leaching below 2020 levels through the year 2025, and a 41% reduction would do so through 2030. Outcomes are similar across major watersheds where corn and soy are intensively farmed."
http://arxiv.org/abs/2206.13652v2,"Reducing Polarization on Abortion, Guns and Immigration: An Experimental Study",2022-06-27 22:21:13+00:00,"['Michele Belot', 'Guglielmo Briscese']",econ.GN,"We study individuals' willingness to engage with others who hold opposite views on polarizing policies. A representative sample of 2,507 Americans are given the opportunity to listen to recordings of fellow countrymen and women expressing their views on immigration, abortion laws and gun ownership laws. We find that most Americans (more than two-thirds) are willing to listen to a view opposite to theirs, and a fraction (ten percent) reports changing their views as a result. We also test whether emphasizing having common grounds with those who think differently helps bridging views. We identify principles the vast majority of people agree upon: (1) a set of fundamental human rights, and (2) a set of simple behavioral etiquette rules. A random subsample of people are made explicitly aware they share common views, either on human rights (one-third of the sample) or etiquette rules (another one-third of the sample), before they have the opportunity to listen to different views. We find that the treatments induce people to adjust their views towards the center on abortion and immigration, relative to a control group, thus reducing polarization."
http://arxiv.org/abs/2206.05966v3,Coordinating Monetary Contributions in Participatory Budgeting,2022-06-13 08:27:16+00:00,"['Haris Aziz', 'Sujit Gujar', 'Manisha Padala', 'Mashbat Suzuki', 'Jeremy Vollen']",cs.GT,"We formalize a framework for coordinating funding and selecting projects, the costs of which are shared among agents with quasi-linear utility functions and individual budgets. Our model contains the classical discrete participatory budgeting model as a special case, while capturing other useful scenarios. We propose several important axioms and objectives and study how well they can be simultaneously satisfied. We show that whereas welfare maximization admits an FPTAS, welfare maximization subject to a natural and very weak participation requirement leads to a strong inapproximability. This result is bypassed if we consider some natural restricted valuations, namely laminar single-minded valuations and symmetric valuations. Our analysis for the former restriction leads to the discovery of a new class of tractable instances for the Set Union Knapsack problem, a classical problem in combinatorial optimization."
http://arxiv.org/abs/2206.05568v1,Bounded strategic reasoning explains crisis emergence in multi-agent market games,2022-06-11 17:15:53+00:00,"['Benjamin Patrick Evans', 'Mikhail Prokopenko']",cs.MA,"The efficient market hypothesis (EMH), based on rational expectations and market equilibrium, is the dominant perspective for modelling economic markets. However, the most notable critique of the EMH is the inability to model periods of out-of-equilibrium behaviour in the absence of any significant external news. When such dynamics emerge endogenously, the traditional economic frameworks provide no explanation for such behaviour and the deviation from equilibrium. This work offers an alternate perspective explaining the endogenous emergence of punctuated out-of-equilibrium dynamics based on bounded rational agents. In a concise market entrance game, we show how boundedly rational strategic reasoning can lead to endogenously emerging crises, exhibiting fat tails in ""returns"". We also show how other common stylised facts of economic markets, such as clustered volatility, can be explained due to agent diversity (or lack thereof) and the varying learning updates across the agents. This work explains various stylised facts and crisis emergence in economic markets, in the absence of any external news, based purely on agent interactions and bounded rational reasoning."
http://arxiv.org/abs/2206.04950v1,The perils of Kremlin's influence: evidence from Ukraine,2022-06-10 08:56:27+00:00,"['Chiara Natalie Focacci', 'Mitja Kovac', 'Rok Spruk']",econ.GN,"We examine the contribution of institutional integration to the institutional quality. To this end, we exploit the 2007 political crisis in Ukraine and examine the effects of staying out of the European Union for 28 Ukrainian provinces in the period 1996-2020. We construct novel subnational estimates of institutional quality for Ukraine and central and eastern European countries based on the latent residual component extraction of institutional quality from the existing governance indicators by making use of Bayesian posterior analysis under non-informative objective prior function. By comparing the residualized institutional quality trajectories of Ukrainian provinces with their central and eastern European peers that were admitted to the European Union in 2004 and after, we assess the institutional quality cost of being under Russian political influence and interference. Based on the large-scale synthetic control analysis, we find evidence of large-scale negative institutional quality effects of staying out of the European Union such as heightened political instability and rampant deterioration of the rule of law and control of corruption. Statistical significance of the estimated effects is evaluated across a comprehensive placebo simulation with more than 34 billion placebo averages for each institutional quality outcome."
http://arxiv.org/abs/2206.04773v3,To What Extent Do Disadvantaged Neighborhoods Mediate Social Assistance Dependency? Evidence from Sweden,2022-06-09 21:36:06+00:00,"['Cheng Lin', 'Adel Daoud', 'Maria Branden']",econ.GN,"Occasional social assistance prevents individuals from a range of social ills, particularly unemployment and poverty. It remains unclear, however, how and to what extent continued reliance on social assistance leads to individuals becoming trapped in social assistance dependency. In this paper, we build on the theory of cumulative disadvantage and examine whether the accumulated use of social assistance over the life course is associated with an increased risk of future social assistance recipiency. We also analyze the extent to which living in disadvantaged neighborhoods constitutes an important mechanism in the explanation of this association. Our analyses use Swedish population registers for the full population of individuals born in 1981, and these individuals are followed for approximately 17 years. While most studies are limited by a lack of granular, life-history data, our granular individual-level data allow us to apply causal-mediation analysis, and thereby quantify the extent to which the likelihood of ending up in social assistance dependency is affected by residing in disadvantaged neighborhoods. Our findings show the accumulation of social assistance over the studied period is associated with a more than four-fold increase on a risk ratio scale for future social assistance recipiency, compared to never having received social assistance during the period examined. Then, we examine how social assistance dependency is mediated by prolonged exposure to disadvantaged neighborhoods. Our results suggest that the indirect effect of disadvantaged neighborhoods is weak to moderate. Therefore, social assistance dependency may be a multilevel process. Future research is to explore how the mediating effects of disadvantaged neighborhoods vary in different contexts."
http://arxiv.org/abs/2206.05022v1,A Fast Third-Step Second-Order Explicit Numerical Approach To Investigating and Forecasting The Dynamic of Corruption And Poverty In Cameroon,2022-06-03 09:39:52+00:00,['Eric Ngondiep'],econ.GN,"This paper constructs a third-step second-order numerical approach for solving a mathematical model on the dynamic of corruption and poverty. The stability and error estimates of the proposed technique are analyzed using the $L^{2}$-norm. The developed algorithm is at less zero-stable and second-order accurate. Furthermore, the new method is explicit, fast and more efficient than a large class of numerical schemes applied to nonlinear systems of ordinary differential equations and can serve as a robust tool for integrating general systems of initial-value problems. Some numerical examples confirm the theory and also consider the corruption and poverty in Cameroon."
http://arxiv.org/abs/2204.08986v1,The 2020 Census Disclosure Avoidance System TopDown Algorithm,2022-04-19 16:35:29+00:00,"['John M. Abowd', 'Robert Ashmead', 'Ryan Cumings-Menon', 'Simson Garfinkel', 'Micah Heineck', 'Christine Heiss', 'Robert Johns', 'Daniel Kifer', 'Philip Leclerc', 'Ashwin Machanavajjhala', 'Brett Moran', 'William Sexton', 'Matthew Spence', 'Pavel Zhuravlev']",cs.CR,"The Census TopDown Algorithm (TDA) is a disclosure avoidance system using differential privacy for privacy-loss accounting. The algorithm ingests the final, edited version of the 2020 Census data and the final tabulation geographic definitions. The algorithm then creates noisy versions of key queries on the data, referred to as measurements, using zero-Concentrated Differential Privacy. Another key aspect of the TDA are invariants, statistics that the Census Bureau has determined, as matter of policy, to exclude from the privacy-loss accounting. The TDA post-processes the measurements together with the invariants to produce a Microdata Detail File (MDF) that contains one record for each person and one record for each housing unit enumerated in the 2020 Census. The MDF is passed to the 2020 Census tabulation system to produce the 2020 Census Redistricting Data (P.L. 94-171) Summary File. This paper describes the mathematics and testing of the TDA for this purpose."
http://arxiv.org/abs/2204.10275v4,Do t-Statistic Hurdles Need to be Raised?,2022-04-21 17:19:35+00:00,['Andrew Y. Chen'],q-fin.GN,"Many scholars have called for raising statistical hurdles to guard against false discoveries in academic publications. I show these calls may be difficult to justify empirically. Published data exhibit bias: results that fail to meet existing hurdles are often unobserved. These unobserved results must be extrapolated, which can lead to weak identification of revised hurdles. In contrast, statistics that can target only published findings (e.g. empirical Bayes shrinkage and the FDR) can be strongly identified, as data on published findings is plentiful. I demonstrate these results theoretically and in an empirical analysis of the cross-sectional return predictability literature."
http://arxiv.org/abs/2204.10359v4,Boundary Adaptive Local Polynomial Conditional Density Estimators,2022-04-21 18:35:22+00:00,"['Matias D. Cattaneo', 'Rajita Chandak', 'Michael Jansson', 'Xinwei Ma']",math.ST,"We begin by introducing a class of conditional density estimators based on local polynomial techniques. The estimators are boundary adaptive and easy to implement. We then study the (pointwise and) uniform statistical properties of the estimators, offering characterizations of both probability concentration and distributional approximation. In particular, we establish uniform convergence rates in probability and valid Gaussian distributional approximations for the Studentized t-statistic process. We also discuss implementation issues such as consistent estimation of the covariance function for the Gaussian approximation, optimal integrated mean squared error bandwidth selection, and valid robust bias-corrected inference. We illustrate the applicability of our results by constructing valid confidence bands and hypothesis tests for both parametric specification and shape constraints, explicitly characterizing their approximation errors. A companion R software package implementing our main results is provided."
http://arxiv.org/abs/2204.11088v1,Trade Facilitation and Economic Growth Among Middle-Income Countries,2022-04-23 15:16:36+00:00,['Victor Ushahemba Ijirshar'],econ.GN,"This study examined the relationship between trade facilitation and economic growth among the middle-income countries from 2010 to 2020 using 94 countries made up of 48 lower-middle-income countries and 46 upper-middle-income countries. The study utilized both difference and system Generalised Method of Moments (GMM) since the cross-sections (N) were greater than the periods (T). The study found that container port traffic, quality of trade and transport-related infrastructure have a strong influence on imports and exports of goods and national income while trade tariff hurts the growth of the countries. The study also found that most of the trade facilitation indicators indicated a weak positive influence on trade flows and economic growth. Based on these findings, the study recommends that reforms aimed at significantly lowering the costs of trading across borders among middle-income countries should be highly prioritized in policy formulations, with a focus on the export side by reducing at-the-border documentation, time, and real costs of trading across borders while the international organizations should continue to report the set of Trade Facilitation Indicators (TFIs) that identify areas for action and enable the potential impact of reforms to be assessed."
http://arxiv.org/abs/2205.03318v1,Benchmarking Econometric and Machine Learning Methodologies in Nowcasting,2022-05-06 15:51:31+00:00,['Daniel Hopp'],stat.ML,"Nowcasting can play a key role in giving policymakers timelier insight to data published with a significant time lag, such as final GDP figures. Currently, there are a plethora of methodologies and approaches for practitioners to choose from. However, there lacks a comprehensive comparison of these disparate approaches in terms of predictive performance and characteristics. This paper addresses that deficiency by examining the performance of 12 different methodologies in nowcasting US quarterly GDP growth, including all the methods most commonly employed in nowcasting, as well as some of the most popular traditional machine learning approaches. Performance was assessed on three different tumultuous periods in US economic history: the early 1980s recession, the 2008 financial crisis, and the COVID crisis. The two best performing methodologies in the analysis were long short-term memory artificial neural networks (LSTM) and Bayesian vector autoregression (BVAR). To facilitate further application and testing of each of the examined methodologies, an open-source repository containing boilerplate code that can be applied to different datasets is published alongside the paper, available at: github.com/dhopp1/nowcasting_benchmark."
http://arxiv.org/abs/2205.02167v1,Knowledge is non-fungible,2022-05-04 16:35:59+00:00,['César A. Hidalgo'],econ.TH,"What would you do if you were asked to ""add"" knowledge? Would you say that ""one plus one knowledge"" is two ""knowledges""? Less than that? More? Or something in between? Adding knowledge sounds strange, but it brings to the forefront questions that are as fundamental as they are eclectic. These are questions about the nature of knowledge and about the use of mathematics to model reality. In this chapter, I explore the mathematics of adding knowledge starting from what I believe is an overlooked but key observation: the idea that knowledge is non-fungible."
http://arxiv.org/abs/2205.02668v2,A Market for Trading Forecasts: A Wagering Mechanism,2022-05-05 14:19:08+00:00,"['Aitazaz Ali Raja', 'Pierre Pinson', 'Jalal Kazempour', 'Sergio Grammatico']",econ.TH,"In many areas of industry and society, e.g., energy, healthcare, logistics, agents collect vast amounts of data that they deem proprietary. These data owners extract predictive information of varying quality and relevance from data depending on quantity, inherent information content, and their own technical expertise. Aggregating these data and heterogeneous predictive skills, which are distributed in terms of ownership, can result in a higher collective value for a prediction task. In this paper, we envision a platform for improving predictions via implicit pooling of private information in return for possible remuneration. Specifically, we design a wagering-based forecast elicitation market platform, where a buyer intending to improve their forecasts posts a prediction task, and sellers respond to it with their forecast reports and wagers. This market delivers an aggregated forecast to the buyer (pre-event) and allocates a payoff to the sellers (post-event) for their contribution. We propose a payoff mechanism and prove that it satisfies several desirable economic properties, including those specific to electronic platforms. Furthermore, we discuss the properties of the forecast aggregation operator and scoring rules to emphasize their effect on the sellers' payoff. Finally, we provide numerical examples to illustrate the structure and properties of the proposed market platform."
http://arxiv.org/abs/2205.02288v1,Choosing Exogeneity Assumptions in Potential Outcome Models,2022-05-04 18:47:17+00:00,"['Matthew A. Masten', 'Alexandre Poirier']",econ.EM,"There are many kinds of exogeneity assumptions. How should researchers choose among them? When exogeneity is imposed on an unobservable like a potential outcome, we argue that the form of exogeneity should be chosen based on the kind of selection on unobservables it allows. Consequently, researchers can assess the plausibility of any exogeneity assumption by studying the distributions of treatment given the unobservables that are consistent with that assumption. We use this approach to study two common exogeneity assumptions: quantile and mean independence. We show that both assumptions require a kind of non-monotonic relationship between treatment and the potential outcomes. We discuss how to assess the plausibility of this kind of treatment selection. We also show how to define a new and weaker version of quantile independence that allows for monotonic treatment selection. We then show the implications of the choice of exogeneity assumption for identification. We apply these results in an empirical illustration of the effect of child soldiering on wages."
http://arxiv.org/abs/2205.06572v2,Dynamic Stochastic Inventory Management in E-Grocery Retailing,2022-05-13 12:00:17+00:00,"['David Winkelmann', 'Matthias Ulrich', 'Michael Römer', 'Roland Langrock', 'Hermann Jahnke']",econ.GN,"E-grocery retailing enables ordering products online to be delivered at a future time slot chosen by the customer. This emerging field of business provides retailers with large and comprehensive new data sets, yet creates several challenges for the inventory management process. For example, the risk of a single item's stock-out leading to a complete cancellation of the shopping process is higher in e-grocery than in traditional store retailing. As a consequence, retailers aim at very high service level targets to provide satisfactory customer service and to ensure long-term business growth. When determining replenishment order quantities, it is of crucial importance to precisely account for the full uncertainty in the inventory process. This requires predictive and prescriptive analytics to (1) estimate suitable underlying probability distributions to represent the uncertainty caused by non-stationary customer demand, shelf lives, and supply, and to (2) integrate those forecasts into a comprehensive multi-period optimisation framework. In this paper, we model this stochastic dynamic problem by a sequential decision process that allows us to avoid simplifying assumptions commonly made in the literature, such as the focus on a single demand period. As the resulting problem will typically be analytically intractable, we propose a stochastic lookahead policy incorporating Monte Carlo techniques to fully propagate the associated uncertainties in order to derive replenishment order quantities. This policy naturally integrates probabilistic forecasts and allows us to explicitly derive the value of accounting for probabilistic information compared to myopic or deterministic approaches in a simulation-based setting. In addition, we evaluate our policy in a case study based on real-world data where underlying probability distributions are estimated from historical data and explanatory variables."
http://arxiv.org/abs/2205.06583v1,The Value of Information in Stopping Problems,2022-05-13 12:14:32+00:00,"['Ehud Lehrer', 'Tao Wang']",econ.TH,"We consider stopping problems in which a decision maker (DM) faces an unknown state of nature and decides sequentially whether to stop and take an irreversible action; pay a fee and obtain additional information; or wait without acquiring information. We discuss the value and quality of information. The former is the maximal discounted expected revenue the DM can generate. We show that among all history-dependent fee schemes, the upfront scheme (as opposed, for instance, to pay-for-use) is optimal: it generates the highest possible value of information. The effects on the optimal strategy of obtaining information from a more accurate source and of having a higher discount factor are distinct, as far as expected stopping time and its distribution are concerned. However, these factors have a similar effect in that they both enlarge the set of cases in which the optimal strategy prescribes waiting."
http://arxiv.org/abs/2205.06363v1,Causal Estimation of Position Bias in Recommender Systems Using Marketplace Instruments,2022-05-12 20:58:25+00:00,"['Rina Friedberg', 'Karthik Rajkumar', 'Jialiang Mao', 'Qian Yao', 'YinYin Yu', 'Min Liu']",econ.EM,"Information retrieval systems, such as online marketplaces, news feeds, and search engines, are ubiquitous in today's digital society. They facilitate information discovery by ranking retrieved items on predicted relevance, i.e. likelihood of interaction (click, share) between users and items. Typically modeled using past interactions, such rankings have a major drawback: interaction depends on the attention items receive. A highly-relevant item placed outside a user's attention could receive little interaction. This discrepancy between observed interaction and true relevance is termed the position bias. Position bias degrades relevance estimation and when it compounds over time, it can silo users into false relevant items, causing marketplace inefficiencies. Position bias may be identified with randomized experiments, but such an approach can be prohibitive in cost and feasibility. Past research has also suggested propensity score methods, which do not adequately address unobserved confounding; and regression discontinuity designs, which have poor external validity. In this work, we address these concerns by leveraging the abundance of A/B tests in ranking evaluations as instrumental variables. Historical A/B tests allow us to access exogenous variation in rankings without manually introducing them, harming user experience and platform revenue. We demonstrate our methodology in two distinct applications at LinkedIn - feed ads and the People-You-May-Know (PYMK) recommender. The marketplaces comprise users and campaigns on the ads side, and invite senders and recipients on PYMK. By leveraging prior experimentation, we obtain quasi-experimental variation in item rankings that is orthogonal to user relevance. Our method provides robust position effect estimates that handle unobserved confounding well, greater generalizability, and easily extends to other information retrieval systems."
http://arxiv.org/abs/2205.05978v1,Welfare compensation in international transmission expansion planning under uncertainty,2022-05-12 09:32:04+00:00,"['E. Ruben van Beesten', 'Ole Kristian Ådnanes', 'Håkon Morken Linde', 'Paolo Pisciella', 'Asgeir Tomasgard']",econ.GN,"In transmission expansion planning, situations can arise in which an expansion plan that is optimal for the system as a whole is detrimental to a specific country in terms of its expected economic welfare. If this country is one of the countries hosting the planned capacity expansion, it has the power to veto the plan and thus, undermine the system-wide social optimum. To solve this issue, welfare compensation mechanisms may be constructed that compensate suffering countries and make them willing to participate in the expansion plan. In the literature, welfare compensation mechanisms have been developed that work in expectation. However, in a stochastic setting, even if the welfare effect after compensation is positive in expectation, countries might still be hesitant to accept the risk that the actual, realized welfare effect may be negative in some scenarios.
  In this paper we analyze welfare compensation mechanisms in a stochastic setting. We consider two existing mechanisms, lump-sum payments and purchase power agreements, and we develop two novel mechanisms, based on the flow through the new transmission line and its economic value. Using a case study of the Northern European power market, we investigate how well these mechanisms succeed in mitigating risk for the countries involved. Using a theoretically ideal model-based mechanism, we show that there is a significant potential for mitigating risk through welfare compensation mechanisms. Out of the four practical mechanisms we consider, our results indicate that a mechanism based on the economic value of the new transmission line is most promising."
http://arxiv.org/abs/2205.05994v3,Recent Contributions to Theories of Discrimination,2022-05-12 10:07:43+00:00,['Paula Onuchic'],econ.TH,"This paper surveys the literature on theories of discrimination, focusing mainly on new contributions. Recent theories expand on the traditional taste-based and statistical discrimination frameworks by considering specific features of learning and signaling environments, often using novel information- and mechanism-design language; analyzing learning and decision making by algorithms; and introducing agents with behavioral biases and misspecified beliefs. This survey also attempts to narrow the gap between the economic perspective on ``theories of discrimination'' and the broader study of discrimination in the social science literature. In that respect, I first contribute by identifying a class of models of discriminatory institutions, made up of theories of discriminatory social norms and discriminatory institutional design. Second, I discuss issues relating to the measurement of discrimination, and the classification of discrimination as bias or statistical, direct or systemic, and accurate or inaccurate."
http://arxiv.org/abs/2205.08223v2,Conditions for Social Preference Transitivity When Cycle Involved and A $\hat{O}\mbox{-}\hat{I}$ Framework,2022-05-17 10:50:15+00:00,['Fujun Hou'],econ.TH,"We present some conditions for social preference transitivity under the majority rule when the individual preferences include cycles. First, our concern is with the restriction on the preference orderings of individuals except those (called cycle members) whose preferences constitute the cycles, but the considered transitivity is, of course, of the society as a whole. In our discussion, the individual preferences are assumed concerned and the cycle members' preferences are assumed as strict orderings. Particularly, for an alternative triple when one cycle is involved and the society is sufficient large (at least 5 individuals in the society), we present a sufficient condition for social transitivity; when two antagonistic cycles are involved and the society has at least 9 individuals, necessary and sufficient conditions are presented which are merely restricted on the preferences of those individuals except the cycle members. Based on the work due to Slutsky (1977) and Gaertner \& Heinecke (1978), we then outline a conceptual $\hat{O}\mbox{-}\hat{I}$ framework of social transitivity in an axiomatic manner. Connections between some already identified conditions and the $\hat{O}\mbox{-}\hat{I}$ framework is examined."
http://arxiv.org/abs/2205.07345v2,Joint Location and Cost Planning in Maximum Capture Facility Location under Multiplicative Random Utility Maximization,2022-05-15 17:45:38+00:00,"['Ngan Ha Duong', 'Tien Thanh Dam', 'Thuy Anh Ta', 'Tien Mai']",math.OC,"We study a joint facility location and cost planning problem in a competitive market under random utility maximization (RUM) models. The objective is to locate new facilities and make decisions on the costs (or budgets) to spend on the new facilities, aiming to maximize an expected captured customer demand, assuming that customers choose a facility among all available facilities according to a RUM model. We examine two RUM frameworks in the discrete choice literature, namely, the additive and multiplicative RUM. While the former has been widely used in facility location problems, we are the first to explore the latter in the context. We numerically show that the two RUM frameworks can well approximate each other in the context of the cost optimization problem. In addition, we show that, under the additive RUM framework, the resultant cost optimization problem becomes highly non-convex and may have several local optima. In contrast, the use of the multiplicative RUM brings several advantages to the competitive facility location problem. For instance, the cost optimization problem under the multiplicative RUM can be solved efficiently by a general convex optimization solver or can be reformulated as a conic quadratic program and handled by a conic solver available in some off-the-shelf solvers such as CPLEX or GUROBI. Furthermore, we consider a joint location and cost optimization problem under the multiplicative RUM and propose three approaches to solve the problem, namely, an equivalent conic reformulation, a multi-cut outer-approximation algorithm, and a local search heuristic. We provide numerical experiments based on synthetic instances of various sizes to evaluate the performances of the proposed algorithms in solving the cost optimization, and the joint location and cost optimization problems."
http://arxiv.org/abs/2205.07486v3,Influencing a Polarized and Connected Legislature,2022-05-16 07:37:14+00:00,"['Ratul Das Chaudhury', 'C. Matthew Leister', 'Birendra Rai']",econ.GN,"When can an interest group exploit polarization between political parties to its advantage? Building upon Battaglini and Patacchini (2018), we study a model where an interest group credibly promises payments to legislators conditional on voting for its preferred policy. A legislator can be directly susceptible to other legislators and value voting like them. The overall pattern of inter-legislator susceptibility determines the relative influence of individual legislators, and therefore the relative influence of the parties. We show that high levels of ideological or affective polarization are more likely to benefit the interest group when the party ideologically aligned with the interest group is relatively more influential. However, ideological and affective polarization operate in different ways. The influence of legislators is independent of ideological polarization. In contrast, affective polarization effectively creates negative links between legislators across parties, and thus modifies the relative influence of individual legislators and parties."
http://arxiv.org/abs/2205.07519v1,"Fair Shares: Feasibility, Domination and Incentives",2022-05-16 08:52:42+00:00,"['Moshe Babaioff', 'Uriel Feige']",econ.TH,"We consider fair allocation of a set $M$ of indivisible goods to $n$ equally-entitled agents, with no monetary transfers. Every agent $i$ has a valuation $v_i$ from some given class of valuation functions. A share $s$ is a function that maps a pair $(v_i,n)$ to a value, with the interpretation that if an allocation of $M$ to $n$ agents fails to give agent $i$ a bundle of value at least equal to $s(v_i,n)$, this serves as evidence that the allocation is not fair towards $i$. For such an interpretation to make sense, we would like the share to be feasible, meaning that for any valuations in the class, there is an allocation that gives every agent at least her share. The maximin share was a natural candidate for a feasible share for additive valuations. However, Kurokawa, Procaccia and Wang [2018] show that it is not feasible.
  We initiate a systematic study of the family of feasible shares. We say that a share is \emph{self maximizing} if truth-telling maximizes the implied guarantee. We show that every feasible share is dominated by some self-maximizing and feasible share. We seek to identify those self-maximizing feasible shares that are polynomial time computable, and offer the highest share values. We show that a SM-dominating feasible share -- one that dominates every self-maximizing (SM) feasible share -- does not exist for additive valuations (and beyond). Consequently, we relax the domination property to that of domination up to a multiplicative factor of $ρ$ (called $ρ$-dominating). For additive valuations we present shares that are feasible, self-maximizing and polynomial-time computable. For $n$ agents we present such a share that is $\frac{2n}{3n-1}$-dominating. For two agents we present such a share that is $(1 - ε)$-dominating. Moreover, for these shares we present poly-time algorithms that compute allocations that give every agent at least her share."
http://arxiv.org/abs/2204.04860v1,"State capital involvement, managerial sentiment and firm innovation performance Evidence from China",2022-04-11 04:20:35+00:00,['Xiangtai Zuo'],stat.AP,"In recent years, more and more state-owned enterprises (SOEs) have been embedded in the restructuring and governance of private enterprises through equity participation, providing a more advantageous environment for private enterprises in financing and innovation. However, there is a lack of knowledge about the underlying mechanisms of SOE intervention on corporate innovation performance. Hence, in this study, we investigated the association of state capital intervention with innovation performance, meanwhile further investigated the potential mediating and moderating role of managerial sentiment and financing constraints, respectively, using all listed non-ST firms from 2010 to 2020 as the sample. The results revealed two main findings: 1) state capital intervention would increase innovation performance through managerial sentiment; 2) financing constraints would moderate the effect of state capital intervention on firms' innovation performance."
http://arxiv.org/abs/2204.03799v1,Optimal allocations to heterogeneous agents with an application to stimulus checks,2022-04-08 01:04:19+00:00,"['Vegard M. Nygaard', 'Bent E. Sørensen', 'Fan Wang']",econ.GN,"A planner allocates discrete transfers of size $D_g$ to $N$ heterogeneous groups labeled $g$ and has CES preferences over the resulting outcomes, $H_g(D_g)$. We derive a closed-form solution for optimally allocating a fixed budget subject to group-specific inequality constraints under the assumption that increments in the $H_g$ functions are non-increasing. We illustrate our method by studying allocations of ""support checks"" from the U.S. government to households during both the Great Recession and the COVID-19 pandemic. We compare the actual allocations to optimal ones under alternative constraints, assuming the government focused on stimulating aggregate consumption during the 2008--2009 crisis and focused on welfare during the 2020--2021 crisis. The inputs for this analysis are obtained from versions of a life-cycle model with heterogeneous households, which predicts household-type-specific consumption and welfare responses to tax rebates and cash transfers."
http://arxiv.org/abs/2204.01933v1,You are what your parents expect: Height and local reference points,2022-04-05 02:04:29+00:00,"['Fan Wang', 'Esteban Puentes', 'Jere R. Behrman', 'Flávio Cunha']",econ.GN,"Recent estimates are that about 150 million children under five years of age are stunted, with substantial negative consequences for their schooling, cognitive skills, health, and economic productivity. Therefore, understanding what determines such growth retardation is significant for designing public policies that aim to address this issue. We build a model for nutritional choices and health with reference-dependent preferences. Parents care about the health of their children relative to some reference population. In our empirical model, we use height as the health outcome that parents target. Reference height is an equilibrium object determined by earlier cohorts' parents' nutritional choices in the same village. We explore the exogenous variation in reference height produced by a protein-supplementation experiment in Guatemala to estimate our model's parameters. We use our model to decompose the impact of the protein intervention on height into price and reference-point effects. We find that the changes in reference points account for 65% of the height difference between two-year-old children in experimental and control villages in the sixth annual cohort born after the initiation of the intervention."
http://arxiv.org/abs/2204.03318v3,What is the effect of EU's fuel-tax cuts on Russia's oil income?,2022-04-07 09:31:44+00:00,"['Johan Gars', 'Daniel Spiro', 'Henrik Wachtmeister']",econ.GN,"Following the oil-price surge in the wake of Russia's invasion of Ukraine, many countries in the EU are cutting taxes on petrol and diesel. Using standard theory and empirical estimates, we assess how such tax cuts influence the oil income in Russia. We find that a tax cut of 20 euro cents per liter increase Russia's oil profits by around 11 million Euros per day in the short run and long run. This is equivalent to 4100 million Euros in a year, 0.3% of Russia's GDP or 7% of its military spending. We show that a cash transfer to EU citizens, with an equivalent fiscal burden as the tax cut, reduces these side effects to a fraction."
http://arxiv.org/abs/2204.07672v4,Abadie's Kappa and Weighting Estimators of the Local Average Treatment Effect,2022-04-15 22:51:50+00:00,"['Tymon Słoczyński', 'S. Derya Uysal', 'Jeffrey M. Wooldridge']",econ.EM,"Recent research has demonstrated the importance of flexibly controlling for covariates in instrumental variables estimation. In this paper we study the finite sample and asymptotic properties of various weighting estimators of the local average treatment effect (LATE), motivated by Abadie's (2003) kappa theorem and offering the requisite flexibility relative to standard practice. We argue that two of the estimators under consideration, which are weight normalized, are generally preferable. Several other estimators, which are unnormalized, do not satisfy the properties of scale invariance with respect to the natural logarithm and translation invariance, thereby exhibiting sensitivity to the units of measurement when estimating the LATE in logs and the centering of the outcome variable more generally. We also demonstrate that, when noncompliance is one sided, certain weighting estimators have the advantage of being based on a denominator that is strictly greater than zero by construction. This is the case for only one of the two normalized estimators, and we recommend this estimator for wider use. We illustrate our findings with a simulation study and three empirical applications, which clearly document the sensitivity of unnormalized estimators to how the outcome variable is coded. We implement the proposed estimators in the Stata package kappalate."
http://arxiv.org/abs/2204.07255v6,The cost of strategy-proofness in school choice,2022-04-14 22:35:43+00:00,"['Josue Ortega', 'Thilo Klein']",econ.TH,"We compare the outcomes of the most prominent strategy-proof and stable algorithm (Deferred Acceptance, DA) and the most prominent strategy-proof and Pareto optimal algorithm (Top Trading Cycles, TTC) to the allocation generated by the rank-minimizing mechanism (RM). While one would expect that RM improves upon both DA and TTC in terms of rank efficiency, the size of the improvement is nonetheless surprising. Moreover, while it is not explicitly designed to do so, RM also significantly improves the placement of the worst-off student. Furthermore, RM generates less justified envy than TTC. We corroborate our findings using data on school admissions in Budapest."
http://arxiv.org/abs/2204.05314v2,Non-equilibrium phase transitions in competitive markets caused by network effects,2022-04-11 18:00:00+00:00,['Andrew Lucas'],cond-mat.stat-mech,"Network effects are the added value derived solely from the popularity of a product in an economic market. Using agent-based models inspired by statistical physics, we propose a minimal theory of a competitive market for (nearly) indistinguishable goods with demand-side network effects, sold by statistically identical sellers. With weak network effects, the model reproduces conventional microeconomics: there is a statistical steady state of (nearly) perfect competition. Increasing network effects, we find a phase transition to a robust non-equilibrium phase driven by the spontaneous formation and collapse of fads in the market. When sellers update prices sufficiently quickly, an emergent monopolist can capture the market and undercut competition, leading to a symmetry- and ergodicity-breaking transition. The non-equilibrium phase simultaneously exhibits three empirically established phenomena not contained in the standard theory of competitive markets: spontaneous price fluctuations, persistent seller profits, and broad distributions of firm market shares."
http://arxiv.org/abs/2204.07891v1,"Bad Weather, Social Network, and Internal Migration; Case of Japanese Sumo Wrestlers 1946-1985",2022-04-17 00:20:18+00:00,['Eiji Yamamura'],econ.GN,"Post-World War II , there was massive internal migration from rural to urban areas in Japan. The location of Sumo stables was concentrated in Tokyo. Hence, supply of Sumo wrestlers from rural areas to Tokyo was considered as migration. Using a panel dataset covering forty years, specifically 1946-1985, this study investigates how weather conditions and social networks influenced the labor supply of Sumo wrestlers. Major findings are; (1) inclemency of the weather in local areas increased supply of Sumo wrestlers in the period 1946-1965, (2) the effect of the bad weather conditions is greater in the locality where large number of Sumo wrestlers were supplied in the pre-war period, (3) neither the occurrence of bad weather conditions nor their interactions with sumo-wrestlers influenced the supply of Sumo wrestlers in the period 1966-1985. These findings imply that the negative shock of bad weather conditions on agriculture in the rural areas incentivized young individuals to be apprenticed in Sumo stables in Tokyo. Additionally, in such situations, the social networks within Sumo wrestler communities from the same locality are important. However, once the share of workers in agricultural sectors became very low, this mechanism did not work."
http://arxiv.org/abs/2204.06588v2,Environmental injustice in America: Racial disparities in exposure to air pollution health damages from freight trucking,2022-04-13 18:14:26+00:00,"['Priyank Lathwal', 'Parth Vaishnav', 'M. Granger Morgan']",econ.GN,"PM2.5 produced by freight trucks has adverse impacts on human health. However, it is unknown to what extent freight trucking affects communities of color and the total public health burden arising from the sector. Based on spatially resolved US federal government data, we explore the geographic distribution of freight trucking emissions and demonstrate that Black and Hispanic populations are more likely to be exposed to elevated emissions from freight trucks. Our results indicate that freight trucks contribute ~10% of NOx and ~12% of CO2 emissions from all sources in the continental US. The annual costs to human health and the environment due to NOx, PM2.5, SO2, and CO2 from freight trucking in the US are estimated respectively to be $11B, $5.5B, $110M, and $30B. Overall, the sector is responsible for nearly two-fifths (~$47B out of $120B) of all transportation-related public health damages."
http://arxiv.org/abs/2205.01387v1,Integration of Behavioral Economic Models to Optimize ML performance and interpretability: a sandbox example,2022-05-03 09:29:36+00:00,"['Emilio Soria-Olivas', 'José E. Vila Gisbert', 'Regino Barranquero Cardeñosa', 'Yolanda Gomez']",econ.TH,"This paper presents a sandbox example of how the integration of models borrowed from Behavioral Economic (specifically Protection-Motivation Theory) into ML algorithms (specifically Bayesian Networks) can improve the performance and interpretability of ML algorithms when applied to Behavioral Data. The integration of Behavioral Economics knowledge to define the architecture of the Bayesian Network increases the accuracy of the predictions in 11 percentage points. Moreover, it simplifies the training process, making unnecessary training computational efforts to identify the optimal structure of the Bayesian Network. Finally, it improves the explicability of the algorithm, avoiding illogical relations among variables that are not supported by previous behavioral cybersecurity literature. Although preliminary and limited to 0ne simple model trained with a small dataset, our results suggest that the integration of behavioral economics and complex ML models may open a promising strategy to improve the predictive power, training costs and explicability of complex ML models. This integration will contribute to solve the scientific issue of ML exhaustion problem and to create a new ML technology with relevant scientific, technological and market implications."
http://arxiv.org/abs/2205.01216v1,"Estimating beneficiaries of the child tax credit: past, present, and future",2022-05-02 21:23:29+00:00,"['Ashley Nunes', 'Chung Yi See', 'Lucas Woodley', 'Nicole A. Divers', 'Audrey L. Cui']",econ.GN,"Government efforts to address child poverty commonly encompass economic assistance programs that bolster household income. The Child Tax Credit (CTC) is the most prominent example of this. Introduced by the United States Congress in 1997, the program endeavors to help working parents via income stabilization. Our work examines the extent to which the CTC has done so. Our study, which documents clear, consistent, and compelling evidence of gender inequity in benefits realization, yields four key findings. First, stringent requisite income thresholds disproportionally disadvantage single mothers, a reflection of the high concentration of this demographic in lower segments of the income distribution. Second, married parents and, to a lesser extent, single fathers, are the primary beneficiaries of the CTC program when benefits are structured as credits rather than refunds. Third, making program benefits more generous disproportionally reduces how many single mothers, relative to married parents and single fathers, can claim this benefit. Fourth and finally, increasing credit refundability can mitigate gender differences in relief eligibility, although doing so imposes externalities of its own. Our findings can inform public policy discourse surrounding the efficacy of programs like the CTC and the effectiveness of programs aimed at alleviating child poverty."
http://arxiv.org/abs/2205.00492v1,Understanding Distance Measures Among Elections,2022-05-01 15:22:46+00:00,"['Niclas Boehmer', 'Piotr Faliszewski', 'Rolf Niedermeier', 'Stanisław Szufa', 'Tomasz Wąs']",cs.GT,"Motivated by putting empirical work based on (synthetic) election data on a more solid mathematical basis, we analyze six distances among elections, including, e.g., the challenging-to-compute but very precise swap distance and the distance used to form the so-called map of elections. Among the six, the latter seems to strike the best balance between its computational complexity and expressiveness."
http://arxiv.org/abs/2205.00577v2,Higher-order Expansions and Inference for Panel Data Models,2022-05-01 23:04:40+00:00,"['Jiti Gao', 'Bin Peng', 'Yayi Yan']",econ.EM,"In this paper, we propose a simple inferential method for a wide class of panel data models with a focus on such cases that have both serial correlation and cross-sectional dependence. In order to establish an asymptotic theory to support the inferential method, we develop some new and useful higher-order expansions, such as Berry-Esseen bound and Edgeworth Expansion, under a set of simple and general conditions. We further demonstrate the usefulness of these theoretical results by explicitly investigating a panel data model with interactive effects which nests many traditional panel data models as special cases. Finally, we show the superiority of our approach over several natural competitors using extensive numerical studies."
http://arxiv.org/abs/2205.00852v1,"A Note on ""A survey of preference estimation with unobserved choice set heterogeneity"" by Gregory S. Crawford, Rachel Griffith, and Alessandro Iaria",2022-05-02 12:33:21+00:00,['C. Angelo Guevara'],econ.EM,"Crawford's et al. (2021) article on estimation of discrete choice models with unobserved or latent consideration sets, presents a unified framework to address the problem in practice by using ""sufficient sets"", defined as a combination of past observed choices. The proposed approach is sustained in a re-interpretation of a consistency result by McFadden (1978) for the problem of sampling of alternatives, but the usage of that result in Crawford et al. (2021) is imprecise in an important matter. It is stated that consistency would be attained if any subset of the true consideration set is used for estimation, but McFadden (1978) shows that, in general, one needs to do a sampling correction that depends on the protocol used to draw the choice set. This note derives the sampling correction that is required when the choice set for estimation is built from past choices. Then, it formalizes the conditions under which such correction would fulfill the uniform condition property and can therefore be ignored when building practical estimators, such as the ones analyzed by Crawford et al. (2021)."
http://arxiv.org/abs/2204.13815v2,Controlling for Latent Confounding with Triple Proxies,2022-04-28 23:12:10+00:00,['Ben Deaner'],econ.EM,"We present new results for nonparametric identification of causal effects using noisy proxies for unobserved confounders. Our approach builds on the results of \citet{Hu2008} who tackle the problem of general measurement error. We call this the `triple proxy' approach because it requires three proxies that are jointly independent conditional on unobservables. We consider three different choices for the third proxy: it may be an outcome, a vector of treatments, or a collection of auxiliary variables. We compare to an alternative identification strategy introduced by \citet{Miao2018a} in which causal effects are identified using two conditionally independent proxies. We refer to this as the `double proxy' approach. The triple proxy approach identifies objects that are not identified by the double proxy approach, including some that capture the variation in average treatment effects between strata of the unobservables. Moreover, the conditional independence assumptions in the double and triple proxy approaches are non-nested."
http://arxiv.org/abs/2205.00032v3,Improving the Deferred Acceptance with Minimal Compromise,2022-04-29 18:30:05+00:00,"['Mustafa Oguz Afacan', 'Umut Dur', 'A. Arda Gitmez', 'Özgür Yılmaz']",econ.TH,"In school choice problems, the motivation for students' welfare (efficiency) is restrained by concerns to respect schools' priorities (fairness). Among the fair matchings, even the best one in terms of welfare (SOSM) is inefficient. Moreover, any mechanism that improves welfare over the SOSM is manipulable by the students. First, we characterize the ""least manipulable"" mechanisms in this class: monotonically-promoting transformation proofness ensures that no student is better off by promoting their assigned school under the true preferences. Second, we use the notion that a matching is less unfair if it yields a smaller set of students whose priorities are violated, and define minimal unfairness accordingly. We then show that the Efficiency Adjusted Deferred Acceptance (EADA) mechanism is minimally unfair in the class of efficient and monotonically-promoting transformation proof mechanisms. When the objective is to improve students' welfare over the SOSM, this characterization implies an important insight into the frontier of the main axioms in school choice."
http://arxiv.org/abs/2204.13392v1,Dynamic screening,2022-04-28 10:19:27+00:00,"['David Lagziel', 'Ehud Lehrer']",econ.TH,"We study dynamic screening problems where elements are subjected to noisy evaluations and, in every stage, some of the elements are rejected while the remaining ones are independently re-evaluated in subsequent stages. We prove that, ceteris paribus, the quality of a dynamic screening process is not monotonic in the number of stages. Specifically, we examine the accepted elements' values and show that adding a single stage to a screening process may produce inferior results, in terms of stochastic dominance, whereas increasing the number of stages substantially leads to a first-best outcome."
http://arxiv.org/abs/2204.13102v1,The Price and Cost of Bitcoin,2022-04-27 17:57:55+00:00,"['John E. Marthinsen', 'Steven R. Gordon']",econ.GN,"Explaining changes in bitcoin's price and predicting its future have been the foci of many research studies. In contrast, far less attention has been paid to the relationship between bitcoin's mining costs and its price. One popular notion is the cost of bitcoin creation provides a support level below which this cryptocurrency's price should never fall because if it did, mining would become unprofitable and threaten the maintenance of bitcoin's public ledger. Other research has used mining costs to explain or forecast bitcoin's price movements. Competing econometric analyses have debunked this idea, showing that changes in mining costs follow changes in bitcoin's price rather than preceding them, but the reason for this behavior remains unexplained in these analyses. This research aims to employ economic theory to explain why econometric studies have failed to predict bitcoin prices and why mining costs follow movements in bitcoin prices rather than precede them. We do so by explaining the chain of causality connecting a bitcoin's price to its mining costs."
http://arxiv.org/abs/2206.08438v1,Fast and Accurate Variational Inference for Large Bayesian VARs with Stochastic Volatility,2022-06-16 20:42:27+00:00,"['Joshua C. C. Chan', 'Xuewen Yu']",econ.EM,"We propose a new variational approximation of the joint posterior distribution of the log-volatility in the context of large Bayesian VARs. In contrast to existing approaches that are based on local approximations, the new proposal provides a global approximation that takes into account the entire support of the joint distribution. In a Monte Carlo study we show that the new global approximation is over an order of magnitude more accurate than existing alternatives. We illustrate the proposed methodology with an application of a 96-variable VAR with stochastic volatility to measure global bank network connectedness."
http://arxiv.org/abs/2206.11847v1,Proposing Dynamic Model of Functional Interactions of IoT Technological Innovation System by Using System Dynamics and Fuzzy DEMATEL,2022-06-17 12:07:43+00:00,"['Mohammad Mousakhani', 'Fatemeh Saghafi', 'Mohammad Hasanzadeh', 'Mohammad Ebrahim Sadeghi']",econ.GN,"One of the emerging technologies, which is expected to have tremendous effects on community development, is the Internet of Things technology. Given the prospects for this technology and the country's efforts to its development, policymaking for this technology is very crucial. The technological innovation system is one of the most important dynamic approaches in the field of modern technology policy. In this approach, by analyzing various functions which are influencing the development of a technology, the proper path to technological advancement is explained. For this reason, 10 major factors influencing the development of emerging technologies have been identified based on previous studies in this area and the effect of ten factors on each other was identified by using the system dynamics and the fuzzy DEMATEL method and the interactions between these functions were modeled. Market formation, resource mobilization, exploitation of the regime and policymaking and coordination functions have the most direct effect on the other functions. Also, policymaking and coordination, market formation, entrepreneurial activities, creating structure and resource mobilization have the most total impact on the other functions. Regard to resource constraint in the system, the policy makers should focus on those factors which have the most direct and total impact on the others that in this research are market formation, entrepreneurial activities, resource mobilization and policymaking and coordination. Given the dynamic nature of technology development, this model can help policymakers in the decision making process for the development of the Internet of Things."
http://arxiv.org/abs/2206.11205v1,Clearing function in the context of the invariant manifold method,2022-06-22 16:44:27+00:00,"['A. Mustafin', 'A. Kantarbayeva']",nlin.AO,"Clearing functions (CFs), which express a mathematical relationship between the expected throughput of a production facility in a planning period and its workload (or work-in-progress, WIP) in that period have shown considerable promise for modeling WIP-dependent cycle times in production planning. While steady-state queueing models are commonly used to derive analytic expressions for CFs, the finite length of planning periods calls their validity into question. We apply a different approach to propose a mechanistic model for one-resource, one-product factory shop based on the analogy between the operation of machine and enzyme molecule. The model is reduced to a singularly perturbed system of two differential equations for slow (WIP) and fast (busy machines) variables, respectively. The analysis of this slow-fast system finds that CF is nothing but a result of the asymptotic expansion of the slow invariant manifold. The validity of CF is ultimately determined by how small is the parameter multiplying the derivative of the fast variable. It is shown that sufficiently small characteristic ratio 'working machines : WIP' guarantees the applicability of CF approximation in unsteady-state operation."
http://arxiv.org/abs/2206.10601v1,Has the Relationship between Urban and Suburban Automobile Travel Changed across Generations? Comparing Millennials and Generation Xers in the United States,2022-06-19 06:53:01+00:00,['Xize Wang'],physics.soc-ph,"Using U.S. nationwide travel surveys for 1995, 2001, 2009 and 2017, this study compares Millennials with their previous generation (Gen Xers) in terms of their automobile travel across different neighborhood patterns. At the age of 16 to 28 years old, Millennials have lower daily personal vehicle miles traveled and car trips than Gen Xers in urban (higher-density) and suburban (lower-density) neighborhoods. Such differences remain unchanged after adjusting for the socio-economic, vehicle ownership, life cycle, year-specific and regional-specific factors. In addition, the associations between residential density and automobile travel for the 16- to 28-year-old Millennials are flatter than that for Gen Xers, controlling for the aforementioned covariates. These generational differences remain for the 24- to 36-year-old Millennials, during the period when the U.S. economy was recovering from the recession. These findings show that, in both urban and suburban neighborhoods, Millennials in the U.S. are less auto-centric than the previous generation during early life stages, regardless of economic conditions. Whether such difference persists over later life stages remains an open question and is worth continuous attention."
http://arxiv.org/abs/2206.10287v2,Superiority of Instantaneous Decisions in Thin Dynamic Matching Markets,2022-06-21 12:13:14+00:00,"['Johannes Bäumler', 'Martin Bullinger', 'Stefan Kober', 'Donghao Zhu']",cs.DS,"We study a dynamic matching procedure where homogeneous agents arrive at random according to a Poisson process and form edges at random yielding a sparse market. Agents leave according to a certain departure distribution and may leave early by forming a pair with a compatible agent. The primary objective is to maximize the number of matched agents. Our main result is to show that a mild condition on the departure distribution suffices to get almost optimal performance of instantaneous matching, despite operating in a thin market. We are thus the first to provide a natural condition under which instantaneous decisions are superior in a market that is both sparse and thin. This result is surprising because similar results in the previous literature are based on market thickness. In addition, instantaneous matching performs well with respect to further objectives such as minimizing waiting times and avoiding the risk of market congestion. We develop new techniques for proving our results going beyond commonly adopted methods for Markov processes."
http://arxiv.org/abs/2206.02722v1,Vicious Cycle of Poverty in Haor Region of Bangladesh- Impact of Formal and Informal Credits,2022-06-06 16:27:06+00:00,['Nazrul Islam'],econ.GN,"This research attempts to explore the key research questions about what are the different microcredit programs in Haor area in Bangladesh? And do microcredit programs have a positive impact on livelihoods of the clients in terms of selected social indicators viz. income, consumption, assets, net worth, education, access to finance, social capacity, food security and handling socks etc. in Haor area in Bangladesh? Utilizing difference-in-difference and factor analysis, we explore the nature and terms of conditions of available formal and informal micro-creditss in Haor region of Bangladesh; and investigate the impact of micro-creditss on the poverty condition of Haor people in Bangladesh. The findings showed that total income of borrowers has been increased over non-borrowers (z=6.75) significantly. Among the components of income, non-agricultural income has been increased significantly on the other hand income from labor sale has been decreased significantly. Total consumption expenditure with its heads of food and non-food consumption of both formal borrowers and informal borrowers have been increased over the period 2016-2019 significantly. Most of the key informants agreed that the findings are very much consistent with prevailing condition of micro-credits in Haor region. However, some of them raised question about the impacts of micro-credits. They argued that there is no straightforward positive impact of micro-credits on poverty condition of the households."
http://arxiv.org/abs/2206.01562v1,Prescriptive maintenance with causal machine learning,2022-06-03 13:35:57+00:00,"['Toon Vanderschueren', 'Robert Boute', 'Tim Verdonck', 'Bart Baesens', 'Wouter Verbeke']",econ.GN,"Machine maintenance is a challenging operational problem, where the goal is to plan sufficient preventive maintenance to avoid machine failures and overhauls. Maintenance is often imperfect in reality and does not make the asset as good as new. Although a variety of imperfect maintenance policies have been proposed in the literature, these rely on strong assumptions regarding the effect of maintenance on the machine's condition, assuming the effect is (1) deterministic or governed by a known probability distribution, and (2) machine-independent. This work proposes to relax both assumptions by learning the effect of maintenance conditional on a machine's characteristics from observational data on similar machines using existing methodologies for causal inference. By predicting the maintenance effect, we can estimate the number of overhauls and failures for different levels of maintenance and, consequently, optimize the preventive maintenance frequency to minimize the total estimated cost. We validate our proposed approach using real-life data on more than 4,000 maintenance contracts from an industrial partner. Empirical results show that our novel, causal approach accurately predicts the maintenance effect and results in individualized maintenance schedules that are more accurate and cost-effective than supervised or non-individualized approaches."
http://arxiv.org/abs/2206.02993v2,"False Consensus, Information Theory, and Prediction Markets",2022-06-07 03:46:11+00:00,"['Yuqing Kong', 'Grant Schoenebeck']",cs.GT,"We study a setting where Bayesian agents with a common prior have private information related to an event's outcome and sequentially make public announcements relating to their information. Our main result shows that when agents' private information is independent conditioning on the event's outcome whenever agents have similar beliefs about the outcome, their information is aggregated. That is, there is no false consensus.
  Our main result has a short proof based on a natural information theoretic framework. A key ingredient of the framework is the equivalence between the sign of the ``interaction information'' and a super/sub-additive property of the value of people's information. This provides an intuitive interpretation and an interesting application of the interaction information, which measures the amount of information shared by three random variables.
  We illustrate the power of this information theoretic framework by reproving two additional results within it: 1) that agents quickly agree when announcing (summaries of) beliefs in round robin fashion [Aaronson 2005]; and 2) results from [Chen et al 2010] on when prediction market agents should release information to maximize their payment. We also interpret the information theoretic framework and the above results in prediction markets by proving that the expected reward of revealing information is the conditional mutual information of the information revealed."
http://arxiv.org/abs/2206.02344v1,"Decentralized, Communication- and Coordination-free Learning in Structured Matching Markets",2022-06-06 04:08:04+00:00,"['Chinmay Maheshwari', 'Eric Mazumdar', 'Shankar Sastry']",cs.AI,"We study the problem of online learning in competitive settings in the context of two-sided matching markets. In particular, one side of the market, the agents, must learn about their preferences over the other side, the firms, through repeated interaction while competing with other agents for successful matches. We propose a class of decentralized, communication- and coordination-free algorithms that agents can use to reach to their stable match in structured matching markets. In contrast to prior works, the proposed algorithms make decisions based solely on an agent's own history of play and requires no foreknowledge of the firms' preferences. Our algorithms are constructed by splitting up the statistical problem of learning one's preferences, from noisy observations, from the problem of competing for firms. We show that under realistic structural assumptions on the underlying preferences of the agents and firms, the proposed algorithms incur a regret which grows at most logarithmically in the time horizon. Our results show that, in the case of matching markets, competition need not drastically affect the performance of decentralized, communication and coordination free online learning algorithms."
http://arxiv.org/abs/2205.11568v3,Quasi Black-Box Variational Inference with Natural Gradients for Bayesian Learning,2022-05-23 18:54:27+00:00,"['Martin Magris', 'Mostafa Shabani', 'Alexandros Iosifidis']",stat.ML,"We develop an optimization algorithm suitable for Bayesian learning in complex models. Our approach relies on natural gradient updates within a general black-box framework for efficient training with limited model-specific derivations. It applies within the class of exponential-family variational posterior distributions, for which we extensively discuss the Gaussian case for which the updates have a rather simple form. Our Quasi Black-box Variational Inference (QBVI) framework is readily applicable to a wide class of Bayesian inference problems and is of simple implementation as the updates of the variational posterior do not involve gradients with respect to the model parameters, nor the prescription of the Fisher information matrix. We develop QBVI under different hypotheses for the posterior covariance matrix, discuss details about its robust and feasible implementation, and provide a number of real-world applications to demonstrate its effectiveness."
http://arxiv.org/abs/2205.11632v1,Evolution of biomedical innovation quantified via billions of distinct article-level MeSH keyword combinations,2022-05-23 20:59:33+00:00,['Alexander M. Petersen'],cs.DL,"We develop a systematic approach to measuring combinatorial innovation in the biomedical sciences based upon the comprehensive ontology of Medical Subject Headings (MeSH). This approach leverages an expert-defined knowledge ontology that features both breadth (27,875 MeSH analyzed across 25 million articles indexed by PubMed from 1902 onwards) and depth (we differentiate between Major and Minor MeSH terms to identify differences in the knowledge network representation constructed from primary research topics only). With this level of uniform resolution we differentiate between three different modes of innovation contributing to the combinatorial knowledge network: (i) conceptual innovation associated with the emergence of new concepts and entities (measured as the entry of new MeSH); and (ii) recombinant innovation, associated with the emergence of new combinations, which itself consists of two types: peripheral (i.e., combinations involving new knowledge) and core (combinations comprised of pre-existing knowledge only). Another relevant question we seek to address is whether examining triplet and quartet combinations, in addition to the more traditional dyadic or pairwise combinations, provide evidence of any new phenomena associated with higher-order combinations. Analysis of the size, growth, and coverage of combinatorial innovation yield results that are largely independent of the combination order, thereby suggesting that the common dyadic approach is sufficient to capture essential phenomena. Our main results are twofold: (a) despite the persistent addition of new MeSH terms, the network is densifying over time meaning that scholars are increasingly exploring and realizing the vast space of all knowledge combinations; and (b) conceptual innovation is increasingly concentrated within single research articles, a harbinger of the recent paradigm shift towards convergence science."
http://arxiv.org/abs/2205.15760v3,Optimized Distortion and Proportional Fairness in Voting,2022-05-31 12:57:48+00:00,"['Soroush Ebadian', 'Anson Kahng', 'Dominik Peters', 'Nisarg Shah']",cs.GT,"A voting rule decides on a probability distribution over a set of m alternatives, based on rankings of those alternatives provided by agents. We assume that agents have cardinal utility functions over the alternatives, but voting rules have access to only the rankings induced by these utilities. We evaluate how well voting rules do on measures of social welfare and of proportional fairness, computed based on the hidden utility functions.
  In particular, we study the distortion of voting rules, which is a worst-case measure. It is an approximation ratio comparing the utilitarian social welfare of the optimum outcome to the social welfare produced by the outcome selected by the voting rule, in the worst case over possible input profiles and utility functions that are consistent with the input. The previous literature has studied distortion with unit-sum utility functions (which are normalized to sum to 1), and left a small asymptotic gap in the best possible distortion. Using tools from the theory of fair multi-winner elections, we propose the first voting rule which achieves the optimal distortion $Θ(\sqrt{m})$ for unit-sum utilities. Our voting rule also achieves optimum $Θ(\sqrt{m})$ distortion for a larger class of utilities, including unit-range and approval (0/1) utilities.
  We then take a worst-case approach to a quantitative measure of the fairness of a voting rule, called proportional fairness. Informally, it measures whether the influence of cohesive groups of agents on the voting outcome is proportional to the group size. We show that there is a voting rule which, without knowledge of the utilities, can achieve a $Θ(\log m)$-approximation to proportional fairness, and thus also to Nash welfare and to the core, making it interesting for applications in participatory budgeting. For all three approximations, we show that $Θ(\log m)$ is the best possible."
http://arxiv.org/abs/2205.15451v1,Economics of 100% renewable power systems,2022-05-30 22:34:40+00:00,['Takuya Hara'],econ.GN,"Studies have evaluated the economic feasibility of 100% renewable power systems using the optimization approach, but the mechanisms determining the results remain poorly understood. Based on a simple but essential model, this study found that the bottleneck formed by the largest mismatch between demand and power generation profiles determines the optimal capacities of generation and storage and their trade-off relationship. Applying microeconomic theory, particularly the duality of quantity and value, this study comprehensively quantified the relationships among the factor cost of technologies, their optimal capacities, and total system cost. Using actual profile data for multiple years/regions in Japan, this study demonstrated that hybrid systems comprising cost-competitive multiple renewable energy sources and different types of storage are critical for the economic feasibility of any profile."
http://arxiv.org/abs/2205.15115v3,A Novel Control-Oriented Cell Transmission Model Including Service Stations on Highways,2022-05-23 15:54:24+00:00,"['Carlo Cenedese', 'Michele Cucuzzella', 'Antonella Ferrara', 'John Lygeros']",eess.SY,"In this paper, we propose a novel model that describes how the traffic evolution on a highway stretch is affected by the presence of a service station. The presented model enhances the classical CTM dynamics by adding the dynamics associated with the service stations, where the vehicles may stop before merging back into the mainstream. We name it CTMs. We discuss its flexibility in describing different complex scenarios where multiple stations are characterized by different drivers' average stopping times corresponding to different services. The model has been developed to help design control strategies aimed at decreasing traffic congestion. Thus, we discuss how classical control schemes can interact with the proposed \gls{CTMs}. Finally, we validate the proposed model through numerical simulations and assess the effects of service stations on traffic evolution, which appear to be beneficial, especially for relatively short congested periods."
http://arxiv.org/abs/2205.12126v2,Estimation and Inference for High Dimensional Factor Model with Regime Switching,2022-05-24 14:57:58+00:00,"['Giovanni Urga', 'Fa Wang']",econ.EM,"This paper proposes maximum (quasi)likelihood estimation for high dimensional factor models with regime switching in the loadings. The model parameters are estimated jointly by the EM (expectation maximization) algorithm, which in the current context only requires iteratively calculating regime probabilities and principal components of the weighted sample covariance matrix. When regime dynamics are taken into account, smoothed regime probabilities are calculated using a recursive algorithm. Consistency, convergence rates and limit distributions of the estimated loadings and the estimated factors are established under weak cross-sectional and temporal dependence as well as heteroscedasticity. It is worth noting that due to high dimension, regime switching can be identified consistently after the switching point with only one observation. Simulation results show good performance of the proposed method. An application to the FRED-MD dataset illustrates the potential of the proposed method for detection of business cycle turning points."
http://arxiv.org/abs/2205.10198v3,"A New Central Limit Theorem for the Augmented IPW Estimator: Variance Inflation, Cross-Fit Covariance and Beyond",2022-05-20 14:17:53+00:00,"['Kuanhao Jiang', 'Rajarshi Mukherjee', 'Subhabrata Sen', 'Pragya Sur']",math.ST,"Estimation of the average treatment effect (ATE) is a central problem in causal inference. In recent times, inference for the ATE in the presence of high-dimensional covariates has been extensively studied. Among the diverse approaches that have been proposed, augmented inverse probability weighting (AIPW) with cross-fitting has emerged a popular choice in practice. In this work, we study this cross-fit AIPW estimator under well-specified outcome regression and propensity score models in a high-dimensional regime where the number of features and samples are both large and comparable. Under assumptions on the covariate distribution, we establish a new central limit theorem for the suitably scaled cross-fit AIPW that applies without any sparsity assumptions on the underlying high-dimensional parameters. Our CLT uncovers two crucial phenomena among others: (i) the AIPW exhibits a substantial variance inflation that can be precisely quantified in terms of the signal-to-noise ratio and other problem parameters, (ii) the asymptotic covariance between the pre-cross-fit estimators is non-negligible even on the root-n scale. These findings are strikingly different from their classical counterparts. On the technical front, our work utilizes a novel interplay between three distinct tools--approximate message passing theory, the theory of deterministic equivalents, and the leave-one-out approach. We believe our proof techniques should be useful for analyzing other two-stage estimators in this high-dimensional regime. Finally, we complement our theoretical results with simulations that demonstrate both the finite sample efficacy of our CLT and its robustness to our assumptions."
http://arxiv.org/abs/2205.12881v1,A Continuum Model of Stable Matching With Finite Capacities,2022-05-25 16:05:41+00:00,['Nick Arnosti'],econ.TH,"This paper introduces a unified framework for stable matching, which nests the traditional definition of stable matching in finite markets and the continuum definition of stable matching from Azevedo and Leshno (2016) as special cases. Within this framework, I identify a novel continuum model, which makes individual-level probabilistic predictions.
  This new model always has a unique stable outcome, which can be found using an analog of the Deferred Acceptance algorithm. The crucial difference between this model and that of Azevedo and Leshno (2016) is that they assume that the amount of student interest at each school is deterministic, whereas my proposed alternative assumes that it follows a Poisson distribution. As a result, this new model accurately predicts the simulated distribution of cutoffs, even for markets with only ten schools and twenty students.
  This model generates new insights about the number and quality of matches. When schools are homogeneous, it provides upper and lower bounds on students' average rank, which match results from Ashlagi, Kanoria and Leshno (2017) but apply to more general settings. This model also provides clean analytical expressions for the number of matches in a platform pricing setting considered by Marx and Schummer (2021)."
http://arxiv.org/abs/2205.12917v2,Identification of Auction Models Using Order Statistics,2022-05-25 17:11:05+00:00,"['Yao Luo', 'Ruli Xiao']",econ.EM,"Auction data often contain information on only the most competitive bids as opposed to all bids. The usual measurement error approaches to unobserved heterogeneity are inapplicable due to dependence among order statistics. We bridge this gap by providing a set of positive identification results. First, we show that symmetric auctions with discrete unobserved heterogeneity are identifiable using two consecutive order statistics and an instrument. Second, we extend the results to ascending auctions with unknown competition and unobserved heterogeneity."
http://arxiv.org/abs/2206.09037v1,Assessing transportation accessibility equity via open data,2022-06-17 22:31:01+00:00,"['Amirhesam Badeanlou', 'Andrea Araldo', 'Marco Diana']",cs.CY,"We propose a methodology to assess transportation accessibility inequity in metropolitan areas. The methodology is based on the classic analysis tools of Lorenz curves and Gini indices, but the novelty resides in the fact that it can be easily applied in an automated way to several cities around the World, with no need for customized data treatment. Indeed, our equity metrics can be computed solely relying on open data, publicly available in standardized form. We showcase our method and study transp"
http://arxiv.org/abs/2206.08745v1,Environmentally extended input-output analysis in complex networks: a multilayer approach,2022-06-17 12:59:59+00:00,"['Alessandra Cornaro', 'Giorgio Rizzini']",econ.GN,"In this paper we propose a methodology suitable for a comprehensive analysis of the global embodied energy flow trough a complex network approach. To this end, we extend the existing literature, providing a multilayer framework based on the environmentally extended input-output analysis. The multilayer structure, with respect to the traditional approach, allows us to unveil the different role of sectors and economies in the system. In order to identify key sectors and economies, we make use of hub and authority scores, by adapting to our framework an extension of the Kleinberg algorithm, called Multi-Dimensional HITS (MD-HITS). A numerical analysis based on multi-region input-output tables shows how the proposed approach provides meaningful insights."
http://arxiv.org/abs/2206.07132v1,Dynamics of a Binary Option Market with Exogenous Information and Price Sensitivity,2022-05-18 14:39:16+00:00,"['Hannah Gampe', 'Christopher Griffin']",q-fin.TR,"In this paper, we derive and analyze a continuous of a binary option market with exogenous information. The resulting non-linear system has a discontinuous right hand side, which can be analyzed using zero-dimensional Filippov surfaces. Under general assumptions on purchasing rules, we show that when exogenous information is constant in the binary asset market, the price always converges. We then investigate market prices in the case of changing information, showing empirically that price sensitivity has a strong effect on price lag vs. information. We conclude with open questions on general $n$-ary option markets. As a by-product of the analysis, we show that these markets are equivalent to a simple recurrent neural network, helping to explain some of the predictive power associated with prediction markets, which are usually designed as $n$-ary option markets."
http://arxiv.org/abs/2206.12610v1,"Can New Light Rail Reduce Personal Vehicle Carbon Emissions? A Before-After, Experimental-Control Evaluation in Los Angeles",2022-06-25 09:40:48+00:00,"['Marlon G. Boarnet', 'Xize Wang', 'Douglas Houston']",econ.GN,"This paper uses a before-after, experimental-control group method to evaluate the impacts of the newly opened Expo light rail transit line in Los Angeles on personal vehicle greenhouse gas (GHG) emissions. We applied the California Air Resources Board's EMFAC 2011 emission model to estimate the amount of daily average CO2 emissions from personal vehicle travel for 160 households across two waves, before and after the light rail opened. The 160 households were part of an experimental-ccontrol group research design. Approximately half of the households live within a half-mile of new Expo light rail stations (the experimental group) and the balance of the sampled households live beyond a half-mile from Expo light rail stations (the control group). Households tracked odometer mileage for all household vehicles for seven days in two sample waves, before the Expo Line opened (fall, 2011) and after the Expo Line opened (fall, 2012). Our analysis indicates that opening the Expo Line had a statistically significant impact on average daily CO2 emissions from motor vehicles. We found that the CO2 emission of households who reside within a half-mile of an Expo Line station was 27.17 percent smaller than those living more than a half-mile from a station after the opening of the light rail, while no significant difference exists before the opening. A difference-in-difference model suggests that the opening of the Expo Line is associated with 3,145 g less of household vehicle CO2 emissions per day as a treatment effect. A sensitivity analysis indicates that the emission reduction effect is also present when the experimental group of households is redefined to be those living within a kilometer from the new light rail stations."
http://arxiv.org/abs/2206.12696v2,Travel time reliability in transportation networks: A review of methodological developments,2022-06-25 17:05:26+00:00,"['Zhaoqi Zang', 'Xiangdong Xu', 'Kai Qu', 'Ruiya Chen', 'Anthony Chen']",econ.GN,"The unavoidable travel time variability in transportation networks, resulted from the widespread supply side and demand side uncertainties, makes travel time reliability (TTR) be a common and core interest of all the stakeholders in transportation systems, including planners, travelers, service providers, and managers. This common and core interest stimulates extensive studies on modeling TTR. Researchers have developed a range of theories and models of TTR, many of which have been incorporated into transportation models, policies, and project appraisals. Adopting the network perspective, this paper aims to provide an integrated framework for reviewing the methodological developments of modeling TTR in transportation networks, including its characterization, evaluation and valuation, and traffic assignment. Specifically, the TTR characterization provides a whole picture of travel time distribution in transportation networks. TTR evaluation and TTR valuation (known as the value of reliability, VOR) simply and intuitively interpret abstract characterized TTR to be well understood by different stakeholders of transportation systems. TTR-based traffic assignment investigates the effects of TTR on the individual users travel behavior and consequently the collective network flow pattern. As the above three topics are mainly separately studied in different disciplines and research areas, the integrated framework allows us to better understand their relationships and may contribute to developing possible combinations of TTR modeling philosophy. Also, the network perspective enables to focus on common challenges of modeling TTR, especially the uncertainty propagation from the uncertainty sources to the TTR at spatial levels including link, route, and the entire network. Some directions for future research are discussed in the era of new data environment, applications, and emerging technologies."
http://arxiv.org/abs/2204.00530v2,Consumption-investment decisions with endogenous reference point and drawdown constraint,2022-04-01 15:45:00+00:00,"['Zongxia Liang', 'Xiaodong Luo', 'Fengyi Yuan']",q-fin.PM,"We propose a consumption-investment decision model where past consumption peak $h$ plays a crucial role. There are two important consumption levels: the lowest constrained level and a reference level, at which the risk aversion in terms of consumption rate is changed. We solve this stochastic control problem and derive the value function, optimal consumption plan, and optimal investment strategy in semi-explicit forms. We find five important thresholds of wealth, all as functions of $h$, and most of them are nonlinear functions. As can be seen from numerical results and theoretical analysis, this intuitive and simple model has significant economic implications, and there are at least three important predictions: the marginal propensity to consume out of wealth is generally decreasing but can be increasing for intermediate wealth levels, and it jumps inversely proportional to the risk aversion at the reference point; the implied relative risk aversion is roughly a smile in wealth; the welfare of the poor is more vulnerable to wealth shocks than the wealthy. Moreover, locally changing the risk aversion influences the optimal strategies globally, revealing some risk allocation behaviors."
http://arxiv.org/abs/2204.00785v1,Rural Pension System and Farmers' Participation in Residents' Social Insurance,2022-04-02 07:12:01+00:00,['Tao Xu'],econ.GN,"As the ageing population and childlessness are increasing in rural China, social pensions will become the mainstream choice for farmers, and the level of social pensions must be supported by better social insurance. The paper compares the history of rural pension insurance system, outlines the current situation and problems, analyses China Family Panel Studies data and explores the key factors influencing farmers' participation through an empirical approach. The paper shows that residents' social pension insurance is facing problems in the rural areas such as low level of protection and weak management capacity, which have contributed to the under-insured rate, and finds that there is a significant impact on farmers' participation in insurance from personal characteristics factors such as gender, age, health and (family) financial factors such as savings, personal income, intergenerational mobility of funds. And use of the Internet can help farmers enroll in pension insurance. The paper argues for the need to continue to implement the rural revitalisation strategy, with the government as the lead and the market as the support, in a concerted effort to improve the protection and popularity of rural pension insurance."
http://arxiv.org/abs/2206.03919v2,Can Mobile Technology Improve Female Entrepreneurship? Evidence from Nepal,2022-06-08 14:26:48+00:00,"['Conner Mullally', 'Sarah Janzen', 'Nicholas Magnan', 'Shruti Sharma', 'Bhola Shrestha']",econ.GN,"Gender norms may constrain the ability of women to develop their entrepreneurial skills, particularly in rural areas. By bringing entrepreneurial training to women rather than requiring extended time away from home, mobile technology could open doors that would otherwise be closed. We randomly selected Nepali women to be trained as veterinary service providers known as community animal health workers. Half of the selected candidates were randomly assigned to a traditional training course requiring 35 consecutive days away from home, and half were assigned to a hybrid distance learning course requiring two shorter stays plus a table-based curriculum to be completed at home. Distance learning strongly increases women's ability to complete training as compared to traditional training. Distance learning has a larger effect than traditional training on boosting the number of livestock responsibilities women carry out at home, while also raising aspirations. Both training types increase women's control over income. Our results indicate that if anything, distance learning produced more effective community animal health workers."
http://arxiv.org/abs/2206.04013v3,The influence of color on prices of abstract paintings,2022-06-08 17:05:23+00:00,"['Maksim Borisov', 'Valeria Kolycheva', 'Alexander Semenov', 'Dmitry Grigoriev']",econ.GN,"Determination of price of an artwork is a fundamental problem in cultural economics. In this work we investigate what impact visual characteristics of a painting have on its price. We construct a number of visual features measuring complexity of the painting, its points of interest, segmentation-based features, local color features, and features based on Itten and Kandinsky theories, and utilize mixed-effects model to study impact of these features on the painting price. We analyze the influence of the color on the example of the most complex art style - abstractionism, by created Kandinsky, for which the color is the primary basis. We use Itten's theory - the most recognized color theory in art history, from which the largest number of subtheories was born. For this day it is taken as the base for teaching artists. We utilize novel dataset of 3885 paintings collected from Christie's and Sotheby's and find that color harmony has some explanatory power, color complexity metrics are insignificant and color diversity explains price well."
http://arxiv.org/abs/2206.03306v2,Household and individual economic responses to different health shocks: The role of medical innovations,2022-06-07 13:51:37+00:00,['Volha Lazuka'],econ.GN,"This study provides new evidence regarding the extent to which medical care mitigates the economic consequences of various health shocks for the individual and a wider family. To obtain causal effects, I focus on the role of medical scientific discoveries and leverage the longitudinal dimension of unique administrative data for Sweden. The results indicate that medical innovations strongly mitigate the negative economic consequences of a health shock for the individual and create spillovers to relatives. Such mitigating effects are highly heterogeneous across prognoses. These results suggest that medical innovation substantially reduces the burden of welfare costs yet produces income inequalities."
http://arxiv.org/abs/2206.03847v2,Time-varying Cost of Distancing: Distancing Fatigue and Lockdowns,2022-06-08 12:41:23+00:00,"['Christoph Carnehl', 'Satoshi Fukuda', 'Nenad Kos']",econ.GN,"We study a behavioral SIR model with time-varying costs of distancing. The two main causes of the variation in the cost of distancing we explore are distancing fatigue and public policies (lockdowns). We show that for a second wave of an epidemic to arise, a steep increase in distancing cost is necessary. Distancing fatigue cannot increase the distancing cost sufficiently fast to create a second wave. However, public policies that discontinuously affect the distancing cost can create a second wave. With that in mind, we characterize the largest change in the distancing cost (due to, for example, lifting a public policy) that will not cause a second wave. Finally, we provide a numerical analysis of public policies under distancing fatigue and show that a strict lockdown at the beginning of an epidemic (as, for example, recently in China) can lead to unintended adverse consequences. When the policy is lifted the disease spreads very fast due to the accumulated distancing fatigue of the individuals causing high prevalence levels."
http://arxiv.org/abs/2205.10434v2,Predicting Choice from Information Costs,2022-05-20 20:49:40+00:00,"['Elliot Lipnowski', 'Doron Ravid']",econ.TH,"An agent acquires a costly flexible signal before making a decision. We explore to what degree knowledge of the agent's information costs helps predict her behavior. We establish an impossibility result: learning costs alone generate no testable restrictions on choice without also imposing constraints on actions' state-dependent utilities. By contrast, choices from a menu often uniquely pin down the agent's decisions in all submenus. To prove the latter result, we define iteratively differentiable cost functions, a tractable class amenable to first-order techniques. Finally, we construct tight tests for a multi-menu data set to be consistent with a given cost."
http://arxiv.org/abs/2205.10472v1,A Simple Characterization of Supply Correspondences,2022-05-21 00:46:37+00:00,"['Alexey Kushnir', 'Vinod Krishnamoorthy']",econ.TH,We prove that supply correspondences are characterized by two properties: the law of supply and being homogeneous of degree zero.
http://arxiv.org/abs/2205.09066v1,"Centralized and decentral approaches to succeed the 100% energiewende in Germany in the European context: A model-based analysis of generation, network, and storage investments",2022-05-18 16:49:57+00:00,"['Mario Kendziorski', 'Leonard Göke', 'Christian von Hirschhausen', 'Claudia Kemfert', 'Elmar Zozmann']",econ.GN,"In this paper, we explore centralized and more decentral approaches to succeed the energiewende in Germany, in the European context. We use the AnyMOD framework to model a future renewable-based European energy system, based on a techno-economic optimization, i.e. cost minimization with given demand, including both investment and the subsequent dispatch of capacity. The model includes 29 regions for European countries, and 38 NUTS-2 regions in Germany. First the entire energy system on the European level is optimized. Based on these results, the electricity system for the German regions is optimized to achieve great regional detail to analyse spatial effects. The model allows a comparison between a stylized central scenario with high amounts of wind offshore deployed, and a decentral scenario using mainly the existing grid, and thus relying more on local capacities. The results reveal that the cost for the second optimization of these two scenarios are about the same: The central scenario is characterized by network expansion in order to transport the electricity from the wind offshore sites, whereas the decentral scenario leads to more photovoltaic and battery deployment closer to the areas with a high demand for energy. A scenarios with higher energy efficiency and lower demand projections lead to a significant reduction of investment requirements, and to different localizations thereof."
http://arxiv.org/abs/2205.08584v3,Revealed Incomplete Preferences,2022-05-17 18:58:47+00:00,"['Kirby Nielsen', 'Luca Rigotti']",econ.GN,"We elicit incomplete preferences over monetary gambles with subjective uncertainty. Subjects rank gambles, and these rankings are used to estimate preferences; payments are based on estimated preferences. About 40\% of subjects express incompleteness, but they do so infrequently. Incompleteness is similar for individuals with precise and imprecise beliefs, and in an environment with objective uncertainty, suggesting that it results from imprecise tastes more than imprecise beliefs. When we force subjects to choose, we observe more inconsistencies and preference reversals. Evidence suggests there is incompleteness that is indirectly revealed -- in up to 98\% of subjects -- in addition to what we directly measure."
http://arxiv.org/abs/2205.13171v1,Immigrant and native export benefiting from business collaborations: a global study,2022-05-26 06:01:48+00:00,"['Shayegheh Ashourizadeh', 'Mehrzad Saeedikiya']",econ.GN,"The authors hypothesised that export develops in the network of business collaborations that are embedded in migration status. In that, collaborative networking positively affects export performance and immigrant entrepreneurs enjoy higher collaborative networking than native entrepreneurs due to their advantage of being embedded in the home and the host country. Moreover, the advantage of being an immigrant promotes the benefits of collaborative networking for export compared to those of native entrepreneurs. A total of 47,200 entrepreneurs starting, running and owning firms in 71 countries were surveyed by Global Entrepreneurship Monitor and analysed through the hierarchical linear modelling technique. Collaborative networking facilitated export and migration status influenced entrepreneur networking, in that, immigrant entrepreneurs had a higher level of collaborative networking than native entrepreneurs. Consequently, immigrant entrepreneurs seemed to have benefited from their network collaborations more than their native counterparts did. This study sheds light on how immigrant entrepreneur network collaborations can be effective for their exporting."
http://arxiv.org/abs/2206.00509v2,Religiosity and Innovation Attitudes: An Instrumental Variables Analysis,2022-06-01 14:10:09+00:00,"['Duygu Buyukyazici', 'Francesco Serti']",econ.GN,"Estimating the influence of religion on innovation is challenging because of both complexness and endogeneity. In order to untangle these issues, we use several measures of religiosity, adopt an individual-level approach to innovation and employ the instrumental variables method. We analyse the effect of religiosity on individual attitudes that are either favourable or unfavourable to innovation, presenting an individual's propensity to innovate. We instrument one's religiosity with the average religiosity of people of the same sex, age range, and religious affiliation who live in countries with the same dominant religious denomination. The results strongly suggest that each measure of religiosity has a somewhat negative effect on innovation attitudes. The diagnostic test results and sensitivity analyses support the main findings. We propose three causality channels from religion to innovation: time allocation, the fear of uncertainty, and conventional roles reinforced by religion."
http://arxiv.org/abs/2204.08798v2,Semantics meets attractiveness: Choice by salience,2022-04-19 10:42:24+00:00,"['Alfio Giarlotta', 'Angelo Petralia', 'Stephen Watson']",econ.TH,"We describe a context-sensitive model of choice, in which the selection process is shaped not only by the attractiveness of items but also by their semantics ('salience'). All items are ranked according to a relation of salience, and a linear order is associated to each item. The selection of a single element from a menu is justified by one of the linear orders indexed by the most salient items in the menu. The general model provides a structured explanation for any observed behavior, and allows us to to model the 'moodiness' of a decision maker, which is typical of choices requiring as many distinct rationales as items. Asymptotically all choices are moody. We single out a model of linear salience, in which the salience order is transitive and complete, and characterize it by a behavioral property, called WARP(S). Choices rationalizable by linear salience can only exhibit non-conflicting violations of WARP. We also provide numerical estimates, which show the high selectivity of this testable model of bounded rationality."
http://arxiv.org/abs/2204.08356v7,Inference for Cluster Randomized Experiments with Non-ignorable Cluster Sizes,2022-04-18 15:00:45+00:00,"['Federico Bugni', 'Ivan Canay', 'Azeem Shaikh', 'Max Tabord-Meehan']",econ.EM,"This paper considers the problem of inference in cluster randomized experiments when cluster sizes are non-ignorable. Here, by a cluster randomized experiment, we mean one in which treatment is assigned at the cluster level. By non-ignorable cluster sizes, we refer to the possibility that the treatment effects may depend non-trivially on the cluster sizes. We frame our analysis in a super-population framework in which cluster sizes are random. In this way, our analysis departs from earlier analyses of cluster randomized experiments in which cluster sizes are treated as non-random. We distinguish between two different parameters of interest: the equally-weighted cluster-level average treatment effect, and the size-weighted cluster-level average treatment effect. For each parameter, we provide methods for inference in an asymptotic framework where the number of clusters tends to infinity and treatment is assigned using a covariate-adaptive stratified randomization procedure. We additionally permit the experimenter to sample only a subset of the units within each cluster rather than the entire cluster and demonstrate the implications of such sampling for some commonly used estimators. A small simulation study and empirical demonstration show the practical relevance of our theoretical results."
http://arxiv.org/abs/2204.11931v1,Pareto Optimization in Categories,2022-04-25 19:13:08+00:00,['Matilde Marcolli'],math.CT,We propose a model of Pareto optimization (multi-objective programming) in the context of a categorical theory of resources. We describe how to adapt multi-objective swarm intelligence algorithms to this categorical formulation.
http://arxiv.org/abs/2204.06967v1,JUE Insight: The (Non-)Effect of Opportunity Zones on Housing Prices,2022-04-14 13:48:40+00:00,"['Jiafeng Chen', 'Edward Glaeser', 'David Wessel']",econ.GN,"Will the Opportunity Zones (OZ) program, America's largest new place-based policy in decades, generate neighborhood change? We compare single-family housing price growth in OZs with price growth in areas that were eligible but not included in the program. We also compare OZs to their nearest geographic neighbors. Our most credible estimates rule out price impacts greater than 0.5 percentage points with 95% confidence, suggesting that, so far, home buyers don't believe that this subsidy will generate major neighborhood change. OZ status reduces prices in areas with little employment, perhaps because buyers think that subsidizing new investment will increase housing supply. Mixed evidence suggests that OZs may have increased residential permitting."
http://arxiv.org/abs/2205.04137v1,Information-Robust Optimal Auctions,2022-05-09 09:25:54+00:00,['Wanchang Zhang'],econ.TH,"A single unit of a good is sold to one of two bidders. Each bidder has either a high prior valuation or a low prior valuation for the good. Their prior valuations are independently and identically distributed. Each bidder may observe an independently and identically distributed signal about her prior valuation. The seller knows the distribution of the prior valuation profile and knows that signals are independently and identically distributed, but does not know the signal distribution. In addition, the seller knows that bidders play undominated strategies. I find that a second-price auction with a random reserve maximizes the worst-case expected revenue over all possible signal distributions and all equilibria in undominated strategies."
http://arxiv.org/abs/2205.04990v2,Stable Outcomes and Information in Games: An Empirical Framework,2022-05-10 15:56:50+00:00,['Paul S. Koh'],econ.EM,"Empirically, many strategic settings are characterized by stable outcomes in which players' decisions are publicly observed, yet no player takes the opportunity to deviate. To analyze such situations in the presence of incomplete information, we build an empirical framework by introducing a novel solution concept that we call Bayes stable equilibrium. Our framework allows the researcher to be agnostic about players' information and the equilibrium selection rule. The Bayes stable equilibrium identified set collapses to the complete information pure strategy Nash equilibrium identified set under strong assumptions on players' information. Furthermore, all else equal, it is weakly tighter than the Bayes correlated equilibrium identified set. We also propose computationally tractable approaches for estimation and inference. In an application, we study the strategic entry decisions of McDonald's and Burger King in the US. Our results highlight the identifying power of informational assumptions and show that the Bayes stable equilibrium identified set can be substantially tighter than the Bayes correlated equilibrium identified set. In a counterfactual experiment, we examine the impact of increasing access to healthy food on the market structures in Mississippi food deserts."
http://arxiv.org/abs/2204.10478v5,On the Robustness of Second-Price Auctions in Prior-Independent Mechanism Design,2022-04-22 03:18:32+00:00,"['Jerry Anunrojwong', 'Santiago R. Balseiro', 'Omar Besbes']",econ.TH,"Classical Bayesian mechanism design relies on the common prior assumption, but such prior is often not available in practice. We study the design of prior-independent mechanisms that relax this assumption: the seller is selling an indivisible item to $n$ buyers such that the buyers' valuations are drawn from a joint distribution that is unknown to both the buyers and the seller; buyers do not need to form beliefs about competitors, and the seller assumes the distribution is adversarially chosen from a specified class. We measure performance through the worst-case regret, or the difference between the expected revenue achievable with perfect knowledge of buyers' valuations and the actual mechanism revenue.
  We study a broad set of classes of valuation distributions that capture a wide spectrum of possible dependencies: independent and identically distributed (i.i.d.) distributions, mixtures of i.i.d. distributions, affiliated and exchangeable distributions, exchangeable distributions, and all joint distributions. We derive in quasi closed form the minimax values and the associated optimal mechanism. In particular, we show that the first three classes admit the same minimax regret value, which is decreasing with the number of competitors, while the last two have the same minimax regret equal to that of the single buyer case. Furthermore, we show that the minimax optimal mechanisms have a simple form across all settings: a second-price auction with random reserve prices, which shows its robustness in prior-independent mechanism design. En route to our results, we also develop a principled methodology to determine the form of the optimal mechanism and worst-case distribution via first-order conditions that should be of independent interest in other minimax problems."
http://arxiv.org/abs/2206.01779v3,Bayesian and Frequentist Inference for Synthetic Controls,2022-06-03 18:48:25+00:00,"['Ignacio Martinez', 'Jaume Vives-i-Bastida']",stat.ME,"The synthetic control method has become a widely popular tool to estimate causal effects with observational data. Despite this, inference for synthetic control methods remains challenging. Often, inferential results rely on linear factor model data generating processes. In this paper, we characterize the conditions on the factor model primitives (the factor loadings) for which the statistical risk minimizers are synthetic controls (in the simplex). Then, we propose a Bayesian alternative to the synthetic control method that preserves the main features of the standard method and provides a new way of doing valid inference. We explore a Bernstein-von Mises style result to link our Bayesian inference to the frequentist inference. For linear factor model frameworks we show that a maximum likelihood estimator (MLE) of the synthetic control weights can consistently estimate the predictive function of the potential outcomes for the treated unit and that our Bayes estimator is asymptotically close to the MLE in the total variation sense. Through simulations, we show that there is convergence between the Bayes and frequentist approach even in sparse settings. Finally, we apply the method to re-visit the study of the economic costs of the German re-unification and the Catalan secession movement. The Bayesian synthetic control method is available in the bsynth R-package."
http://arxiv.org/abs/2206.11359v2,Non-Obvious Manipulability of the Rank-Minimizing Mechanism,2022-06-22 20:12:08+00:00,['Peter Troyan'],econ.TH,"In assignment problems, the rank distribution of assigned objects is often used to evaluate match quality. Rank-minimizing (RM) mechanisms directly optimize for average rank. While appealing, a drawback is RM mechanisms are not strategyproof. This paper investigates whether RM satisfies the weaker incentive notion of non-obvious manipulability (NOM, Troyan and Morrill, 2020). I show any RM mechanism with full support - placing positive probability on all rank-minimizing allocations - is NOM. In particular, uniform randomization satisfies this condition. Without full support, whether an RM mechanism is NOM or not depends on the details of the selection rule."
http://arxiv.org/abs/2206.08503v4,Semiparametric Single-Index Estimation for Average Treatment Effects,2022-06-17 01:44:53+00:00,"['Difang Huang', 'Jiti Gao', 'Tatsushi Oka']",econ.EM,"We propose a semiparametric method to estimate the average treatment effect under the assumption of unconfoundedness given observational data. Our estimation method alleviates misspecification issues of the propensity score function by estimating the single-index link function involved through Hermite polynomials. Our approach is computationally tractable and allows for moderately large dimension covariates. We provide the large sample properties of the estimator and show its validity. Also, the average treatment effect estimator achieves the parametric rate and asymptotic normality. Our extensive Monte Carlo study shows that the proposed estimator is valid in finite samples. Applying our method to maternal smoking and infant health, we find that conventional estimates of smoking's impact on birth weight may be biased due to propensity score misspecification, and our analysis of job training programs reveals earnings effects that are more precisely estimated than in prior work. These applications demonstrate how addressing model misspecification can substantively affect our understanding of key policy-relevant treatment effects."
http://arxiv.org/abs/2204.13481v2,Bunching and Taxing Multidimensional Skills,2022-04-28 13:14:41+00:00,"['Job Boerma', 'Aleh Tsyvinski', 'Alexander P. Zimin']",econ.GN,"We characterize optimal policy in a multidimensional nonlinear taxation model with bunching. We develop an empirically relevant model with cognitive and manual skills, firm heterogeneity, and labor market sorting. We first derive two conditions for the optimality of taxes that take into account bunching. The first condition $-$ a stochastic dominance optimal tax condition $-$ shows that at the optimum the schedule of benefits dominates the schedule of distortions in terms of second-order stochastic dominance. The second condition $-$ a global optimal tax formula $-$ provides a representation that balances the local costs and benefits of optimal taxation while explicitly accounting for global incentive constraints. Second, we use Legendre transformations to represent our problem as a linear program. This linearization allows us to solve the model quantitatively and to precisely characterize bunching. At an optimum, 10 percent of workers is bunched. We introduce two notions of bunching $-$ blunt bunching and targeted bunching. Blunt bunching constitutes 30 percent of all bunching, occurs at the lowest regions of cognitive and manual skills, and lumps the allocations of these workers resulting in a significant distortion. Targeted bunching constitutes 70 percent of all bunching and recognizes the workers' comparative advantage. The planner separates workers on their dominant skill and bunches them on their weaker skill, thus mitigating distortions along the dominant skill dimension."
http://arxiv.org/abs/2204.11748v3,Optimal Decision Rules when Payoffs are Partially Identified,2022-04-25 16:06:16+00:00,"['Timothy Christensen', 'Hyungsik Roger Moon', 'Frank Schorfheide']",econ.EM,We derive asymptotically optimal statistical decision rules for discrete choice problems when payoffs depend on a partially-identified parameter $θ$ and the decision maker can use a point-identified parameter $μ$ to deduce restrictions on $θ$. Examples include treatment choice under partial identification and pricing with rich unobserved heterogeneity. Our notion of optimality combines a minimax approach to handle the ambiguity from partial identification of $θ$ given $μ$ with an average risk minimization approach for $μ$. We show how to implement optimal decision rules using the bootstrap and (quasi-)Bayesian methods in both parametric and semiparametric settings. We provide detailed applications to treatment choice and optimal pricing. Our asymptotic approach is well suited for realistic empirical settings in which the derivation of finite-sample optimal rules is intractable.
http://arxiv.org/abs/2205.10910v1,Mechanisms without transfers for fully biased agents,2022-05-22 19:28:01+00:00,"['Deniz Kattwinkel', 'Axel Niemeyer', 'Justus Preusser', 'Alexander Winter']",econ.TH,"A principal must decide between two options. Which one she prefers depends on the private information of two agents. One agent always prefers the first option; the other always prefers the second. Transfers are infeasible. One application of this setting is the efficient division of a fixed budget between two competing departments. We first characterize all implementable mechanisms under arbitrary correlation. Second, we study when there exists a mechanism that yields the principal a higher payoff than she could receive by choosing the ex-ante optimal decision without consulting the agents. In the budget example, such a profitable mechanism exists if and only if the information of one department is also relevant for the expected returns of the other department. We generalize this insight to derive necessary and sufficient conditions for the existence of a profitable mechanism in the n-agent allocation problem with independent types."
http://arxiv.org/abs/2206.05160v3,Classes of Aggregation Rules for Ethical Decision Making in Automated Systems,2022-06-10 14:59:13+00:00,"['Federico Fioravanti', 'Iyad Rahwan', 'Fernando Abel Tohmé']",econ.TH,"We study a class of {\em aggregation rules} that could be applied to ethical AI decision-making. These rules yield the decisions to be made by automated systems based on the information of profiles of preferences over possible choices. We consider two different but very intuitive notions of preferences of an alternative over another one, namely {\it pairwise majority} and {\it position} dominance. Preferences are represented by permutation processes over alternatives and aggregation rules are applied to obtain results that are socially considered to be ethically correct. In this setting, we find many aggregation rules that satisfy desirable properties for an autonomous system. We also address the problem of the stability of the aggregation process, which is important when the information is variable. These results are a contribution for an AI designer that wants to justify the decisions made by an autonomous system.\\ \textit{Keywords:} Aggregation Operators; Permutation Process; Decision Analysis."
http://arxiv.org/abs/2205.07950v4,The Power of Tests for Detecting $p$-Hacking,2022-05-16 19:18:55+00:00,"['Graham Elliott', 'Nikolay Kudrin', 'Kaspar Wüthrich']",econ.EM,A flourishing empirical literature investigates the prevalence of $p$-hacking based on the distribution of $p$-values across studies. Interpreting results in this literature requires a careful understanding of the power of methods for detecting $p$-hacking. We theoretically study the implications of likely forms of $p$-hacking on the distribution of $p$-values to understand the power of tests for detecting it. Power can be low and depends crucially on the $p$-hacking strategy and the distribution of true effects. Combined tests for upper bounds and monotonicity and tests for continuity of the $p$-curve tend to have the highest power for detecting $p$-hacking.
http://arxiv.org/abs/2205.06132v2,Assignment Markets with Budget Constraints,2022-05-12 14:50:19+00:00,"['Eleni Batziou', 'Martin Bichler', 'Maximilian Fichtl']",cs.GT,"This paper studies markets where a set of indivisible items is sold to bidders with quasilinear, unit-demand valuations, subject to a hard budget constraint. Without financial constraints the well-known assignment market model of Shapley and Shubik (1971) allows for a simple ascending auction format that is incentive-compatible, and strongly Pareto-optimal. However, this auction model does not capture the possibility that bidders face hard budget constraints. We design an iterative auction that depends on demand queries and an easily verifiable additional condition to maintain the properties in the presence of budget constraints. If instead this additional condition does not hold, incentive compatibility and core stability are at odds, and we cannot hope to achieve strong Pareto optimality in a simple ascending auction even with truthful bidding. Moreover, even in a complete information model where the auctioneer has access to valuations and budget constraints, the problem is NP-hard."
http://arxiv.org/abs/2204.10304v4,Measuring artificial intelligence: a systematic assessment and implications for governance,2022-04-21 17:39:25+00:00,"['Kerstin Hötte', 'Taheya Tarannum', 'Vilhelm Verendel', 'Lauren Bennett']",econ.GN,"Governing artificial intelligence (AI) inventions is a major policy concern, yet definitions and measurement remain contested. We compare four patent-based approaches reflecting distinct understandings of AI. Using US patents (1990-2019), we assess the degree to which each approach classifies AI as a general-purpose technology (GPT) and examine patent concentration--two central policy-relevant dimensions. The approaches overlap in just 1.37% of patents, defining between 3-17% of all US patents in 2019 as AI. All approaches confirm AI's GPT characteristics, with the smallest keyword-based set exhibiting the highest growth and generality. High GPTness indicates public good characteristics, justifying public support. Across methods, AI patents concentrate among a few firms, highlighting market power and regulatory challenges. Policy implementation, thus, requires careful consideration of multiple classification methods to ensure robust, inclusive, and effective AI governance."
http://arxiv.org/abs/2205.05002v5,Estimating Discrete Games of Complete Information: Bringing Logit Back in the Game,2022-05-10 16:17:55+00:00,['Paul S. Koh'],econ.EM,"Estimating discrete games of complete information is often computationally difficult due to partial identification and the absence of closed-form moment characterizations. This paper proposes computationally tractable approaches to estimation and inference that remove the computational burden associated with equilibria enumeration, numerical simulation, and grid search. Separately for unordered and ordered-actions games, I construct an identified set characterized by a finite set of generalized likelihood-based conditional moment inequalities that are convex in (a subvector of) structural model parameters under the standard logit assumption on unobservables. I use simulation and empirical examples to show that the proposed approaches generate informative identified sets and can be several orders of magnitude faster than existing estimation methods."
http://arxiv.org/abs/2208.06675v1,From the historical Roman road network to modern infrastructure in Italy,2022-08-13 15:34:15+00:00,"['Luca De Benedictis', 'Vania Licio', 'Anna Pinna']",econ.GN,"An integrated and widespread road system, like the one built during the Roman Empire in Italy, plays an important role today in facilitating the construction of new infrastructure. This paper investigates the historical path of Roman roads as main determinant of both motorways and railways in the country. The empirical analysis shows how the modern Italian transport infrastructure followed the path traced in ancient times by the Romans in constructing their roads. Being paved and connecting Italy from North to South, consular trajectories lasted in time, representing the starting physical capital for developing the new transport networks."
http://arxiv.org/abs/2207.13939v4,Stable Matching with Mistaken Agents,2022-07-28 08:04:12+00:00,"['Georgy Artemov', 'Yeon-Koo Che', 'YingHua He']",econ.TH,"Motivated by growing evidence of agents' mistakes in strategically simple environments, we propose a solution concept -- robust equilibrium -- that requires only an asymptotically optimal behavior. We use it to study large random matching markets operated by the applicant-proposing Deferred Acceptance (DA). Although truth-telling is a dominant strategy, almost all applicants may be non-truthful in robust equilibrium; however, the outcome must be arbitrarily close to the stable matching. Our results imply that one can assume truthful agents to study DA outcomes, theoretically or counterfactually. However, to estimate the preferences of mistaken agents, one should assume stable matching but not truth-telling."
http://arxiv.org/abs/2209.08380v2,A Structural Model for Detecting Communities in Networks,2022-09-17 17:58:01+00:00,['Alex Centeno'],econ.TH,"The objective of this paper is to identify and analyze the response actions of a set of players embedded in sub-networks in the context of interaction and learning. We characterize strategic network formation as a static game of interactions where players maximize their utility depending on the connections they establish and multiple interdependent actions that permit group-specific parameters of players. It is challenging to apply this type of model to real-life scenarios for two reasons: The computation of the Bayesian Nash Equilibrium is highly demanding and the identification of social influence requires the use of excluded variables that are oftentimes unavailable. Based on the theoretical proposal, we propose a set of simulant equations and discuss the identification of the social interaction effect employing multi-modal network autoregressive."
http://arxiv.org/abs/2208.03737v5,Finite Tests from Functional Characterizations,2022-08-07 14:29:18+00:00,"['Charles Gauthier', 'Raghav Malhotra', 'Agustin Troccoli Moretti']",econ.TH,"Classically, testing whether decision makers belong to specific preference classes involves two main approaches. The first, known as the functional approach, assumes access to an entire demand function. The second, the revealed preference approach, constructs inequalities to test finite demand data. This paper bridges these methods by using the functional approach to test finite data through preference learnability results. We develop a computationally efficient algorithm that generates tests for choice data based on functional characterizations of preference families. We provide these restrictions for various applications, including homothetic and weakly separable preferences, where the latter's revealed preference characterization is provably NP-Hard. We also address choice under uncertainty, offering tests for betweenness preferences. Lastly, we perform a simulation exercise demonstrating that our tests are effective in finite samples and accurately reject demands not belonging to a specified class."
http://arxiv.org/abs/2208.09638v3,Optimal Pre-Analysis Plans: Statistical Decisions Subject to Implementability,2022-08-20 08:54:39+00:00,"['Maximilian Kasy', 'Jann Spiess']",econ.EM,"What is the purpose of pre-analysis plans, and how should they be designed? We model the interaction between an agent who analyzes data and a principal who makes a decision based on agent reports. The agent could be the manufacturer of a new drug, and the principal a regulator deciding whether the drug is approved. Or the agent could be a researcher submitting a research paper, and the principal an editor deciding whether it is published. The agent decides which statistics to report to the principal. The principal cannot verify whether the analyst reported selectively. Absent a pre-analysis message, if there are conflicts of interest, then many desirable decision rules cannot be implemented. Allowing the agent to send a message before seeing the data increases the set of decision rules that can be implemented, and allows the principal to leverage agent expertise. The optimal mechanisms that we characterize require pre-analysis plans. Applying these results to hypothesis testing, we show that optimal rejection rules pre-register a valid test, and make worst-case assumptions about unreported statistics. Optimal tests can be found as a solution to a linear-programming problem."
http://arxiv.org/abs/2207.04480v1,Strategic Choices of Migrants and Smugglers in the Central Mediterranean Sea,2022-07-10 15:06:34+00:00,"['Katherine Hoffmann Pham', 'Junpei Komiyama']",econ.GN,"The sea crossing from Libya to Italy is one of the world's most dangerous and politically contentious migration routes, and yet over half a million people have attempted the crossing since 2014. Leveraging data on aggregate migration flows and individual migration incidents, we estimate how migrants and smugglers have reacted to changes in border enforcement, namely the rise in interceptions by the Libyan Coast Guard starting in 2017 and the corresponding decrease in the probability of rescue at sea. We find support for a deterrence effect in which attempted crossings along the Central Mediterranean route declined, and a diversion effect in which some migrants substituted to the Western Mediterranean route. At the same time, smugglers adapted their tactics. Using a strategic model of the smuggler's choice of boat size, we estimate how smugglers trade off between the short-run payoffs to launching overcrowded boats and the long-run costs of making less successful crossing attempts under different levels of enforcement. Taken together, these analyses shed light on how the integration of incident- and flow-level datasets can inform ongoing migration policy debates and identify potential consequences of changing enforcement regimes."
http://arxiv.org/abs/2207.04481v2,Detecting Grouped Local Average Treatment Effects and Selecting True Instruments,2022-07-10 15:08:39+00:00,"['Nicolas Apfel', 'Helmut Farbmacher', 'Rebecca Groh', 'Martin Huber', 'Henrika Langen']",econ.EM,"Under an endogenous binary treatment with heterogeneous effects and multiple instruments, we propose a two-step procedure for identifying complier groups with identical local average treatment effects (LATE) despite relying on distinct instruments, even if several instruments violate the identifying assumptions. We use the fact that the LATE is homogeneous for instruments which (i) satisfy the LATE assumptions (instrument validity and treatment monotonicity in the instrument) and (ii) generate identical complier groups in terms of treatment propensities given the respective instruments. We propose a two-step procedure, where we first cluster the propensity scores in the first step and find groups of IVs with the same reduced form parameters in the second step. Under the plurality assumption that within each set of instruments with identical treatment propensities, instruments truly satisfying the LATE assumptions are the largest group, our procedure permits identifying these true instruments in a data driven way. We show that our procedure is consistent and provides consistent and asymptotically normal estimators of underlying LATEs. We also provide a simulation study investigating the finite sample properties of our approach and an empirical application investigating the effect of incarceration on recidivism in the US with judge assignments serving as instruments."
http://arxiv.org/abs/2207.04557v1,Mechanisms that Incentivize Data Sharing in Federated Learning,2022-07-10 22:36:52+00:00,"['Sai Praneeth Karimireddy', 'Wenshuo Guo', 'Michael I. Jordan']",cs.GT,"Federated learning is typically considered a beneficial technology which allows multiple agents to collaborate with each other, improve the accuracy of their models, and solve problems which are otherwise too data-intensive / expensive to be solved individually. However, under the expectation that other agents will share their data, rational agents may be tempted to engage in detrimental behavior such as free-riding where they contribute no data but still enjoy an improved model. In this work, we propose a framework to analyze the behavior of such rational data generators. We first show how a naive scheme leads to catastrophic levels of free-riding where the benefits of data sharing are completely eroded. Then, using ideas from contract theory, we introduce accuracy shaping based mechanisms to maximize the amount of data generated by each agent. These provably prevent free-riding without needing any payment mechanism."
http://arxiv.org/abs/2207.11003v1,Time-Varying Poisson Autoregression,2022-07-22 10:43:18+00:00,"['Giovanni Angelini', 'Giuseppe Cavaliere', ""Enzo D'Innocenzo"", 'Luca De Angelis']",econ.EM,"In this paper we propose a new time-varying econometric model, called Time-Varying Poisson AutoRegressive with eXogenous covariates (TV-PARX), suited to model and forecast time series of counts. {We show that the score-driven framework is particularly suitable to recover the evolution of time-varying parameters and provides the required flexibility to model and forecast time series of counts characterized by convoluted nonlinear dynamics and structural breaks.} We study the asymptotic properties of the TV-PARX model and prove that, under mild conditions, maximum likelihood estimation (MLE) yields strongly consistent and asymptotically normal parameter estimates. Finite-sample performance and forecasting accuracy are evaluated through Monte Carlo simulations. The empirical usefulness of the time-varying specification of the proposed TV-PARX model is shown by analyzing the number of new daily COVID-19 infections in Italy and the number of corporate defaults in the US."
http://arxiv.org/abs/2207.02064v1,Climate-Contingent Finance,2022-07-05 14:15:19+00:00,['John Nay'],q-fin.GN,"Climate adaptation could yield significant benefits. However, the uncertainty of which future climate scenarios will occur decreases the feasibility of proactively adapting. Climate adaptation projects could be underwritten by benefits paid for in the climate scenarios that each adaptation project is designed to address because other entities would like to hedge the financial risk of those scenarios. Because the return on investment is a function of the level of climate change, it is optimal for the adapting entity to finance adaptation with repayment as a function of the climate. It is also optimal for entities with more financial downside under a more extreme climate to serve as an investing counterparty because they can obtain higher than market rates of return when they need it most.
  In this way, parties proactively adapting would reduce the risk they over-prepare, while their investors would reduce the risk they under-prepare. This is superior to typical insurance because, by investing in climate-contingent mechanisms, investors are not merely financially hedging but also outright preventing physical damage, and therefore creating economic value. This coordinates capital through time and place according to parties' risk reduction capabilities and financial profiles, while also providing a diversifying investment return.
  Climate-contingent finance can be generalized to any situation where entities share exposure to a risk where they lack direct control over whether it occurs (e.g., climate change, or a natural pandemic), and one type of entity can take proactive actions to benefit from addressing the effects of the risk if it occurs (e.g., through innovating on crops that would do well under extreme climate change or vaccination technology that could address particular viruses) with funding from another type of entity that seeks a targeted return to ameliorate the downside."
http://arxiv.org/abs/2207.01664v1,Optimal Multi-Dimensional Auctions: Conjectures and Simulations,2022-07-04 18:28:23+00:00,"['Alexey Kushnir', 'James Michelson']",econ.TH,"We explore the properties of optimal multi-dimensional auctions in a model where a single object of multiple qualities is sold to several buyers. Using simulations, we test some hypotheses conjectured by Belloni et al. [3] and Kushnir and Shourideh [7]. As part of this work, we provide the first open-source library for multi-dimensional auction simulations written in Python."
http://arxiv.org/abs/2207.12199v3,A meta-analysis of the total economic impact of climate change,2022-07-25 13:39:24+00:00,['Richard S. J. Tol'],econ.GN,"Earlier meta-analyses of the economic impact of climate change are updated with more data, with three new results: (1) The central estimate of the economic impact of global warming is always negative. (2) The confidence interval about the estimates is much wider. (3) Elicitation methods are most pessimistic, econometric studies most optimistic. Two previous results remain: (4) The uncertainty about the impact is skewed towards negative surprises. (5) Poorer countries are much more vulnerable than richer ones. A meta-analysis of the impact of weather shocks reveals that studies, which relate economic growth to temperature levels, cannot agree on the sign of the impact whereas studies, which make economic growth a function of temperature change do agree on the sign but differ an order of magnitude in effect size. The former studies posit that climate change has a permanent effect on economic growth, the latter that the effect is transient. The impact on economic growth implied by studies of the impact of climate change is close to the growth impact estimated as a function of weather shocks. The social cost of carbon shows a similar pattern to the total impact estimates, but with more emphasis on the impacts of moderate warming in the near and medium term."
http://arxiv.org/abs/2207.12225v1,Forecasting euro area inflation using a huge panel of survey expectations,2022-07-25 14:24:32+00:00,"['Florian Huber', 'Luca Onorante', 'Michael Pfarrhofer']",econ.EM,"In this paper, we forecast euro area inflation and its main components using an econometric model which exploits a massive number of time series on survey expectations for the European Commission's Business and Consumer Survey. To make estimation of such a huge model tractable, we use recent advances in computational statistics to carry out posterior simulation and inference. Our findings suggest that the inclusion of a wide range of firms and consumers' opinions about future economic developments offers useful information to forecast prices and assess tail risks to inflation. These predictive improvements do not only arise from surveys related to expected inflation but also from other questions related to the general economic environment. Finally, we find that firms' expectations about the future seem to have more predictive content than consumer expectations."
http://arxiv.org/abs/2209.01013v2,Intrinsic fluctuations of reinforcement learning promote cooperation,2022-09-01 09:14:47+00:00,"['Wolfram Barfuss', 'Janusz Meylahn']",cs.LG,"In this work, we ask for and answer what makes classical temporal-difference reinforcement learning with epsilon-greedy strategies cooperative. Cooperating in social dilemma situations is vital for animals, humans, and machines. While evolutionary theory revealed a range of mechanisms promoting cooperation, the conditions under which agents learn to cooperate are contested. Here, we demonstrate which and how individual elements of the multi-agent learning setting lead to cooperation. We use the iterated Prisoner's dilemma with one-period memory as a testbed. Each of the two learning agents learns a strategy that conditions the following action choices on both agents' action choices of the last round. We find that next to a high caring for future rewards, a low exploration rate, and a small learning rate, it is primarily intrinsic stochastic fluctuations of the reinforcement learning process which double the final rate of cooperation to up to 80%. Thus, inherent noise is not a necessary evil of the iterative learning process. It is a critical asset for the learning of cooperation. However, we also point out the trade-off between a high likelihood of cooperative behavior and achieving this in a reasonable amount of time. Our findings are relevant for purposefully designing cooperative algorithms and regulating undesired collusive effects."
http://arxiv.org/abs/2209.06310v2,Representations of cones and applications to decision theory,2022-09-13 21:36:19+00:00,"['Paolo Leonetti', 'Giulio Principi']",math.CA,"Let $C$ be a cone in a locally convex Hausdorff topological vector space $X$ containing $0$. We show that there exists a (essentially unique) nonempty family $\mathscr{K}$ of nonempty subsets of the topological dual $X^\prime$ such that $$ C=\{x \in X: \forall K \in \mathscr{K}, \exists f \in K, \,\, f(x) \ge 0\}. $$ Then, we identify the additional properties on the family $\mathscr{K}$ which characterize, among others, closed convex cones, open convex cones, closed cones, and convex cones. For instance, if $X$ is a Banach space, then $C$ is a closed cone if and only if the family $\mathscr{K}$ can be chosen with nonempty convex compact sets.
  These representations provide abstract versions of several recent results in decision theory and give us the proper framework to obtain new ones. This allows us to characterize preorders which satisfy the independence axiom over certain probability measures, answering an open question in [Econometrica~\textbf{87} (2019), no. 3, 933--980]."
http://arxiv.org/abs/2209.06624v1,Digital 'nudges' to increase childhood vaccination compliance: Evidence from Pakistan,2022-09-14 13:20:57+00:00,"['Shehryar Munir', 'Farah Said', 'Umar Taj', 'Maida Zafar']",econ.GN,"Pakistan has one of the lowest rates of routine childhood immunization worldwide, with only two-thirds of infants 2 years or younger being fully immunized (Pakistan Demographic and Health Survey 2019). Government-led, routine information campaigns have been disrupted over the last few years due to the on-going COVID-19 pandemic. We use data from a mobile-based campaign that involved sending out short audio dramas emphasizing the importance of vaccines and parental responsibilities in Quetta, Pakistan. Five out of eleven areas designated by the provincial government were randomly selected to receive the audio calls with a lag of 3 months and form the comparison group in our analysis. We conduct a difference-in-difference analysis on data collected by the provincial Department of Health in the 3-month study and find a significant 30% increase over the comparison mean in the number of fully vaccinated children in campaign areas on average. We find evidence that suggests vaccination increased in UCs where vaccination centers were within a short 30-minute travel distance, and that the campaign was successful in changing perceptions about vaccination and reliable sources of advice. Results highlight the need for careful design and targeting of similar soft behavioral change campaigns, catering to the constraints and abilities of the context."
http://arxiv.org/abs/2207.08941v2,Circulation of a digital community currency,2022-07-18 21:01:17+00:00,"['Carolina E S Mattsson', 'Teodoro Criscione', 'Frank W Takes']",physics.soc-ph,"Circulation is the characteristic feature of successful currency systems, from community currencies to cryptocurrencies to national currencies. In this paper, we propose a network analysis approach especially suited for studying circulation given a system's digital transaction records. Sarafu is a digital community currency that was active in Kenya over a period that saw considerable economic disruption due to the COVID-19 pandemic. We represent its circulation as a network of monetary flow among the 40,000 Sarafu users. Network flow analysis reveals that circulation was highly modular, geographically localized, and occurring among users with diverse livelihoods. Across localized sub-populations, network cycle analysis supports the intuitive notion that circulation requires cycles. Moreover, the sub-networks underlying circulation are consistently degree disassortative and we find evidence of preferential attachment. Community-based institutions often take on the role of local hubs, and network centrality measures confirm the importance of early adopters and of women's participation. This work demonstrates that networks of monetary flow enable the study of circulation within currency systems at a striking level of detail, and our findings can be used to inform the development of community currencies in marginalized areas."
http://arxiv.org/abs/2207.07990v1,The Roads One Must Walk Down: Commute and Depression for Beijing's Residents,2022-07-16 17:44:39+00:00,"['Xize Wang', 'Tao Liu']",econ.GN,"As a vital aspect of individual's quality of life, mental health has been included as an important component of the U.N. Sustainable Development Goals. This study focuses on a specific aspect of mental health: depression, and examines its relationship with commute patterns. Using survey data from 1,528 residents in Beijing, China, we find that every 10 additional minutes of commute time is associated with 1.1% higher likelihood of depression. We test for the mechanisms of the commute-depression link and find that commute is associated with depression as a direct stressor rather than triggering higher work stress. When decomposing commute time into mode-specific time, we found that time on mopeds/motorcycles has the strongest association with depression. Moreover, the commute-depression associations are stronger for older workers and blue-collar workers. Hence, policies that could reduce commute time, encourage work from home, improve job-housing balance or increase motorcyclists' safety would help promote mental health."
http://arxiv.org/abs/2207.02379v2,Some Tradeoffs of Competition in Grant Contests,2022-07-06 01:02:38+00:00,['Kyle R. Myers'],econ.GN,"When funding public goods, resources are often allocated via mechanisms that resemble contests, especially in the case of research grants. A common critique of these contests is that they induce ``too much'' effort from participants. This need not be true if the effort in the contest is itself directed towards the public good. This papers analyzes survey data on scientists' time use and finds that scientists allocate their time in a way that is consistent with fundraising effort (e.g., grant writing) having inherent scientific value -- scientists who spend more time fundraising do not spent significantly less time on research even after conditioning on confounding factors. Theoretical models of contests are used to show that the presence of such a positive effort externality, where scientists generate social value when pursuing grants, changes the relationship between competition and the aggregate productivity of a grant contest. Ensuring that scientists exert socially valuable effort to obtain grants is increasingly important as grant contests become more competitive."
http://arxiv.org/abs/2207.06396v2,On Market Clearing of Day Ahead Auctions for European Power Markets: Cost Minimisation versus Social Welfare Maximisation,2022-07-13 17:52:44+00:00,"['Ioan Alexandru Puiu', 'Raphael Andreas Hauser']",cs.GT,"For the case of inflexible demand and considering network constraints, we introduce a Cost Minimisation (CM) based market clearing mechanism, and a model representing the standard Social Welfare Maximisation mechanism used in European Day Ahead Electricity Markets. Since the CM model corresponds to a more challenging optimisation problem, we propose four numerical algorithms that leverage the problem structure, each with different trade-offs between computational cost and convergence guarantees. These algorithms are evaluated on synthetic data to provide some intuition of their performance. We also provide strong (but partial) analytical results to facilitate efficient solution of the CM problem, which call for the introduction of a new concept: optimal zonal stack curves, and these results are used to devise one of the four solution algorithms. An evaluation of the CM and SWM models and their comparison is performed, under the assumption of truthful bidding, on the real world data of Central Western European Day Ahead Power Market during the period of 2019-2020. We show that the SWM model we introduce gives a good representation of the historical time series of the real prices. Further, the CM reduces the market power of producers, as generally this results in decreased zonal prices and always decreases the total cost of electricity procurement when compared to the currently employed SWM."
http://arxiv.org/abs/2207.06925v1,Adjacencies on random ordering polytopes and flow polytopes,2022-07-12 21:51:52+00:00,"['Jean-Paul Doignon', 'Kota Saito']",math.CO,"The Multiple Choice Polytope (MCP) is the prediction range of a random utility model due to Block and Marschak (1960). Fishburn (1998) offers a nice survey of the findings on random utility models at the time. A complete characterization of the MCP is a remarkable achievement of Falmagne (1978). Apart for a recognition of the facets by Suck (2002), the geometric structure of the MCP was apparently not much investigated. Recently, Chang, Narita and Saito (2022) refer to the adjacency of vertices while Turansick (2022) uses a condition which we show to be equivalent to the non-adjacency of two vertices. We characterize the adjacency of vertices and the adjacency of facets. To derive a more enlightening proof of Falmagne Theorem and of Suck result, Fiorini (2004) assimilates the MCP with the flow polytope of some acyclic network. Our results on adjacencies also hold for the flow polytope of any acyclic network. In particular, they apply not only to the MCP, but also to three polytopes which Davis-Stober, Doignon, Fiorini, Glineur and Regenwetter (2018) introduced as extended formulations of the weak order polytope, interval order polytope and semiorder polytope (the prediction ranges of other models, see for instance Fishburn and Falmagne, 1989, and Marley and Regenwetter, 2017)."
http://arxiv.org/abs/2209.00822v1,Optimal design of lottery with cumulative prospect theory,2022-09-02 05:10:12+00:00,"['Shunta Akiyama', 'Mitsuaki Obara', 'Yasushi Kawase']",cs.GT,"A lottery is a popular form of gambling between a seller and multiple buyers, and its profitable design is of primary interest to the seller. Designing a lottery requires modeling the buyer decision-making process for uncertain outcomes. One of the most promising descriptive models of such decision-making is the cumulative prospect theory (CPT), which represents people's different attitudes towards gain and loss, and their overestimation of extreme events. In this study, we design a lottery that maximizes the seller's profit when the buyers follow CPT. The derived problem is nonconvex and constrained, and hence, it is challenging to directly characterize its optimal solution. We overcome this difficulty by reformulating the problem as a three-level optimization problem. The reformulation enables us to characterize the optimal solution. Based on this characterization, we propose an algorithm that computes the optimal lottery in linear time with respect to the number of lottery tickets. In addition, we provide an efficient algorithm for a more general setting in which the ticket price is constrained. To the best of the authors' knowledge, this is the first study that employs the CPT framework for designing an optimal lottery."
http://arxiv.org/abs/2209.00900v1,Costs and Benefits of the Paris Climate Targets,2022-09-02 09:14:13+00:00,['Richard S. J. Tol'],econ.GN,"The temperature targets in the Paris Agreement cannot be met without very rapid reduction of greenhouse gas emissions and removal of carbon dioxide from the atmosphere. The latter requires large, perhaps prohibitively large subsidies. The central estimate of the costs of climate policy, unrealistically assuming least-cost implementation, is 3.8-5.6\% of GDP in 2100. The central estimate of the benefits of climate policy, unrealistically assuming constant vulnerability, is 2.8-3.2\% of GDP. The uncertainty about the benefits is larger than the uncertainty about the costs. The Paris targets do not pass the cost-benefit test unless risk aversion is high and discount rate low."
http://arxiv.org/abs/2209.01429v2,Instrumental variable quantile regression under random right censoring,2022-09-03 14:08:13+00:00,"['Jad Beyhum', 'Lorenzo Tedesco', 'Ingrid Van Keilegom']",econ.EM,This paper studies a semiparametric quantile regression model with endogenous variables and random right censoring. The endogeneity issue is solved using instrumental variables. It is assumed that the structural quantile of the logarithm of the outcome variable is linear in the covariates and censoring is independent. The regressors and instruments can be either continuous or discrete. The specification generates a continuum of equations of which the quantile regression coefficients are a solution. Identification is obtained when this system of equations has a unique solution. Our estimation procedure solves an empirical analogue of the system of equations. We derive conditions under which the estimator is asymptotically normal and prove the validity of a bootstrap procedure for inference. The finite sample performance of the approach is evaluated through numerical simulations. An application to the national Job Training Partnership Act study illustrates the method.
http://arxiv.org/abs/2209.01453v1,Learning by Consuming: Optimal Pricing with Endogenous Information Provision,2022-09-03 15:55:08+00:00,"['Huiyi Guo', 'Wei He', 'Bin Liu']",econ.TH,"We study the revenue-maximizing mechanism when a buyer's value evolves endogenously because of learning-by-consuming. A seller sells one unit of a divisible good, while the buyer relies on his private, rough valuation to choose his first-stage consumption level. Consuming more leads to a more precise valuation estimate, after which the buyer determines the second-stage consumption level. The optimum is a menu of try-and-decide contracts, consisting of a first-stage price-quantity pair and a second-stage per-unit price for the remaining quantity. In equilibrium, a higher first-stage valuation buyer pays more for higher first-stage consumption and enjoys a lower second-stage per-unit price. Methodologically, we deal with the difficulty that due to the failure of single-crossing condition, monotonicity in allocation plus the envelope condition is insufficient for incentive compatibility. Our results help to understand contracts about sequential consumption with the learning feature; e.g., leasing contracts for experience goods and trial sessions for certain courses."
http://arxiv.org/abs/2209.02683v1,Classicals versus Keynesians: Fifty Distinctions between Two Major Schools of Economic Thought,2022-09-06 17:52:33+00:00,['Seyyed Ali Zeytoon Nejad Moosavian'],econ.GN,"Macroeconomics essentially discusses macroeconomic phenomena from the perspectives of various schools of economic thought, each of which takes different views on how macroeconomic agents make decisions and how the corresponding markets operate. Therefore, developing a clear, comprehensive understanding of how and in what ways these schools of economic thought differ is a key and a prerequisite for economics students to prosper academically and professionally in the discipline. This becomes even more crucial as economics students pursue their studies toward higher levels of education and graduate school, during which students are expected to attain higher levels of Bloom's taxonomy, including analysis, synthesis, evaluation, and creation. Teaching the distinctions and similarities of the two major schools of economic thought has never been an easy task to undertake in the classroom. Although the reason for such a hardship can be multi-fold, one reason has undoubtedly been students' lack of a holistic view on how the two mainstream economic schools of thought differ. There is strong evidence that students make smoother transition to higher levels of education after building up such groundwork, on which they can build further later on (e.g. Didia and Hasnat, 1998; Marcal and Roberts, 2001; Islam, et al., 2008; Green, et al., 2009; White, 2016). The paper starts with a visual spectrum of various schools of economic thought, and then narrows down the scope to the classical and Keynesian schools, i.e. the backbone of modern macroeconomics. Afterwards, a holistic table contrasts the two schools in terms of 50 aspects. Not only does this table help economics students enhance their comprehension, retention, and critical-thinking capability, it also benefits macroeconomic instructors to ..."
http://arxiv.org/abs/2210.00138v1,School closures and educational path: how the Covid-19 pandemic affected transitions to college,2022-09-30 23:39:21+00:00,"['Fernanda Estevan', 'Lucas Finamor']",econ.GN,"We investigate the impact of the Covid-19 pandemic on the transition between high school and college in Brazil. Using microdata from the universe of students that applied to a selective university, we document how the Covid-19 shock increased enrollment for students in the top 10% high-quality public and private high schools. This increase comes at the expense of graduates from relatively lower-quality schools. Furthermore, this effect is entirely driven by applicants who were at high school during the Covid pandemic. The effect is large and completely offsets the gains in student background diversity achieved by a bold quota policy implemented years before Covid. These results suggest that not only students from underprivileged backgrounds endured larger negative effects on learning during the pandemic, but they also experienced a stall in their educational paths."
http://arxiv.org/abs/2209.05592v1,"Impact of interpersonal influences on Employee engagement and Psychological contract: Effects of guanxi, wasta, jeitinho, blat and pulling strings",2022-09-12 20:17:31+00:00,['Elizabeth Kassab Sfeir'],econ.GN,"This study puts forward a conceptual model linking interpersonal influences' impact on Employee Engagement, Psychological contracts, and Human Resource Practices. It builds on human and social capital, as well as the social exchange theory (SET), projecting how interpersonal influences can impact the psychological contract (PC) and employee engagement (EE) of employees. This research analyzes the interpersonal influences of Wasta in the Middle East, Guanxi in China, Jeitinho in Brazil, Blat in Russia, and Pulling Strings in England. Interpersonal influences draw upon nepotism, favoritism, and corruption in organizations in many countries. This paper draws on the qualitative methods of analyzing previous theories. It uses the Model Paper method of predicting relationships by examining the question of how do interpersonal influences impact employee engagement and psychological contract?. It is vital to track the effects of interpersonal influences on PC and EE, acknowledging that the employer can either empower or disengage our human capital."
http://arxiv.org/abs/2209.05914v1,Estimation of Average Derivatives of Latent Regressors: With an Application to Inference on Buffer-Stock Saving,2022-09-13 11:57:37+00:00,"['Hao Dong', 'Yuya Sasaki']",econ.EM,"This paper proposes a density-weighted average derivative estimator based on two noisy measures of a latent regressor. Both measures have classical errors with possibly asymmetric distributions. We show that the proposed estimator achieves the root-n rate of convergence, and derive its asymptotic normal distribution for statistical inference. Simulation studies demonstrate excellent small-sample performance supporting the root-n asymptotic normality. Based on the proposed estimator, we construct a formal test on the sub-unity of the marginal propensity to consume out of permanent income (MPCP) under a nonparametric consumption model and a permanent-transitory model of income dynamics with nonparametric distribution. Applying the test to four recent waves of U.S. Panel Study of Income Dynamics (PSID), we reject the null hypothesis of the unit MPCP in favor of a sub-unit MPCP, supporting the buffer-stock model of saving."
http://arxiv.org/abs/2209.05998v1,Interpreting and predicting the economy flows: A time-varying parameter global vector autoregressive integrated the machine learning model,2022-07-31 06:24:15+00:00,"['Yukang Jiang', 'Xueqin Wang', 'Zhixi Xiong', 'Haisheng Yang', 'Ting Tian']",econ.EM,"The paper proposes a time-varying parameter global vector autoregressive (TVP-GVAR) framework for predicting and analysing developed region economic variables. We want to provide an easily accessible approach for the economy application settings, where a variety of machine learning models can be incorporated for out-of-sample prediction. The LASSO-type technique for numerically efficient model selection of mean squared errors (MSEs) is selected. We show the convincing in-sample performance of our proposed model in all economic variables and relatively high precision out-of-sample predictions with different-frequency economic inputs. Furthermore, the time-varying orthogonal impulse responses provide novel insights into the connectedness of economic variables at critical time points across developed regions. We also derive the corresponding asymptotic bands (the confidence intervals) for orthogonal impulse responses function under standard assumptions."
http://arxiv.org/abs/2208.14650v1,Changing Electricity Markets: Quantifying the Price Effects of Greening the Energy Matrix,2022-08-31 06:18:12+00:00,"['Emanuel Kohlscheen', 'Richhild Moessner']",econ.GN,"We analyse the drivers of European Power Exchange (EPEX) wholesale electricity prices between 2012 and early 2022 using machine learning. The agnostic random forest approach that we use is able to reduce in-sample root mean square errors (RMSEs) by around 50% when compared to a standard linear least square model. This indicates that non-linearities and interaction effects are key in wholesale electricity markets. Out-of-sample prediction errors using machine learning are (slightly) lower than even in-sample least square errors using a least square model. The effects of efforts to limit power consumption and green the energy matrix on wholesale electricity prices are first order. CO2 permit prices strongly impact electricity prices, as do the prices of source energy commodities. And carbon permit prices impact has clearly increased post-2021 (particularly for baseload prices). Among energy sources, natural gas has the largest effect on electricity prices. Importantly, the role of wind energy feed-in has slowly risen over time, and its impact is now roughly on par with that of coal."
http://arxiv.org/abs/2208.14121v3,Prolonged Learning and Hasty Stopping: the Wald Problem with Ambiguity,2022-08-30 10:09:29+00:00,"['Sarah Auster', 'Yeon-Koo Che', 'Konrad Mierendorff']",econ.TH,"This paper studies sequential information acquisition by an ambiguity-averse decision maker (DM), who decides how long to collect information before taking an irreversible action. The agent optimizes against the worst-case belief and updates prior by prior. We show that the consideration of ambiguity gives rise to rich dynamics: compared to the Bayesian DM, the DM here tends to experiment excessively when facing modest uncertainty and, to counteract it, may stop experimenting prematurely when facing high uncertainty. In the latter case, the DM's stopping rule is non-monotonic in beliefs and features randomized stopping."
http://arxiv.org/abs/2208.14254v2,"Quantifying the Role of Interest Rates, the Dollar and Covid in Oil Prices",2022-08-30 13:28:07+00:00,['Emanuel Kohlscheen'],econ.GN,"This study analyses oil price movements through the lens of an agnostic random forest model, which is based on 1,000 regression trees. It shows that this highly disciplined, yet flexible computational model reduces in sample root mean square errors by 65% relative to a standard linear least square model that uses the same set of 11 explanatory factors. In forecasting exercises the RMSE reduction ranges between 51% and 68%, highlighting the relevance of non linearities in oil markets. The results underscore the importance of incorporating financial factors into oil models: US interest rates, the dollar and the VIX together account for 39% of the models RMSE reduction in the post 2010 sample, rising to 48% in the post 2020 sample. If Covid 19 is also considered as a risk factor, these shares become even larger."
http://arxiv.org/abs/2208.14902v2,A nation-wide experiment: fuel tax cuts and almost free public transport for three months in Germany -- Report 3 Second wave results,2022-08-31 14:53:28+00:00,"['Allister Loder', 'Fabienne Cantner', 'Andrea Cadavid', 'Markus B. Siewert', 'Stefan Wurster', 'Sebastian Goerg', 'Klaus Bogenberger']",econ.GN,"In spring 2022, the German federal government agreed on a set of measures that aimed at reducing households' financial burden resulting from a recent price increase, especially in energy and mobility. These measures included among others, a nation-wide public transport ticket for 9\ EUR per month and a fuel tax cut that reduced fuel prices by more than 15\,\%. In transportation research this is an almost unprecedented behavioral experiment. It allows to study not only behavioral responses in mode choice and induced demand but also to assess the effectiveness of transport policy instruments. We observe this natural experiment with a three-wave survey and an app-based travel diary on a sample of hundreds of participants as well as an analysis of traffic counts. In this third report, we provide first findings from the second survey, conducted during the experiment."
http://arxiv.org/abs/2207.04856v4,Research Joint Ventures: The Role of Financial Constraints,2022-07-11 13:33:03+00:00,"['Philipp Brunner', 'Igor Letina', 'Armin Schmutzler']",econ.GN,"This paper provides a novel theory of research joint ventures for financially constrained firms. When firms choose R&D portfolios, an RJV can help to coordinate research efforts, reducing investments in duplicate projects. This can free up resources, increase the variety of pursued projects and thereby increase the probability of discovering the innovation. RJVs improve innovation outcomes when market competition is weak and external financing conditions are bad. An RJV may increase the innovation probability and nevertheless lower total R&D costs. RJVs that increase innovation also increase consumer surplus and tend to be profitable, but innovation-reducing RJVs also exist. Finally, we compare RJVs to innovation-enhancing mergers."
http://arxiv.org/abs/2208.09372v3,Non-Stationary Dynamic Pricing Via Actor-Critic Information-Directed Pricing,2022-08-19 14:37:37+00:00,"['Po-Yi Liu', 'Chi-Hua Wang', 'Henghsiu Tsai']",stat.ML,"This paper presents a novel non-stationary dynamic pricing algorithm design, where pricing agents face incomplete demand information and market environment shifts. The agents run price experiments to learn about each product's demand curve and the profit-maximizing price, while being aware of market environment shifts to avoid high opportunity costs from offering sub-optimal prices. The proposed ACIDP extends information-directed sampling (IDS) algorithms from statistical machine learning to include microeconomic choice theory, with a novel pricing strategy auditing procedure to escape sub-optimal pricing after market environment shift. The proposed ACIDP outperforms competing bandit algorithms including Upper Confidence Bound (UCB) and Thompson sampling (TS) in a series of market environment shifts."
http://arxiv.org/abs/2207.14481v2,Same Root Different Leaves: Time Series and Cross-Sectional Methods in Panel Data,2022-07-29 05:12:32+00:00,"['Dennis Shen', 'Peng Ding', 'Jasjeet Sekhon', 'Bin Yu']",econ.EM,"A central goal in social science is to evaluate the causal effect of a policy. One dominant approach is through panel data analysis in which the behaviors of multiple units are observed over time. The information across time and space motivates two general approaches: (i) horizontal regression (i.e., unconfoundedness), which exploits time series patterns, and (ii) vertical regression (e.g., synthetic controls), which exploits cross-sectional patterns. Conventional wisdom states that the two approaches are fundamentally different. We establish this position to be partly false for estimation but generally true for inference. In particular, we prove that both approaches yield identical point estimates under several standard settings. For the same point estimate, however, each approach quantifies uncertainty with respect to a distinct estimand. In turn, the confidence interval developed for one estimand may have incorrect coverage for another. This emphasizes that the source of randomness that researchers assume has direct implications for the accuracy of inference."
http://arxiv.org/abs/2208.09102v1,On the Estimation of Peer Effects for Sampled Networks,2022-08-19 00:30:43+00:00,['Mamadou Yauck'],econ.EM,"This paper deals with the estimation of exogeneous peer effects for partially observed networks under the new inferential paradigm of design identification, which characterizes the missing data challenge arising with sampled networks with the central idea that two full data versions which are topologically compatible with the observed data may give rise to two different probability distributions. We show that peer effects cannot be identified by design when network links between sampled and unsampled units are not observed. Under realistic modeling conditions, and under the assumption that sampled units report on the size of their network of contacts, the asymptotic bias arising from estimating peer effects with incomplete network data is characterized, and a bias-corrected estimator is proposed. The finite sample performance of our methodology is investigated via simulations."
http://arxiv.org/abs/2207.14724v2,The IPCC and the challenge of ex post policy evaluation,2022-07-29 14:56:45+00:00,['Richard S. J. Tol'],econ.GN,"The IPCC started at a time when climate policy was an aspiration for the future. The research assessed in the early IPCC reports was necessarily about potential climate policies, always stylized and often optimized. The IPCC has continued on this path, even though there is now a considerable literature studying actual climate policy, in all its infuriating detail, warts and all. Four case studies suggest that the IPCC, in its current form, will not be able to successfully switch from ex ante to ex post policy evaluation. This transition is key as AR7 will most likely have to confront the failure to meet the 1.5K target. The four cases are as follows. (1) The scenarios first build and later endorsed by the IPCC all project a peaceful future with steady if not rapid economic growth everywhere, more closely resembling political manifestos than facts on the ground. (2) Successive IPCC reports have studiously avoided discussing the voluminous literature suggesting that political targets for greenhouse gas emission reduction are far from optimal, although a central part of that work was awarded the Nobel Prize in 2018. (3) IPCC AR5 found it impossible to acknowledge that the international climate policy negotiations from COP1 (Berlin) to COP19 (Warsaw) were bound to fail, just months before the radical overhaul at COP20 (Lima) proved that point. (4) IPCC AR6 by and large omitted the nascent literature on \textit{ex post} climate policy evaluation. Together, these cases suggest that the IPCC finds self-criticism difficult and is too close to policy makers to criticize past and current policy mistakes. One solution would be to move control over the IPCC to the national authorities on research and higher education."
http://arxiv.org/abs/2208.06849v1,On spatial majority voting with an even (vis-a-vis odd) number of voters: a note,2022-08-14 13:11:55+00:00,"['Anindya Bhattacharya', 'Francesco Ciardiello']",econ.TH,"In this note we consider situations of (multidimensional) spatial majority voting. We show that under some assumptions usual in this literature, with an even number of voters if the core of the voting situation is singleton (and in the interior of the policy space) then the element in the core is never a Condorcet winner. This is in sharp contrast with what happens with an odd number of voters: in that case, under identical assumptions, it is well known that if the core of the voting situation is non-empty then the single element in the core is the Condorcet winner as well."
http://arxiv.org/abs/2208.01967v1,"Weak Instruments, First-Stage Heteroskedasticity, the Robust F-Test and a GMM Estimator with the Weight Matrix Based on First-Stage Residuals",2022-08-03 10:34:20+00:00,['Frank Windmeijer'],econ.EM,"This paper is concerned with the findings related to the robust first-stage F-statistic in the Monte Carlo analysis of Andrews (2018), who found in a heteroskedastic grouped-data design that even for very large values of the robust F-statistic, the standard 2SLS confidence intervals had large coverage distortions. This finding appears to discredit the robust F-statistic as a test for underidentification. However, it is shown here that large values of the robust F-statistic do imply that there is first-stage information, but this may not be utilized well by the 2SLS estimator, or the standard GMM estimator. An estimator that corrects for this is a robust GMM estimator, denoted GMMf, with the robust weight matrix not based on the structural residuals, but on the first-stage residuals. For the grouped-data setting of Andrews (2018), this GMMf estimator gives the weights to the group specific estimators according to the group specific concentration parameters in the same way as 2SLS does under homoskedasticity, which is formally shown using weak instrument asymptotics. The GMMf estimator is much better behaved than the 2SLS estimator in the Andrews (2018) design, behaving well in terms of relative bias and Wald-test size distortion at more standard values of the robust F-statistic. We show that the same patterns can occur in a dynamic panel data model when the error variance is heteroskedastic over time. We further derive the conditions under which the Stock and Yogo (2005) weak instruments critical values apply to the robust F-statistic in relation to the behaviour of the GMMf estimator."
http://arxiv.org/abs/2208.00972v1,A penalized two-pass regression to predict stock returns with time-varying risk premia,2022-08-01 16:21:20+00:00,"['Gaetan Bakalli', 'Stéphane Guerrier', 'Olivier Scaillet']",econ.EM,"We develop a penalized two-pass regression with time-varying factor loadings. The penalization in the first pass enforces sparsity for the time-variation drivers while also maintaining compatibility with the no-arbitrage restrictions by regularizing appropriate groups of coefficients. The second pass delivers risk premia estimates to predict equity excess returns. Our Monte Carlo results and our empirical results on a large cross-sectional data set of US individual stocks show that penalization without grouping can yield to nearly all estimated time-varying models violating the no-arbitrage restrictions. Moreover, our results demonstrate that the proposed method reduces the prediction errors compared to a penalized approach without appropriate grouping or a time-invariant factor model."
http://arxiv.org/abs/2208.02073v2,Coherence without Rationality at the Zero Lower Bound,2022-08-03 13:48:36+00:00,"['Guido Ascari', 'Sophocles Mavroeidis', 'Nigel McClung']",econ.GN,"Standard rational expectations models with an occasionally binding zero lower bound constraint either admit no solutions (incoherence) or multiple solutions (incompleteness). This paper shows that deviations from full-information rational expectations mitigate concerns about incoherence and incompleteness. Models with no rational expectations equilibria admit self-confirming equilibria involving the use of simple mis-specified forecasting models. Completeness and coherence is restored if expectations are adaptive or if agents are less forward-looking due to some information or behavioral friction. In the case of incompleteness, the E-stability criterion selects an equilibrium."
http://arxiv.org/abs/2208.02154v1,Child Care Provider Survival Analysis,2022-08-03 15:42:53+00:00,"['Phillip Sherlock', 'Herman T. Knopf', 'Robert Chapman', 'Maya Schreiber', 'Courtney K. Blackwell']",econ.GN,"The aggregate ability of child care providers to meet local demand for child care is linked to employment rates in many sectors of the economy. Amid growing concern regarding child care provider sustainability due to the COVID-19 pandemic, state and local governments have received large amounts of new funding to better support provider stability. In response to this new funding aimed at bolstering the child care market in Florida, this study was devised as an exploratory investigation into features of child care providers that lead to business longevity. In this study we used optimal survival trees, a machine learning technique designed to better understand which providers are expected to remain operational for longer periods of time, supporting stabilization of the child care market. This tree-based survival analysis detects and describes complex interactions between provider characteristics that lead to differences in expected business survival rates. Results show that small providers who are religiously affiliated, and all providers who are serving children in Florida's universal Prekindergarten program and/or children using child care subsidy, are likely to have the longest expected survival rates."
http://arxiv.org/abs/2208.11606v1,Will the last be the first? School closures and educational outcomes,2022-08-24 15:20:17+00:00,"['Michele Battisti', 'Giuseppe Maggio']",econ.GN,"Governments have implemented school closures and online learning as one of the main tools to reduce the spread of Covid-19. Despite the potential benefits in terms of reduction of cases, the educational costs of these policies may be dramatic. This work identifies the educational costs, expressed as decrease in test scores, for the whole universe of Italian students attending the 5th, 8th and 13th grade of the school cycle during the 2021/22 school year. The analysis relies on a difference-in-difference model in relative time, where the control group is the closest generation before the Covid-19 pandemic. The results suggest a national average loss between 1.6-4.1% and 0.5-2.4% in Mathematics and Italian test scores, respectively. After collecting the precise number of days of school closures for the universe of students in Sicily, we estimate that 30 additional days of closure decrease the test score by 1%. However, the impact is much larger for students from high schools (1.8%) compared to students from low and middle schools (0.5%). This is likely explained by the lower relevance of parental inputs and higher reliance on peers inputs, within the educational production function, for higher grades. Findings are also heterogeneous across class size and parental job conditions, pointing towards potential growing inequalities driven by the lack of in front teaching."
http://arxiv.org/abs/2207.13033v1,An Axiomatic Framework for Cost-Benefit Analysis,2022-07-26 16:51:06+00:00,['Ganesh Karapakula'],econ.GN,"In recent years, the Marginal Value of Public Funds (MVPF) has become a popular tool for conducting cost-benefit analysis; the MVPF relies on the ratio of willingness-to-pay for a policy divided by its net fiscal cost. The MVPF gives policymakers important information about the equity-efficiency trade-off that is not necessarily conveyed by absolute welfare measures. However, I show in this paper that the usefulness of MVPF for comparative welfare analysis is limited, because it suffers from several empirically important economic paradoxes and statistical irregularities. There are also several practical issues in using the MVPF to aggregate welfare across policies or across population subgroups. To address these problems, I develop a new axiomatic framework to construct a measure that quantifies the equity-efficiency trade-off in a better way. I do so without compromising on the core features of the MVPF: its unit-free property, and the main preference orderings underlying it. My axiomatic framework delivers a unique (econo)metric that I call the Relative Policy Value (RPV), which can be weighted to conduct both comparative and absolute welfare analyses (or a hybrid combination thereof) and to intuitively aggregate welfare (without encountering the issues in MVPF-based aggregation). I also propose computationally convenient methods to make uniformly valid statistical inferences on welfare measures. After reanalyzing several government policies using my new econometric methods, I conclude that there is substantial economic and statistical uncertainty about welfare of some policies that were previously reported to have very high or even ""precisely estimated infinite"" MVPF values."
http://arxiv.org/abs/2208.02516v2,Weak convergence to derivatives of fractional Brownian motion,2022-08-04 08:04:54+00:00,"['Søren Johansen', 'Morten Ørregaard Nielsen']",math.PR,"It is well known that, under suitable regularity conditions, the normalized fractional process with fractional parameter $d$ converges weakly to fractional Brownian motion for $d>1/2$. We show that, for any non-negative integer $M$, derivatives of order $m=0,1,\dots,M$ of the normalized fractional process with respect to the fractional parameter $d$, jointly converge weakly to the corresponding derivatives of fractional Brownian motion. As an illustration we apply the results to the asymptotic distribution of the score vectors in the multifractional vector autoregressive model."
http://arxiv.org/abs/2208.03719v1,Strategic differences between regional investments into graphene technology and how corporations and universities manage patent portfolios,2022-08-07 13:28:07+00:00,"['Ai Linh Nguyen', 'Wenyuan Liu', 'Khiam Aik Khor', 'Andrea Nanetti', 'Siew Ann Cheong']",cs.DL,"Nowadays, patenting activities are essential in converting applied science to technology in the prevailing innovation model. To gain strategic advantages in the technological competitions between regions, nations need to leverage the investments of public and private funds to diversify over all technologies or specialize in a small number of technologies. In this paper, we investigated who the leaders are at the regional and assignee levels, how they attained their leadership positions, and whether they adopted diversification or specialization strategies, using a dataset of 176,193 patent records on graphene between 1986 and 2017 downloaded from Derwent Innovation. By applying a co-clustering method to the IPC subclasses in the patents and using a z-score method to extract keywords from their titles and abstracts, we identified seven graphene technology areas emerging in the sequence synthesis - composites - sensors - devices - catalyst - batteries - water treatment. We then examined the top regions in their investment preferences and their changes in rankings over time and found that they invested in all seven technology areas. In contrast, at the assignee level, some were diversified while others were specialized. We found that large entities diversified their portfolios across multiple technology areas, while small entities specialized around their core competencies. In addition, we found that universities had higher entropy values than corporations on average, leading us to the hypothesis that corporations file, buy, or sell patents to enable product development. In contrast, universities focus only on licensing their patents. We validated this hypothesis through an aggregate analysis of reassignment and licensing and a more detailed analysis of three case studies - SAMSUNG, RICE UNIVERSITY, and DYSON."
http://arxiv.org/abs/2208.07926v1,Mental health concerns prelude the Great Resignation: Evidence from Social Media,2022-08-16 19:49:34+00:00,"['R. Maria del Rio-Chanona', 'Alejandro Hermida-Carrillo', 'Melody Sepahpour-Fard', 'Luning Sun', 'Renata Topinkova', 'Ljubica Nedelkoska']",econ.GN,"To study the causes of the 2021 Great Resignation, we use text analysis to investigate the changes in work- and quit-related posts between 2018 and 2021 on Reddit. We find that the Reddit discourse evolution resembles the dynamics of the U.S. quit and layoff rates. Furthermore, when the COVID-19 pandemic started, conversations related to working from home, switching jobs, work-related distress, and mental health increased. We distinguish between general work-related and specific quit-related discourse changes using a difference-in-differences method. Our main finding is that mental health and work-related distress topics disproportionally increased among quit-related posts since the onset of the pandemic, likely contributing to the Great Resignation. Along with better labor market conditions, some relief came beginning-to-mid-2021 when these concerns decreased. Our study validates the use of forums such as Reddit for studying emerging economic phenomena in real time, complementing traditional labor market surveys and administrative data."
http://arxiv.org/abs/2208.08348v1,"Ban The Box? Information, Incentives, and Statistical Discrimination",2022-08-17 15:22:14+00:00,"['John W. Patty', 'Elizabeth Maggie Penn']",econ.TH,"""Banning the Box"" refers to a policy campaign aimed at prohibiting employers from soliciting applicant information that could be used to statistically discriminate against categories of applicants (in particular, those with criminal records). In this article, we examine how the concealing or revealing of informative features about an applicant's identity affects hiring both directly and, in equilibrium, by possibly changing applicants' incentives to invest in human capital. We show that there exist situations in which an employer and an applicant are in agreement about whether to ban the box. Specifically, depending on the structure of the labor market, banning the box can be (1) Pareto dominant, (2) Pareto dominated, (3) benefit the applicant while harming the employer, or (4) benefit the employer while harming the applicant. Our results have policy implications spanning beyond employment decisions, including the use of credit checks by landlords and standardized tests in college admissions."
http://arxiv.org/abs/2208.08471v4,"An unexpected stochastic dominance: Pareto distributions, dependence, and diversification",2022-08-17 18:17:01+00:00,"['Yuyu Chen', 'Paul Embrechts', 'Ruodu Wang']",q-fin.RM,"We find the perhaps surprising inequality that the weighted average of independent and identically distributed Pareto random variables with infinite mean is larger than one such random variable in the sense of first-order stochastic dominance. This result holds for more general models including super-Pareto distributions, negative dependence, and triggering events, and yields superadditivity of the risk measure Value-at-Risk for these models."
http://arxiv.org/abs/2208.07533v4,An axiomatic theory for anonymized risk sharing,2022-08-16 04:37:00+00:00,"['Zhanyi Jiao', 'Steven Kou', 'Yang Liu', 'Ruodu Wang']",econ.TH,"We study an axiomatic framework for anonymized risk sharing. In contrast to traditional risk sharing settings, our framework requires no information on preferences, identities, private operations and realized losses from the individual agents, and thereby it is useful for modeling risk sharing in decentralized systems. Four axioms natural in such a framework -- actuarial fairness, risk fairness, risk anonymity, and operational anonymity -- are put forward and discussed. We establish the remarkable fact that the four axioms characterizes the conditional mean risk sharing rule, revealing the unique and prominent role of this popular risk sharing rule among all others in relevant applications of anonymized risk sharing. Several other properties and their relations to the four axioms are studied, as well as their implications in rationalizing the design of some sharing mechanisms in practice."
http://arxiv.org/abs/2208.08169v1,Time is limited on the road to asymptopia,2022-08-17 09:15:07+00:00,"['Ivonne Schwartz', 'Mark Kirstein']",econ.EM,"One challenge in the estimation of financial market agent-based models (FABMs) is to infer reliable insights using numerical simulations validated by only a single observed time series. Ergodicity (besides stationarity) is a strong precondition for any estimation, however it has not been systematically explored and is often simply presumed. For finite-sample lengths and limited computational resources empirical estimation always takes place in pre-asymptopia. Thus broken ergodicity must be considered the rule, but it remains largely unclear how to deal with the remaining uncertainty in non-ergodic observables. Here we show how an understanding of the ergodic properties of moment functions can help to improve the estimation of (F)ABMs. We run Monte Carlo experiments and study the convergence behaviour of moment functions of two prototype models. We find infeasibly-long convergence times for most. Choosing an efficient mix of ensemble size and simulated time length guided our estimation and might help in general."
http://arxiv.org/abs/2208.08171v2,Generic catastrophic poverty when selfish investors exploit a degradable common resource,2022-08-17 09:19:14+00:00,['Claudius Gros'],econ.TH,"The productivity of a common pool of resources may degrade when overly exploited by a number of selfish investors, a situation known as the tragedy of the commons (TOC). Without regulations, agents optimize the size of their individual investments into the commons by balancing incurring costs with the returns received. The resulting Nash equilibrium involves a self-consistency loop between individual investment decisions and the state of the commons. As a consequence, several non-trivial properties emerge. For $N$ investing actors we prove rigorously that typical payoffs do not scale as $1/N$, the expected result for cooperating agents, but as $(1/N)^2$. Payoffs are hence reduced with regard to the functional dependence on $N$, a situation denoted catastrophic poverty. We show that catastrophic poverty results from a fine-tuned balance between returns and costs. Additionally, a finite number of oligarchs may be present. Oligarchs are characterized by payoffs that are finite and not decreasing when $N$ increases. Our results hold for generic classes of models, including convex and moderately concave cost functions. For strongly concave cost functions the Nash equilibrium undergoes a collective reorganization, being characterized instead by entry barriers and sudden death forced market exits."
http://arxiv.org/abs/2208.08291v3,Inference on Strongly Identified Functionals of Weakly Identified Functions,2022-08-17 13:38:31+00:00,"['Andrew Bennett', 'Nathan Kallus', 'Xiaojie Mao', 'Whitney Newey', 'Vasilis Syrgkanis', 'Masatoshi Uehara']",stat.ME,"In a variety of applications, including nonparametric instrumental variable (NPIV) analysis, proximal causal inference under unmeasured confounding, and missing-not-at-random data with shadow variables, we are interested in inference on a continuous linear functional (e.g., average causal effects) of nuisance function (e.g., NPIV regression) defined by conditional moment restrictions. These nuisance functions are generally weakly identified, in that the conditional moment restrictions can be severely ill-posed as well as admit multiple solutions. This is sometimes resolved by imposing strong conditions that imply the function can be estimated at rates that make inference on the functional possible. In this paper, we study a novel condition for the functional to be strongly identified even when the nuisance function is not; that is, the functional is amenable to asymptotically-normal estimation at $\sqrt{n}$-rates. The condition implies the existence of debiasing nuisance functions, and we propose penalized minimax estimators for both the primary and debiasing nuisance functions. The proposed nuisance estimators can accommodate flexible function classes, and importantly they can converge to fixed limits determined by the penalization regardless of the identifiability of the nuisances. We use the penalized nuisance estimators to form a debiased estimator for the functional of interest and prove its asymptotic normality under generic high-level conditions, which provide for asymptotically valid confidence intervals. We also illustrate our method in a novel partially linear proximal causal inference problem and a partially linear instrumental variable regression problem."
http://arxiv.org/abs/2209.08574v1,Skills and Liquidity Barriers to Youth Employment: Medium-term Evidence from a Cash Benchmarking Experiment in Rwanda,2022-09-18 14:24:30+00:00,"['Craig McIntosh', 'Andrew Zeitlin']",econ.GN,"We present results of an experiment benchmarking a workforce training program against cash transfers for underemployed young adults in Rwanda. 3.5 years after treatment, the training program enhances productive time use and asset investment, while the cash transfers drive productive assets, livestock values, savings, and subjective well-being. Both interventions have powerful effects on entrepreneurship. But while labor, sales, and profits all go up, the implied wage rate in these businesses is low. Our results suggest that credit is a major barrier to self-employment, but deeper reforms may be required to enable entrepreneurship to provide a transformative pathway out of poverty."
http://arxiv.org/abs/2209.00409v2,The Impact of the #MeToo Movement on Language at Court -- A text-based causal inference approach,2022-09-01 12:34:36+00:00,['Henrika Langen'],econ.GN,"This study assesses the effect of the #MeToo movement on the language used in judicial opinions on sexual violence related cases from 51 U.S. state and federal appellate courts. The study introduces various indicators to quantify the extent to which actors in courtrooms employ language that implicitly shifts responsibility away from the perpetrator and onto the victim. One indicator measures how frequently the victim is mentioned as the grammatical subject, as research in the field of psychology suggests that victims are assigned more blame the more often they are referred to as the grammatical subject. The other two indices designed to gauge the level of victim-blaming capture the sentiment of and the context in sentences referencing the victim and/or perpetrator. Additionally, judicial opinions are transformed into bag-of-words and tf-idf vectors to facilitate the examination of the evolution of language over time. The causal effect of the #MeToo movement is estimated by means of a Difference-in-Differences approach comparing the development of the language in opinions on sexual offenses and other crimes against persons as well as a Panel Event Study approach. The results do not clearly identify a #MeToo-movement-induced change in the language in court but suggest that the movement may have accelerated the evolution of court language slightly, causing the effect to materialize with a significant time lag. Additionally, the study considers potential effect heterogeneity with respect to the judge's gender and political affiliation. The study combines causal inference with text quantification methods that are commonly used for classification as well as with indicators that rely on sentiment analysis, word embedding models and grammatical tagging."
http://arxiv.org/abs/2209.10498v1,International institutions and power politics in the context of Chinese Belt and Road Initiative,2022-09-21 16:53:53+00:00,['Mandeep Singh Rai'],econ.GN,"The subject of international institutions and power politics continues to occupy a central position in the field of International Relations and to the world politics. It revolves around key questions on how rising states, regional powers and small states leverage international institutions for achieving social, political, economic gains for themselves. Taking into account one of the rising powers China and the role of international institutions in the contemporary international politics, this paper aims to demonstrate, how in pursuit of power politics, various states (Small, Regional and Great powers) utilise international institutions by making them adapt to the new power realities critical to world politics."
http://arxiv.org/abs/2209.09847v2,Rationality and correctness in n-player games,2022-09-20 16:46:22+00:00,"['Lorenzo Bastianello', 'Mehmet S. Ismail']",econ.TH,"There are two well-known sufficient conditions for Nash equilibrium in two-player games: mutual knowledge of rationality (MKR) and mutual knowledge of conjectures. MKR assumes that the concept of rationality is mutually known. In contrast, mutual knowledge of conjectures assumes that a given profile of conjectures is mutually known, which has long been recognized as a strong assumption. In this note, we introduce a notion of ""mutual assumption of rationality and correctness"" (MARC), which conceptually aligns more closely with the MKR assumption. We present two main results. Our first result establishes that MARC holds in every two-person zero-sum game. In our second theorem, we show that MARC does not in general hold in n-player games."
http://arxiv.org/abs/2209.10128v3,Efficient Integrated Volatility Estimation in the Presence of Infinite Variation Jumps via Debiased Truncated Realized Variations,2022-09-21 05:36:17+00:00,"['B. Cooper Boniece', 'José E. Figueroa-López', 'Yuchen Han']",econ.EM,"Statistical inference for stochastic processes based on high-frequency observations has been an active research area for more than two decades. One of the most well-known and widely studied problems has been the estimation of the quadratic variation of the continuous component of an Itô semimartingale with jumps. Several rate- and variance-efficient estimators have been proposed in the literature when the jump component is of bounded variation. However, to date, very few methods can deal with jumps of unbounded variation. By developing new high-order expansions of the truncated moments of a locally stable Lévy process, we propose a new rate- and variance-efficient volatility estimator for a class of Itô semimartingales whose jumps behave locally like those of a stable Lévy process with Blumenthal-Getoor index $Y\in (1,8/5)$ (hence, of unbounded variation). The proposed method is based on a two-step debiasing procedure for the truncated realized quadratic variation of the process and can also cover the case $Y<1$. Our Monte Carlo experiments indicate that the method outperforms other efficient alternatives in the literature in the setting covered by our theoretical framework."
http://arxiv.org/abs/2209.10363v1,Insurance Contract for High Renewable Energy Integration,2022-09-21 13:57:04+00:00,"['Dongwei Zhao', 'Hao Wang', 'Jianwei Huang', 'Xiaojun Lin']",eess.SY,"The increasing penetration of renewable energy poses significant challenges to power grid reliability. There have been increasing interests in utilizing financial tools, such as insurance, to help end-users hedge the potential risk of lost load due to renewable energy variability. With insurance, a user pays a premium fee to the utility, so that he will get compensated in case his demand is not fully satisfied. A proper insurance design needs to resolve the following two challenges: (i) users' reliability preference is private information; and (ii) the insurance design is tightly coupled with the renewable energy investment decision. To address these challenges, we adopt the contract theory to elicit users' private reliability preferences, and we study how the utility can jointly optimize the insurance contract and the planning of renewable energy. A key analytical challenge is that the joint optimization of the insurance design and the planning of renewables is non-convex. We resolve this difficulty by revealing important structural properties of the optimal solution, using the help of two benchmark problems: the no-insurance benchmark and the social-optimum benchmark. Compared with the no-insurance benchmark, we prove that the social cost and users' total energy cost are always no larger under the optimal contract. Simulation results show that the largest benefit of the insurance contract is achieved at a medium electricity-bill price together with a low type heterogeneity and a high renewable uncertainty."
http://arxiv.org/abs/2209.11079v1,The effect of ambiguity in strategic environments: an experiment,2022-09-22 15:09:35+00:00,"['Pablo Brañas-Garza', 'Antonio Cabrales', 'María Paz Espinosa', 'Diego Jorrat']",econ.GN,"We experimentally study a game in which success requires a sufficient total contribution by members of a group. There are significant uncertainties surrounding the chance and the total effort required for success. A theoretical model with max-min preferences towards ambiguity predicts higher contributions under ambiguity than under risk. However, in a large representative sample of the Spanish population (1,500 participants) we find that the ATE of ambiguity on contributions is zero. The main significant interaction with the personal characteristics of the participants is with risk attitudes, and it increases contributions. This suggests that policymakers concerned with ambiguous problems (like climate change) do not need to worry excessively about ambiguity."
http://arxiv.org/abs/2209.01805v1,Robust Causal Learning for the Estimation of Average Treatment Effects,2022-09-05 07:35:58+00:00,"['Yiyan Huang', 'Cheuk Hang Leung', 'Xing Yan', 'Qi Wu', 'Shumin Ma', 'Zhiri Yuan', 'Dongdong Wang', 'Zhixiang Huang']",econ.EM,"Many practical decision-making problems in economics and healthcare seek to estimate the average treatment effect (ATE) from observational data. The Double/Debiased Machine Learning (DML) is one of the prevalent methods to estimate ATE in the observational study. However, the DML estimators can suffer an error-compounding issue and even give an extreme estimate when the propensity scores are misspecified or very close to 0 or 1. Previous studies have overcome this issue through some empirical tricks such as propensity score trimming, yet none of the existing literature solves this problem from a theoretical standpoint. In this paper, we propose a Robust Causal Learning (RCL) method to offset the deficiencies of the DML estimators. Theoretically, the RCL estimators i) are as consistent and doubly robust as the DML estimators, and ii) can get rid of the error-compounding issue. Empirically, the comprehensive experiments show that i) the RCL estimators give more stable estimations of the causal parameters than the DML estimators, and ii) the RCL estimators outperform the traditional estimators and their variants when applying different machine learning models on both simulation and benchmark datasets."
http://arxiv.org/abs/2209.04764v3,A Mathematical Analysis of the 2022 Alaska Special Election for US House,2022-09-11 01:09:15+00:00,"['Adam Graham-Squire', 'David McCune']",econ.GN,"The August 2022 Alaska Special Election for US House contained many interesting features from the perspective of social choice theory. This election used instant runoff voting (often referred to as ranked choice voting) to elect a winner, and many of the weaknesses of this voting method were on display in this election. For example, the Condorcet winner is different from the instant runoff winner, and the election demonstrated a monotonicity paradox. The election also demonstrated a no show paradox; as far as we are aware, this election represents the first document American ranked choice election to demonstrate this paradox."
http://arxiv.org/abs/2209.04770v2,Testing the martingale difference hypothesis in high dimension,2022-09-11 02:59:39+00:00,"['Jinyuan Chang', 'Qing Jiang', 'Xiaofeng Shao']",econ.EM,"In this paper, we consider testing the martingale difference hypothesis for high-dimensional time series. Our test is built on the sum of squares of the element-wise max-norm of the proposed matrix-valued nonlinear dependence measure at different lags. To conduct the inference, we approximate the null distribution of our test statistic by Gaussian approximation and provide a simulation-based approach to generate critical values. The asymptotic behavior of the test statistic under the alternative is also studied. Our approach is nonparametric as the null hypothesis only assumes the time series concerned is martingale difference without specifying any parametric forms of its conditional moments. As an advantage of Gaussian approximation, our test is robust to the cross-series dependence of unknown magnitude. To the best of our knowledge, this is the first valid test for the martingale difference hypothesis that not only allows for large dimension but also captures nonlinear serial dependence. The practical usefulness of our test is illustrated via simulation and a real data analysis. The test is implemented in a user-friendly R-function."
http://arxiv.org/abs/2209.08340v1,Peer Networks and Malleability of Educational Aspirations,2022-09-17 14:28:15+00:00,"['Michelle González Amador', 'Robin Cowan', 'Eleonora Nillesen']",econ.GN,"Continuing education beyond the compulsory years of schooling is one of the most important choices an adolescent has to make. Higher education is associated with a host of social and economic benefits both for the person and its community. Today, there is ample evidence that educational aspirations are an important determinant of said choice. We implement a multilevel, networked experiment in 45 Mexican high schools and provide evidence of the malleability of educational aspirations. We also show there exists an interdependence of students' choices and the effect of our intervention with peer networks. We find that a video intervention, which combines role models and information about returns to education is successful in updating students' beliefs and consequently educational aspirations."
http://arxiv.org/abs/2207.14666v1,Loss aversion in strategy-proof school-choice mechanisms,2022-07-29 13:20:10+00:00,"['Vincent Meisner', 'Jonas von Wangenheim']",econ.TH,"Evidence suggests that participants in strategy-proof matching mechanisms play dominated strategies. To explain the data, we introduce expectation-based loss aversion into a school-choice setting and characterize choice-acclimating personal equilibria. We find that non-truthful preference submissions can be strictly optimal if and only if they are top-rank monotone. In equilibrium, inefficiency or justified envy may arise in seemingly stable or efficient mechanisms. Specifically, students who are more loss averse or less confident than their peers obtain suboptimal allocations."
http://arxiv.org/abs/2208.01690v2,A look back at the core of games in characteristic function form: some new axiomatization results,2022-08-02 18:33:14+00:00,['Anindya Bhattacharya'],econ.TH,"In this paper we provide three new results axiomatizing the core of games in characteristic function form (not necessarily having transferable utility) obeying an innocuous condition (that the set of individually rational pay-off vectors is bounded). One novelty of this exercise is that our domain is the {\em entire} class of such games: i.e., restrictions like ""non-levelness"" or ""balancedness"" are not required."
http://arxiv.org/abs/2207.03076v2,Playing Divide-and-Choose Given Uncertain Preferences,2022-07-07 04:15:26+00:00,"['Jamie Tucker-Foltz', 'Richard Zeckhauser']",cs.GT,"We study the classic divide-and-choose method for equitably allocating divisible goods between two players who are rational, self-interested Bayesian agents. The players have additive values for the goods. The prior distributions on those values are common knowledge. We consider both the cases of independent values and values that are correlated across players (as occurs when there is a common-value component).
  We describe the structure of optimal divisions in the divide-and-choose game and identify several cases where it is possible to efficiently compute equilibria. An approximation algorithm is presented for the case when the distribution over the chooser's value for each good follows a normal distribution, along with a randomized approximation algorithm for the case of uniform distributions over intervals.
  A mixture of analytic results and computational simulations illuminates several striking differences between optimal strategies in the cases of known versus unknown preferences. Most notably, given unknown preferences, the divider has a compelling ""diversification"" incentive in creating the chooser's two options. This incentive leads to multiple goods being divided at equilibrium, quite contrary to the divider's optimal strategy when preferences are known.
  In many contexts, such as buy-and-sell provisions between partners, or in judging fairness, it is important to assess the relative expected utilities of the divider and chooser. Those utilities, we show, depend on the players' levels of knowledge about each other's values, the correlations between the players' values, and the number of goods being divided. Under fairly mild assumptions, we show that the chooser is strictly better off for a small number of goods, while the divider is strictly better off for a large number of goods."
http://arxiv.org/abs/2207.02930v6,Rawlsian Assignments,2022-07-06 19:15:02+00:00,"['Tom Demeulemeester', 'Juan S. Pereyra']",econ.TH,"We study the assignment of indivisible goods to individuals without monetary transfers. Previous literature has mainly focused on efficiency and individually fair assignments; consequently, egalitarian concerns have been overlooked. Drawing inspiration from the allocation of apartments in housing cooperatives, where families prioritize egalitarianism in assignments, we introduce the concept of Rawlsian assignment. We demonstrate the uniqueness, efficiency and anonymity of the Rawlsian rule. Our findings are validated using cooperative housing preference data, showing significant improvements in egalitarian outcomes over both the probabilistic serial rule and the currently employed rule."
http://arxiv.org/abs/2208.04985v2,Pricing Novel Goods,2022-08-09 18:26:14+00:00,"['Francesco Giovannoni', 'Toomas Hinnosaar']",econ.TH,"We study a bilateral trade problem where a principal has private information that is revealed with delay, such as a seller who does not yet know her production cost. Postponing the contracting process incurs a costly delay, while early contracting with limited information can create incentive issues, as the principal might misrepresent private information that will be revealed later. We show that the optimal mechanism can effectively address these challenges by leveraging the sequential nature of the problem. The optimal mechanism is a menu of two-part tariffs, where the variable part is determined by the principal's incentives and the fixed part by the agent's incentives. As two-part tariffs might be impractical in some applications, we also study price mechanisms. We show that the optimal price mechanism often entails trade at both the ex-ante and ex-post stages. Dynamic price mechanisms can lower the cost of delay by transacting with high-type agents early and relax the incentive constraints by postponing contracts with lower-type agents. We also generalize our analysis to costly learning and study ex-post efficiency in our context."
http://arxiv.org/abs/2208.03632v3,Quantile Random-Coefficient Regression with Interactive Fixed Effects: Heterogeneous Group-Level Policy Evaluation,2022-08-07 03:56:12+00:00,"['Ruofan Xu', 'Jiti Gao', 'Tatsushi Oka', 'Yoon-Jae Whang']",econ.EM,"We propose a quantile random-coefficient regression with interactive fixed effects to study the effects of group-level policies that are heterogeneous across individuals. Our approach is the first to use a latent factor structure to handle the unobservable heterogeneities in the random coefficient. The asymptotic properties and an inferential method for the policy estimators are established. The model is applied to evaluate the effect of the minimum wage policy on earnings between 1967 and 1980 in the United States. Our results suggest that the minimum wage policy has significant and persistent positive effects on black workers and female workers up to the median. Our results also indicate that the policy helps reduce income disparity up to the median between two groups: black, female workers versus white, male workers. However, the policy is shown to have little effect on narrowing the income gap between low- and high-income workers within the subpopulations."
http://arxiv.org/abs/2207.12494v3,Extending the Range of Robust PCE Inflation Measures,2022-07-25 19:46:55+00:00,"['Sergio Ocampo', 'Raphael Schoenle', 'Dominic A. Smith']",econ.GN,"Robust inflation measures gauge inflation behavior by excluding volatile expenditure categories from headline inflation. We evaluate the forecasting performance of a wide set of such measures between 1970 and 2024, including core, median, and trimmed mean personal-consumption-expenditure (PCE) inflation. Core inflation performs significantly worse than official median and trimmed mean inflation. Among a set of alternative trimmed mean measures, there is no single best trim based on forecasting performance: A wide set of trims generates statistically indistinguishable average errors. Nonetheless, different trims imply different predictions for trend inflation in any given month, within a range of 0.5 to 1 percentage points. In tracking trend inflation, this range and its midpoint outperform all trimmed mean inflation measures, suggesting the use of the range of inflation implied by the set of near-optimal trims as a valuable complement to any single inflation measure."
http://arxiv.org/abs/2209.03199v1,An Assessment Tool for Academic Research Managers in the Third World,2022-09-07 14:59:25+00:00,"['Fernando Delbianco', 'Andres Fioriti', 'Fernando Tohmé']",econ.EM,"The academic evaluation of the publication record of researchers is relevant for identifying talented candidates for promotion and funding. A key tool for this is the use of the indexes provided by Web of Science and SCOPUS, costly databases that sometimes exceed the possibilities of academic institutions in many parts of the world. We show here how the data in one of the bases can be used to infer the main index of the other one. Methods of data analysis used in Machine Learning allow us to select just a few of the hundreds of variables in a database, which later are used in a panel regression, yielding a good approximation to the main index in the other database. Since the information of SCOPUS can be freely scraped from the Web, this approach allows to infer for free the Impact Factor of publications, the main index used in research assessments around the globe."
http://arxiv.org/abs/2208.06115v6,A Nonparametric Approach with Marginals for Modeling Consumer Choice,2022-08-12 04:43:26+00:00,"['Yanqiu Ruan', 'Xiaobo Li', 'Karthyek Murthy', 'Karthik Natarajan']",stat.ML,"Given data on the choices made by consumers for different offer sets, a key challenge is to develop parsimonious models that describe and predict consumer choice behavior while being amenable to prescriptive tasks such as pricing and assortment optimization. The marginal distribution model (MDM) is one such model, which requires only the specification of marginal distributions of the random utilities. This paper aims to establish necessary and sufficient conditions for given choice data to be consistent with the MDM hypothesis, inspired by the usefulness of similar characterizations for the random utility model (RUM). This endeavor leads to an exact characterization of the set of choice probabilities that the MDM can represent. Verifying the consistency of choice data with this characterization is equivalent to solving a polynomial-sized linear program. Since the analogous verification task for RUM is computationally intractable and neither of these models subsumes the other, MDM is helpful in striking a balance between tractability and representational power. The characterization is then used with robust optimization for making data-driven sales and revenue predictions for new unseen assortments. When the choice data lacks consistency with the MDM hypothesis, finding the best-fitting MDM choice probabilities reduces to solving a mixed integer convex program. Numerical results using real world data and synthetic data demonstrate that MDM exhibits competitive representational power and prediction performance compared to RUM and parametric models while being significantly faster in computation than RUM."
http://arxiv.org/abs/2208.06907v3,Impossibility theorems involving weakenings of expansion consistency and resoluteness in voting,2022-08-14 20:12:19+00:00,"['Wesley H. Holliday', 'Chase Norman', 'Eric Pacuit', 'Saam Zahedian']",econ.TH,"A fundamental principle of individual rational choice is Sen's $γ$ axiom, also known as expansion consistency, stating that any alternative chosen from each of two menus must be chosen from the union of the menus. Expansion consistency can also be formulated in the setting of social choice. In voting theory, it states that any candidate chosen from two fields of candidates must be chosen from the combined field of candidates. An important special case of the axiom is binary expansion consistency, which states that any candidate chosen from an initial field of candidates and chosen in a head-to-head match with a new candidate must also be chosen when the new candidate is added to the field, thereby ruling out spoiler effects. In this paper, we study the tension between this weakening of expansion consistency and weakenings of resoluteness, an axiom demanding the choice of a single candidate in any election. As is well known, resoluteness is inconsistent with basic fairness conditions on social choice, namely anonymity and neutrality. Here we prove that even significant weakenings of resoluteness, which are consistent with anonymity and neutrality, are inconsistent with binary expansion consistency. The proofs make use of SAT solving, with the correctness of a SAT encoding formally verified in the Lean Theorem Prover, as well as a strategy for generalizing impossibility theorems obtained for special types of voting methods (namely majoritarian and pairwise voting methods) to impossibility theorems for arbitrary voting methods. This proof strategy may be of independent interest for its potential applicability to other impossibility theorems in social choice."
http://arxiv.org/abs/2209.13689v2,Optimally Biased Expertise,2022-09-27 21:03:58+00:00,"['Pavel Ilinov', 'Andrei Matveenko', 'Maxim Senkov', 'Egor Starkov']",econ.TH,"We show that in delegation problems, a principal benefits from belief misalignment vis-à-vis an agent when the latter can flexibly acquire costly information. The agent optimally succumbs to confirmatory learning, leading him to favor the ex ante optimal action. We show that the principal prefers to mitigate this by hiring an agent who is ex ante more uncertain about which action is optimal. This is optimal even when the principal is herself biased towards some action: the benefit always outweighs the cost of a small misalignment. Optimally misaligned agent considers weakly more actions than an aligned agent. All results continue to hold when delegation is replaced by communication."
http://arxiv.org/abs/2209.09354v2,A Dynamic Stochastic Block Model for Multidimensional Networks,2022-09-19 21:23:17+00:00,"['Ovielt Baltodano López', 'Roberto Casarin']",stat.ME,"The availability of relational data can offer new insights into the functioning of the economy. Nevertheless, modeling the dynamics in network data with multiple types of relationships is still a challenging issue. Stochastic block models provide a parsimonious and flexible approach to network analysis. We propose a new stochastic block model for multidimensional networks, where layer-specific hidden Markov-chain processes drive the changes in community formation. The changes in the block membership of a node in a given layer may be influenced by its own past membership in other layers. This allows for clustering overlap, clustering decoupling, or more complex relationships between layers, including settings of unidirectional, or bidirectional, non-linear Granger block causality. We address the overparameterization issue of a saturated specification by assuming a Multi-Laplacian prior distribution within a Bayesian framework. Data augmentation and Gibbs sampling are used to make the inference problem more tractable. Through simulations, we show that standard linear models and the pairwise approach are unable to detect block causality in most scenarios. In contrast, our model can recover the true Granger causality structure. As an application to international trade, we show that our model offers a unified framework, encompassing community detection and Gravity equation modeling. We found new evidence of block Granger causality of trade agreements and flows and core-periphery structure in both layers on a large sample of countries."
http://arxiv.org/abs/2208.00552v4,The Effect of Omitted Variables on the Sign of Regression Coefficients,2022-08-01 00:50:42+00:00,"['Matthew A. Masten', 'Alexandre Poirier']",econ.EM,"We show that, depending on how the impact of omitted variables is measured, it can be substantially easier for omitted variables to flip coefficient signs than to drive them to zero. This behavior occurs with ""Oster's delta"" (Oster 2019), a widely reported robustness measure. Consequently, any time this measure is large -- suggesting that omitted variables may be unimportant -- a much smaller value reverses the sign of the parameter of interest. We propose a modified measure of robustness to address this concern. We illustrate our results in four empirical applications and two meta-analyses. We implement our methods in the companion Stata module regsensitivity."
http://arxiv.org/abs/2207.04690v7,Dynamic Budget Throttling in Repeated Second-Price Auctions,2022-07-11 08:12:02+00:00,"['Zhaohua Chen', 'Chang Wang', 'Qian Wang', 'Yuqi Pan', 'Zhuming Shi', 'Zheng Cai', 'Yukun Ren', 'Zhihua Zhu', 'Xiaotie Deng']",cs.GT,"In today's online advertising markets, a crucial requirement for an advertiser is to control her total expenditure within a time horizon under some budget. Among various budget control methods, throttling has emerged as a popular choice, managing an advertiser's total expenditure by selecting only a subset of auctions to participate in. This paper provides a theoretical panorama of a single advertiser's dynamic budget throttling process in repeated second-price auctions. We first establish a lower bound on the regret and an upper bound on the asymptotic competitive ratio for any throttling algorithm, respectively, when the advertiser's values are stochastic and adversarial. Regarding the algorithmic side, we propose the OGD-CB algorithm, which guarantees a near-optimal expected regret with stochastic values. On the other hand, when values are adversarial, we prove that this algorithm also reaches the upper bound on the asymptotic competitive ratio. We further compare throttling with pacing, another widely adopted budget control method, in repeated second-price auctions. In the stochastic case, we demonstrate that pacing is generally superior to throttling for the advertiser, supporting the well-known result that pacing is asymptotically optimal in this scenario. However, in the adversarial case, we give an exciting result indicating that throttling is also an asymptotically optimal dynamic bidding strategy. Our results bridge the gaps in theoretical research of throttling in repeated auctions and comprehensively reveal the ability of this popular budget-smoothing strategy."
http://arxiv.org/abs/2207.04441v2,Nobel begets Nobel,2022-07-10 11:33:45+00:00,['Richard S. J. Tol'],econ.GN,"I construct the professor-student network for laureates of and candidates for the Nobel Prize in Economics. I study the effect of proximity to previous Nobelists on winning the Nobel Prize. Conditional on being Nobel-worthy, students and grandstudents of Nobel laureates are significantly less likely to win. Professors and fellow students of Nobel Prize winners, however, are significantly more likely to win."
http://arxiv.org/abs/2207.04299v1,Model diagnostics of discrete data regression: a unifying framework using functional residuals,2022-07-09 17:00:18+00:00,"['Zewei Lin', 'Dungang Liu']",stat.ME,"Model diagnostics is an indispensable component of regression analysis, yet it is not well addressed in standard textbooks on generalized linear models. The lack of exposition is attributed to the fact that when outcome data are discrete, classical methods (e.g., Pearson/deviance residual analysis and goodness-of-fit tests) have limited utility in model diagnostics and treatment. This paper establishes a novel framework for model diagnostics of discrete data regression. Unlike the literature defining a single-valued quantity as the residual, we propose to use a function as a vehicle to retain the residual information. In the presence of discreteness, we show that such a functional residual is appropriate for summarizing the residual randomness that cannot be captured by the structural part of the model. We establish its theoretical properties, which leads to the innovation of new diagnostic tools including the functional-residual-vs covariate plot and Function-to-Function (Fn-Fn) plot. Our numerical studies demonstrate that the use of these tools can reveal a variety of model misspecifications, such as not properly including a higher-order term, an explanatory variable, an interaction effect, a dispersion parameter, or a zero-inflation component. The functional residual yields, as a byproduct, Liu-Zhang's surrogate residual mainly developed for cumulative link models for ordinal data (Liu and Zhang, 2018, JASA). As a general notion, it considerably broadens the diagnostic scope as it applies to virtually all parametric models for binary, ordinal and count data, all in a unified diagnostic scheme."
http://arxiv.org/abs/2209.11837v1,Doubly Fair Dynamic Pricing,2022-09-23 20:02:09+00:00,"['Jianyu Xu', 'Dan Qiao', 'Yu-Xiang Wang']",cs.LG,"We study the problem of online dynamic pricing with two types of fairness constraints: a ""procedural fairness"" which requires the proposed prices to be equal in expectation among different groups, and a ""substantive fairness"" which requires the accepted prices to be equal in expectation among different groups. A policy that is simultaneously procedural and substantive fair is referred to as ""doubly fair"". We show that a doubly fair policy must be random to have higher revenue than the best trivial policy that assigns the same price to different groups. In a two-group setting, we propose an online learning algorithm for the 2-group pricing problems that achieves $\tilde{O}(\sqrt{T})$ regret, zero procedural unfairness and $\tilde{O}(\sqrt{T})$ substantive unfairness over $T$ rounds of learning. We also prove two lower bounds showing that these results on regret and unfairness are both information-theoretically optimal up to iterated logarithmic factors. To the best of our knowledge, this is the first dynamic pricing algorithm that learns to price while satisfying two fairness constraints at the same time."
http://arxiv.org/abs/2209.10841v1,Multiscale Comparison of Nonparametric Trend Curves,2022-09-22 08:05:16+00:00,"['Marina Khismatullina', 'Michael Vogt']",econ.EM,"We develop new econometric methods for the comparison of nonparametric time trends. In many applications, practitioners are interested in whether the observed time series all have the same time trend. Moreover, they would often like to know which trends are different and in which time intervals they differ. We design a multiscale test to formally approach these questions. Specifically, we develop a test which allows to make rigorous confidence statements about which time trends are different and where (that is, in which time intervals) they differ. Based on our multiscale test, we further develop a clustering algorithm which allows to cluster the observed time series into groups with the same trend. We derive asymptotic theory for our test and clustering methods. The theory is complemented by a simulation study and two applications to GDP growth data and house pricing data."
http://arxiv.org/abs/2209.12346v2,Exploring the Constraints on Artificial General Intelligence: A Game-Theoretic No-Go Theorem,2022-09-25 23:17:20+00:00,['Mehmet S. Ismail'],econ.TH,"The emergence of increasingly sophisticated artificial intelligence (AI) systems have sparked intense debate among researchers, policymakers, and the public due to their potential to surpass human intelligence and capabilities in all domains. In this paper, I propose a game-theoretic framework that captures the strategic interactions between a human agent and a potential superhuman machine agent. I identify four key assumptions: Strategic Unpredictability, Access to Machine's Strategy, Rationality, and Superhuman Machine. The main result of this paper is an impossibility theorem: these four assumptions are inconsistent when taken together, but relaxing any one of them results in a consistent set of assumptions. Two straightforward policy recommendations follow: first, policymakers should control access to specific human data to maintain Strategic Unpredictability; and second, they should grant select AI researchers access to superhuman machine research to ensure Access to Machine's Strategy holds. My analysis contributes to a better understanding of the context that can shape the theoretical development of superhuman AI."
http://arxiv.org/abs/2209.10405v1,Effects of Work-From-Home on University Students and Faculty,2022-09-13 07:21:18+00:00,['Avni Singh'],econ.GN,"The work-from-home policy affected people of all demographics and professions, including students and faculty at universities. After the onset of the COVID-19 pandemic in 2020, institutions moved their operations online, affecting the motivation levels, communication abilities, and mental health of students and faculty around the world. This paper is based mainly on primary data collected from students from around the world, and professors at universities in Bengaluru, India. It explores the effects of work-from-home as a policy in terms of how it changed learning during the pandemic and how it has permanently altered it in a post-pandemic future. Further, it suggests and evaluates policies on how certain negative effects of the work-from-home policy can be mitigated."
http://arxiv.org/abs/2209.10518v1,Sustainable Venture Capital,2022-09-13 01:17:39+00:00,['Sam Johnston'],cs.CY,"Sustainability initiatives are set to benefit greatly from the growing involvement of venture capital, in the same way that other technological endeavours have been enabled and accelerated in the post-war period. With the spoils increasingly being shared between shareholders and other stakeholders, this requires a more nuanced view than the finance-first methodologies deployed to date. Indeed, it is possible for a venture-backed sustainability startup to deliver outstanding results to society in general without returning a cent to investors, though the most promising outcomes deliver profit with purpose, satisfying all stakeholders in ways that make existing 'extractive' venture capital seem hollow.
  To explore this nascent area, a review of related research was conducted and social entrepreneurs & investors interviewed to construct a questionnaire assessing the interests and intentions of current & future ecosystem participants. Analysis of 114 responses received via several sampling methods revealed statistically significant relationships between investing preferences and genders, generations, sophistication, and other variables, all the way down to the level of individual UN Sustainable Development Goals (SDGs)."
http://arxiv.org/abs/2207.01793v3,The Short-term Impact of Congestion Taxes on Ridesourcing Demand and Traffic Congestion: Evidence from Chicago,2022-07-05 03:42:52+00:00,"['Yuan Liang', 'Bingjie Yu', 'Xiaojian Zhang', 'Yi Lu', 'Linchuan Yang']",econ.GN,"Ridesourcing is popular in many cities. Despite its theoretical benefits, a large body of studies have claimed that ridesourcing also brings (negative) externalities (e.g., inducing trips and aggravating traffic congestion). Therefore, many cities are planning to enact or have already enacted policies to regulate its use. However, these policies' effectiveness or impact on ridesourcing demand and traffic congestion is uncertain. To this end, this study applies difference-in-differences (i.e., a regression-based causal inference approach) to empirically evaluate the effects of the congestion tax policy on ridesourcing demand and traffic congestion in Chicago. It shows that this congestion tax policy significantly curtails overall ridesourcing demand but marginally alleviates traffic congestion. The results are robust to the choice of time windows and data sets, additional control variables, alternative model specifications, alternative control groups, and alternative modeling approaches (i.e., regression discontinuity in time). Moreover, considerable heterogeneity exists. For example, the policy notably reduces ridesourcing demand with short travel distances, but such an impact is gradually attenuated as the distance increases."
http://arxiv.org/abs/2207.03816v4,The welfare effects of nonlinear health dynamics,2022-07-08 10:50:27+00:00,"['Chiara Dal Bianco', 'Andrea Moro']",econ.GN,"We generate a continuous measure of health to estimate a non-parametric model of health dynamics, showing that adverse health shocks are highly persistent when suffered by people in poor health. Canonical models cannot account for this pattern. We incorporate this health dynamic into a life-cycle model of consumption, savings, and labor force participation. After estimating the model parameters, we simulate the effects of health shocks on economic outcomes. We find that bad health shocks have long-term adverse economic effects that are more extreme for those in poor health. Furthermore, bad health shocks also increase the disparity of asset accumulation among this group of people. A canonical model of health dynamics would not reveal these effects."
http://arxiv.org/abs/2207.02902v1,Homo economicus to model human behavior is ethically doubtful and mathematically inconsistent,2022-07-05 07:21:37+00:00,"['M. Lunkenheimer', 'A. Kracklauer', 'G. Klinkova', 'M. Grabinski']",econ.GN,"In many models in economics or business a dominantly self-interested homo economicus is assumed. Unfortunately (or fortunately), humans are in general not homines economici as e.g. the ultimatum game shows. This leads to the fact that all these models are at least doubtful. Moreover, economists started to set a quantitative value for the feeling of social justice, altruism, or envy and the like to execute utilitarian calculation. Besides being ethically doubtful, it delivers an explanation in hindsight with little predicting power. We use examples from game theory to show its arbitrariness. It is even possible that a stable Nash equilibrium can be calculated while it does not exist at all, due to the wide differences in human values. Finally, we show that assigned numbers for envy or altruism and the like do not build a field (in a mathematical sense). As there is no homomorphism to real numbers or a subset of it, any calculation is generally invalid or arbitrary. There is no (easy) way to fix the problem. One has to go back to ethical concepts like the categorical imperative or use at most semi quantitative approaches like considering knaves and knights. Mathematically one can only speculate whether e.g. surreal numbers can make ethics calculable."
http://arxiv.org/abs/2207.04082v1,Spatial Econometrics for Misaligned Data,2022-07-08 18:12:50+00:00,['Guillaume Allaire Pouliot'],econ.EM,"We produce methodology for regression analysis when the geographic locations of the independent and dependent variables do not coincide, in which case we speak of misaligned data. We develop and investigate two complementary methods for regression analysis with misaligned data that circumvent the need to estimate or specify the covariance of the regression errors. We carry out a detailed reanalysis of Maccini and Yang (2009) and find economically significant quantitative differences but sustain most qualitative conclusions."
http://arxiv.org/abs/2208.11281v2,Robust Tests of Model Incompleteness in the Presence of Nuisance Parameters,2022-08-24 02:51:05+00:00,"['Shuowen Chen', 'Hiroaki Kaido']",econ.EM,"Economic models may exhibit incompleteness depending on whether or not they admit certain policy-relevant features such as strategic interaction, self-selection, or state dependence. We develop a novel test of model incompleteness and analyze its asymptotic properties. A key observation is that one can identify the least-favorable parametric model that represents the most challenging scenario for detecting local alternatives without knowledge of the selection mechanism. We build a robust test of incompleteness on a score function constructed from such a model. The proposed procedure remains computationally tractable even with nuisance parameters because it suffices to estimate them only under the null hypothesis of model completeness. We illustrate the test by applying it to a market entry model and a triangular model with a set-valued control function."
http://arxiv.org/abs/2208.09690v1,Gradient Descent Ascent in Min-Max Stackelberg Games,2022-08-20 14:19:43+00:00,"['Denizalp Goktas', 'Amy Greenwald']",cs.GT,"Min-max optimization problems (i.e., min-max games) have attracted a great deal of attention recently as their applicability to a wide range of machine learning problems has become evident. In this paper, we study min-max games with dependent strategy sets, where the strategy of the first player constrains the behavior of the second. Such games are best understood as sequential, i.e., Stackelberg, games, for which the relevant solution concept is Stackelberg equilibrium, a generalization of Nash. One of the most popular algorithms for solving min-max games is gradient descent ascent (GDA). We present a straightforward generalization of GDA to min-max Stackelberg games with dependent strategy sets, but show that it may not converge to a Stackelberg equilibrium. We then introduce two variants of GDA, which assume access to a solution oracle for the optimal Karush Kuhn Tucker (KKT) multipliers of the games' constraints. We show that such an oracle exists for a large class of convex-concave min-max Stackelberg games, and provide proof that our GDA variants with such an oracle converge in $O(\frac{1}{\varepsilon^2})$ iterations to an $\varepsilon$-Stackelberg equilibrium, improving on the most efficient algorithms currently known which converge in $O(\frac{1}{\varepsilon^3})$ iterations. We then show that solving Fisher markets, a canonical example of a min-max Stackelberg game, using our novel algorithm, corresponds to buyers and sellers using myopic best-response dynamics in a repeated market, allowing us to prove the convergence of these dynamics in $O(\frac{1}{\varepsilon^2})$ iterations in Fisher markets. We close by describing experiments on Fisher markets which suggest potential ways to extend our theoretical results, by demonstrating how different properties of the objective function can affect the convergence and convergence rate of our algorithms."
http://arxiv.org/abs/2208.10804v1,Limit Orders and Knightian Uncertainty,2022-08-23 08:26:25+00:00,"['Michael Greinecker', 'Christoph Kuzmics']",econ.TH,"A range of empirical puzzles in finance has been explained as a consequence of traders being averse to ambiguity. Ambiguity averse traders can behave in financial portfolio problems in ways that cannot be rationalized as maximizing subjective expected utility. However, this paper shows that when traders have access to limit orders, all investment behavior of an ambiguity-averse decision-maker is observationally equivalent to the behavior of a subjective expected utility maximizer with the same risk preferences; ambiguity aversion has no additional explanatory power."
http://arxiv.org/abs/2209.08793v1,A Generalized Argmax Theorem with Applications,2022-09-19 06:53:56+00:00,['Gregory Cox'],econ.EM,"The argmax theorem is a useful result for deriving the limiting distribution of estimators in many applications. The conclusion of the argmax theorem states that the argmax of a sequence of stochastic processes converges in distribution to the argmax of a limiting stochastic process. This paper generalizes the argmax theorem to allow the maximization to take place over a sequence of subsets of the domain. If the sequence of subsets converges to a limiting subset, then the conclusion of the argmax theorem continues to hold. We demonstrate the usefulness of this generalization in three applications: estimating a structural break, estimating a parameter on the boundary of the parameter space, and estimating a weakly identified parameter. The generalized argmax theorem simplifies the proofs for existing results and can be used to prove new results in these literatures."
http://arxiv.org/abs/2207.07984v1,Characterization of Group-Fair Social Choice Rules under Single-Peaked Preferences,2022-07-16 17:12:54+00:00,"['Gogulapati Sreedurga', 'Soumyarup Sadhukhan', 'Souvik Roy', 'Yadati Narahari']",cs.GT,"We study fairness in social choice settings under single-peaked preferences. Construction and characterization of social choice rules in the single-peaked domain has been extensively studied in prior works. In fact, in the single-peaked domain, it is known that unanimous and strategy-proof deterministic rules have to be min-max rules and those that also satisfy anonymity have to be median rules. Further, random social choice rules satisfying these properties have been shown to be convex combinations of respective deterministic rules. We non-trivially add to this body of results by including fairness considerations in social choice. Our study directly addresses fairness for groups of agents. To study group-fairness, we consider an existing partition of the agents into logical groups, based on natural attributes such as gender, race, and location. To capture fairness within each group, we introduce the notion of group-wise anonymity. To capture fairness across the groups, we propose a weak notion as well as a strong notion of fairness. The proposed fairness notions turn out to be natural generalizations of existing individual-fairness notions and moreover provide non-trivial outcomes for strict ordinal preferences, unlike the existing group-fairness notions. We provide two separate characterizations of random social choice rules that satisfy group-fairness: (i) direct characterization (ii) extreme point characterization (as convex combinations of fair deterministic social choice rules). We also explore the special case where there are no groups and provide sharper characterizations of rules that achieve individual-fairness."
http://arxiv.org/abs/2208.08442v1,Peculiaridades de la Economia islandesa en los albores del siglo XXI,2022-08-17 13:45:50+00:00,['I. Martin-de-Santos'],econ.GN,"Se repasa brevemente la historia y las finanzas islandesas de manera diacrónica. Se presenta a Islandia como bastión del estallido de la crisis financiera internacional que comienza a gestarse a principios del siglo XXI y cuyo origen se hace evidente en la fecha simbólica del año 2008. Se analizan las razones fundamentales de esta crisis, centrandonos en las particularidades de la estructura económica islandesa. Se consideran las diferencias y parecidos de esta situación en relación a algunos otros países en similares circunstancias. Se estudia el caso del banco Icesave. Se considera la repercusión que la crisis experimentada por Islandia tiene en el ámbito internacional, especialmente en los inversores extranjeros y en los conflictos jurídicos surgidos a raíz de las medidas adoptadas por el gobierno islandés para sacar al país de la bancarrota.
  --
  Icelandic history and diachronically finances are briefly reviewed. Iceland is presented as a bastion of the outbreak of the global financial crisis begins to take shape in the early twenty-first century and whose origin is evident in the symbolic date of 2008. The main reasons for this crisis are analyzed, focusing on the particularities of Iceland's economic structure. The differences and similarities of this in relation to some other countries in similar circumstances are considered. Bank Icesave case is studied. The impact of the crises experienced by Iceland has in the international arena, especially foreign investors and legal disputes arising out of actions taken by the Icelandic government to pull the country out of bankruptcy is considered."
http://arxiv.org/abs/2209.02651v1,The Coronavirus Tradeoff -- Life vs. Economy: Handling the Tradeoff Rationally and Optimally,2022-09-06 17:06:59+00:00,"['Ali Zeytoon-Nejad', 'Tanzid Hasnain']",econ.GN,"The recent coronavirus outbreak has made governments face an inconvenient tradeoff choice, i.e. the choice between saving lives and saving the economy, forcing them to make immensely consequential decisions among alternative courses of actions without knowing what the ultimate results would be for the society as a whole. This paper attempts to frame the coronavirus tradeoff problem as an economic optimization problem and proposes mathematical optimization methods to make rationally optimal decisions when faced with trade-off situations such as those involved in managing through the recent coronavirus pandemic. The framework introduced and the method proposed in this paper are on the basis of the theory of rational choice at a societal level, which assumes that the government is a rational, benevolent agent that systematically and purposefully takes into account the social marginal costs and social marginal benefits of its actions to its citizens and makes decisions that maximize the society's well-being as a whole. We approach solving this tradeoff problem from a static as well as a dynamic point of view. Finally, we provide several numerical examples clarifying how the proposed framework and methods can be applied in the real-world context."
http://arxiv.org/abs/2209.03239v1,Technical and Economic Feasibility Analysis of Underground Hydrogen Storage: A Case Study in Intermountain-West Region USA,2022-09-07 15:47:46+00:00,"['Fangxuan Chen', 'Zhiwei Ma', 'Hadi Nasrabadi', 'Bailian Chen', 'Mohamed Mehana', 'Jolante Wieke Van Wijk']",physics.geo-ph,"Hydrogen is an integral component of the current energy transition roadmap to decarbonize the economy and create an environmentally-sustainable future. However, surface storage options (e.g., tanks) do not provide the required capacity or durability to deploy a regional or nationwide hydrogen economy. In this study, we have analyzed the techno-economic feasibility of the geologic storage of hydrogen in depleted gas reservoirs, salt caverns, and aquifers in the Intermountain-West (I-WEST) region. We have identified the most favorable candidate sites for hydrogen storage and estimated the volumetric storage capacity. Our results show that the geologic storage of hydrogen can provide at least 72% of total energy consumption of I-WEST region in 2020. We also calculated the capital and levelized costs of each storage option. We found that a depleted gas reservoir is the most cost-effective candidate among the three geologic storage options. Interestingly, the cushion gas type and volume play a significant role in the storage cost when we consider hydrogen storage in saline aquifers. The levelized costs of hydrogen storage in depleted gas reservoirs, salt caverns, and saline aquifers with large-scale storage capacity are approximately $1.3, $2.3, and $3.4 per kg of H2, respectively. This work provides essential guidance for the geologic hydrogen storage in the I-WEST region."
http://arxiv.org/abs/2209.03259v2,A Ridge-Regularised Jackknifed Anderson-Rubin Test,2022-09-07 16:08:51+00:00,"['Max-Sebastian Dovì', 'Anders Bredahl Kock', 'Sophocles Mavroeidis']",econ.EM,"We consider hypothesis testing in instrumental variable regression models with few included exogenous covariates but many instruments -- possibly more than the number of observations. We show that a ridge-regularised version of the jackknifed Anderson Rubin (1949, henceforth AR) test controls asymptotic size in the presence of heteroskedasticity, and when the instruments may be arbitrarily weak. Asymptotic size control is established under weaker assumptions than those imposed for recently proposed jackknifed AR tests in the literature. Furthermore, ridge-regularisation extends the scope of jackknifed AR tests to situations in which there are more instruments than observations. Monte-Carlo simulations indicate that our method has favourable finite-sample size and power properties compared to recently proposed alternative approaches in the literature. An empirical application on the elasticity of substitution between immigrants and natives in the US illustrates the usefulness of the proposed method for practitioners."
http://arxiv.org/abs/2210.03572v1,Power in the Pipeline,2022-10-07 14:15:13+00:00,"['Quentin Gallea', 'Massimo Morelli', 'Dominic Rohner']",econ.GN,"This paper provides the first comprehensive empirical analysis of the role of natural gas for the domestic and international distribution of power. The crucial role of pipelines for the trade of natural gas determines a set of network effects that are absent for other natural resources such as oil and minerals. Gas rents are not limited to producers but also accrue to key players occupying central nodes in the gas network. Drawing on our new gas pipeline data, this paper shows that gas betweenness-centrality of a country increases substantially the ruler's grip on power as measured by leader turnover. A main mechanism at work is the reluctance of connected gas trade partners to impose sanctions, meaning that bad behavior of gas-central leaders is tolerated for longer before being sanctioned. Overall, this reinforces the notion that fossil fuels are not just poison for the environment but also for political pluralism and healthy regime turnover."
http://arxiv.org/abs/2210.03200v1,"Agenda manipulation-proofness, stalemates, and redundant elicitation in preference aggregation. Exposing the bright side of Arrow's theorem",2022-10-06 20:41:55+00:00,['Stefano Vannucci'],econ.TH,"This paper provides a general framework to explore the possibility of agenda manipulation-proof and proper consensus-based preference aggregation rules, so powerfully called in doubt by a disputable if widely shared understanding of Arrow's `general possibility theorem'. We consider two alternative versions of agenda manipulation-proofness for social welfare functions, that are distinguished by `parallel' vs. `sequential' execution of agenda formation and preference elicitation, respectively. Under the `parallel' version, it is shown that a large class of anonymous and idempotent social welfare functions that satisfy both agenda manipulation-proofness and strategy-proofness on a natural domain of single-peaked `meta-preferences' induced by arbitrary total preference preorders are indeed available. It is only under the second, `sequential' version that agenda manipulation-proofness on the same natural domain of single-peaked `meta-preferences' is in fact shown to be tightly related to the classic Arrowian `independence of irrelevant alternatives' (IIA) for social welfare functions. In particular, it is shown that using IIA to secure such `sequential' version of agenda manipulation-proofness and combining it with a very minimal requirement of distributed responsiveness results in a characterization of the `global stalemate' social welfare function, the constant function which invariably selects universal social indifference. It is also argued that, altogether, the foregoing results provide new significant insights concerning the actual content and the constructive implications of Arrow's `general possibility theorem' from a mechanism-design perspective."
http://arxiv.org/abs/2210.04086v2,A Structural Equation Modeling Approach to Understand User's Perceptions of Acceptance of Ride-Sharing Services in Dhaka City,2022-10-08 18:47:53+00:00,"['Md. Mohaimenul Islam Sourav', 'Mohammed Russedul Islam', 'H M Imran Kays', 'Md. Hadiuzzaman']",stat.AP,"This research aims at building a multivariate statistical model for assessing users' perceptions of acceptance of ride-sharing services in Dhaka City. A structured questionnaire is developed based on the users' reported attitudes and perceived risks. A total of 350 normally distributed responses are collected from ride-sharing service users and stakeholders of Dhaka City. Respondents are interviewed to express their experience and opinions on ride-sharing services through the stated preference questionnaire. Structural Equation Modeling (SEM) is used to validate the research hypotheses. Statistical parameters and several trials are used to choose the best SEM. The responses are also analyzed using the Relative Importance Index (RII) method, validating the chosen SEM. Inside SEM, the quality of ride-sharing services is measured by two latent and eighteen observed variables. The latent variable 'safety & security' is more influential than 'service performance' on the overall quality of service index. Under 'safety & security' the other two variables, i.e., 'account information' and 'personal information' are found to be the most significant that impact the decision to share rides with others. In addition, 'risk of conflict' and 'possibility of accident' are identified using the perception model as the lowest contributing variables. Factor analysis reveals the suitability and reliability of the proposed SEM. Identifying the influential parameters in this will help the service providers understand and improve the quality of ride-sharing service for users."
http://arxiv.org/abs/2210.01282v3,Structural Estimation of Markov Decision Processes in High-Dimensional State Space with Finite-Time Guarantees,2022-10-04 00:11:38+00:00,"['Siliang Zeng', 'Mingyi Hong', 'Alfredo Garcia']",cs.LG,"We consider the task of estimating a structural model of dynamic decisions by a human agent based upon the observable history of implemented actions and visited states. This problem has an inherent nested structure: in the inner problem, an optimal policy for a given reward function is identified while in the outer problem, a measure of fit is maximized. Several approaches have been proposed to alleviate the computational burden of this nested-loop structure, but these methods still suffer from high complexity when the state space is either discrete with large cardinality or continuous in high dimensions. Other approaches in the inverse reinforcement learning (IRL) literature emphasize policy estimation at the expense of reduced reward estimation accuracy. In this paper we propose a single-loop estimation algorithm with finite time guarantees that is equipped to deal with high-dimensional state spaces without compromising reward estimation accuracy. In the proposed algorithm, each policy improvement step is followed by a stochastic gradient step for likelihood maximization. We show that the proposed algorithm converges to a stationary solution with a finite-time guarantee. Further, if the reward is parameterized linearly, we show that the algorithm approximates the maximum likelihood estimator sublinearly. Finally, by using robotics control problems in MuJoCo and their transfer settings, we show that the proposed algorithm achieves superior performance compared with other IRL and imitation learning benchmarks."
http://arxiv.org/abs/2210.01392v5,Collaborative knowledge exchange promotes innovation,2022-10-04 05:50:58+00:00,"['Tomoya Mori', 'Jonathan Newton', 'Shosei Sakaguchi']",econ.GN,"Considering collaborative patent development, we provide micro-level evidence for innovation through exchanges of differentiated knowledge. Knowledge embodied in a patent is proxied by word pairs appearing in its abstract, while novelty is measured by the frequency with which these word pairs have appeared in past patents. Inventors are assumed to possess the knowledge associated with patents in which they have previously participated. We find that collaboration by inventors with more mutually differentiated knowledge sets is likely to result in patents with higher novelty."
http://arxiv.org/abs/2210.02504v2,Bikeability and the induced demand for cycling,2022-10-05 18:42:21+00:00,"['Mogens Fosgerau', 'Miroslawa Lukawska', 'Mads Paulsen', 'Thomas Kjær Rasmussen']",econ.EM,"To what extent is the volume of urban bicycle traffic affected by the provision of bicycle infrastructure? In this study, we exploit a large dataset of observed bicycle trajectories in combination with a fine-grained representation of the Copenhagen bicycle-relevant network. We apply a novel model for bicyclists' choice of route from origin to destination that takes the complete network into account. This enables us to determine bicyclists' preferences for a range of infrastructure and land-use types. We use the estimated preferences to compute a subjective cost of bicycle travel, which we correlate with the number of bicycle trips across a large number of origin-destination pairs. Simulations suggest that the extensive Copenhagen bicycle lane network has caused the number of bicycle trips and the bicycle kilometers traveled to increase by 60% and 90%, respectively, compared with a counterfactual without the bicycle lane network. This translates into an annual benefit of EUR 0.4M per km of bicycle lane owing to changes in subjective travel cost, health, and accidents. Our results thus strongly support the provision of bicycle infrastructure."
http://arxiv.org/abs/2210.01846v3,Shock propagation from the Russia-Ukraine conflict on international multilayer food production network determines global food availability,2022-10-04 18:28:58+00:00,"['Moritz Laber', 'Peter Klimek', 'Martin Bruckner', 'Liuhuaying Yang', 'Stefan Thurner']",econ.GN,"Dependencies in the global food production network can lead to shortages in numerous regions, as demonstrated by the impacts of the Russia-Ukraine conflict on global food supplies. Here, we reveal the losses of $125$ food products after a localized shock to agricultural production in $192$ countries and territories using a multilayer network model of trade (direct) and conversion of food products (indirect), thereby quantifying $10^8$ shock transmissions. We find that a complete agricultural production loss in Ukraine has heterogeneous impacts on other countries, causing relative losses of up to $89\%$ in sunflower oil and $85\%$ in maize via direct effects, and up to $25\%$ in poultry meat via indirect impacts. Whilst previous studies often treated products in isolation and did not account for product conversion during production, our model studies the global propagation of local supply shocks along both production and trade relations, allowing comparison of different response strategies."
http://arxiv.org/abs/2210.07152v1,"Smooth Calibration, Leaky Forecasts, Finite Recall, and Nash Dynamics",2022-10-13 16:34:55+00:00,"['Dean P. Foster', 'Sergiu Hart']",econ.TH,"We propose to smooth out the calibration score, which measures how good a forecaster is, by combining nearby forecasts. While regular calibration can be guaranteed only by randomized forecasting procedures, we show that smooth calibration can be guaranteed by deterministic procedures. As a consequence, it does not matter if the forecasts are leaked, i.e., made known in advance: smooth calibration can nevertheless be guaranteed (while regular calibration cannot). Moreover, our procedure has finite recall, is stationary, and all forecasts lie on a finite grid. To construct the procedure, we deal also with the related setups of online linear regression and weak calibration. Finally, we show that smooth calibration yields uncoupled finite-memory dynamics in n-person games ""smooth calibrated learning"" in which the players play approximate  Nash equilibria in almost all periods (by contrast, calibrated learning, which uses regular calibration, yields only that the time-averages of play are approximate correlated equilibria)."
http://arxiv.org/abs/2212.03092v3,Can Machine Learning discover the determining factors in participation in insurance schemes? A comparative analysis,2022-12-06 16:02:09+00:00,"['Luigi Biagini', 'Simone Severini']",econ.GN,"Identifying factors that affect participation is key to a successful insurance scheme. This study's challenges involve using many factors that could affect insurance participation to make a better forecast.Huge numbers of factors affect participation, making evaluation difficult. These interrelated factors can mask the influence on adhesion predictions, making them misleading.This study evaluated how 66 common characteristics affect insurance participation choices. We relied on individual farm data from FADN from 2016 to 2019 with type 1 (Fieldcrops) farming with 10,926 observations.We use three Machine Learning (ML) approaches (LASSO, Boosting, Random Forest) compare them to the GLM model used in insurance modelling. ML methodologies can use a large set of information efficiently by performing the variable selection. A highly accurate parsimonious model helps us understand the factors affecting insurance participation and design better products.ML predicts fairly well despite the complexity of insurance participation problem. Our results suggest Boosting performs better than the other two ML tools using a smaller set of regressors. The proposed ML tools identify which variables explain participation choice. This information includes the number of cases in which single variables are selected and their relative importance in affecting participation.Focusing on the subset of information that best explains insurance participation could reduce the cost of designing insurance schemes."
http://arxiv.org/abs/2212.02585v1,Identification of Unobservables in Observations,2022-12-05 20:24:19+00:00,['Yingyao Hu'],econ.EM,"In empirical studies, the data usually don't include all the variables of interest in an economic model. This paper shows the identification of unobserved variables in observations at the population level. When the observables are distinct in each observation, there exists a function mapping from the observables to the unobservables. Such a function guarantees the uniqueness of the latent value in each observation. The key lies in the identification of the joint distribution of observables and unobservables from the distribution of observables. The joint distribution of observables and unobservables then reveal the latent value in each observation. Three examples of this result are discussed."
http://arxiv.org/abs/2211.13100v7,"Leverage, Endogenous Unbalanced Growth, and Asset Price Bubbles",2022-11-23 16:40:09+00:00,"['Tomohiro Hirano', 'Ryo Jinnai', 'Alexis Akira Toda']",econ.TH,"We present a general equilibrium macro-finance model with a positive feedback loop between capital investment and land price. As leverage is relaxed beyond a critical value, through the financial accelerator, a phase transition occurs from balanced growth where land prices reflect fundamentals (present value of rents) to unbalanced growth where land prices grow faster than rents, generating land price bubbles. Unbalanced growth dynamics and bubbles are associated with financial loosening and technological progress. In an analytically tractable two-sector large open economy model with unique equilibria, financial loosening simultaneously leads to low interest rates, asset overvaluation, and top-end wealth concentration."
http://arxiv.org/abs/2212.05554v1,Robust Inference in High Dimensional Linear Model with Cluster Dependence,2022-12-11 17:36:05+00:00,['Ng Cheuk Fai'],econ.EM,"Cluster standard error (Liang and Zeger, 1986) is widely used by empirical researchers to account for cluster dependence in linear model. It is well known that this standard error is biased. We show that the bias does not vanish under high dimensional asymptotics by revisiting Chesher and Jewitt (1987)'s approach. An alternative leave-cluster-out crossfit (LCOC) estimator that is unbiased, consistent and robust to cluster dependence is provided under high dimensional setting introduced by Cattaneo, Jansson and Newey (2018). Since LCOC estimator nests the leave-one-out crossfit estimator of Kline, Saggio and Solvsten (2019), the two papers are unified. Monte Carlo comparisons are provided to give insights on its finite sample properties. The LCOC estimator is then applied to Angrist and Lavy's (2009) study of the effects of high school achievement award and Donohue III and Levitt's (2001) study of the impact of abortion on crime."
http://arxiv.org/abs/2212.05841v1,Dominant Drivers of National Inflation,2022-12-12 11:58:21+00:00,"['Jan Ditzen', 'Francesco Ravazzolo']",econ.EM,"For western economies a long-forgotten phenomenon is on the horizon: rising inflation rates. We propose a novel approach christened D2ML to identify drivers of national inflation. D2ML combines machine learning for model selection with time dependent data and graphical models to estimate the inverse of the covariance matrix, which is then used to identify dominant drivers. Using a dataset of 33 countries, we find that the US inflation rate and oil prices are dominant drivers of national inflation rates. For a more general framework, we carry out Monte Carlo simulations to show that our estimator correctly identifies dominant drivers."
http://arxiv.org/abs/2212.06080v7,Logs with zeros? Some problems and solutions,2022-12-12 17:56:15+00:00,"['Jiafeng Chen', 'Jonathan Roth']",econ.EM,"When studying an outcome $Y$ that is weakly-positive but can equal zero (e.g. earnings), researchers frequently estimate an average treatment effect (ATE) for a ""log-like"" transformation that behaves like $\log(Y)$ for large $Y$ but is defined at zero (e.g. $\log(1+Y)$, $\mathrm{arcsinh}(Y)$). We argue that ATEs for log-like transformations should not be interpreted as approximating percentage effects, since unlike a percentage, they depend on the units of the outcome. In fact, we show that if the treatment affects the extensive margin, one can obtain a treatment effect of any magnitude simply by re-scaling the units of $Y$ before taking the log-like transformation. This arbitrary unit-dependence arises because an individual-level percentage effect is not well-defined for individuals whose outcome changes from zero to non-zero when receiving treatment, and the units of the outcome implicitly determine how much weight the ATE for a log-like transformation places on the extensive margin. We further establish a trilemma: when the outcome can equal zero, there is no treatment effect parameter that is an average of individual-level treatment effects, unit-invariant, and point-identified. We discuss several alternative approaches that may be sensible in settings with an intensive and extensive margin, including (i) expressing the ATE in levels as a percentage (e.g. using Poisson regression), (ii) explicitly calibrating the value placed on the intensive and extensive margins, and (iii) estimating separate effects for the two margins (e.g. using Lee bounds). We illustrate these approaches in three empirical applications."
http://arxiv.org/abs/2211.06046v2,Are Large Traders Harmed by Front-running HFTs?,2022-11-11 07:52:42+00:00,"['Ziyi Xu', 'Xue Cheng']",q-fin.TR,"This paper studies the influences of a high-frequency trader (HFT) on a large trader whose future trading is predicted by the former. We conclude that HFT always front-runs and the large trader is benefited when: (1) there is sufficient high-speed noise trading; (2) HFT's prediction is vague enough. Besides, we find surprisingly that (1) making HFT's prediction less accurate might decrease large trader's profit; (2) when there is little high-speed noise trading, although HFT nearly does nothing, the large trader is still hurt."
http://arxiv.org/abs/2211.03244v1,Arbitrage from a Bayesian's Perspective,2022-11-07 00:33:28+00:00,['Ayan Bhattacharya'],econ.TH,"This paper builds a model of interactive belief hierarchies to derive the conditions under which judging an arbitrage opportunity requires Bayesian market participants to exercise their higher-order beliefs. As a Bayesian, an agent must carry a complete recursion of priors over the uncertainty about future asset payouts, the strategies employed by other market participants that are aggregated in the price, other market participants' beliefs about the agent's strategy, other market participants beliefs about what the agent believes their strategies to be, and so on ad infinitum. Defining this infinite recursion of priors -- the belief hierarchy so to speak -- along with how they update gives the Bayesian decision problem equivalent to the standard asset pricing formulation of the question. The main results of the paper show that an arbitrage trade arises only when an agent updates his recursion of priors about the strategies and beliefs employed by other market participants. The paper thus connects the foundations of finance to the foundations of game theory by identifying a bridge from market arbitrage to market participant belief hierarchies."
http://arxiv.org/abs/2212.04814v2,The Falsification Adaptive Set in Linear Models with Instrumental Variables that Violate the Exclusion or Conditional Exogeneity Restriction,2022-12-09 12:42:17+00:00,"['Nicolas Apfel', 'Frank Windmeijer']",econ.EM,"Masten and Poirier (2021) introduced the falsification adaptive set (FAS) in linear models with a single endogenous variable estimated with multiple correlated instrumental variables (IVs). The FAS reflects the model uncertainty that arises from falsification of the baseline model. We show that it applies to cases where a conditional exogeneity assumption holds and invalid instruments violate the exclusion assumption only. We propose a generalized FAS that reflects the model uncertainty when some instruments violate the exclusion assumption and/or some instruments violate the conditional exogeneity assumption. Under the assumption that invalid instruments are not themselves endogenous explanatory variables, if there is at least one relevant instrument that satisfies both the exclusion and conditional exogeneity assumptions then this generalized FAS is guaranteed to contain the parameter of interest."
http://arxiv.org/abs/2212.05281v1,Exploring non-residential technology adoption: an empirical analysis of factors associated with the adoption of photovoltaic systems by municipal authorities in Germany,2022-12-10 11:49:33+00:00,"['Maren Springsklee', 'Fabian Scheller']",econ.GN,"This research article explores potential influencing factors of solar photovoltaic (PV) system adoption by municipal authorities in Germany in the year 2019. We derive seven hypothesized relationships from the empirical literature on residential PV adoption, organizational technology adoption, and sustainability policy adoption by local governments, and apply a twofold empirical approach to examine them. First, we explore the associations of a set of explanatory variables on the installed capacity of adopter municipalities (N=223) in an OLS model. Second, we use a logit model to analyze whether the identified relationships are also apparent between adopter and non-adopter municipalities (N=423). Our findings suggest that fiscal capacity (measured by per capita debt and per capita tax revenue) and peer effects (measured by the pre-existing installed capacity) are positively associated with both the installed capacity and adoption. Furthermore, we find that institutional capacity (measured by the presence of a municipal utility) and environmental concern (measured by the share of green party votes) are positively associated with municipal PV adoption. Economic factors (measured by solar irradiation) show a significant positive but small effect in both regression models. No evidence was found to support the influence of political will. Results for the role of municipal characteristics are mixed, although the population size was consistently positively associated with municipal PV adoption and installed capacity. Our results support previous studies on PV system adoption determinants and offer a starting point for additional research on non-residential decision-making and PV adoption."
http://arxiv.org/abs/2212.04043v1,Optimal Model Selection in RDD and Related Settings Using Placebo Zones,2022-12-08 02:44:59+00:00,"['Nathan Kettlewell', 'Peter Siminski']",econ.EM,"We propose a new model-selection algorithm for Regression Discontinuity Design, Regression Kink Design, and related IV estimators. Candidate models are assessed within a 'placebo zone' of the running variable, where the true effects are known to be zero. The approach yields an optimal combination of bandwidth, polynomial, and any other choice parameters. It can also inform choices between classes of models (e.g. RDD versus cohort-IV) and any other choices, such as covariates, kernel, or other weights. We outline sufficient conditions under which the approach is asymptotically optimal. The approach also performs favorably under more general conditions in a series of Monte Carlo simulations. We demonstrate the approach in an evaluation of changes to Minimum Supervised Driving Hours in the Australian state of New South Wales. We also re-evaluate evidence on the effects of Head Start and Minimum Legal Drinking Age. Our Stata commands implement the procedure and compare its performance to other approaches."
http://arxiv.org/abs/2212.04203v2,A Characterization of Maximum Nash Welfare for Indivisible Goods,2022-12-08 11:39:22+00:00,['Warut Suksompong'],econ.TH,"In the allocation of indivisible goods, the maximum Nash welfare (MNW) rule, which chooses an allocation maximizing the product of the agents' utilities, has received substantial attention for its fairness. We characterize MNW as the only additive welfarist rule that satisfies envy-freeness up to one good. Our characterization holds even in the simplest setting of two agents."
http://arxiv.org/abs/2212.04525v1,Monetary Uncertainty as a Determinant of the Response of Stock Market to Macroeconomic News,2022-12-08 19:16:20+00:00,['Mykola Pinchuk'],q-fin.PR,"This paper examines the effect of macroeconomic news announcements (MNA) on the stock market. Stocks exhibit a strong positive response to major MNA: 1 standard deviation of MNA surprise causes 11-25 bps higher returns. This response is highly time-varying and is weaker during periods of high monetary uncertainty. I decompose this response into cash flow and risk-free rate channels. 1 standard deviation of good MNA surprise leads to plus 30 bps returns from the cash flow channel and minus 23 bps per 1\% of monetary uncertainty from the risk-free rate channel. Risk-free rate channel is time-varying and is stronger when monetary uncertainty is high. High levels of monetary uncertainty mask the strong positive response of stocks to MNA, which explains why past research failed to detect this relation."
http://arxiv.org/abs/2212.04277v1,Lie detection algorithms attract few users but vastly increase accusation rates,2022-12-08 14:07:21+00:00,"['Alicia von Schenk', 'Victor Klockmann', 'Jean-François Bonnefon', 'Iyad Rahwan', 'Nils Köbis']",econ.GN,"People are not very good at detecting lies, which may explain why they refrain from accusing others of lying, given the social costs attached to false accusations - both for the accuser and the accused. Here we consider how this social balance might be disrupted by the availability of lie-detection algorithms powered by Artificial Intelligence. Will people elect to use lie detection algorithms that perform better than humans, and if so, will they show less restraint in their accusations? We built a machine learning classifier whose accuracy (67\%) was significantly better than human accuracy (50\%) in a lie-detection task and conducted an incentivized lie-detection experiment in which we measured participants' propensity to use the algorithm, as well as the impact of that use on accusation rates. We find that the few people (33\%) who elect to use the algorithm drastically increase their accusation rates (from 25\% in the baseline condition up to 86% when the algorithm flags a statement as a lie). They make more false accusations (18pp increase), but at the same time, the probability of a lie remaining undetected is much lower in this group (36pp decrease). We consider individual motivations for using lie detection algorithms and the social implications of these algorithms."
http://arxiv.org/abs/2211.11876v1,Structural Modelling of Dynamic Networks and Identifying Maximum Likelihood,2022-11-21 22:00:23+00:00,"['Christian Gourieroux', 'Joann Jasiak']",econ.EM,"This paper considers nonlinear dynamic models where the main parameter of interest is a nonnegative matrix characterizing the network (contagion) effects. This network matrix is usually constrained either by assuming a limited number of nonzero elements (sparsity), or by considering a reduced rank approach for nonnegative matrix factorization (NMF). We follow the latter approach and develop a new probabilistic NMF method. We introduce a new Identifying Maximum Likelihood (IML) method for consistent estimation of the identified set of admissible NMF's and derive its asymptotic distribution. Moreover, we propose a maximum likelihood estimator of the parameter matrix for a given non-negative rank, derive its asymptotic distribution and the associated efficiency bound."
http://arxiv.org/abs/2211.14236v4,Strategyproof Decision-Making in Panel Data Settings and Beyond,2022-11-25 16:56:42+00:00,"['Keegan Harris', 'Anish Agarwal', 'Chara Podimata', 'Zhiwei Steven Wu']",econ.EM,"We consider the problem of decision-making using panel data, in which a decision-maker gets noisy, repeated measurements of multiple units (or agents). We consider a setup where there is a pre-intervention period, when the principal observes the outcomes of each unit, after which the principal uses these observations to assign a treatment to each unit. Unlike this classical setting, we permit the units generating the panel data to be strategic, i.e. units may modify their pre-intervention outcomes in order to receive a more desirable intervention. The principal's goal is to design a strategyproof intervention policy, i.e. a policy that assigns units to their utility-maximizing interventions despite their potential strategizing. We first identify a necessary and sufficient condition under which a strategyproof intervention policy exists, and provide a strategyproof mechanism with a simple closed form when one does exist. Along the way, we prove impossibility results for strategic multiclass classification, which may be of independent interest. When there are two interventions, we establish that there always exists a strategyproof mechanism, and provide an algorithm for learning such a mechanism. For three or more interventions, we provide an algorithm for learning a strategyproof mechanism if there exists a sufficiently large gap in the principal's rewards between different interventions. Finally, we empirically evaluate our model using real-world panel data collected from product sales over 18 months. We find that our methods compare favorably to baselines which do not take strategic interactions into consideration, even in the presence of model misspecification."
http://arxiv.org/abs/2211.15241v1,Synthetic Principal Component Design: Fast Covariate Balancing with Synthetic Controls,2022-11-28 11:45:54+00:00,"['Yiping Lu', 'Jiajin Li', 'Lexing Ying', 'Jose Blanchet']",econ.EM,"The optimal design of experiments typically involves solving an NP-hard combinatorial optimization problem. In this paper, we aim to develop a globally convergent and practically efficient optimization algorithm. Specifically, we consider a setting where the pre-treatment outcome data is available and the synthetic control estimator is invoked. The average treatment effect is estimated via the difference between the weighted average outcomes of the treated and control units, where the weights are learned from the observed data. {Under this setting, we surprisingly observed that the optimal experimental design problem could be reduced to a so-called \textit{phase synchronization} problem.} We solve this problem via a normalized variant of the generalized power method with spectral initialization. On the theoretical side, we establish the first global optimality guarantee for experiment design when pre-treatment data is sampled from certain data-generating processes. Empirically, we conduct extensive experiments to demonstrate the effectiveness of our method on both the US Bureau of Labor Statistics and the Abadie-Diemond-Hainmueller California Smoking Data. In terms of the root mean square error, our algorithm surpasses the random design by a large margin."
http://arxiv.org/abs/2212.00101v1,mCube: Multinomial Micro-level reserving Model,2022-11-30 20:17:48+00:00,"['Emmanuel Jordy Menvouta', 'Jolien Ponnet', 'Robin Van Oirbeek', 'Tim Verdonck']",stat.AP,"This paper presents a multinomial multi-state micro-level reserving model, denoted mCube. We propose a unified framework for modelling the time and the payment process for IBNR and RBNS claims and for modeling IBNR claim counts. We use multinomial distributions for the time process and spliced mixture models for the payment process. We illustrate the excellent performance of the proposed model on a real data set of a major insurance company consisting of bodily injury claims. It is shown that the proposed model produces a best estimate distribution that is centered around the true reserve."
http://arxiv.org/abs/2211.16641v3,Predicting China's CPI by Scanner Big Data,2022-11-30 00:11:31+00:00,"['Zhenkun Zhou', 'Zikun Song', 'Tao Ren']",econ.GN,"Scanner big data has potential to construct Consumer Price Index (CPI). This work utilizes the scanner data of supermarket retail sales, which are provided by China Ant Business Alliance (CAA), to construct the Scanner-data Food Consumer Price Index (S-FCPI) in China, and the index reliability is verified by other macro indicators, especially by China's CPI. And not only that, we build multiple machine learning models based on S-FCPI to quantitatively predict the CPI growth rate in months, and qualitatively predict those directions and levels. The prediction models achieve much better performance than the traditional time series models in existing research. This work paves the way to construct and predict price indexes through using scanner big data in China. S-FCPI can not only reflect the changes of goods prices in higher frequency and wider geographic dimension than CPI, but also provide a new perspective for monitoring macroeconomic operation, predicting inflation and understanding other economic issues, which is beneficial supplement to China's CPI."
http://arxiv.org/abs/2211.11322v1,Influence of Economic Decoupling in assessing carbon budget quotas for the European Union,2022-11-21 10:08:26+00:00,"['Ilaria Perissi', 'Aled Jones']",econ.GN,"In the present study, for the first time, an effort sharing approach based on Inertia and Capability principles is proposed to assess European Union (EU27) carbon budget distribution among the Member States. This is done within the context of achieving the Green Deal objective and EU27 carbon neutrality by 2050. An in-depth analysis is carried out about the role of Economic Decoupling embedded in the Capability principle to evaluate the correlation between the expected increase of economic production and the level of carbon intensity in the Member States. As decarbonization is a dynamic process, the study proposes a simple mathematical model as a policy tool to assess and redistribute Member States carbon budgets as frequently as necessary to encourage progress or overcome the difficulties each Member State may face during the decarbonization pathways."
http://arxiv.org/abs/2212.01553v2,"Short-term shock, long-lasting payment: Evidence from the Lushan Earthquake",2022-12-03 06:52:01+00:00,['Yujue Wang'],econ.GN,"Abrupt catastrophic events bring business risks into firms. The paper introduces the Great Lushan Earthquake in 2013 in China as an unexpected shock to explore the causal effects on public firms in both the long and short term. DID-PSM methods are conducted to examine the robustness of causal inference. The identifications and estimations indicate that catastrophic shock significantly negatively impacts cash flow liquidity and profitability in the short term. Besides, the practical influences on firms' manufacturing and operation emerge in the treated group. Firms increase non-business expenditures and retained earnings as a financial measure to resist series risk during the shock period. As the long-term payment, the decline in production factors, particularly in employment level and the loss in fixed assets, are permanent. The earthquake's comprehensive interactions are also reflected. The recovery from the disaster would benefit the companies by raising the growth rate of R\&D and enhancing competitiveness through increasing market share, though these effects are temporary. PSM-DID and event study methods are implemented to investigate the general effects of specific strong earthquakes on local public firms nationwide. Consistent with the Lushan Earthquake, the ratio of cash flow to sales dropped drastically and recovered in 3 subsequent semesters. The shock on sales was transitory, only in the current semester."
http://arxiv.org/abs/2212.00640v1,3 Lessons from Hyperinflationary Periods,2022-12-01 16:40:55+00:00,"['Mark Bergen', 'Thomas Bergen', 'Daniel Levy', 'Rose Semenov']",econ.GN,"Inflation is painful, for firms, customers, employees, and society. But careful study of periods of hyperinflation point to ways that firms can adapt. In particular, companies need to think about how to change prices regularly and cheaply, because constant price changes can ultimately be very, very expensive. And they should consider how to communicate those price changes to customers. Providing clarity and predictability can increase consumer trust and help firms in the long run."
http://arxiv.org/abs/2210.14388v2,Revealed Preferences of One-Sided Matching,2022-10-25 23:35:15+00:00,['Andrew Tai'],econ.TH,"Consider the object allocation (one-sided matching) model of Shapley and Scarf (1974). When final allocations are observed but agents' preferences are unknown, when might the allocation be in the core? This is a one-sided analogue of the model in Echenique, Lee, Shum, and Yenmez (2013). I build a model in which the strict core is testable -- an allocation is ""rationalizable"" if there is a preference profile putting it in the core. In this manner, I develop a theory of the revealed preferences of one-sided matching. I study rationalizability in both non-transferrable and transferrable utility settings. In the non-transferrable utility setting, an allocation is rationalizable if and only if: whenever agents with the same preferences are in the same potential trading cycle, they receive the same allocation. In the transferrable utility setting, an allocation is rationalizable if and only if: there exists a price vector supporting the allocation as a competitive equilibrium; or equivalently, it satisfies a cyclic monotonicity condition. The proofs leverage simple graph theory and combinatorial optimization and tie together classic theories of consumer demand revealed preferences and competitive equilibrium."
http://arxiv.org/abs/2210.13843v2,GLS under Monotone Heteroskedasticity,2022-10-25 09:04:54+00:00,"['Yoichi Arai', 'Taisuke Otsu', 'Mengshan Xu']",econ.EM,"The generalized least square (GLS) is one of the most basic tools in regression analyses. A major issue in implementing the GLS is estimation of the conditional variance function of the error term, which typically requires a restrictive functional form assumption for parametric estimation or smoothing parameters for nonparametric estimation. In this paper, we propose an alternative approach to estimate the conditional variance function under nonparametric monotonicity constraints by utilizing the isotonic regression method. Our GLS estimator is shown to be asymptotically equivalent to the infeasible GLS estimator with knowledge of the conditional error variance, and involves only some tuning to trim boundary observations, not only for point estimation but also for interval estimation or hypothesis testing. Our analysis extends the scope of the isotonic regression method by showing that the isotonic estimates, possibly with generated variables, can be employed as first stage estimates to be plugged in for semiparametric objects. Simulation studies illustrate excellent finite sample performances of the proposed method. As an empirical example, we revisit Acemoglu and Restrepo's (2017) study on the relationship between an aging population and economic growth to illustrate how our GLS estimator effectively reduces estimation errors."
http://arxiv.org/abs/2211.12475v1,The impact of moving expenses on social segregation: a simulation with RL and ABM,2022-11-22 18:40:41+00:00,['Xinyu Li'],econ.GN,"Over the past decades, breakthroughs such as Reinforcement Learning (RL) and Agent-based modeling (ABM) have made simulations of economic models feasible. Recently, there has been increasing interest in applying ABM to study the impact of residential preferences on neighborhood segregation in the Schelling Segregation Model. In this paper, RL is combined with ABM to simulate a modified Schelling Segregation model, which incorporates moving expenses as an input parameter. In particular, deep Q network (DQN) is adopted as RL agents' learning algorithm to simulate the behaviors of households and their preferences. This paper studies the impact of moving expenses on the overall segregation pattern and its role in social integration. A more comprehensive simulation of the segregation model is built for policymakers to forecast the potential consequences of their policies."
http://arxiv.org/abs/2211.12669v1,Revenue Comparisons of Auctions with Ambiguity Averse Sellers,2022-11-23 02:44:32+00:00,"['Sosung Baik', 'Sung-Ha Hwang']",econ.TH,"We study the revenue comparison problem of auctions when the seller has a maxmin expected utility preference. The seller holds a set of priors around some reference belief, interpreted as an approximating model of the true probability law or the focal point distribution. We develop a methodology for comparing the revenue performances of auctions: the seller prefers auction X to auction Y if their transfer functions satisfy a weak form of the single-crossing condition. Intuitively, this condition means that a bidder's payment is more negatively associated with the competitor's type in X than in Y. Applying this methodology, we show that when the reference belief is independent and identically distributed (IID) and the bidders are ambiguity neutral, (i) the first-price auction outperforms the second-price and all-pay auctions, and (ii) the second-price and all-pay auctions outperform the war of attrition. Our methodology yields results opposite to those of the Linkage Principle."
http://arxiv.org/abs/2211.07416v1,Collective models and the marriage market,2022-11-14 14:42:31+00:00,['Simon Weber'],econ.GN,"In this paper, I develop an integrated approach to collective models and matching models of the marriage market. In the collective framework, both household formation and the intra-household allocation of bargaining power are taken as given. This is no longer the case in the present contribution, where both are endogenous to the determination of equilibrium on the marriage market. I characterize a class of ""proper"" collective models which can be embedded into a general matching framework with imperfectly transferable utility. In such models, the bargaining sets are parametrized by an analytical device called distance function, which plays a key role both for writing down the usual stability conditions and for estimation. In general, however, distance functions are not known in closed-form. I provide an efficient method for computing distance functions, that works even with the most complex collective models. Finally, I provide a fully-fledged application using PSID data. I identify the sharing rule and its distribution and study the evolution of the sharing rule and housework time sharing in the United States since 1969. In a counterfactual experiment, I simulate the impact of closing the gender wage gap."
http://arxiv.org/abs/2211.12004v1,Contextual Bandits in a Survey Experiment on Charitable Giving: Within-Experiment Outcomes versus Policy Learning,2022-11-22 04:44:17+00:00,"['Susan Athey', 'Undral Byambadalai', 'Vitor Hadad', 'Sanath Kumar Krishnamurthy', 'Weiwen Leung', 'Joseph Jay Williams']",econ.EM,"We design and implement an adaptive experiment (a ``contextual bandit'') to learn a targeted treatment assignment policy, where the goal is to use a participant's survey responses to determine which charity to expose them to in a donation solicitation. The design balances two competing objectives: optimizing the outcomes for the subjects in the experiment (``cumulative regret minimization'') and gathering data that will be most useful for policy learning, that is, for learning an assignment rule that will maximize welfare if used after the experiment (``simple regret minimization''). We evaluate alternative experimental designs by collecting pilot data and then conducting a simulation study. Next, we implement our selected algorithm. Finally, we perform a second simulation study anchored to the collected data that evaluates the benefits of the algorithm we chose. Our first result is that the value of a learned policy in this setting is higher when data is collected via a uniform randomization rather than collected adaptively using standard cumulative regret minimization or policy learning algorithms. We propose a simple heuristic for adaptive experimentation that improves upon uniform randomization from the perspective of policy learning at the expense of increasing cumulative regret relative to alternative bandit algorithms. The heuristic modifies an existing contextual bandit algorithm by (i) imposing a lower bound on assignment probabilities that decay slowly so that no arm is discarded too quickly, and (ii) after adaptively collecting data, restricting policy learning to select from arms where sufficient data has been gathered."
http://arxiv.org/abs/2211.01116v2,Medical Bill Shock and Imperfect Moral Hazard,2022-11-02 13:45:50+00:00,"['Alex Hoagland', 'David M. Anderson', 'Ed Zhu']",econ.GN,"Consumers are sensitive to medical prices when consuming care, but delays in price information may distort moral hazard. We study how medical bills affect household spillover spending following utilization, leveraging variation in insurer claim processing times. Households increase spending by 22\% after a scheduled service, but then reduce spending by 11\% after the bill arrives. Observed bill effects are consistent with resolving price uncertainty; bill effects are strongest when pricing information is particularly salient. A model of demand for healthcare with delayed pricing information suggests households misperceive pricing signals prior to bills, and that correcting these perceptions reduce average (median) spending by 16\% (7\%) annually."
http://arxiv.org/abs/2211.02215v2,Boosted p-Values for High-Dimensional Vector Autoregression,2022-11-04 01:52:55+00:00,['Xiao Huang'],econ.EM,"Assessing the statistical significance of parameter estimates is an important step in high-dimensional vector autoregression modeling. Using the least-squares boosting method, we compute the p-value for each selected parameter at every boosting step in a linear model. The p-values are asymptotically valid and also adapt to the iterative nature of the boosting procedure. Our simulation experiment shows that the p-values can keep false positive rate under control in high-dimensional vector autoregressions. In an application with more than 100 macroeconomic time series, we further show that the p-values can not only select a sparser model with good prediction performance but also help control model stability. A companion R package boostvar is developed."
http://arxiv.org/abs/2211.01575v1,Are Synthetic Control Weights Balancing Score?,2022-11-03 03:52:28+00:00,['Harsh Parikh'],stat.ME,"In this short note, I outline conditions under which conditioning on Synthetic Control (SC) weights emulates a randomized control trial where the treatment status is independent of potential outcomes. Specifically, I demonstrate that if there exist SC weights such that (i) the treatment effects are exactly identified and (ii) these weights are uniformly and cumulatively bounded, then SC weights are balancing scores."
http://arxiv.org/abs/2211.02742v1,The debt aversion survey module: An experimentally validated tool to measure individual debt aversion,2022-11-04 20:36:58+00:00,"['David Albrecht', 'Thomas Meissner']",econ.GN,"We develop an experimentally validated, short and easy-to-use survey module for measuring individual debt aversion. To this end, we first estimate debt aversion on an individual level, using choice data from Meissner and Albrecht (2022). This data also contains responses to a large set of debt aversion survey items, consisting of existing items from the literature and novel items developed for this study. Out of these, we identify a survey module comprising two qualitative survey items to best predict debt aversion in the incentivized experiment."
http://arxiv.org/abs/2211.02441v1,Computing Economic Chaos,2022-11-04 13:29:33+00:00,"['Richard H. Day', 'Oleg V. Pavlov']",econ.GN,"Existence theory in economics is usually in real domains such as the findings of chaotic trajectories in models of economic growth, tatonnement, or overlapping generations models. Computational examples, however, sometimes converge rapidly to cyclic orbits when in theory they should be nonperiodic almost surely. We explain this anomaly as the result of digital approximation and conclude that both theoretical and numerical behavior can still illuminate essential features of the real data."
http://arxiv.org/abs/2210.15181v3,Optimal Mechanism Design for Agents with DSL Strategies: The Case of Sybil Attacks in Combinatorial Auctions,2022-10-27 05:18:00+00:00,"['Yotam Gafni', 'Moshe Tennenholtz']",econ.TH,"In robust decision making under uncertainty, a natural choice is to go with safety (aka security) level strategies. However, in many important cases, most notably auctions, there is a large multitude of safety level strategies, thus making the choice unclear.  We consider two refined notions: 
  (i) a term we call DSL (distinguishable safety level), and is based on the notion of ``discrimin'', which uses a pairwise comparison of actions while removing trivial equivalencies. This captures the fact that when comparing two actions an agent should not care about payoffs in situations where they lead to identical payoffs. 
  (ii) The well-known Leximin notion from social choice theory, which we apply for robust decision-making. In particular, the leximin is always DSL but not vice-versa. 
  We study the relations of these notions to other robust notions, and illustrate the results of their use in auctions and other settings. Economic design aims to maximize social welfare when facing self-motivated participants. In online environments, such as the Web, participants' incentives take a novel form originating from the lack of clear agent identity -- the ability to create Sybil attacks, i.e., the ability of each participant to act using multiple identities. It is well-known that Sybil attacks are a major obstacle for welfare-maximization.  Our main result proves that when DSL attackers face uncertainty over the auction's bids, the celebrated VCG mechanism is welfare-maximizing even under Sybil attacks. Altogether, our work shows a successful fundamental synergy between robustness under uncertainty, economic design, and agents' strategic manipulations in online multi-agent systems."
http://arxiv.org/abs/2210.16042v1,Eigenvalue tests for the number of latent factors in short panels,2022-10-28 10:24:52+00:00,"['Alain-Philippe Fortin', 'Patrick Gagliardini', 'Olivier Scaillet']",econ.EM,"This paper studies new tests for the number of latent factors in a large cross-sectional factor model with small time dimension. These tests are based on the eigenvalues of variance-covariance matrices of (possibly weighted) asset returns, and rely on either the assumption of spherical errors, or instrumental variables for factor betas. We establish the asymptotic distributional results using expansion theorems based on perturbation theory for symmetric matrices. Our framework accommodates semi-strong factors in the systematic components. We propose a novel statistical test for weak factors against strong or semi-strong factors. We provide an empirical application to US equity data. Evidence for a different number of latent factors according to market downturns and market upturns, is statistically ambiguous in the considered subperiods. In particular, our results contradicts the common wisdom of a single factor model in bear markets."
http://arxiv.org/abs/2211.08078v1,Relevance of financial development and fiscal stability in dealing with disasters in Emerging Economies,2022-11-15 11:58:35+00:00,"['Valeria Terrones', 'Richard S. J. Tol']",econ.GN,"Previous studies show that natural disasters decelerate economic growth, and more so in countries with lower financial development. We confirm these results with more recent data. We are the first to show that fiscal stability reduces the negative economic impact of natural disasters in poorer countries, and that catastrophe bonds have the same effect in richer countries."
http://arxiv.org/abs/2211.04752v4,Bayesian Neural Networks for Macroeconomic Analysis,2022-11-09 09:10:57+00:00,"['Niko Hauzenberger', 'Florian Huber', 'Karin Klieber', 'Massimiliano Marcellino']",econ.EM,"Macroeconomic data is characterized by a limited number of observations (small T), many time series (big K) but also by featuring temporal dependence. Neural networks, by contrast, are designed for datasets with millions of observations and covariates. In this paper, we develop Bayesian neural networks (BNNs) that are well-suited for handling datasets commonly used for macroeconomic analysis in policy institutions. Our approach avoids extensive specification searches through a novel mixture specification for the activation function that appropriately selects the form of nonlinearities. Shrinkage priors are used to prune the network and force irrelevant neurons to zero. To cope with heteroskedasticity, the BNN is augmented with a stochastic volatility model for the error term. We illustrate how the model can be used in a policy institution by first showing that our different BNNs produce precise density forecasts, typically better than those from other machine learning methods. Finally, we showcase how our model can be used to recover nonlinearities in the reaction of macroeconomic aggregates to financial shocks."
http://arxiv.org/abs/2211.08649v2,Causal Bandits: Online Decision-Making in Endogenous Settings,2022-11-16 03:51:14+00:00,"['Jingwen Zhang', 'Yifang Chen', 'Amandeep Singh']",econ.EM,"The deployment of Multi-Armed Bandits (MAB) has become commonplace in many economic applications. However, regret guarantees for even state-of-the-art linear bandit algorithms (such as Optimism in the Face of Uncertainty Linear bandit (OFUL)) make strong exogeneity assumptions w.r.t. arm covariates. This assumption is very often violated in many economic contexts and using such algorithms can lead to sub-optimal decisions. Further, in social science analysis, it is also important to understand the asymptotic distribution of estimated parameters. To this end, in this paper, we consider the problem of online learning in linear stochastic contextual bandit problems with endogenous covariates. We propose an algorithm we term $ε$-BanditIV, that uses instrumental variables to correct for this bias, and prove an $\tilde{\mathcal{O}}(k\sqrt{T})$ upper bound for the expected regret of the algorithm. Further, we demonstrate the asymptotic consistency and normality of the $ε$-BanditIV estimator. We carry out extensive Monte Carlo simulations to demonstrate the performance of our algorithms compared to other methods. We show that $ε$-BanditIV significantly outperforms other existing methods in endogenous settings. Finally, we use data from real-time bidding (RTB) system to demonstrate how $ε$-BanditIV can be used to estimate the causal impact of advertising in such settings and compare its performance with other existing methods."
http://arxiv.org/abs/2211.09591v2,Personal Privacy Protection Problems in the Digital Age,2022-11-17 15:38:32+00:00,"['Zhiheng Yi', 'Xiaoli Chen']",econ.GN,"With the development of Internet technology, the issue of privacy leakage has attracted more and more attention from the public. In our daily life, mobile phone applications and identity documents that we use may bring the risk of privacy leakage, which had increasingly aroused public concern. The path of privacy protection in the digital age remains to be explored. To explore the source of this risk and how it can be reduced, we conducted this study by using personal experience, collecting data and applying the theory."
http://arxiv.org/abs/2211.10864v1,Borrowing Constraints in Emerging Markets,2022-11-20 04:18:27+00:00,"['Santiago Camara', 'Maximo Sangiacomo']",econ.GN,"Borrowing constraints are a key component of modern international macroeconomic models. The analysis of Emerging Markets (EM) economies generally assumes collateral borrowing constraints, i.e., firms access to debt is constrained by the value of their collateralized assets. Using credit registry data from Argentina for the period 1998-2020 we show that less than 15% of firms debt is based on the value of collateralized assets, with the remaining 85% based on firms cash flows. Exploiting central bank regulations over banks capital requirements and credit policies we argue that the most prevalent borrowing constraints is defined in terms of the ratio of their interest payments to a measure of their present and past cash flows, akin to the interest coverage borrowing constraint studied by the corporate finance literature. Lastly, we argue that EMs exhibit a greater share of interest sensitive borrowing constraints than the US and other Advanced Economies. From a structural point of view, we show that in an otherwise standard small open economy DSGE model, an interest coverage borrowing constraints leads to significantly stronger amplification of foreign interest rate shocks compared to the standard collateral constraint. This greater amplification provides a solution to the Spillover Puzzle of US monetary policy rates by which EMs experience greater negative effects than Advanced Economies after a US interest rate hike. In terms of policy implications, this greater amplification leads to managed exchange rate policy being more costly in the presence of an interest coverage constraint, given their greater interest rate sensitivity, compared to the standard collateral borrowing constraint."
http://arxiv.org/abs/2211.06830v2,Two-Person Bargaining when the Disagreement Point is Private Information,2022-11-13 06:12:39+00:00,"['Eric van Damme', 'Xu Lang']",econ.TH,"We consider two-person bargaining problems in which (only) the disagreement outcome is private (and possibly correlated) information and it is common knowledge that disagreement is inefficient. We show that if the Pareto frontier is linear, the outcome of an ex post efficient mechanism cannot depend on the disagreement payoffs. If the frontier is non-linear, the result continues to hold when the disagreement payoffs are independent or there is a player with at most two types. We discuss implications of these results for axiomatic bargaining theory and for full surplus extraction in mechanism design."
http://arxiv.org/abs/2211.06887v3,Firm-worker hypergraphs,2022-11-13 12:25:54+00:00,['Chao Huang'],econ.TH,A firm-worker hypergraph consists of edges in which each edge joins a firm and its possible employees. We show that a stable matching exists in both many-to-one matching with transferable utilities and discrete many-to-one matching when the firm-worker hypergraph has no nontrivial odd-length cycle. Firms' preferences satisfying this condition arise in a problem of matching specialized firms with specialists.
http://arxiv.org/abs/2211.07362v4,Optimal Pricing Schemes in the Presence of Social Learning and Costly Reporting,2022-11-14 13:58:35+00:00,"['Kaiwei Zhang', 'Xi Weng', 'Xienan Cheng']",econ.TH,"A monopoly platform sells either a risky product (with unknown utility) or a safe product (with known utility) to agents who sequentially arrive and learn the utility of the risky product by the reporting of previous agents. It is costly for agents to report utility; hence the platform has to design both the prices and the reporting bonus to motivate the agents to explore and generate new information. By allowing sellers to set bonuses, we are essentially enabling them to dynamically control the supply of learning signals without significantly affecting the demand for the product. We characterize the optimal bonus and pricing schemes offered by the profit-maximizing platform. It turns out that the optimal scheme falls into one of four types: Full Coverage, Partial Coverage, Immediate Revelation, and Non-Bonus. In a model of exponential bandit, we find that there is a dynamical switch of the types along the learning trajectory. Although learning stops efficiently, information is revealed too slowly compared with the planner's optimal solution."
http://arxiv.org/abs/2210.09426v5,Party On: The Labor Market Returns to Social Networks in Adolescence,2022-10-17 20:43:21+00:00,"['Adriana Lleras-Muney', 'Matthew Miller', 'Shuyang Sheng', 'Veronica Sovero']",econ.EM,"We investigate the returns to adolescent friendships on earnings in adulthood using data from the National Longitudinal Study of Adolescent to Adult Health. Because both education and friendships are jointly determined in adolescence, OLS estimates of their returns are likely biased. We implement a novel procedure to obtain bounds on the causal returns to friendships: we assume that the returns to schooling range from 5 to 15% (based on prior literature), and instrument for friendships using similarity in age among peers. Having one more friend in adolescence increases earnings between 7 and 14%, substantially more than OLS estimates would suggest."
http://arxiv.org/abs/2210.10146v1,Housing Forecasts via Stock Market Indicators,2022-10-18 20:23:43+00:00,"['Varun Mittal', 'Laura P. Schaposnik']",econ.GN,"Through the reinterpretation of housing data as candlesticks, we extend Nature Scientific Reports' article by Liang and Unwin [LU22] on stock market indicators for COVID-19 data, and utilize some of the most prominent technical indicators from the stock market to estimate future changes in the housing market, comparing the findings to those one would obtain from studying real estate ETF's. By providing an analysis of MACD, RSI, and Candlestick indicators (Bullish Engulfing, Bearish Engulfing, Hanging Man, and Hammer), we exhibit their statistical significance in making predictions for USA data sets (using Zillow Housing data) and also consider their applications within three different scenarios: a stable housing market, a volatile housing market, and a saturated market. In particular, we show that bearish indicators have a much higher statistical significance then bullish indicators, and we further illustrate how in less stable or more populated countries, bearish trends are only slightly more statistically present compared to bullish trends."
http://arxiv.org/abs/2210.08147v1,A New Method for Generating Random Correlation Matrices,2022-10-15 00:30:24+00:00,"['Ilya Archakov', 'Peter Reinhard Hansen', 'Yiyao Luo']",econ.EM,"We propose a new method for generating random correlation matrices that makes it simple to control both location and dispersion. The method is based on a vector parameterization, gamma = g(C), which maps any distribution on R^d, d = n(n-1)/2 to a distribution on the space of non-singular nxn correlation matrices. Correlation matrices with certain properties, such as being well-conditioned, having block structures, and having strictly positive elements, are simple to generate. We compare the new method with existing methods."
http://arxiv.org/abs/2210.06594v1,Sample Constrained Treatment Effect Estimation,2022-10-12 21:13:47+00:00,"['Raghavendra Addanki', 'David Arbour', 'Tung Mai', 'Cameron Musco', 'Anup Rao']",cs.LG,"Treatment effect estimation is a fundamental problem in causal inference. We focus on designing efficient randomized controlled trials, to accurately estimate the effect of some treatment on a population of $n$ individuals. In particular, we study sample-constrained treatment effect estimation, where we must select a subset of $s \ll n$ individuals from the population to experiment on. This subset must be further partitioned into treatment and control groups. Algorithms for partitioning the entire population into treatment and control groups, or for choosing a single representative subset, have been well-studied. The key challenge in our setting is jointly choosing a representative subset and a partition for that set.
  We focus on both individual and average treatment effect estimation, under a linear effects model. We give provably efficient experimental designs and corresponding estimators, by identifying connections to discrepancy minimization and leverage-score-based sampling used in randomized numerical linear algebra. Our theoretical results obtain a smooth transition to known guarantees when $s$ equals the population size. We also empirically demonstrate the performance of our algorithms."
http://arxiv.org/abs/2210.12881v1,A Control Theoretic Approach to Infrastructure-Centric Blockchain Tokenomics,2022-10-23 23:23:13+00:00,"['Oguzhan Akcin', 'Robert P. Streit', 'Benjamin Oommen', 'Sriram Vishwanath', 'Sandeep Chinchali']",cs.DC,"There are a multitude of Blockchain-based physical infrastructure systems, operating on a crypto-currency enabled token economy, where infrastructure suppliers are rewarded with tokens for enabling, validating, managing and/or securing the system. However, today's token economies are largely designed without infrastructure systems in mind, and often operate with a fixed token supply (e.g., Bitcoin). This paper argues that token economies for infrastructure networks should be structured differently - they should continually incentivize new suppliers to join the network to provide services and support to the ecosystem. As such, the associated token rewards should gracefully scale with the size of the decentralized system, but should be carefully balanced with consumer demand to manage inflation and be designed to ultimately reach an equilibrium. To achieve such an equilibrium, the decentralized token economy should be adaptable and controllable so that it maximizes the total utility of all users, such as achieving stable (overall non-inflationary) token economies.
  Our main contribution is to model infrastructure token economies as dynamical systems - the circulating token supply, price, and consumer demand change as a function of the payment to nodes and costs to consumers for infrastructure services. Crucially, this dynamical systems view enables us to leverage tools from mathematical control theory to optimize the overall decentralized network's performance. Moreover, our model extends easily to a Stackelberg game between the controller and the nodes, which we use for robust, strategic pricing. In short, we develop predictive, optimization-based controllers that outperform traditional algorithmic stablecoin heuristics by up to $2.4 \times$ in simulations based on real demand data from existing decentralized wireless networks."
http://arxiv.org/abs/2210.16885v1,Rationalization of indecisive choice behavior by majoritarian ballots,2022-10-30 16:50:24+00:00,"['José Carlos R. Alcantud', 'Domenico Cantone', 'Alfio Giarlotta', 'Stephen Watson']",econ.TH,"We describe a model that explains possibly indecisive choice behavior, that is, quasi-choices (choice correspondences that may be empty on some menus). The justification is here provided by a proportion of ballots, which are quasi-choices rationalizable by an arbitrary binary relation. We call a quasi-choice $s$-majoritarian if all options selected from a menu are endorsed by a share of ballots larger than $s$. We prove that all forms of majoritarianism are equivalent to a well-known behavioral property, namely Chernoff axiom. Then we focus on two paradigms of majoritarianism, whereby either a simple majority of ballots justifies a quasi-choice, or the endorsement by a single ballot suffices - a liberal justification. These benchmark explanations typically require a different minimum number of ballots. We determine the asymptotic minimum size of a liberal justification."
http://arxiv.org/abs/2210.08294v3,Setting Interim Deadlines to Persuade,2022-10-15 13:35:51+00:00,['Maxim Senkov'],econ.TH,"A principal funds a multistage project and retains the right to cut the funding if it stagnates at some point. An agent wants to convince the principal to fund the project as long as possible, and can design the flow of information about the progress of the project in order to persuade the principal. If the project is sufficiently promising ex ante, then the agent commits to providing only the good news that the project is accomplished. If the project is not promising enough ex ante, the agent persuades the principal to start the funding by committing to provide not only good news but also the bad news that a project milestone has not been reached by an interim deadline. I demonstrate that the outlined structure of optimal information disclosure holds irrespective of the agent's profit share, benefit from the flow of funding, and the common discount rate."
http://arxiv.org/abs/2210.11355v2,Network Synthetic Interventions: A Causal Framework for Panel Data Under Network Interference,2022-10-20 15:44:05+00:00,"['Anish Agarwal', 'Sarah H. Cen', 'Devavrat Shah', 'Christina Lee Yu']",econ.EM,"We propose a generalization of the synthetic controls and synthetic interventions methodology to incorporate network interference. We consider the estimation of unit-specific potential outcomes from panel data in the presence of spillover across units and unobserved confounding. Key to our approach is a novel latent factor model that takes into account network interference and generalizes the factor models typically used in panel data settings. We propose an estimator, Network Synthetic Interventions (NSI), and show that it consistently estimates the mean outcomes for a unit under an arbitrary set of counterfactual treatments for the network. We further establish that the estimator is asymptotically normal. We furnish two validity tests for whether the NSI estimator reliably generalizes to produce accurate counterfactual estimates. We provide a novel graph-based experiment design that guarantees the NSI estimator produces accurate counterfactual estimates, and also analyze the sample complexity of the proposed design. We conclude with simulations that corroborate our theoretical findings."
http://arxiv.org/abs/2212.08509v1,Moate Simulation of Stochastic Processes,2022-12-16 14:42:45+00:00,['Michael E. Mura'],q-fin.CP,"A novel approach called Moate Simulation is presented to provide an accurate numerical evolution of probability distribution functions represented on grids arising from stochastic differential processes where initial conditions are specified. Where the variables of stochastic differential equations may be transformed via Itô-Doeblin calculus into stochastic differentials with a constant diffusion term, the probability distribution function for these variables can be simulated in discrete time steps. The drift is applied directly to a volume element of the distribution while the stochastic diffusion term is applied through the use of convolution techniques such as Fast or Discrete Fourier Transforms. This allows for highly accurate distributions to be efficiently simulated to a given time horizon and may be employed in one, two or higher dimensional expectation integrals, e.g. for pricing of financial derivatives. The Moate Simulation approach forms a more accurate and considerably faster alternative to Monte Carlo Simulation for many applications while retaining the opportunity to alter the distribution in mid-simulation."
http://arxiv.org/abs/2212.10790v1,Inference for Model Misspecification in Interest Rate Term Structure using Functional Principal Component Analysis,2022-12-21 06:19:58+00:00,['Kaiwen Hou'],econ.EM,"Level, slope, and curvature are three commonly-believed principal components in interest rate term structure and are thus widely used in modeling. This paper characterizes the heterogeneity of how misspecified such models are through time. Presenting the orthonormal basis in the Nelson-Siegel model interpretable as the three factors, we design two nonparametric tests for whether the basis is equivalent to the data-driven functional principal component basis underlying the yield curve dynamics, considering the ordering of eigenfunctions or not, respectively. Eventually, we discover high dispersion between the two bases when rare events occur, suggesting occasional misspecification even if the model is overall expressive."
http://arxiv.org/abs/2212.12251v1,The connection between Arrow theorem and Sperner lemma,2022-12-23 10:54:40+00:00,['Nikita Miku'],math.CO,"It is well known that Sperner lemma is equivalent to Brouwer fixed-point theorem. Tanaka [12] proved that Brouwer theorem is equivalent to Arrow theorem, hence Arrow theorem is equivalent to Sperner lemma. In this paper we will prove this result directly. Moreover, we describe a number of other statements equivalent to Arrow theorem."
http://arxiv.org/abs/2212.13226v3,An Effective Treatment Approach to Difference-in-Differences with General Treatment Patterns,2022-12-26 17:20:59+00:00,['Takahide Yanagi'],econ.EM,"We consider a general difference-in-differences model in which the treatment variable of interest may be non-binary and its value may change in each period. It is generally difficult to estimate treatment parameters defined with the potential outcome given the entire path of treatment adoption, because each treatment path may be experienced by only a small number of observations. We propose an alternative approach using the concept of effective treatment, which summarizes the treatment path into an empirically tractable low-dimensional variable, and develop doubly robust identification, estimation, and inference methods. We also provide a companion R software package."
http://arxiv.org/abs/2212.13324v2,Spectral and post-spectral estimators for grouped panel data models,2022-12-26 23:30:37+00:00,"['Denis Chetverikov', 'Elena Manresa']",econ.EM,"In this paper, we develop spectral and post-spectral estimators for grouped panel data models. Both estimators are consistent in the asymptotics where the number of observations $N$ and the number of time periods $T$ simultaneously grow large. In addition, the post-spectral estimator is $\sqrt{NT}$-consistent and asymptotically normal with mean zero under the assumption of well-separated groups even if $T$ is growing much slower than $N$. The post-spectral estimator has, therefore, theoretical properties that are comparable to those of the grouped fixed-effect estimator developed by Bonhomme and Manresa (2015). In contrast to the grouped fixed-effect estimator, however, our post-spectral estimator is computationally straightforward."
http://arxiv.org/abs/2212.12797v1,New trends in South-South migration: The economic impact of COVID-19 and immigration enforcement,2022-12-24 18:27:30+00:00,"['Roxana Gutiérrez-Romero', 'Nayeli Salgado']",econ.GN,"This paper evaluates the impact of the pandemic and enforcement at the US and Mexican borders on the emigration of Guatemalans during 2017-2020. During this period, the number of crossings from Guatemala fell by 10%, according to the Survey of Migration to the Southern Border of Mexico. Yet, there was a rise of nearly 30% in the number of emigration crossings of male adults travelling with their children. This new trend was partly driven by the recent reduction in the number of children deported from the US. For a one-point reduction in the number of children deported from the US to Guatemalan municipalities, there was an increase of nearly 14 in the number of crossings made by adult males leaving from Guatemala for Mexico; and nearly 0.5 additional crossings made by male adults travelling with their children. However, the surge of emigrants travelling with their children was also driven by the acute economic shock that Guatemala experienced during the pandemic. During this period, air pollution in the analysed Guatemalan municipalities fell by 4%, night light per capita fell by 15%, and homicide rates fell by 40%. Unlike in previous years, emigrants are fleeing poverty rather than violence. Our findings suggest that a reduction in violence alone will not be sufficient to reduce emigration flows from Central America, but that economic recovery is needed."
http://arxiv.org/abs/2301.00092v2,Inference on Time Series Nonparametric Conditional Moment Restrictions Using General Sieves,2022-12-31 01:44:17+00:00,"['Xiaohong Chen', 'Yuan Liao', 'Weichen Wang']",stat.ML,"General nonlinear sieve learnings are classes of nonlinear sieves that can approximate nonlinear functions of high dimensional variables much more flexibly than various linear sieves (or series). This paper considers general nonlinear sieve quasi-likelihood ratio (GN-QLR) based inference on expectation functionals of time series data, where the functionals of interest are based on some nonparametric function that satisfy conditional moment restrictions and are learned using multilayer neural networks. While the asymptotic normality of the estimated functionals depends on some unknown Riesz representer of the functional space, we show that the optimally weighted GN-QLR statistic is asymptotically Chi-square distributed, regardless whether the expectation functional is regular (root-$n$ estimable) or not. This holds when the data are weakly dependent beta-mixing condition. We apply our method to the off-policy evaluation in reinforcement learning, by formulating the Bellman equation into the conditional moment restriction framework, so that we can make inference about the state-specific value functional using the proposed GN-QLR method with time series data. In addition, estimating the averaged partial means and averaged partial derivatives of nonparametric instrumental variables and quantile IV models are also presented as leading examples. Finally, a Monte Carlo study shows the finite sample performance of the procedure"
http://arxiv.org/abs/2301.04609v1,The Effects of Hofstede's Cultural Dimensions on Pro-Environmental Behaviour: How Culture Influences Environmentally Conscious Behaviour,2022-12-26 09:53:29+00:00,"['Szabolcs Nagy', 'Csilla Konyha Molnarne']",econ.GN,"The need for a more sustainable lifestyle is a key focus for several countries. Using a questionnaire survey conducted in Hungary, this paper examines how culture influences environmentally conscious behaviour. Having investigated the direct impact of Hofstedes cultural dimensions on pro-environmental behaviour, we found that the culture of a country hardly affects actual environmentally conscious behaviour. The findings indicate that only individualism and power distance have a significant but weak negative impact on pro-environmental behaviour. Based on the findings, we can state that a positive change in culture is a necessary but not sufficient condition for making a country greener."
http://arxiv.org/abs/2301.00666v2,E-commerce users' preferences for delivery options,2022-12-30 00:31:58+00:00,"['Yuki Oyama', 'Daisuke Fukuda', 'Naoto Imura', 'Katsuhiro Nishinari']",econ.GN,"Many e-commerce marketplaces offer their users fast delivery options for free to meet the increasing needs of users, imposing an excessive burden on city logistics. Therefore, understanding e-commerce users' preference for delivery options is a key to designing logistics policies. To this end, this study designs a stated choice survey in which respondents are faced with choice tasks among different delivery options and time slots, which was completed by 4,062 users from the three major metropolitan areas in Japan. To analyze the data, mixed logit models capturing taste heterogeneity as well as flexible substitution patterns have been estimated. The model estimation results indicate that delivery attributes including fee, time, and time slot size are significant determinants of the delivery option choices. Associations between users' preferences and socio-demographic characteristics, such as age, gender, teleworking frequency and the presence of a delivery box, were also suggested. Moreover, we analyzed two willingness-to-pay measures for delivery, namely, the value of delivery time savings (VODT) and the value of time slot shortening (VOTS), and applied a non-semiparametric approach to estimate their distributions in a data-oriented manner. Although VODT has a large heterogeneity among respondents, the estimated median VODT is 25.6 JPY/day, implying that more than half of the respondents would wait an additional day if the delivery fee were increased by only 26 JPY, that is, they do not necessarily need a fast delivery option but often request it when cheap or almost free. Moreover, VOTS was found to be low, distributed with the median of 5.0 JPY/hour; that is, users do not highly value the reduction in time slot size in monetary terms. These findings on e-commerce users' preferences can help in designing levels of service for last-mile delivery to significantly improve its efficiency."
http://arxiv.org/abs/2301.00292v6,Inference for Large Panel Data with Many Covariates,2022-12-31 21:07:24+00:00,"['Markus Pelger', 'Jiacheng Zou']",econ.EM,"This paper proposes a novel testing procedure for selecting a sparse set of covariates that explains a large dimensional panel. Our selection method provides correct false detection control while having higher power than existing approaches. We develop the inferential theory for large panels with many covariates by combining post-selection inference with a novel multiple testing adjustment. Our data-driven hypotheses are conditional on the sparse covariate selection. We control for family-wise error rates for covariate discovery for large cross-sections. As an easy-to-use and practically relevant procedure, we propose Panel-PoSI, which combines the data-driven adjustment for panel multiple testing with valid post-selection p-values of a generalized LASSO, that allows us to incorporate priors. In an empirical study, we select a small number of asset pricing factors that explain a large cross-section of investment strategies. Our method dominates the benchmarks out-of-sample due to its better size and power."
http://arxiv.org/abs/2212.07099v1,Research on The Cultivation Path of Craftsman Spirit in Higher Vocational Education Based on Survey Data,2022-12-14 08:45:56+00:00,"['Yufei Xie', 'Jing Cui', 'Mengdie Wang']",econ.GN,"With the development of China's economy and society, the importance of ""craftsman's spirit"" has become more and more prominent. As the main educational institution for training technical talents, higher vocational colleges vigorously promote the exploration of the cultivation path of craftsman spirit in higher vocational education, which provides new ideas and directions for the reform and development of higher vocational education, and is the fundamental need of the national innovation driven development strategy. Based on the questionnaire survey of vocational students in a certain range, this paper analyzes the problems existing in the cultivation path of craftsman spirit in Higher Vocational Education from multiple levels and the countermeasures."
http://arxiv.org/abs/2212.07108v1,School Choice with Farsighted Students,2022-12-14 08:57:28+00:00,"['Ata Atay', 'Ana Mauleon', 'Vincent Vannetelbosch']",econ.TH,"We consider priority-based school choice problems with farsighted students. We show that a singleton set consisting of the matching obtained from the Top Trading Cycles (TTC) mechanism is a farsighted stable set. However, the matching obtained from the Deferred Acceptance (DA) mechanism may not belong to any farsighted stable set. Hence, the TTC mechanism provides an assignment that is not only Pareto efficient but also farsightedly stable. Moreover, looking forward three steps ahead is already sufficient for stabilizing the matching obtained from the TTC."
http://arxiv.org/abs/2212.07280v3,Amenity complexity and urban locations of socio-economic mixing,2022-12-14 15:27:50+00:00,"['Sándor Juhász', 'Gergő Pintér', 'Ádám Kovács', 'Endre Borza', 'Gergely Mónus', 'László Lőrincz', 'Balázs Lengyel']",physics.soc-ph,"Cities host diverse people and their mixing is the engine of prosperity. In turn, segregation and inequalities are common features of most cities and locations that enable the meeting of people with different socio-economic status are key for urban inclusion. In this study, we adopt the concept of economic complexity to quantify the sophistication of amenity supply at urban locations. We propose that neighborhood complexity and amenity complexity are connected to the ability of locations to attract diverse visitors from various socio-economic backgrounds across the city. We construct the measures of amenity complexity based on the local portfolio of diverse and non-ubiquitous amenities in Budapest, Hungary. Socio-economic mixing at visited third places is investigated by tracing the daily mobility of individuals and by characterizing their status by the real-estate price of their home locations. Results suggest that measures of ubiquity and diversity of amenities do not, but neighborhood complexity and amenity complexity are correlated with the urban centrality of locations. Urban centrality is a strong predictor of socio-economic mixing, but both neighborhood complexity and amenity complexity add further explanatory power to our models. Our work combines urban mobility data with economic complexity thinking to show that the diversity of non-ubiquitous amenities, central locations, and the potentials for socio-economic mixing are interrelated."
http://arxiv.org/abs/2212.02881v3,Respecting priorities versus respecting preferences in school choice: When is there a trade-off?,2022-12-06 10:55:35+00:00,"['Estelle Cantillon', 'Li Chen', 'Juan S. Pereyra']",econ.TH,"A classic trade-off that school districts face when deciding which matching algorithm to use is that it is not possible to always respect both priorities and preferences. The student-proposing deferred acceptance algorithm (DA) respects priorities but can lead to inefficient allocations. We identify a new condition on school choice markets under which DA is efficient. Our condition generalizes earlier conditions by placing restrictions on how preferences and priorities relate to one another only on the parts that are relevant for the assignment. Whenever there is a unique allocation that respects priorities, our condition captures all the environments for which DA is efficient. We show through stylized examples and simulations that our condition significantly expands the range of known environments for which DA is efficient. We also discuss how our condition sheds light on existing empirical findings."
http://arxiv.org/abs/2211.14219v2,The Informational Role of Online Recommendations: Evidence from a Field Experiment,2022-11-25 16:31:10+00:00,"['Guy Aridor', 'Duarte Goncalves', 'Daniel Kluver', 'Ruoyan Kong', 'Joseph Konstan']",econ.GN,"We conduct a field experiment on a movie-recommendation platform to investigate whether and how online recommendations influence consumption choices. Using a within-subjects design, our experiment measures the causal effect of recommendations on consumption and decomposes the relative importance of two economic mechanisms: expanding consumers' consideration sets and providing information about their idiosyncratic match value. We find that the informational component exerts a stronger influence - recommendations shape consumer beliefs, which in turn drive consumption, particularly among less experienced consumers. Our findings and experimental design provide valuable insights for the economic evaluation and optimisation of online recommendation systems."
http://arxiv.org/abs/2212.14075v2,Forward Orthogonal Deviations GMM and the Absence of Large Sample Bias,2022-12-28 19:38:03+00:00,['Robert F. Phillips'],econ.EM,"It is well known that generalized method of moments (GMM) estimators of dynamic panel data regressions can have significant bias when the number of time periods ($T$) is not small compared to the number of cross-sectional units ($n$). The bias is attributed to the use of many instrumental variables. This paper shows that if the maximum number of instrumental variables used in a period increases with $T$ at a rate slower than $T^{1/2}$, then GMM estimators that exploit the forward orthogonal deviations (FOD) transformation do not have asymptotic bias, regardless of how fast $T$ increases relative to $n$. This conclusion is specific to using the FOD transformation. A similar conclusion does not necessarily apply when other transformations are used to remove fixed effects. Monte Carlo evidence illustrating the analytical results is provided."
http://arxiv.org/abs/2211.09360v1,Achieving Social Optimality for Energy Communities via Dynamic NEM Pricing,2022-11-17 06:20:52+00:00,"['Ahmed S. Alahmed', 'Lang Tong']",eess.SY,"We propose a social welfare maximizing mechanism for an energy community that aggregates individual and shared community resources under a general net energy metering (NEM) policy. Referred to as Dynamic NEM, the proposed mechanism adopts the standard NEM tariff model and sets NEM prices dynamically based on the total shared renewables within the community. We show that Dynamic NEM guarantees a higher benefit to each community member than possible outside the community. We further show that Dynamic NEM aligns the individual member's incentive with that of the overall community; each member optimizing individual surplus under Dynamic NEM results in maximum community's social welfare. Dynamic NEM is also shown to satisfy the cost-causation principle. Empirical studies using real data on a hypothetical energy community demonstrate the benefits to community members and grid operators."
http://arxiv.org/abs/2211.06850v3,Approximate Optimality of Linear Contracts Under Uncertainty,2022-11-13 08:38:51+00:00,"['Tal Alon', 'Paul Dütting', 'Yingkai Li', 'Inbal Talgam-Cohen']",cs.GT,"We consider a hidden-action principal-agent model, in which actions require different amounts of effort, and the agent privately knows his ability that determines his cost of effort. We show that linear contracts admit approximation guarantees that improve with a natural metric that captures the degree of uncertainty in the contracting setting. We thus show that linear contracts are near-optimal whenever there is enough uncertainty. In contrast, other simple contract formats such as debt contracts may suffer from a loss linear in the number of possible actions, even when there is sufficient uncertainty."
http://arxiv.org/abs/2210.15841v7,How to sample and when to stop sampling: The generalized Wald problem and minimax policies,2022-10-28 02:23:43+00:00,['Karun Adusumilli'],econ.EM,"We study sequential experiments where sampling is costly and a decision-maker aims to determine the best treatment for full scale implementation by (1) adaptively allocating units between two possible treatments, and (2) stopping the experiment when the expected welfare (inclusive of sampling costs) from implementing the chosen treatment is maximized. Working under a continuous time limit, we characterize the optimal policies under the minimax regret criterion. We show that the same policies also remain optimal under both parametric and non-parametric outcome distributions in an asymptotic regime where sampling costs approach zero. The minimax optimal sampling rule is just the Neyman allocation: it is independent of sampling costs and does not adapt to observed outcomes. The decision-maker halts sampling when the product of the average treatment difference and the number of observations surpasses a specific threshold. The results derived also apply to the so-called best-arm identification problem, where the number of observations is exogenously specified."
http://arxiv.org/abs/2211.04388v3,Profit Shifting and International Tax Reforms,2022-11-08 17:23:02+00:00,"['Alessandro Ferrari', 'Sébastien Laffitte', 'Mathieu Parenti', 'Farid Toubal']",econ.GN,"International taxation rules are outdated, allowing multinationals to shift profits to tax havens. This paper examines how tax reforms affect profit shifting and cross-country welfare. We propose a model that separates real economic profits from paper profits, introducing 'triangle identities' to estimate bilateral profit-shifting flows. Using macro- and firm-level data, paper profits' elasticity is three times that of the tax base. Global minimum tax reforms improve welfare by increasing public goods funding and reducing tax competition. We also identify optimal minimum rates under various taxing-right scenarios and demonstrate that unilateral destination-based-cash-flow-tax reforms' welfare effects depend highly on trade imbalances."
http://arxiv.org/abs/2210.06639v4,Robust Estimation and Inference in Panels with Interactive Fixed Effects,2022-10-13 00:32:58+00:00,"['Timothy B. Armstrong', 'Martin Weidner', 'Andrei Zeleneev']",econ.EM,"We consider estimation and inference for a regression coefficient in panels with interactive fixed effects (i.e., with a factor structure). We demonstrate that existing estimators and confidence intervals (CIs) can be heavily biased and size-distorted when some of the factors are weak. We propose estimators with improved rates of convergence and bias-aware CIs that remain valid uniformly, regardless of factor strength. Our approach applies the theory of minimax linear estimation to form a debiased estimate, using a nuclear norm bound on the error of an initial estimate of the interactive fixed effects. Our resulting bias-aware CIs take into account the remaining bias caused by weak factors. Monte Carlo experiments show substantial improvements over conventional methods when factors are weak, with minimal costs to estimation accuracy when factors are strong."
http://arxiv.org/abs/2212.03360v2,Screening with Persuasion,2022-12-06 23:02:43+00:00,"['Dirk Bergemann', 'Tibor Heumann', 'Stephen Morris']",econ.TH,"We analyze a nonlinear pricing model where the seller controls both product pricing (screening) and buyer information about their own values (persuasion). We prove that the optimal mechanism always consists of finitely many signals and items, even with a continuum of buyer values. The seller optimally pools buyer values and reduces product variety to minimize informational rents. We show that value pooling is optimal even for finite value distributions if their entropy exceeds a critical threshold. We also provide sufficient conditions under which the optimal menu restricts offering to a single item."
http://arxiv.org/abs/2211.13605v4,Efficient Communication in Organizations,2022-11-24 13:46:08+00:00,['Federico Vaccari'],econ.TH,"This paper studies the organization of communication between biased senders and a receiver. Senders can misreport their private information at a cost. Efficiency is achieved by clearing information asymmetries without incurring costs. Results show that only one communication protocol is efficient, robust to collusion, and free from unnecessary complexities. This protocol has a simple, adversarial, and public structure. It always induces efficient equilibria, for which a closed-form characterization is provided. The findings are relevant for the design of organizations that seek to improve decision-making while limiting wasteful influence activities."
http://arxiv.org/abs/2212.03931v3,A Better Test of Choice Overload,2022-12-07 19:54:59+00:00,"['Mark Dean', 'Dilip Ravindran', 'Jörg Stoye']",econ.GN,"Choice overload - in which larger choice sets are detrimental to a chooser's well-being - is potentially of great importance in the design of economic policy. Yet the current evidence on its prevalence is inconclusive. We argue that existing tests are likely to be underpowered and hence that choice overload may occur more often than the literature suggests. We propose more powerful tests based on richer data and characterization theorems for the Random Utility Model. These new approaches come with significant econometric challenges, which we show how to address. We apply our tests to new experimental data and find strong evidence of choice overload that would likely be missed using current approaches."
http://arxiv.org/abs/2211.00291v2,"Distinguishable Cash, Bosonic Bitcoin, and Fermionic Non-fungible Token",2022-11-01 05:59:56+00:00,"['Zae Young Kim', 'Jeong-Hyuck Park']",econ.GN,"Modern technology has brought novel types of wealth. In contrast to hard cashes, digital currencies do not have a physical form. They exist in electronic forms only. Yet, it has not been clear what impacts their ongoing growth will make, if any, on wealth distribution. Here we propose to identify all forms of contemporary wealth into two classes: 'distinguishable' or 'identical'. Traditional tangible moneys are all distinguishable. Financial assets and cryptocurrencies, such as bank deposits and Bitcoin, are boson-like, while non-fungible tokens are fermion-like. We derive their ownership-based distributions in a unified manner. Each class follows essentially the Poisson or the geometric distribution. We contrast their distinct features such as Gini coefficients. Further, aggregating different kinds of wealth corresponds to a weighted convolution where the number of banks matters and Bitcoin follows Bose-Einstein distribution. Our proposal opens a new avenue to understand the deepened inequality in modern economy, which is based on the statistical physics property of wealth rather than the individual ability of owners. We call for verifications with real data."
http://arxiv.org/abs/2211.01921v4,Asymptotic Theory of Principal Component Analysis for High-Dimensional Time Series Data under a Factor Structure,2022-11-03 16:01:49+00:00,['Matteo Barigozzi'],econ.EM,"We review Principal Components (PC) estimation of a large approximate factor model for a panel of $n$ stationary time series and we provide new derivations of the asymptotic properties of the estimators, which are derived under a minimal set of assumptions requiring only the existence of 4th order moments. To this end, we also review various alternative sets of primitive sufficient conditions for mean-squared consistency of the sample covariance matrix. Finally, we discuss in detail the issue of identification of the loadings and factors as well as its implications for inference."
http://arxiv.org/abs/2210.08698v3,A General Design-Based Framework and Estimator for Randomized Experiments,2022-10-17 02:19:11+00:00,"['Christopher Harshaw', 'Fredrik Sävje', 'Yitan Wang']",stat.ME,"We describe a design-based framework for drawing causal inference in general randomized experiments. Causal effects are defined as linear functionals evaluated at unit-level potential outcome functions. Assumptions about the potential outcome functions are encoded as function spaces. This makes the framework expressive, allowing experimenters to formulate and investigate a wide range of causal questions, including about interference, that previously could not be investigated with design-based methods. We describe a class of estimators for estimands defined using the framework and investigate their properties. We provide necessary and sufficient conditions for unbiasedness and consistency. We also describe a class of conservative variance estimators, which facilitate the construction of confidence intervals. Finally, we provide several examples of empirical settings that previously could not be examined with design-based methods to illustrate the use of our approach in practice."
http://arxiv.org/abs/2210.10642v2,Public Good Provision with a Governor,2022-10-19 15:15:38+00:00,"['Chowdhury Mohammad Sakib Anwar', 'Alexander Matros', 'Sonali SenGupta']",econ.TH,"We study a public good game with N citizens and a Governor who allocates resources from a common fund. Citizens may voluntarily contribute or be compelled to do so if audited, in which case shirkers face a penalty. The Governor decides how much of the fund to devote to public good provision, with the remainder embezzled. Crucially, the Governor's utility combines material payoffs from embezzlement with belief-dependent reputational concerns. We fully characterize the symmetric subgame perfect equilibria (SSPE) of the game. The model always admits at least one pure-strategy equilibrium, ranging from universal free-riding with complete embezzlement to full contribution with efficient provision. Mixed-strategy equilibria exist only in a narrow region of parameter values and may involve multiple equilibria. Our analysis highlights the roles of penalties, audits, and reputational incentives in sustaining contribution and provision, thereby linking public good provision with the broader literature on corruption, embezzlement, and psychological game theory."
http://arxiv.org/abs/2212.13638v6,Battling the Coronavirus Infodemic Among Social Media Users in Kenya and Nigeria,2022-12-27 23:09:44+00:00,"['Molly Offer-Westort', 'Leah R. Rosenzweig', 'Susan Athey']",cs.SI,"How can we induce social media users to be discerning when sharing information during a pandemic? An experiment on Facebook Messenger with users from Kenya (n = 7,498) and Nigeria (n = 7,794) tested interventions designed to decrease intentions to share COVID-19 misinformation without decreasing intentions to share factual posts. The initial stage of the study incorporated: (i) a factorial design with 40 intervention combinations; and (ii) a contextual adaptive design, increasing the probability of assignment to treatments that worked better for previous subjects with similar characteristics. The second stage evaluated the best-performing treatments and a targeted treatment assignment policy estimated from the data. We precisely estimate null effects from warning flags and related article suggestions, tactics used by social media platforms. However, nudges to consider information's accuracy reduced misinformation sharing relative to control by 4.9% (estimate = -2.3 pp, s.e. = 1.0 , Z = -2.31, p = 0.021, 95% CI = [-4.2 , -0.35]). Such low-cost scalable interventions may improve the quality of information circulating online."
http://arxiv.org/abs/2210.17063v4,Shrinkage Methods for Treatment Choice,2022-10-31 04:41:56+00:00,"['Takuya Ishihara', 'Daisuke Kurisu']",econ.EM,"This study examines the problem of determining whether to treat individuals based on observed covariates. The most common decision rule is the conditional empirical success (CES) rule proposed by Manski (2004), which assigns individuals to treatments that yield the best experimental outcomes conditional on the observed covariates. Conversely, using shrinkage estimators, which shrink unbiased but noisy preliminary estimates toward the average of these estimates, is a common approach in statistical estimation problems because it is well-known that shrinkage estimators may have smaller mean squared errors than unshrunk estimators. Inspired by this idea, we propose a computationally tractable shrinkage rule that selects the shrinkage factor by minimizing an upper bound of the maximum regret. Then, we compare the maximum regret of the proposed shrinkage rule with those of the CES and pooling rules when the space of conditional average treatment effects (CATEs) is correctly specified or misspecified. Our theoretical results demonstrate that the shrinkage rule performs well in many cases and these findings are further supported by numerical experiments. Specifically, we show that the maximum regret of the shrinkage rule can be strictly smaller than those of the CES and pooling rules in certain cases when the space of CATEs is correctly specified. In addition, we find that the shrinkage rule is robust against misspecification of the space of CATEs. Finally, we apply our method to experimental data from the National Job Training Partnership Act Study."
http://arxiv.org/abs/2211.06288v3,A Residuals-Based Nonparametric Variance Ratio Test for Cointegration,2022-11-11 15:54:26+00:00,['Karsten Reichold'],econ.EM,"This paper derives asymptotic theory for Breitung's (2002, Journal of Econometrics 108, 343-363) nonparameteric variance ratio unit root test when applied to regression residuals. The test requires neither the specification of the correlation structure in the data nor the choice of tuning parameters. Compared with popular residuals-based no-cointegration tests, the variance ratio test is less prone to size distortions but has smaller local asymptotic power. However, this paper shows that local asymptotic power properties do not serve as a useful indicator for the power of residuals-based no-cointegration tests in finite samples. In terms of size-corrected power, the variance ratio test performs relatively well and, in particular, does not suffer from power reversal problems detected for, e.g., the frequently used augmented Dickey-Fuller type no-cointegration test. An application to daily prices of cryptocurrencies illustrates the usefulness of the variance ratio test in practice."
http://arxiv.org/abs/2210.02548v1,Regression discontinuity design with right-censored survival data,2022-10-05 20:23:04+00:00,['Emil Aas Stoltenberg'],stat.ME,"In this paper the regression discontinuity design is adapted to the survival analysis setting with right-censored data, studied in an intensity based counting process framework. In particular, a local polynomial regression version of the Aalen additive hazards estimator is introduced as an estimator of the difference between two covariate dependent cumulative hazard rate functions. Large-sample theory for this estimator is developed, including confidence intervals that take into account the uncertainty associated with bias correction. As is standard in the causality literature, the models and the theory are embedded in the potential outcomes framework. Two general results concerning potential outcomes and the multiplicative hazards model for survival data are presented."
http://arxiv.org/abs/2210.02824v2,Testing the Number of Components in Finite Mixture Normal Regression Model with Panel Data,2022-10-06 11:25:35+00:00,"['Yu Hao', 'Hiroyuki Kasahara']",econ.EM,"This paper develops the likelihood ratio-based test of the null hypothesis of a M0-component model against an alternative of (M0 + 1)-component model in the normal mixture panel regression by extending the Expectation-Maximization (EM) test of Chen and Li (2009a) and Kasahara and Shimotsu (2015) to the case of panel data. We show that, unlike the cross-sectional normal mixture, the first-order derivative of the density function for the variance parameter in the panel normal mixture is linearly independent of its second-order derivatives for the mean parameter. On the other hand, like the cross-sectional normal mixture, the likelihood ratio test statistic of the panel normal mixture is unbounded. We consider the Penalized Maximum Likelihood Estimator to deal with the unboundedness, where we obtain the data-driven penalty function via computational experiments. We derive the asymptotic distribution of the Penalized Likelihood Ratio Test (PLRT) and EM test statistics by expanding the log-likelihood function up to five times for the reparameterized parameters. The simulation experiment indicates good finite sample performance of the proposed EM test. We apply our EM test to estimate the number of production technology types for the finite mixture Cobb-Douglas production function model studied by Kasahara et al. (2022) used the panel data of the Japanese and Chilean manufacturing firms. We find the evidence of heterogeneity in elasticities of output for intermediate goods, suggesting that production function is heterogeneous across firms beyond their Hicks-neutral productivity terms."
http://arxiv.org/abs/2212.03947v1,Macroeconomic evaluation of the growth of the UK economy over the period 2000 to 2019,2022-11-08 16:57:31+00:00,['Laurence Francis Lacey'],stat.AP,"An information entropy statistical methodology was used to evaluate the growth of the UK economy over the period 2000 to 2019, with an emphasis on the impact of labour productivity on gross domestic product (GDP) per capita and the average growth in real wages, during this time period. The growth of the UK economy over the period 2000 to 2019 can be described in terms of three distinct phases: 1) 2000 to 2007 - strong sustained economic growth 2) 2008 to 2013 - the impact of the international financial crisis, its immediate aftermath, and period of recovery 3) 2014 to 2019 - weak sustained economic growth The key determinant of the UK economic performance over this period would appear to the annual rate of growth in labour productivity. It was closely related to the annual rate of growth in GDP per capita, and it was significantly weaker in the period 2014 to 2019 compared to the period 2000 to 2007. This also corresponded with a weaker rate of growth in annual average real wages over the period 2014 to 2019 compared to the period 2000 to 2007. Throughout the period 2000 to 2019, UK CPI was maintained, on average, at approximately 2.1% per annum. More rapid UK economic growth would be expected to be achieved by sustained investment in measures that enhance labour productivity, with the further expectation that a sustained improvement in labour productivity would increase the annual rate of growth of UK GDP per capita and average real wages. While the results given in this paper are specific to the UK over the time period 2000 to 2019, the expectation is that the methodology and approach adopted can be applied to quantifying the dynamics of any developed economy over any time period."
http://arxiv.org/abs/2212.06223v2,The Effect of Financial Resources on Fertility: Evidence from Administrative Data on Lottery Winners,2022-12-12 20:16:46+00:00,"['Yung-Yu Tsai', 'Hsing-Wen Han', 'Kuang-Ta Lo', 'Tzu-Ting Yang']",econ.GN,"This paper utilizes wealth shocks from winning lottery prizes to examine the causal effect of financial resources on fertility. We employ extensive panels of administrative data encompassing over 0.4 million lottery winners in Taiwan and implement a triple-differences design. Our analyses reveal that a substantial lottery win can significantly increase fertility, the implied wealth elasticity of which is around 0.06. Moreover, the primary channel through which fertility increases is by prompting first births among previously childless individuals. Finally, our analysis reveals that approximately 25% of the total fertility effect stems from increased marriage rates following a lottery win."
http://arxiv.org/abs/2212.00934v1,Shifting to Telework and Firms' Location: Does Telework Make Our Society Efficient?,2022-12-02 02:42:15+00:00,['Kazufumi Tsuboi'],econ.GN,"Although it has been suggested that the shift from on-site work to telework will change the city structure, the mechanism of this change is not clear. This study clarifies how the location of firms changes when the cost of teleworking decreases and how this affects the urban economy. The two main results obtained are as follows. (i) The expansion of teleworking causes firms to be located closer to urban centers or closer to urban fringes. (ii) Teleworking makes urban production more efficient and cities more compact. This is the first paper to show that two empirical studies can be represented in a unified theoretical model and that existing studies obtained by simulation can be explained analytically."
http://arxiv.org/abs/2211.14272v3,A Rigorous Proof of the Index Theorem for Economists,2022-11-25 18:10:38+00:00,['Yuhki Hosoya'],econ.TH,"This paper provides a rigorous and gap-free proof of the index theorem used in the theory of regular economy. In the index theorem that is the subject of this paper, the assumptions for the excess demand function are only several usual assumptions and continuous differentiability around any equilibrium price, and thus it has a form that is applicable to many economies. However, the textbooks on this theme contain only abbreviated proofs and there is no known monograph that contains a rigorous proof of this theorem. Hence, the purpose of this paper is to make this theorem available to more economists by constructing a readable proof."
http://arxiv.org/abs/2211.16714v3,Incorporating Prior Knowledge of Latent Group Structure in Panel Data Models,2022-11-30 03:32:58+00:00,['Boyuan Zhang'],econ.EM,"The assumption of group heterogeneity has become popular in panel data models. We develop a constrained Bayesian grouped estimator that exploits researchers' prior beliefs on groups in a form of pairwise constraints, indicating whether a pair of units is likely to belong to a same group or different groups. We propose a prior to incorporate the pairwise constraints with varying degrees of confidence. The whole framework is built on the nonparametric Bayesian method, which implicitly specifies a distribution over the group partitions, and so the posterior analysis takes the uncertainty of the latent group structure into account. Monte Carlo experiments reveal that adding prior knowledge yields more accurate estimates of coefficient and scores predictive gains over alternative estimators. We apply our method to two empirical applications. In a first application to forecasting U.S. CPI inflation, we illustrate that prior knowledge of groups improves density forecasts when the data is not entirely informative. A second application revisits the relationship between a country's income and its democratic transition; we identify heterogeneous income effects on democracy with five distinct groups over ninety countries."
http://arxiv.org/abs/2211.14354v1,A Design-Based Approach to Spatial Correlation,2022-11-25 19:21:14+00:00,"['Ruonan Xu', 'Jeffrey M. Wooldridge']",econ.EM,"When observing spatial data, what standard errors should we report? With the finite population framework, we identify three channels of spatial correlation: sampling scheme, assignment design, and model specification. The Eicker-Huber-White standard error, the cluster-robust standard error, and the spatial heteroskedasticity and autocorrelation consistent standard error are compared under different combinations of the three channels. Then, we provide guidelines for whether standard errors should be adjusted for spatial correlation for both linear and nonlinear estimators. As it turns out, the answer to this question also depends on the magnitude of the sampling probability."
http://arxiv.org/abs/2211.15509v1,Uncovering the Dynamics of the Wealth Distribution,2022-11-24 15:43:51+00:00,['Thomas Blanchet'],econ.GN,"I introduce a new way of decomposing the evolution of the wealth distribution using a simple continuous time stochastic model, which separates the effects of mobility, savings, labor income, rates of return, demography, inheritance, and assortative mating. Based on two results from stochastic calculus, I show that this decomposition is nonparametrically identified and can be estimated based solely on repeated cross-sections of the data. I estimate it in the United States since 1962 using historical data on income, wealth, and demography. I find that the main drivers of the rise of the top 1% wealth share since the 1980s have been, in decreasing level of importance, higher savings at the top, higher rates of return on wealth (essentially in the form of capital gains), and higher labor income inequality. I then use the model to study the effects of wealth taxation. I derive simple formulas for how the tax base reacts to the net-of-tax rate in the long run, which nest insights from several existing models, and can be calibrated using estimable elasticities. In the benchmark calibration, the revenue-maximizing wealth tax rate at the top is high (around 12%), but the revenue collected from the tax is much lower than in the static case."
http://arxiv.org/abs/2212.03471v2,Bayesian Forecasting in Economics and Finance: A Modern Review,2022-12-07 05:41:04+00:00,"['Gael M. Martin', 'David T. Frazier', 'Worapree Maneesoonthorn', 'Ruben Loaiza-Maya', 'Florian Huber', 'Gary Koop', 'John Maheu', 'Didier Nibbering', 'Anastasios Panagiotelis']",econ.EM,"The Bayesian statistical paradigm provides a principled and coherent approach to probabilistic forecasting. Uncertainty about all unknowns that characterize any forecasting problem -- model, parameters, latent states -- is able to be quantified explicitly, and factored into the forecast distribution via the process of integration or averaging. Allied with the elegance of the method, Bayesian forecasting is now underpinned by the burgeoning field of Bayesian computation, which enables Bayesian forecasts to be produced for virtually any problem, no matter how large, or complex. The current state of play in Bayesian forecasting in economics and finance is the subject of this review. The aim is to provide the reader with an overview of modern approaches to the field, set in some historical context; and with sufficient computational detail given to assist the reader with implementation."
http://arxiv.org/abs/2301.09438v1,"Composite distributions in the social sciences: A comparative empirical study of firms' sales distribution for France, Germany, Italy, Japan, South Korea, and Spain",2023-01-20 11:04:06+00:00,"['Arturo Ramos', 'Till Massing', 'Atushi Ishikawa', 'Shouji Fujimoto', 'Takayuki Mizuno']",econ.GN,"We study 17 different statistical distributions for sizes obtained {}from the classical and recent literature to describe a relevant variable in the social sciences and Economics, namely the firms' sales distribution in six countries over an ample period. We find that the best results are obtained with mixtures of lognormal (LN), loglogistic (LL), and log Student's $t$ (LSt) distributions. The single lognormal, in turn, is strongly not selected. We then find that the whole firm size distribution is better described by a mixture, and there exist subgroups of firms. Depending on the method of measurement, the best fitting distribution cannot be defined by a single one, but as a mixture of at least three distributions or even four or five. We assess a full sample analysis, an in-sample and out-of-sample analysis, and a doubly truncated sample analysis. We also provide the formulation of the preferred models as solutions of the Fokker--Planck or forward Kolmogorov equation."
http://arxiv.org/abs/2302.08323v1,Reevaluating the Taylor Rule with Machine Learning,2023-02-07 21:41:11+00:00,['Alper Deniz Karakas'],econ.GN,"This paper aims to reevaluate the Taylor Rule, through a linear and a nonlinear method, such that its estimated federal funds rates match those actually previously implemented by the Federal Reserve Bank. In the linear method, this paper uses an OLS regression model to find more accurate coefficients within the same Taylor Rule equation in which the dependent variable is the federal funds rate, and the independent variables are the inflation rate, the inflation gap, and the output gap. The intercept in the OLS regression model would capture the constant equilibrium target real interest rate set at 2. The linear OLS method suggests that the Taylor Rule overestimates the output gap and standalone inflation rate's coefficients for the Taylor Rule. The coefficients this paper suggests are shown in equation (2). In the nonlinear method, this paper uses a machine learning system in which the two inputs are the inflation rate and the output gap and the output is the federal funds rate. This system utilizes gradient descent error minimization to create a model that minimizes the error between the estimated federal funds rate and the actual previously implemented federal funds rate. Since the machine learning system allows the model to capture the more realistic nonlinear relationship between the variables, it significantly increases the estimation accuracy as a result. The actual and estimated federal funds rates are almost identical besides three recessions caused by bubble bursts, which the paper addresses in the concluding remarks. Overall, the first method provides theoretical insight while the second suggests a model with improved applicability."
http://arxiv.org/abs/2303.01231v5,Consumer Welfare Under Individual Heterogeneity,2023-03-01 10:57:56+00:00,"['Charles Gauthier', 'Sebastiaan Maes', 'Raghav Malhotra']",econ.TH,"We propose a nonparametric method for estimating the distribution of consumer welfare from cross-sectional data with no restrictions on individual preferences. First demonstrating that moments of demand identify the curvature of the expenditure function, we use these moments to approximate money-metric welfare measures. Our approach captures both nonhomotheticity and heterogeneity in preferences in the behavioral responses to price changes. We apply our method to US household scanner data to evaluate the impacts of the price shock between December 2020 and 2021 on the cost-of-living index. We document substantial heterogeneity in welfare losses within and across demographic groups. For most groups, a naive measure of consumer welfare would significantly underestimate the welfare loss. By decomposing the behavioral responses into the components arising from nonhomotheticity and heterogeneity in preferences, we find that both factors are essential for accurate welfare measurement, with heterogeneity contributing more substantially."
http://arxiv.org/abs/2301.02648v1,Climate change heterogeneity: A new quantitative approach,2023-01-06 18:49:24+00:00,"['Maria Dolores Gadea', 'Jesus Gonzalo']",econ.EM,"Climate change is a non-uniform phenomenon. This paper proposes a new quantitative methodology to characterize, measure, and test the existence of climate change heterogeneity. It consists of three steps. First, we introduce a new testable warming typology based on the evolution of the trend of the whole temperature distribution and not only on the average. Second, we define the concepts of warming acceleration and warming amplification in a testable format. And third, we introduce the new testable concept of warming dominance to determine whether region A is suffering a worse warming process than region B. Applying this three-step methodology, we find that Spain and the Globe experience a clear distributional warming process (beyond the standard average) but of different types. In both cases, this process is accelerating over time and asymmetrically amplified. Overall, warming in Spain dominates the Globe in all the quantiles except the lower tail of the global temperature distribution that corresponds to the Arctic region. Our climate change heterogeneity results open the door to the need for a non-uniform causal-effect climate analysis that goes beyond the standard causality in mean as well as for a more efficient design of the mitigation-adaptation policies. In particular, the heterogeneity we find suggests that these policies should contain a common global component and a clear local-regional element. Future climate agreements should take the whole temperature distribution into account."
http://arxiv.org/abs/2303.06106v1,The Nobel Family,2023-03-10 17:46:57+00:00,['Richard S. J. Tol'],econ.GN,"Nobel laureates cluster together. 696 of the 727 winners of the Nobel Prize in physics, chemistry, medicine, and economics belong to one single academic family tree. 668 trace their ancestry to Emmanuel Stupanus, 228 to Lord Rayleigh (physics, 1904). Craig Mello (medicine, 2006) counts 51 Nobelists among his ancestors. Chemistry laureates have the most Nobel ancestors and descendants, economics laureates the fewest. Chemistry is the central discipline. Its Nobelists have trained and are trained by Nobelists in other fields. Nobelists in physics (medicine) have trained (by) others. Economics stands apart. Openness to other disciplines is the same in recent and earlier times. The familial concentration of Nobelists is lower now than it used to be."
http://arxiv.org/abs/2303.06564v2,Redesigning the US Army's Branching Process: A Case Study in Minimalist Market Design,2023-03-12 04:10:12+00:00,"['Kyle Greenberg', 'Parag A. Pathak', 'Tayfun Sönmez']",econ.TH,"We present the proof-of-concept for minimalist market design (Sönmez, 2023) as an effective methodology to enhance an institution based on the desiderata of stakeholders with minimal interference. Four objectives-respecting merit, increasing retention, aligning talent, and enhancing trust-guided reforms to US Army's centralized branching process of cadets to military specialties since 2006. USMA's mechanism for the Class of 2020 exacerbated challenges implementing these objectives. Formulating the Army's desiderata as rigorous axioms, we analyze their implications. Under our minimalist approach to institution redesign, the Army's objectives uniquely identify a branching mechanism. Our design is now adopted at USMA and ROTC."
http://arxiv.org/abs/2303.05781v2,Strategy-proofness with single-peaked and single-dipped preferences,2023-03-10 08:32:35+00:00,"['Jorge Alcalde-Unzu', 'Oihane Gallo', 'Marc Vorsatz']",econ.TH,"We analyze the problem of locating a public facility in a domain of single-peaked and single-dipped preferences when the social planner knows the type of preference (single-peaked or single-dipped) of each agent. Our main result characterizes all strategy-proof rules and shows that they can be decomposed into two steps. In the first step, the agents with single-peaked preferences are asked about their peaks and, for each profile of reported peaks, at most two alternatives are preselected. In the second step, the agents with single-dipped preferences are asked to reveal their dips to complete the decision between the preselected alternatives. Our result generalizes the findings of Moulin (1980) and Barberà and Jackson (1994) for single-peaked and of Manjunath (2014) for single-dipped preferences. Finally, we show that all strategy-proof rules are also group strategy-proof and analyze the implications of Pareto efficiency."
http://arxiv.org/abs/2303.01601v1,Time-inconsistent contract theory,2023-03-02 21:52:39+00:00,"['Camilo Hernández', 'Dylan Possamaï']",econ.TH,"This paper investigates the moral hazard problem in finite horizon with both continuous and lump-sum payments, involving a time-inconsistent sophisticated agent and a standard utility maximiser principal. Building upon the so-called dynamic programming approach in Cvitanić, Possamaï, and Touzi [18] and the recently available results in Hernández and Possamaï [43], we present a methodology that covers the previous contracting problem. Our main contribution consists in a characterisation of the moral hazard problem faced by the principal. In particular, it shows that under relatively mild technical conditions on the data of the problem, the supremum of the principal's expected utility over a smaller restricted family of contracts is equal to the supremum over all feasible contracts. Nevertheless, this characterisation yields, as far as we know, a novel class of control problems that involve the control of a forward Volterra equation via Volterra-type controls, and infinite-dimensional stochastic target constraints. Despite the inherent challenges associated to such a problem, we study the solution under three different specifications of utility functions for both the agent and the principal, and draw qualitative implications from the form of the optimal contract. The general case remains the subject of future research."
http://arxiv.org/abs/2303.01824v1,Revisiting the effect of search frictions on market concentration,2023-03-03 10:09:28+00:00,"['Jules Depersin', 'Bérengère Patault']",econ.GN,"Search frictions can impede the formation of optimal matches between consumer and supplier, or employee and employer, and lead to inefficiencies. This paper revisits the effect of search frictions on the firm size distribution when challenging two common but strong assumptions: that all agents share the same ranking of firms, and that agents meet all firms, whether small or large, at the same rate. We build a random search model in which we relax those two assumptions and show that the intensity of search frictions has a non monotonic effect on market concentration. An increase in friction intensity increases market concentration up to a certain threshold of frictions, that depends on the slope of the meeting rate with respect to firm size. We leverage unique French customs data to estimate this slope. First, we find that in a range of plausible scenarios, search frictions intensity increases market concentration. Second, we show that slopes have increased over time, which unambiguously increases market concentration in our model. Overall, we shed light on the importance of the structure of frictions, rather than their intensity, to understand market concentration."
http://arxiv.org/abs/2303.14947v2,Measuring Self-Preferencing on Digital Platforms,2023-03-27 07:10:28+00:00,"['Lukas Jürgensmeier', 'Bernd Skiera']",econ.GN,"Digital platforms use recommendations to facilitate exchanges between platform actors, such as trade between buyers and sellers. Aiming to protect consumers and guarantee fair competition on platforms, legislators increasingly require that recommendations on market-dominating platforms be free from self-preferencing. That is, platforms that also act as sellers (e.g., Amazon) or information providers (e.g., Google) must not prefer their own offers over comparable third-party offers. Yet, successful enforcement of self-preferencing bans -- to the potential benefit of consumers and third-party actors -- requires defining and measuring self-preferencing across a platform. In the context of recommendations through search results, this research contributes by i) conceptualizing a ""recommendation"" as an offer's level of search engine visibility across an entire platform (instead of its position in specific search queries, as in previous research); ii) discussing two tests for self-preferencing, and iii) implementing them in two empirical studies across three international Amazon marketplaces. Contrary to consumer expectations and emerging literature, our analysis finds almost no evidence for self-preferencing. A survey reveals that even if Amazon were proven to engage in self-preferencing, most consumers would not change their shopping behavior on the platform -- highlighting Amazon's significant market power and suggesting the need for robust protections for sellers and consumers."
http://arxiv.org/abs/2303.14428v1,A New Production Function Approach,2023-03-25 10:21:22+00:00,['Samidh Pal'],econ.TH,"This paper presents a new nested production function that is specifically designed for analyzing capital and labor intensity of manufacturing industries in developing and developed regions. The paper provides a rigorous theoretical foundation for this production function, as well as an empirical analysis of its performance in a sample of industries. The analysis shows that the production function can be used to accurately estimate the level of capital and labor intensity in industries, as well as to analyze the capacity utilization of these industries."
http://arxiv.org/abs/2304.00081v1,Reconstructing firm-level input-output networks from partial information,2023-03-31 19:10:23+00:00,"['Andrea Bacilieri', 'Pablo Austudillo-Estevez']",econ.GN,"There is a large consensus on the fundamental role of firm-level supply chain networks in macroeconomics. However, data on supply chains at the fine-grained, firm level are scarce and frequently incomplete. For listed firms, some commercial datasets exist but only contain information about the existence of a trade relationship between two companies, not the value of the monetary transaction. We use a recently developed maximum entropy method to reconstruct the values of the transactions based on information about their existence and aggregate information disclosed by firms in financial statements. We test the method on the administrative dataset of Ecuador and reconstruct a commercial dataset (FactSet). We test the method's performance on the weights, the technical and allocation coefficients (microscale quantities), two measures of firms' systemic importance and GDP volatility. The method reconstructs the distribution of microscale quantities reasonably well but shows diverging results for the measures of firms' systemic importance. Due to the network structure of supply chains and the sampling process of firms and links, quantities relying on the number of customers firms have (out-degrees) are harder to reconstruct. We also reconstruct the input-output table of globally listed firms and merge it with a global input-output table at the sector level (the WIOD). Differences in accounting standards between national accounts and firms' financial statements significantly reduce the quality of the reconstruction."
http://arxiv.org/abs/2303.13406v2,Sequential Cauchy Combination Test for Multiple Testing Problems with Financial Applications,2023-03-23 16:28:16+00:00,"['Nabil Bouamara', 'Sébastien Laurent', 'Shuping Shi']",econ.EM,"We introduce a simple tool to control for false discoveries and identify individual signals in scenarios involving many tests, dependent test statistics, and potentially sparse signals. The tool applies the Cauchy combination test recursively on a sequence of expanding subsets of $p$-values and is referred to as the sequential Cauchy combination test. While the original Cauchy combination test aims to make a global statement about a set of null hypotheses by summing transformed $p$-values, our sequential version determines which $p$-values trigger the rejection of the global null. The sequential test achieves strong familywise error rate control, exhibits less conservatism compared to existing controlling procedures when dealing with dependent test statistics, and provides a power boost. As illustrations, we revisit two well-known large-scale multiple testing problems in finance for which the test statistics have either serial dependence or cross-sectional dependence, namely monitoring drift bursts in asset prices and searching for assets with a nonzero alpha. In both applications, the sequential Cauchy combination test proves to be a preferable alternative. It overcomes many of the drawbacks inherent to inequality-based controlling procedures, extreme value approaches, resampling and screening methods, and it improves the power in simulations, leading to distinct empirical outcomes."
http://arxiv.org/abs/2303.17130v1,Entrepreneurial Capability And Engagement Of Persons With Disabilities Toward A Framework For Inclusive Entrepreneurship,2023-03-30 03:26:36+00:00,['Xavier Lawrence D. Mendoza'],econ.GN,"The study was designed to determine the entrepreneurial capability and engagement of persons with disabilities toward a framework for inclusive entrepreneurship. The researcher used descriptive and correlational approaches through purposive random sampling. The sample came from the City of General Trias and the Municipality of Rosario, registered under their respective Persons with Disabilities Affairs Offices (PDAO). The findings indicated that the respondents are from the working class, are primarily female, are mostly single, have college degrees, live in a medium-sized home, and earn the bare minimum. Furthermore, PWDs' perceived capability level in entrepreneurship was somehow capable, and the majority of engagement level responses were somehow engaged. Considerably, age and civil status have significant relationships with most of the variables under study. Finally, the perceived challenges of PWDs' respondents noted the following: lack of financial capacity, access to credit and other financial institutions, absence of business information, absence of access to data, lack of competent business skills, lack of family support, and lack of personal motivation. As a result, the author proposed a framework that emphasizes interaction and cooperation between national and local government units in the formulation of policies promoting inclusive entrepreneurship for people with disabilities."
http://arxiv.org/abs/2302.14602v1,On the Estimation of Cross-Firm Productivity Spillovers with an Application to FDI,2023-02-26 22:52:40+00:00,"['Emir Malikov', 'Shunan Zhao']",econ.GN,"We develop a novel methodology for the proxy variable identification of firm productivity in the presence of productivity-modifying learning and spillovers which facilitates a unified ""internally consistent"" analysis of the spillover effects between firms. Contrary to the popular two-step empirical approach, ours does not postulate contradictory assumptions about firm productivity across the estimation steps. Instead, we explicitly accommodate cross-sectional dependence in productivity induced by spillovers which facilitates identification of both the productivity and spillover effects therein simultaneously. We apply our model to study cross-firm spillovers in China's electric machinery manufacturing, with a particular focus on productivity effects of inbound FDI."
http://arxiv.org/abs/2302.13427v1,Detecting Learning by Exporting and from Exporters,2023-02-26 22:31:39+00:00,"['Jingfang Zhang', 'Emir Malikov']",econ.GN,"Existing literature at the nexus of firm productivity and export behavior mostly focuses on ""learning by exporting,"" whereby firms can improve their performance by engaging in exports. Whereas, the secondary channel of learning via cross-firm spillovers from exporting peers, or ""learning from exporters,"" has largely been neglected. Omitting this important mechanism, which can benefit both exporters and non-exporters, may provide an incomplete assessment of the total productivity benefits of exporting. In this paper, we develop a unified empirical framework for productivity measurement that explicitly accommodates both channels. To do this, we formalize the evolution of firm productivity as an export-controlled process, allowing future productivity to be affected by both the firm's own export behavior as well as export behavior of spatially proximate, same-industry peers. This facilitates a simultaneous, ""internally consistent"" identification of firm productivity and the corresponding effects of exporting. We apply our methodology to a panel of manufacturing plants in Chile in 1995-2007 and find significant evidence in support of both direct and spillover effects of exporting that substantially boost the productivity of domestic firms."
http://arxiv.org/abs/2303.10818v1,A Re-Examination of the Foundations of Cost of Capital for Regulatory Purposes,2023-03-20 01:22:28+00:00,['Darryl Biggar'],econ.GN,"In regulatory proceedings, few issues are more hotly debated than the cost of capital. This article formalises the theoretical foundation of cost of capital estimation for regulatory purposes. Several common regulatory practices lack a solid foundation in the theory. For example, the common practice of estimating a single cost of capital for the regulated firm suffers from a circularity problem, especially in the context of a multi-year regulatory period. In addition, the relevant cost of debt cannot be estimated using the yield-to-maturity on a corporate bond. We suggest possible directions for reform of cost of capital practices in regulatory proceedings."
http://arxiv.org/abs/2303.10835v1,Bifurcation analysis of the Keynesian cross model,2023-03-20 02:41:16+00:00,['Xinyu Li'],econ.GN,"This study rigorously investigates the Keynesian cross model of a national economy with a focus on the dynamic relationship between government spending and economic equilibrium. The model consists of two ordinary differential equations regarding the rate of change of national income and the rate of consumer spending. Three dynamic relationships between national income and government spending are studied. This study aims to classify the stabilities of equilibrium states for the economy by discussing different cases of government spending. Furthermore, the implication of government spending on the national economy is investigated based on phase portraits and bifurcation analysis of the dynamical system in each scenario."
http://arxiv.org/abs/2303.10684v1,School-based malaria chemoprevention as a cost-effective approach to improve cognitive and educational outcomes: a meta-analysis,2023-03-19 15:07:25+00:00,"['Noam Angrist', 'Matthew C. H. Jukes', 'Sian Clarke', 'R. Matthew Chico', 'Charles Opondo', 'Donald Bundy', 'Lauren M. Cohee']",econ.GN,"There is limited evidence of health interventions impact on cognitive function and educational outcomes. We build on two prior systematic reviews to conduct a meta-analysis, exploring the effects of one of the most consequential health interventions, malaria chemoprevention, on education outcomes. We pool data from nine study treatment groups (N=4,075) and outcomes across four countries. We find evidence of a positive effect (Cohen's d = 0.12, 95% CI [0.08, 0.16]) on student cognitive function, achieved at low cost. These results show that malaria chemoprevention can be highly cost effective in improving some cognitive skills, such as sustained attention. Moreover, we conduct simulations using a new common metric (learning-adjusted years of development) to compare cost-effectiveness across diverse interventions. While we might expect that traditional education interventions provide an immediate learning gain, health interventions such as malaria prevention can have surprisingly cost-effective education benefits, enabling children to achieve their full human capital potential."
http://arxiv.org/abs/2301.09173v1,Labor Income Risk and the Cross-Section of Expected Returns,2023-01-22 18:18:11+00:00,['Mykola Pinchuk'],q-fin.PR,"This paper explores asset pricing implications of unemployment risk from sectoral shifts. I proxy for this risk using cross-industry dispersion (CID), defined as a mean absolute deviation of returns of 49 industry portfolios. CID peaks during periods of accelerated sectoral reallocation and heightened uncertainty. I find that expected stock returns are related cross-sectionally to the sensitivities of returns to innovations in CID. Annualized returns of the stocks with high sensitivity to CID are 5.9% lower than the returns of the stocks with low sensitivity. Abnormal returns with respect to the best factor model are 3.5%, suggesting that common factors can not explain this return spread. Stocks with high sensitivity to CID are likely to be the stocks, which benefited from sectoral shifts. CID positively predicts unemployment through its long-term component, consistent with the hypothesis that CID is a proxy for unemployment risk from sectoral shifts."
http://arxiv.org/abs/2301.08666v2,Haves and Have-Nots: A Theory of Economic Sufficientarianism,2023-01-20 16:35:44+00:00,"['Christopher P. Chambers', 'Siming Ye']",econ.TH,"We introduce a generalization of the concept of sufficientarianism, intended to rank allocations involving multiple consumption goods. In ranking allocations of goods for a fixed society of agents, sufficientarianism posits that allocations are compared according to the number of individuals whose consumption is deemed sufficient. We base our analysis on a novel ethical concept, which we term sufficientarian judgment. Sufficientarian judgment asserts that if in starting from an allocation in which all agents have identical consumption, a change in one agent's consumption hurts society, then there is no change in any other agent's consumption which could subsequently benefit society. Sufficientarianism is shown to be equivalent to sufficientarian judgment, symmetry, and separability. We investigate our axioms in an abstract environment, and in specific economic environments. Finally, we argue formally that sufficientarian judgment is closely related to the leximin principle."
http://arxiv.org/abs/2301.10944v1,A Framework of Transaction Packaging in High-throughput Blockchains,2023-01-26 05:31:24+00:00,"['Yuxuan Lu', 'Qian Qi', 'Xi Chen']",econ.GN,"We develop a model of coordination and allocation of decentralized multi-sided markets, in which our theoretical analysis is promisingly optimizing the decentralized transaction packaging process at high-throughput blockchains or Web 3.0 platforms. In contrast to the stylized centralized platform, the decentralized platform is powered by blockchain technology, which allows for secure and transparent Peer-to-Peer transactions among users. Traditional single-chain-based blockchains suffer from the well-known blockchain trilemma. Beyond the single-chain-based scheme, decentralized high-throughput blockchains adopt parallel protocols to reconcile the blockchain trilemma, implementing any tasking and desired allocation. However, unneglectable network latency may induce partial observability, resulting in incoordination and misallocation issues for the decentralized transaction packaging process at the current high-throughput blockchain protocols.
  To address this problem, we consider a strategic coordination mechanism for the decentralized transaction packaging process by using a game-theoretic approach. Under a tractable two-period model, we find a Bayesian Nash equilibrium of the miner's strategic transaction packaging under partial observability. Along with novel algorithms for computing equilibrium payoffs, we show that the decentralized platform can achieve an efficient and stable market outcome. The model also highlights that the proposed mechanism can endogenously offer a base fee per gas without any restructuration of the initial blockchain transaction fee mechanism. The theoretical results that underlie the algorithms also imply bounds on the computational complexity of equilibrium payoffs."
http://arxiv.org/abs/2302.11376v1,Institutional reforms and the employment effects of spatially targeted investment grants: The case of Germany's GRW,2023-02-22 13:49:51+00:00,"['Björn Alecke', 'Timo Mitze']",econ.GN,"Spatially targeted investment grant schemes are a common tool to support firms in lagging regions. We exploit exogenous variations in Germany's main regional policy instrument (GRW) arriving from institutional reforms to analyse local employment effects of investment grants. Findings for reduced-form and IV regressions point to a significant policy channel running from higher funding rates to increased firm-level investments and newly created jobs. When we contrast effects for regions with high but declining funding rates to those with low but rising rates, we find that GRW reforms led to diminishing employment increases. Especially small firms responded to changing funding conditions."
http://arxiv.org/abs/2302.11729v2,Factor Exposure Heterogeneity in Green and Brown Stocks,2023-02-23 01:28:38+00:00,"['David Ardia', 'Keven Bluteau', 'Gabriel Lortie-Cloutier', 'Thien-Duy Tran']",econ.GN,"Using the peer-exposure ratio, we explore the factor exposure heterogeneity in green and brown stocks. By looking at peer groups of S&P 500 index firms over 2014-2020 based on their greenhouse gas emission levels, we find that, on average, green stocks exhibit less factor exposure heterogeneity than brown stocks for most of the traditional equity factors but the value factor. Hence, investment managers shifting their investments from brown stocks to green stocks have less room to differentiate themselves regarding their factor exposures. Finally, we find that factor exposure heterogeneity has increased for green stocks compared to earlier periods."
http://arxiv.org/abs/2303.08460v2,Identifying an Earnings Process With Dependent Contemporaneous Income Shocks,2023-03-15 09:04:10+00:00,['Dan Ben-Moshe'],econ.EM,"This paper proposes a novel approach for identifying coefficients in an earnings dynamics model with arbitrarily dependent contemporaneous income shocks. Traditional methods relying on second moments fail to identify these coefficients, emphasizing the need for nongaussianity assumptions that capture information from higher moments. Our results contribute to the literature on earnings dynamics by allowing models of earnings to have, for example, the permanent income shock of a job change to be linked to the contemporaneous transitory income shock of a relocation bonus."
http://arxiv.org/abs/2303.09399v1,"Main Concepts and Principles of Political Economy -- Production and Values, Distribution and Prices, Reproduction and Profits",2023-03-13 10:25:50+00:00,['Christian Flamant'],econ.GN,"This book starts from the basic questions that had been raised by the founders of Economic theory, Smith, Ricardo, and Marx: what makes the value of commodities, what are production, exchange, money and incomes like profits, wages and rents. The answers that these economists had provided were mostly wrong, above all by defining the equivalence of commodities at the level of exchange, but also because of a confusion made between values and prices, and wrong views of what production really is and the role of fixed capital. Using the mathematical theory of measurement and the physical theory of dimensional analysis, this book provides a coherent theory of value based on an equivalence relation not at the level of exchange, but of production. Indeed exchange is considered here as an equivalence relation between money and a monetary price, and not between commodities, modern monetary theory having demonstrated that money is not a commodity. The book rejects the conception of production as a surplus, which owes much to Sraffa's theory of production prices, and is shown to be severely flawed. It founds the equivalence of commodities at the level of a production process considered as a transformation process. It rehabilitates the labor theory of value, based on the connection between money and labor due the monetary payment of wages, which allows the homogenization of various kinds of concrete labor into abstract labor. It shows that value is then a dimension of commodities and that this dimension is time, i.e. the time of physics. On this background, the book shows that the calculation of values for all commodities is always possible, even in the case of joint production, and that there cannot be any commodity residue left by this calculation. As a further step, this book provides a coherent theory of the realization of the product, which occurs in the circulation process. Using an idea - the widow's cruse - introduced by Keynes in his Treatise on Money, it brings to light the mechanism behind the transformation of money values into money prices and of surplus-value into profits and other transfer incomes, ensuring the formation of monetary profits. The book sheds some light on the rate of profit, its determinants and its evolution, showing in particular the paramount importance of capitalist consumption as one of its main determinants. In passing it explains the reasons why in the real world there is a multiplicity of profit rates. Finally, it allows to solve in a precise and illustrated way the problems raised by the Marxist law of the tendency of the rate of profit to fall. Most of the results obtained translate into principles, the first ones being truly basic, the following ones less basic, but all of them being fundamental. All in all, this book might provide the first building blocks to develop a full-fledged and scientific economic theory to many fellow economists, critical of neo-classical theory, but who have not yet dicovered the bases of a complete and coherent alternative."
http://arxiv.org/abs/2303.09652v4,On Using Proportional Representation Methods as Alternatives to Pro-Rata Based Order Matching Algorithms in Stock Exchanges,2023-03-16 21:07:45+00:00,"['Sanjay Bhattacherjee', 'Palash Sarkar']",econ.GN,"The first observation of the paper is that methods for determining proportional representation in electoral systems may be suitable as alternatives to the pro-rata order matching algorithm used in stock exchanges. The main part of our work is to comprehensively consider various well known proportional representation methods and analyse in details their suitability for replacing the pro-rata algorithm. Our analysis consists of a theoretical study as well as simulation studies based on data sampled from a distribution which has been suggested in the literature as models of limit orders. Based on our analysis, we put forward the suggestion that the well known Hamilton's method is a superior alternative to the pro-rata algorithm for order matching applications."
http://arxiv.org/abs/2303.09675v1,Dynamic Information Provision: Rewarding the Past and Guiding the Future,2023-03-16 22:41:30+00:00,['Ian Ball'],econ.TH,"I study the optimal provision of information in a long-term relationship between a sender and a receiver. The sender observes a persistent, evolving state and commits to send signals over time to the receiver, who sequentially chooses public actions that affect the welfare of both players. I solve for the sender's optimal policy in closed form: the sender reports the value of the state with a delay that shrinks over time and eventually vanishes. Even when the receiver knows the current state, the sender retains leverage by threatening to conceal the future evolution of the state."
http://arxiv.org/abs/2303.10117v2,Estimation of Grouped Time-Varying Network Vector Autoregression Models,2023-03-17 16:57:29+00:00,"['Degui Li', 'Bin Peng', 'Songqiao Tang', 'Weibiao Wu']",stat.ME,"This paper introduces a flexible time-varying network vector autoregressive model framework for large-scale time series. A latent group structure is imposed on the heterogeneous and node-specific time-varying momentum and network spillover effects so that the number of unknown time-varying coefficients to be estimated can be reduced considerably. A classic agglomerative clustering algorithm with nonparametrically estimated distance matrix is combined with a ratio criterion to consistently estimate the latent group number and membership. A post-grouping local linear smoothing method is proposed to estimate the group-specific time-varying momentum and network effects, substantially improving the convergence rates of the preliminary estimates which ignore the latent structure. We further modify the methodology and theory to allow for structural breaks in either the group membership, group number or group-specific coefficient functions. Numerical studies including Monte-Carlo simulation and an empirical application are presented to examine the finite-sample performance of the developed model and methodology."
http://arxiv.org/abs/2303.10130v5,GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models,2023-03-17 17:15:20+00:00,"['Tyna Eloundou', 'Sam Manning', 'Pamela Mishkin', 'Daniel Rock']",econ.GN,"We investigate the potential implications of large language models (LLMs), such as Generative Pre-trained Transformers (GPTs), on the U.S. labor market, focusing on the increased capabilities arising from LLM-powered software compared to LLMs on their own. Using a new rubric, we assess occupations based on their alignment with LLM capabilities, integrating both human expertise and GPT-4 classifications. Our findings reveal that around 80% of the U.S. workforce could have at least 10% of their work tasks affected by the introduction of LLMs, while approximately 19% of workers may see at least 50% of their tasks impacted. We do not make predictions about the development or adoption timeline of such LLMs. The projected effects span all wage levels, with higher-income jobs potentially facing greater exposure to LLM capabilities and LLM-powered software. Significantly, these impacts are not restricted to industries with higher recent productivity growth. Our analysis suggests that, with access to an LLM, about 15% of all worker tasks in the US could be completed significantly faster at the same level of quality. When incorporating software and tooling built on top of LLMs, this share increases to between 47 and 56% of all tasks. This finding implies that LLM-powered software will have a substantial effect on scaling the economic impacts of the underlying models. We conclude that LLMs such as GPTs exhibit traits of general-purpose technologies, indicating that they could have considerable economic, social, and policy implications."
http://arxiv.org/abs/2303.07468v2,Distributionally Robust Principal-Agent Problems and Optimality of Contracts,2023-03-13 21:09:41+00:00,['Peter Zhang'],econ.TH,"We propose a distributionally robust principal agent formulation, which generalizes some common variants of worst-case and Bayesian principal agent problems. We construct a theoretical framework to certify whether any surjective contract family is optimal, and bound its sub-optimality. We then apply the framework to study the optimality of affine contracts. We show with geometric intuition that these simple contract families are optimal when the surplus function is convex and there exists a technology type that is simultaneously least productive and least efficient. We also provide succinct expressions to quantify the optimality gap of any surplus function, based on its concave biconjugate. This new framework complements the current literature in two ways: invention of a new toolset; understanding affine contracts' performance in a larger landscape. Our results also shed light on the technical roots of this question: why are there more positive results in the recent literature that show simple contracts' optimality in robust settings rather than stochastic settings? This phenomenon is related to two technical facts: the sum of quasi-concave functions is not quasi-concave, and the maximization and expectation operators do not commute."
http://arxiv.org/abs/2303.07786v1,A Commons-Compatible Implementation of the Sharing Economy: Blockchain-Based Open Source Mediation,2023-03-14 10:56:50+00:00,"['Petra Tschuchnig', 'Manfred Mayr', 'Maximilian Tschuchnig', 'Peter Haber']",econ.GN,"The network economical sharing economy, with direct exchange as a core characteristic, is implemented both, on a commons and platform economical basis. This is due to a gain in importance of trust, collaborative consumption and democratic management as well as technological progress, in the form of near zero marginal costs, open source contributions and digital transformation. Concurrent to these commons-based drivers, the grey area between commerce and private exchange is used to exploit work, safety and tax regulations by central platform economists. Instead of central intermediators, the blockchain technology makes decentralized consensus finding, using Proof-of-Work (PoW) within a self-sustaining Peer-to-Peer network, possible. Therefore, a blockchain-based open source mediation seems to offer a commons-compatible implementation of the sharing economy. This thesis is investigated through a qualitative case study of Sardex and Interlace with their blockchain application, based on expert interviews and a structured content analysis. To detect the most commons-compatible implementation, the different implementation options through conventional platform intermediators, an open source blockchain with PoW as well as Interlaces' permissioned blockchain approach, are compared. The following confrontation is based on deductive criteria, which illustrates the inherent characteristics of a commons-based sharing economy."
http://arxiv.org/abs/2303.04833v1,Finding Regularized Competitive Equilibria of Heterogeneous Agent Macroeconomic Models with Reinforcement Learning,2023-02-24 17:16:27+00:00,"['Ruitu Xu', 'Yifei Min', 'Tianhao Wang', 'Zhaoran Wang', 'Michael I. Jordan', 'Zhuoran Yang']",econ.GN,"We study a heterogeneous agent macroeconomic model with an infinite number of households and firms competing in a labor market. Each household earns income and engages in consumption at each time step while aiming to maximize a concave utility subject to the underlying market conditions. The households aim to find the optimal saving strategy that maximizes their discounted cumulative utility given the market condition, while the firms determine the market conditions through maximizing corporate profit based on the household population behavior. The model captures a wide range of applications in macroeconomic studies, and we propose a data-driven reinforcement learning framework that finds the regularized competitive equilibrium of the model. The proposed algorithm enjoys theoretical guarantees in converging to the equilibrium of the market at a sub-linear rate."
http://arxiv.org/abs/2301.05649v1,Filtering Down to Size: A Theory of Consideration,2023-01-13 16:43:18+00:00,['Tonna Emenuga'],econ.TH,"The standard rational choice model describes individuals as making choices by selecting the best option from a menu. A wealth of evidence instead suggests that individuals often filter menus into smaller sets - consideration sets - from which choices are then made. I provide a theoretical foundation for this phenomenon, developing a formal language of axioms to characterize how consideration sets are formed from menus. I posit that consideration filters - mappings that translate a menu into one of its subsets - capture this process, and I introduce several properties that consideration filters can have. I then extend this core model to provide linkages with the sequential choice and rational attention literatures. Finally, I explore whether utility representation is feasible under this consideration model, conjecturing necessary and sufficient conditions for consideration-mediated choices to be rationalizable."
http://arxiv.org/abs/2301.05703v2,Stable Probability Weighting: Large-Sample and Finite-Sample Estimation and Inference Methods for Heterogeneous Causal Effects of Multivalued Treatments Under Limited Overlap,2023-01-13 18:52:18+00:00,['Ganesh Karapakula'],econ.EM,"In this paper, I try to tame ""Basu's elephants"" (data with extreme selection on observables). I propose new practical large-sample and finite-sample methods for estimating and inferring heterogeneous causal effects (under unconfoundedness) in the empirically relevant context of limited overlap. I develop a general principle called ""Stable Probability Weighting"" (SPW) that can be used as an alternative to the widely used Inverse Probability Weighting (IPW) technique, which relies on strong overlap. I show that IPW (or its augmented version), when valid, is a special case of the more general SPW (or its doubly robust version), which adjusts for the extremeness of the conditional probabilities of the treatment states. The SPW principle can be implemented using several existing large-sample parametric, semiparametric, and nonparametric procedures for conditional moment models. In addition, I provide new finite-sample results that apply when unconfoundedness is plausible within fine strata. Since IPW estimation relies on the problematic reciprocal of the estimated propensity score, I develop a ""Finite-Sample Stable Probability Weighting"" (FPW) set-estimator that is unbiased in a sense. I also propose new finite-sample inference methods for testing a general class of weak null hypotheses. The associated computationally convenient methods, which can be used to construct valid confidence sets and to bound the finite-sample confidence distribution, are of independent interest. My large-sample and finite-sample frameworks extend to the setting of multivalued treatments."
http://arxiv.org/abs/2301.05798v2,Regulating For-Hire Autonomous Vehicles for An Equitable Multimodal Transportation Network,2023-01-14 00:56:01+00:00,"['Jing Gao', 'Sen Li']",math.OC,"This paper assesses the equity impacts of for-hire autonomous vehicles (AVs) and investigates regulatory policies that promote spatial and social equity in future autonomous mobility ecosystems. To this end, we consider a multimodal transportation network, where a ride-hailing platform operates a fleet of AVs to offer mobility-on-demand services in competition with a public transit agency that offers transit services on a transportation network. A game-theoretic model is developed to characterize the intimate interactions between the ride-hailing platform, the transit agency, and multiclass passengers with distinct income levels. An algorithm is proposed to compute the Nash equilibrium of the game and conduct an ex-post evaluation of the performance of the obtained solution. Based on the proposed framework, we evaluate the spatial and social equity in transport accessibility using the Theil index, and find that although the proliferation of for-hire AVs in the ride-hailing network improves overall accessibility, the benefits are not fairly distributed among distinct locations or population groups, implying that the deployment of AVs will enlarge the existing spatial and social inequity gaps in the transportation network if no regulatory intervention is in place. To address this concern, we investigate two regulatory policies that can improve transport equity: (a) a minimum service-level requirement on ride-hailing services, which improves the spatial equity in the transport network; (b) a subsidy on transit services by taxing ride-hailing services, which promotes the use of public transit and improves the spatial and social equity of the transport network. We show that the minimum service-level requirement entails a trade-off: as a higher minimum service level is imposed, the spatial inequity reduces, but the social inequity will be exacerbated. On the other hand ..."
http://arxiv.org/abs/2301.06658v1,Statistical inference for the logarithmic spatial heteroskedasticity model with exogenous variables,2023-01-17 01:48:14+00:00,"['Bing Su', 'Fukang Zhu', 'Ke Zhu']",econ.EM,"The spatial dependence in mean has been well studied by plenty of models in a large strand of literature, however, the investigation of spatial dependence in variance is lagging significantly behind. The existing models for the spatial dependence in variance are scarce, with neither probabilistic structure nor statistical inference procedure being explored. To circumvent this deficiency, this paper proposes a new generalized logarithmic spatial heteroscedasticity model with exogenous variables (denoted by the log-SHE model) to study the spatial dependence in variance. For the log-SHE model, its spatial near-epoch dependence (NED) property is investigated, and a systematic statistical inference procedure is provided, including the maximum likelihood and generalized method of moments estimators, the Wald, Lagrange multiplier and likelihood-ratio-type D tests for model parameter constraints, and the overidentification test for the model diagnostic checking. Using the tool of spatial NED, the asymptotics of all proposed estimators and tests are established under regular conditions. The usefulness of the proposed methodology is illustrated by simulation results and a real data example on the house selling price."
http://arxiv.org/abs/2301.05999v4,Common Subcontracting and Airline Prices,2023-01-15 02:09:45+00:00,"['Gaurab Aryal', 'Dennis J. Campbell', 'Federico Ciliberto', 'Ekaterina A. Khmelnitskaya']",econ.GN,"In the US airline industry, independent regional airlines fly passengers on behalf of several national airlines across different markets, giving rise to $\textit{common subcontracting}$. On the one hand, we find that subcontracting is associated with lower prices, consistent with the notion that regional airlines tend to fly passengers at lower costs than major airlines. On the other hand, we find that $\textit{common}$ subcontracting is associated with higher prices. These two countervailing effects suggest that the growth of regional airlines can have anticompetitive implications for the industry."
http://arxiv.org/abs/2301.07312v2,Auctions without commitment in the auto-bidding world,2023-01-18 05:23:17+00:00,"['Aranyak Mehta', 'Andres Perlroth']",econ.TH,"Advertisers in online ad auctions are increasingly using auto-bidding mechanisms to bid into auctions instead of directly bidding their value manually. One prominent auto-bidding format is the target cost-per-acquisition (tCPA) which maximizes the volume of conversions subject to a return-of-investment constraint. From an auction theoretic perspective however, this trend seems to go against foundational results that postulate that for profit-maximizing bidders, it is optimal to use a classic bidding system like marginal CPA (mCPA) bidding rather than using strategies like tCPA.
  In this paper we rationalize the adoption of such seemingly sub-optimal bidding within the canonical quasi-linear framework. The crux of the argument lies in the notion of commitment. We consider a multi-stage game where first the auctioneer declares the auction rules; then bidders select either the tCPA or mCPA bidding format and then, if the auctioneer lacks commitment, it can revisit the rules of the auction (e.g., may readjust reserve prices depending on the observed bids). Our main result is that so long as a bidder believes that the auctioneer lacks commitment to follow the rule of the declared auction then the bidder will make a higher profit by choosing the tCPA format over the mCPA format.
  We then explore the commitment consequences for the auctioneer. In a simplified version of the model where there is only one bidder, we show that the tCPA subgame admits a credible equilibrium while the mCPA format does not. That is, when the bidder chooses the tCPA format the auctioneer can credibly implement the auction rules announced at the beginning of the game. We also show that, under some mild conditions, the auctioneer's revenue is larger when the bidder uses the tCPA format rather than mCPA. We further quantify the value for the auctioneer to be able to commit to the declared auction rules."
http://arxiv.org/abs/2301.02937v1,Quantile Autoregression-based Non-causality Testing,2023-01-07 21:15:17+00:00,['Weifeng Jin'],econ.EM,"Non-causal processes have been drawing attention recently in Macroeconomics and Finance for their ability to display nonlinear behaviors such as asymmetric dynamics, clustering volatility, and local explosiveness. In this paper, we investigate the statistical properties of empirical conditional quantiles of non-causal processes. Specifically, we show that the quantile autoregression (QAR) estimates for non-causal processes do not remain constant across different quantiles in contrast to their causal counterparts. Furthermore, we demonstrate that non-causal autoregressive processes admit nonlinear representations for conditional quantiles given past observations. Exploiting these properties, we propose three novel testing strategies of non-causality for non-Gaussian processes within the QAR framework. The tests are constructed either by verifying the constancy of the slope coefficients or by applying a misspecification test of the linear QAR model over different quantiles of the process. Some numerical experiments are included to examine the finite sample performance of the testing strategies, where we compare different specification tests for dynamic quantiles with the Kolmogorov-Smirnov constancy test. The new methodology is applied to some time series from financial markets to investigate the presence of speculative bubbles. The extension of the approach based on the specification tests to AR processes driven by innovations with heteroskedasticity is studied through simulations. The performance of QAR estimates of non-causal processes at extreme quantiles is also explored."
http://arxiv.org/abs/2301.02052v3,Relaxing Instrument Exogeneity with Common Confounders,2023-01-05 12:59:57+00:00,['Christian Tien'],econ.EM,"Instruments can be used to identify causal effects in the presence of unobserved confounding, under the famous relevance and exogeneity (unconfoundedness and exclusion) assumptions. As exogeneity is difficult to justify and to some degree untestable, it often invites criticism in applications. Hoping to alleviate this problem, we propose a novel identification approach, which relaxes traditional IV exogeneity to exogeneity conditional on some unobserved common confounders. We assume there exist some relevant proxies for the unobserved common confounders. Unlike typical proxies, our proxies can have a direct effect on the endogenous regressor and the outcome. We provide point identification results with a linearly separable outcome model in the disturbance, and alternatively with strict monotonicity in the first stage. General doubly robust and Neyman orthogonal moments are derived consecutively to enable the straightforward root-n estimation of low-dimensional parameters despite the high-dimensionality of nuisances, themselves non-uniquely defined by Fredholm integral equations. Using this novel method with NLS97 data, we separate ability bias from general selection bias in the economic returns to education problem."
http://arxiv.org/abs/2301.02575v1,"Cognitive Endurance, Talent Selection, and the Labor Market Returns to Human Capital",2023-01-06 16:08:35+00:00,['Germán Reyes'],econ.GN,"Cognitive endurance -- the ability to sustain performance on a cognitively-demanding task over time -- is thought to be a crucial productivity determinant. However, a lack of data on this variable has limited researchers' ability to understand its role for success in college and the labor market. This paper uses college-admission-exam records from 15 million Brazilian high school students to measure cognitive endurance based on changes in performance throughout the exam. By exploiting exogenous variation in the order of exam questions, I show that students are 7.1 percentage points more likely to correctly answer a given question when it appears at the beginning of the day versus the end (relative to a sample mean of 34.3%). I develop a method to decompose test scores into fatigue-adjusted ability and cognitive endurance. I then merge these measures into a higher-education census and the earnings records of the universe of Brazilian formal-sector workers to quantify the association between endurance and long-run outcomes. I find that cognitive endurance has a statistically and economically significant wage return. Controlling for fatigue-adjusted ability and other student characteristics, a one-standard-deviation higher endurance predicts a 5.4% wage increase. This wage return to endurance is sizable, equivalent to a third of the wage return to ability. I also document positive associations between endurance and college attendance, college quality, college graduation, firm quality, and other outcomes. Finally, I show how systematic differences in endurance across students interact with the exam design to determine the sorting of students to colleges. I discuss the implications of these findings for the use of cognitive assessments for talent selection and investments in interventions that build cognitive endurance."
http://arxiv.org/abs/2301.04439v1,Uniform Inference in Linear Error-in-Variables Models: Divide-and-Conquer,2023-01-11 12:46:09+00:00,"['Tom Boot', 'Artūras Juodis']",econ.EM,"It is customary to estimate error-in-variables models using higher-order moments of observables. This moments-based estimator is consistent only when the coefficient of the latent regressor is assumed to be non-zero. We develop a new estimator based on the divide-and-conquer principle that is consistent for any value of the coefficient of the latent regressor. In an application on the relation between investment, (mismeasured) Tobin's $q$ and cash flow, we find time periods in which the effect of Tobin's $q$ is not statistically different from zero. The implausibly large higher-order moment estimates in these periods disappear when using the proposed estimator."
http://arxiv.org/abs/2301.01007v1,A Bertrand duopoly game with differentiated products reconsidered,2023-01-03 08:56:59+00:00,"['Xiaoliang Li', 'Bo Li']",econ.TH,"In this paper, we explore a dynamic Bertrand duopoly game with differentiated products, where firms are boundedly rational and consumers are assumed to possess an underlying CES utility function. We mainly focus on two distinct degrees of product substitutability. Several tools based on symbolic computations such as the triangular decomposition method and the PCAD method are employed in the analytical investigation of the model. The uniqueness of the non-vanishing equilibrium is proved and rigorous conditions for the local stability of this equilibrium are established for the first time. Most importantly, we find that increasing the substitutability degree or decreasing the product differentiation has an effect of destabilization for our Bertrand model, which is in contrast with the relative conclusions for the Cournot models. This finding could be conducive to the revelation of the essential difference between dynamic Cournot and Bertrand oligopolies with differentiated goods. In the special case of identical marginal costs, we derive that lower degrees of product differentiation mean lower prices, higher supplies, lower profits, and lower social welfare. Furthermore, complex dynamics such as periodic orbits and chaos are reported through our numerical simulations."
http://arxiv.org/abs/2301.00509v1,Time-Varying Coefficient DAR Model and Stability Measures for Stablecoin Prices: An Application to Tether,2023-01-02 03:07:09+00:00,"['Antoine Djobenou', 'Emre Inan', 'Joann Jasiak']",econ.EM,"This paper examines the dynamics of Tether, the stablecoin with the largest market capitalization. We show that the distributional and dynamic properties of Tether/USD rates have been evolving from 2017 to 2021. We use local analysis methods to detect and describe the local patterns, such as short-lived trends, time-varying volatility and persistence. To accommodate these patterns, we consider a time varying parameter Double Autoregressive tvDAR(1) model under the assumption of local stationarity of Tether/USD rates. We estimate the tvDAR model non-parametrically and test hypotheses on the functional parameters. In the application to Tether, the model provides a good fit and reliable out-of-sample forecasts at short horizons, while being robust to time-varying persistence and volatility. In addition, the model yields a simple plug-in measure of stability for Tether and other stablecoins for assessing and comparing their stability."
http://arxiv.org/abs/2301.13321v2,Censorship Resistance in On-Chain Auctions,2023-01-30 23:05:01+00:00,"['Elijah Fox', 'Mallesh Pai', 'Max Resnick']",econ.TH,"Modern blockchains guarantee that submitted transactions will be included eventually; a property formally known as liveness. But financial activity requires transactions to be included in a timely manner. Unfortunately, classical liveness is not strong enough to guarantee this, particularly in the presence of a motivated adversary who benefits from censoring transactions. We define censorship resistance as the amount it would cost the adversary to censor a transaction for a fixed interval of time as a function of the associated tip. This definition has two advantages, first it captures the fact that transactions with a higher miner tip can be more costly to censor, and therefore are more likely to swiftly make their way onto the chain. Second, it applies to a finite time window, so it can be used to assess whether a blockchain is capable of hosting financial activity that relies on timely inclusion.
  We apply this definition in the context of auctions. Auctions are a building block for many financial applications, and censoring competing bids offers an easy-to-model motivation for our adversary. Traditional proof-of-stake blockchains have poor enough censorship resistance that it is difficult to retain the integrity of an auction when bids can only be submitted in a single block. As the number of bidders $n$ in a single block auction increases, the probability that the winner is not the adversary, and the economic efficiency of the auction, both decrease faster than $1/n$. Running the auction over multiple blocks, each with a different proposer, alleviates the problem only if the number of blocks grows faster than the number of bidders. We argue that blockchains with more than one concurrent proposer have can have strong censorship resistance. We achieve this by setting up a prisoner's dilemma among the proposers using conditional tips."
http://arxiv.org/abs/2302.10490v1,Creating Disasters: Recession Forecasting with GAN-Generated Synthetic Time Series Data,2023-02-21 07:38:46+00:00,['Sam Dannels'],cs.LG,"A common problem when forecasting rare events, such as recessions, is limited data availability. Recent advancements in deep learning and generative adversarial networks (GANs) make it possible to produce high-fidelity synthetic data in large quantities. This paper uses a model called DoppelGANger, a GAN tailored to producing synthetic time series data, to generate synthetic Treasury yield time series and associated recession indicators. It is then shown that short-range forecasting performance for Treasury yields is improved for models trained on synthetic data relative to models trained only on real data. Finally, synthetic recession conditions are produced and used to train classification models to predict the probability of a future recession. It is shown that training models on synthetic recessions can improve a model's ability to predict future recessions over a model trained only on real data."
http://arxiv.org/abs/2302.10026v4,What is essential is visible to the eye: Saliency in primary school ranking and its effect on academic achievements,2023-02-20 15:17:08+00:00,"['Francois-Xavier Ladant', 'Julien Hedou', 'Paolo Sestito', 'Falco J. Bargagli-Stoffi']",econ.GN,"We propose a new strategy to identify the impact of class rank, exploiting a ""visible"" primary school rank from teachers' exam grades, and an ""invisible"" rank from unreported standardized test scores. Leveraging a unique panel dataset on Italian students, we show that the visible rank has a substantial impact on students' perceptions, which affects subsequent academic performance. However, the effect of being surrounded by higher-SES or higher-achieving peers remains positive even accounting for the decrease in rank. Higher-ranked students self-select into high schools with higher average student achievements. Finally, exploiting an extensive survey, we identify psychological mechanisms channeling the rank effect."
http://arxiv.org/abs/2302.09871v1,Attitudes and Latent Class Choice Models using Machine learning,2023-02-20 10:03:01+00:00,"['Lorena Torres Lahoz', 'Francisco Camara Pereira', 'Georges Sfeir', 'Ioanna Arkoudi', 'Mayara Moraes Monteiro', 'Carlos Lima Azevedo']",econ.EM,"Latent Class Choice Models (LCCM) are extensions of discrete choice models (DCMs) that capture unobserved heterogeneity in the choice process by segmenting the population based on the assumption of preference similarities. We present a method of efficiently incorporating attitudinal indicators in the specification of LCCM, by introducing Artificial Neural Networks (ANN) to formulate latent variables constructs. This formulation overcomes structural equations in its capability of exploring the relationship between the attitudinal indicators and the decision choice, given the Machine Learning (ML) flexibility and power in capturing unobserved and complex behavioural features, such as attitudes and beliefs. All of this while still maintaining the consistency of the theoretical assumptions presented in the Generalized Random Utility model and the interpretability of the estimated parameters. We test our proposed framework for estimating a Car-Sharing (CS) service subscription choice with stated preference data from Copenhagen, Denmark. The results show that our proposed approach provides a complete and realistic segmentation, which helps design better policies."
http://arxiv.org/abs/2302.09916v1,Goal oriented indicators for food systems based on FAIR data,2023-02-20 11:20:44+00:00,['Ronit Purian'],cs.CY,"Throughout the food supply chain, between production, transportation, packaging, and green employment, a plethora of indicators cover the environmental footprint and resource use. By defining and tracking the more inefficient practices of the food supply chain and their effects, we can better understand how to improve agricultural performance, track nutrition values, and focus on the reduction of a major risk to the environment while contributing to food security. Our aim is to propose a framework for a food supply chain, devoted to the vision of zero waste and zero emissions, and at the same time, fulfilling the broad commitment on inclusive green economy within the climate action. To set the groundwork for a smart city solution which achieves this vision, main indicators and evaluation frameworks are introduced, followed by the drill down into most crucial problems, both globally and locally, in a case study in north Italy. Methane is on the rise in the climate agenda, and specifically in Italy emission mitigation is difficult to achieve in the farming sector. Accordingly, going from the generic frameworks towards a federation deployment, we provide the reasoning for a cost-effective use case in the domain of food, to create a valuable digital twin. A Bayesian approach to assess use cases and select preferred scenarios is proposed, realizing the potential of the digital twin flexibility with FAIR data, while understanding and acting to achieve environmental and social goals, i.e., coping uncertainties, and combining green employment and food security. The proposed framework can be adjusted to organizational, financial, and political considerations in different locations worldwide, rethinking the value of information in the context of FAIR data in digital twins."
http://arxiv.org/abs/2302.09438v1,Does Machine Learning Amplify Pricing Errors in the Housing Market? -- The Economics of Machine Learning Feedback Loops,2023-02-18 23:20:57+00:00,"['Nikhil Malik', 'Emaad Manzoor']",econ.TH,"Machine learning algorithms are increasingly employed to price or value homes for sale, properties for rent, rides for hire, and various other goods and services. Machine learning-based prices are typically generated by complex algorithms trained on historical sales data. However, displaying these prices to consumers anchors the realized sales prices, which will in turn become training samples for future iterations of the algorithms. The economic implications of this machine learning ""feedback loop"" - an indirect human-algorithm interaction - remain relatively unexplored. In this work, we develop an analytical model of machine learning feedback loops in the context of the housing market. We show that feedback loops lead machine learning algorithms to become overconfident in their own accuracy (by underestimating its error), and leads home sellers to over-rely on possibly erroneous algorithmic prices. As a consequence at the feedback loop equilibrium, sale prices can become entirely erratic (relative to true consumer preferences in absence of ML price interference). We then identify conditions (choice of ML models, seller characteristics and market characteristics) where the economic payoffs for home sellers at the feedback loop equilibrium is worse off than no machine learning. We also empirically validate primitive building blocks of our analytical model using housing market data from Zillow. We conclude by prescribing algorithmic corrective strategies to mitigate the effects of machine learning feedback loops, discuss the incentives for platforms to adopt these strategies, and discuss the role of policymakers in regulating the same."
http://arxiv.org/abs/2302.09537v1,The Globalization-Inequality Nexus: A Comparative Study of Developed and Developing Countries,2023-02-19 11:08:38+00:00,['Md Shah Naoaj'],econ.GN,"This study examines the relationship between globalization and income inequality, utilizing panel data spanning from 1992 to 2020. Globalization is measured by the World Bank global-link indicators such as FDI, Remittance, Trade Openness, and Migration while income inequality is measured by Gini Coefficient and the median income of 50% of the population. The fixed effect panel data analysis provides empirical evidence indicating that globalization tends to reduce income inequality, though its impact varies between developed and developing countries. The analysis reveals a strong negative correlation between net foreign direct investment (FDI) inflows and inequality in developing countries, while no such relationship was found for developed countries.The relationship holds even if we consider an alternative measure of inequality. However, when dividing countries by developed and developing groups, no statistically significant relationship was observed. Policymakers can use these findings to support efforts to increase FDI, trade, tourism, and migration to promote growth and reduce income inequality."
http://arxiv.org/abs/2302.09548v3,Legitimacy of collective decisions: a mechanism design approach,2023-02-19 11:58:11+00:00,"['Kirneva Margarita', 'Núñez Matías']",econ.TH,"We design two mechanisms that ensure that the majority preferred option wins in all equilibria. The first one is a simultaneous game where agents choose other agents to cooperate with on top of the vote for an alternative, thus overcoming recent impossibility results concerning the implementation of majority rule. The second one adds sequential ratification to the standard majority voting procedure allowing to reach the (correct) outcome in significantly fewer steps than the widely used roll call voting. Both mechanisms use off-equilibrium lotteries to incentivize truthful voting. We discuss different extensions, including the possibility for agents to abstain."
http://arxiv.org/abs/2302.08987v3,Firm-level supply chains to minimize unemployment and economic losses in rapid decarbonization scenarios,2023-02-17 16:44:06+00:00,"['Johannes Stangl', 'András Borsos', 'Christian Diem', 'Tobias Reisch', 'Stefan Thurner']",econ.GN,"Urgently needed carbon emissions reductions might lead to strict command-and-control decarbonization strategies with potentially negative economic consequences. Analysing the entire firm-level production network of a European economy, we have explored how the worst outcomes of such approaches can be avoided. We compared the systemic relevance of every firm in Hungary with its annual CO2 emissions to identify optimal emission-reducing strategies with a minimum of additional unemployment and economic losses. Setting specific reduction targets, we studied various decarbonization scenarios and quantified their economic consequences. We determined that for an emissions reduction of 20%, the most effective strategy leads to losses of about 2% of jobs and 2% of economic output. In contrast, a naive scenario targeting the largest emitters first results in 28% job losses and 33% output reduction for the same target. This demonstrates that it is possible to use firm-level production networks to design highly effective decarbonization strategies that practically preserve employment and economic output."
http://arxiv.org/abs/2302.09131v1,Nash equilibrium selection by eigenvalue control,2023-02-17 20:46:32+00:00,['Wang Zhijian'],econ.TH,"People choose their strategies through a trial-and-error learning process in which they gradually discover that some strategies work better than others. The process can be modelled as an evolutionary game dynamics system, which may be controllable. In modern control theory, eigenvalue (pole) assignment is a basic approach to designing a full-state feedback controller, which can influence the outcome of a game. This study shows that, in a game with two Nash equilibria, the long-running strategy distribution can be controlled by pole assignment. We illustrate a theoretical workflow to design and evaluate the controller. To our knowledge, this is the first realisation of the control of equilibrium selection by design in the game dynamics theory paradigm. We hope the controller can be verified in a laboratory human subject game experiment."
http://arxiv.org/abs/2301.11207v1,Inflation targeting strategy and its credibility,2023-01-26 16:33:08+00:00,['Carlos Esteban Posada'],econ.GN,"The money supply is endogenous if the monetary policy strategy is the so called Inflation and Interest Rate Targeting, IRT. With that and perfect credibility, the theory of the price level and inflation only needs the Fisher equation, but it interprets causality in a new sense: if the monetary authority raises the policy rate, it will raise the inflation target, and vice versa, given the natural interest rate. If credibility is not perfect or if expectations are not completely rational, the theory needs something more. Here I present a model corresponding to this theory that includes both the steady state case and the recovery dynamics after a supply shock, with and without policy reactions to such a shock. But, under the finite horizon assumption for IRT, at some future point in time the money supply must become exogenous. This creates the incentive for agents to examine, as of today, statistics on monetary aggregates and form their forecasts of money supply growth and inflation rates. Additionally, inflation models of the small open economy allow us to deduce that the IRT in this case is much more powerful than otherwise, and for the same degree of credibility. But things are not necessarily easier for the monetary authority: it must monitor not only internal indicators, but also external inflation and its determinants, and it must, in certain circumstances, make more intense adjustments to the interest rate."
http://arxiv.org/abs/2301.11237v3,The Hazards and Benefits of Condescension in Social Learning,2023-01-26 17:19:42+00:00,"['Itai Arieli', 'Yakov Babichenko', 'Stephan Müller', 'Farzad Pourbabaee', 'Omer Tamuz']",econ.TH,"In a misspecified social learning setting, agents are condescending if they perceive their peers as having private information that is of lower quality than it is in reality. Applying this to a standard sequential model, we show that outcomes improve when agents are mildly condescending. In contrast, too much condescension leads to worse outcomes, as does anti-condescension."
http://arxiv.org/abs/2302.02747v2,Testing Quantile Forecast Optimality,2023-02-06 12:47:34+00:00,"['Jack Fosten', 'Daniel Gutknecht', 'Marc-Oliver Pohle']",econ.EM,"Quantile forecasts made across multiple horizons have become an important output of many financial institutions, central banks and international organisations. This paper proposes misspecification tests for such quantile forecasts that assess optimality over a set of multiple forecast horizons and/or quantiles. The tests build on multiple Mincer-Zarnowitz quantile regressions cast in a moment equality framework. Our main test is for the null hypothesis of autocalibration, a concept which assesses optimality with respect to the information contained in the forecasts themselves. We provide an extension that allows to test for optimality with respect to larger information sets and a multivariate extension. Importantly, our tests do not just inform about general violations of optimality, but may also provide useful insights into specific forms of sub-optimality. A simulation study investigates the finite sample performance of our tests, and two empirical applications to financial returns and U.S. macroeconomic series illustrate that our tests can yield interesting insights into quantile forecast sub-optimality and its causes."
http://arxiv.org/abs/2302.02762v1,Does higher capital maintenance drive up banks cost of equity? Evidence from Bangladesh,2023-01-30 19:33:26+00:00,"['Md Shah Naoaj', 'Mir Md Moyazzem Hosen']",econ.GN,This paper assesses whether the higher capital maintenance drives up banks cost of equity. We investigate the hypothesis using fixed effect panel estimation with the data from a sample of 28 publicly listed commercial banks over the 2013 to 2019 periods. We find a significant negative relationship between banks capital and cost of equity. Empirically our baseline estimates entail that a 10 percent increase in capital would reduce the cost of equity by 4.39 percent.
http://arxiv.org/abs/2302.02767v2,Being at the core: firm product specialisation,2023-02-06 13:26:14+00:00,"['Filippo Bontadini', 'Mercedes Campi', 'Marco Dueñas']",econ.GN,"We propose a novel measure to investigate firms' product specialisation: product coreness, that captures the centrality of exported products within the firm's export basket. We study product coreness using firm-product level data between 2018 and 2020 for Colombia, Ecuador, and Peru. Three main findings emerge from our analysis. First, the composition of firms' export baskets changes relatively little from one year to the other, and products far from the firm's core competencies, with low coreness, are more likely to be dropped. Second, higher coreness is associated with larger export flows at the firm level. Third, such firm-level patterns also have implications at the aggregate level: products that are, on average, exported with higher coreness have higher export flows at the country level, which holds across all levels of product complexity. Therefore, the paper shows that how closely a product fits within a firm's capabilities is important for economic performance at both the firm and country level. We explore these issues within an econometric framework, finding robust evidence both across our three countries and for each country separately."
http://arxiv.org/abs/2302.02833v2,What may future electricity markets look like?,2023-02-06 14:51:01+00:00,['Pierre Pinson'],econ.GN,"Should the organization, design and functioning of electricity markets be taken for granted? Definitely not. While decades of evolution of electricity markets in countries that committed early to restructure their electric power sector made us believe that we may have found the right and future-proof model, the substantially and rapidly evolving context of our power and energy systems is challenging this idea in many ways. Actually, that situation brings both challenges and opportunities. Challenges include accommodation of renewable energy generation, decentralization and support to investment, while opportunities are mainly that advances in technical and social sciences provide us with many more options in terms of future market design. We here take a holistic point of view, by trying to understand where we are coming from with electricity markets and where we may be going. Future electricity markets should be made fit for purpose by considering them as a way to organize and operate a socio-techno-economic system."
http://arxiv.org/abs/2302.08065v1,Wargames as Data: Addressing the Wargamer's Trilemma,2023-02-16 04:04:15+00:00,"['Andrew W. Reddie', 'Ruby E. Booth', 'Bethany L. Goldblum', 'Kiran Lakkaraju', 'Jason Reinhardt']",econ.GN,"Policymakers often want the very best data with which to make decisions--particularly when concerned with questions of national and international security. But what happens when this data is not available? In those instances, analysts have come to rely on synthetic data-generating processes--turning to modeling and simulation tools and survey experiments among other methods. In the cyber domain, where empirical data at the strategic level are limited, this is no different--cyber wargames are quickly becoming a principal method for both exploring and analyzing the security challenges posed by state and non-state actors in cyberspace. In this chapter, we examine the design decisions associated with this method."
http://arxiv.org/abs/2302.07627v5,LP-Duality Theory and the Cores of Games,2023-02-15 12:46:50+00:00,['Vijay V. Vazirani'],cs.GT,"LP-duality theory has played a central role in the study of the core, right from its early days to the present time. However, despite the extensive nature of this work, basic gaps still remain. We address these gaps using the following building blocks from LP-duality theory: 1. Total unimodularity (TUM). 2. Complementary slackness conditions and strict complementarity. Our exploration of TUM leads to defining new games, characterizing their cores and giving novel ways of using core imputations to enforce constraints that arise naturally in applications of these games. The latter include: 1. Efficient algorithms for finding min-max fair, max-min fair and equitable core imputations. 2. Encouraging diversity and avoiding over-representation in a generalization of the assignment game. Complementarity enables us to prove new properties of core imputations of the assignment game and its generalizations."
http://arxiv.org/abs/2302.08456v1,Adverse weather amplifies social media activity,2023-02-16 18:00:55+00:00,"['Kelton Minor', 'Esteban Moro', 'Nick Obradovich']",econ.GN,"Humanity spends an increasing proportion of its time interacting online. Scholars are intensively investigating the societal drivers and resultant impacts of this collective shift in our allocation of time and attention. Yet, the external factors that regularly shape online behavior remain markedly understudied. Do environmental factors alter rates of online activity? Here we show that adverse meteorological conditions markedly increase social media use in the United States. To do so, we employ climate econometric methods alongside over three and a half billion social media posts from tens of millions of individuals from both Facebook and Twitter between 2009 and 2016. We find that more extreme temperatures and added precipitation each independently amplify social media activity. Weather that is adverse on both the temperature and precipitation dimensions produces markedly larger increases in social media activity. On average across both platforms, compared to the temperate weather baseline, days colder than -5°C with 1.5-2cm of precipitation elevate social media activity by 35%. This effect is nearly three times the typical increase in social media activity observed on New Year's Eve in New York City. We observe meteorological effects on social media participation at both the aggregate and individual level, even accounting for individual-specific, temporal, and location-specific potential confounds."
http://arxiv.org/abs/2302.08920v1,A tale of two tails: 130 years of growth-at-risk,2023-02-17 14:50:48+00:00,"['Martin Gächter', 'Elias Hasler', 'Florian Huber']",econ.GN,"We extend the existing growth-at-risk (GaR) literature by examining a long time period of 130 years in a time-varying parameter regression model. We identify several important insights for policymakers. First, both the level as well as the determinants of GaR vary significantly over time. Second, the stability of upside risks to GDP growth reported in earlier research is specific to the period known as the Great Moderation, with the distribution of risks being more balanced before the 1970s. Third, the distribution of GDP growth has significantly narrowed since the end of the Bretton Woods system. Fourth, financial stress is always linked to higher downside risks, but it does not affect upside risks. Finally, other risk indicators, such as credit growth and house prices, not only drive downside risks, but also contribute to increased upside risks during boom periods. In this context, the paper also adds to the financial cycle literature by completing the picture of drivers (and risks) for both booms and recessions over time."
http://arxiv.org/abs/2302.07695v1,Genetic multi-armed bandits: a reinforcement learning approach for discrete optimization via simulation,2023-02-15 14:46:19+00:00,"['Deniz Preil', 'Michael Krapp']",cs.NE,"This paper proposes a new algorithm, referred to as GMAB, that combines concepts from the reinforcement learning domain of multi-armed bandits and random search strategies from the domain of genetic algorithms to solve discrete stochastic optimization problems via simulation. In particular, the focus is on noisy large-scale problems, which often involve a multitude of dimensions as well as multiple local optima. Our aim is to combine the property of multi-armed bandits to cope with volatile simulation observations with the ability of genetic algorithms to handle high-dimensional solution spaces accompanied by an enormous number of feasible solutions. For this purpose, a multi-armed bandit framework serves as a foundation, where each observed simulation is incorporated into the memory of GMAB. Based on this memory, genetic operators guide the search, as they provide powerful tools for exploration as well as exploitation. The empirical results demonstrate that GMAB achieves superior performance compared to benchmark algorithms from the literature in a large variety of test problems. In all experiments, GMAB required considerably fewer simulations to achieve similar or (far) better solutions than those generated by existing methods. At the same time, GMAB's overhead with regard to the required runtime is extremely small due to the suggested tree-based implementation of its memory. Furthermore, we prove its convergence to the set of global optima as the simulation effort goes to infinity."
http://arxiv.org/abs/2302.04380v3,Covariate Adjustment in Experiments with Matched Pairs,2023-02-09 00:12:32+00:00,"['Yuehao Bai', 'Liang Jiang', 'Joseph P. Romano', 'Azeem M. Shaikh', 'Yichong Zhang']",econ.EM,"This paper studies inference on the average treatment effect in experiments in which treatment status is determined according to ""matched pairs"" and it is additionally desired to adjust for observed, baseline covariates to gain further precision. By a ""matched pairs"" design, we mean that units are sampled i.i.d. from the population of interest, paired according to observed, baseline covariates and finally, within each pair, one unit is selected at random for treatment. Importantly, we presume that not all observed, baseline covariates are used in determining treatment assignment. We study a broad class of estimators based on a ""doubly robust"" moment condition that permits us to study estimators with both finite-dimensional and high-dimensional forms of covariate adjustment. We find that estimators with finite-dimensional, linear adjustments need not lead to improvements in precision relative to the unadjusted difference-in-means estimator. This phenomenon persists even if the adjustments are interacted with treatment; in fact, doing so leads to no changes in precision. However, gains in precision can be ensured by including fixed effects for each of the pairs. Indeed, we show that this adjustment is the ""optimal"" finite-dimensional, linear adjustment. We additionally study two estimators with high-dimensional forms of covariate adjustment based on the LASSO. For each such estimator, we show that it leads to improvements in precision relative to the unadjusted difference-in-means estimator and also provide conditions under which it leads to the ""optimal"" nonparametric, covariate adjustment. A simulation study confirms the practical relevance of our theoretical analysis, and the methods are employed to reanalyze data from an experiment using a ""matched pairs"" design to study the effect of macroinsurance on microenterprise."
http://arxiv.org/abs/2302.05677v1,A Tractable Truthful Profit Maximization Mechanism Design with Autonomous Agents,2023-02-11 12:22:57+00:00,"['Mina Montazeri', 'Hamed Kebriaei', 'Babak N. Araabi']",econ.TH,"Task allocation is a crucial process in modern systems, but it is often challenged by incomplete information about the utilities of participating agents. In this paper, we propose a new profit maximization mechanism for the task allocation problem, where the task publisher seeks an optimal incentive function to maximize its own profit and simultaneously ensure the truthful announcing of the agent's private information (type) and its participation in the task, while an autonomous agent aims at maximizing its own utility function by deciding on its participation level and announced type. Our mechanism stands out from the classical contract theory-based truthful mechanisms as it empowers agents to make their own decisions about their level of involvement, making it more practical for many real-world task allocation scenarios. It has been proven that by considering a linear form of incentive function consisting of two decision functions for the task publisher the mechanism's goals are met. The proposed truthful mechanism is initially modeled as a non-convex functional optimization with the double continuum of constraints, nevertheless, we demonstrate that by deriving an equivalent form of the incentive constraints, it can be reformulated as a tractable convex optimal control problem. Further, we propose a numerical algorithm to obtain the solution."
http://arxiv.org/abs/2302.05772v1,Set-Asides in USDA Food Procurement Auctions,2023-02-11 20:34:16+00:00,"['Ni Yan', 'WenTing Tao']",econ.GN,"We study the partial and full set-asides and their implication for changes in bidding behavior in first-price sealed-bid auctions in the context of United States Department of Agriculture (USDA) food procurement auctions. Using five years of bid data on different beef products, we implement weighted least squares regression models to show that partial set-aside predicts decreases in both offer prices and winning prices among large and small business bidders. Full set-aside predicts a small increase in offer prices and winning prices among small businesses. With these predictions, we infer that net profit of small businesses is unlikely to increase when set-asides are present."
http://arxiv.org/abs/2301.13449v1,Certification Design for a Competitive Market,2023-01-31 07:04:04+00:00,"['Andreas A. Haupt', 'Nicole Immorlica', 'Brendan Lucier']",cs.GT,"Motivated by applications such as voluntary carbon markets and educational testing, we consider a market for goods with varying but hidden levels of quality in the presence of a third-party certifier. The certifier can provide informative signals about the quality of products, and can charge for this service. Sellers choose both the quality of the product they produce and a certification. Prices are then determined in a competitive market. Under a single-crossing condition, we show that the levels of certification chosen by producers are uniquely determined at equilibrium. We then show how to reduce a revenue-maximizing certifier's problem to a monopolistic pricing problem with non-linear valuations, and design an FPTAS for computing the optimal slate of certificates and their prices. In general, both the welfare-optimal and revenue-optimal slate of certificates can be arbitrarily large."
http://arxiv.org/abs/2301.11776v1,'Good job!' The impact of positive and negative feedback on performance,2023-01-27 15:29:39+00:00,"['Daniel Goller', 'Maximilian Späth']",econ.GN,"We analyze the causal impact of positive and negative feedback on professional performance. We exploit a unique data source in which quasi-random, naturally occurring variations within subjective ratings serve as positive and negative feedback. The analysis shows that receiving positive feedback has a favorable impact on subsequent performance, while negative feedback does not have an effect. These main results are found in two different environments and for distinct cultural backgrounds, experiences, and gender of the feedback recipients. The findings imply that managers should focus on giving positive motivational feedback."
http://arxiv.org/abs/2301.11859v3,Synthetic Difference In Differences Estimation,2023-01-27 17:05:42+00:00,"['Damian Clarke', 'Daniel Pailañir', 'Susan Athey', 'Guido Imbens']",econ.EM,"In this paper, we describe a computational implementation of the Synthetic difference-in-differences (SDID) estimator of Arkhangelsky et al. (2021) for Stata. Synthetic difference-in-differences can be used in a wide class of circumstances where treatment effects on some particular policy or event are desired, and repeated observations on treated and untreated units are available over time. We lay out the theory underlying SDID, both when there is a single treatment adoption date and when adoption is staggered over time, and discuss estimation and inference in each of these cases. We introduce the sdid command which implements these methods in Stata, and provide a number of examples of use, discussing estimation, inference, and visualization of results."
http://arxiv.org/abs/2301.13099v1,Prediction of Customer Churn in Banking Industry,2023-01-30 17:36:33+00:00,['Sina Esmaeilpour Charandabi'],stat.ML,"With the growing competition in banking industry, banks are required to follow customer retention strategies while they are trying to increase their market share by acquiring new customers. This study compares the performance of six supervised classification techniques to suggest an efficient model to predict customer churn in banking industry, given 10 demographic and personal attributes from 10000 customers of European banks. The effect of feature selection, class imbalance, and outliers will be discussed for ANN and random forest as the two competing models. As shown, unlike random forest, ANN does not reveal any serious concern regarding overfitting and is also robust to noise. Therefore, ANN structure with five nodes in a single hidden layer is recognized as the best performing classifier."
http://arxiv.org/abs/2301.12075v2,"An Examination of Ranked Choice Voting in the United States, 2004-2022",2023-01-28 03:17:08+00:00,"['Adam Graham-Squire', 'David McCune']",econ.GN,"From the perspective of social choice theory, ranked-choice voting (RCV) is known to have many flaws. RCV can fail to elect a Condorcet winner and is susceptible to monotonicity paradoxes and the spoiler effect, for example. We use a database of 182 American ranked-choice elections for political office from the years 2004-2022 to investigate empirically how frequently RCV's deficiencies manifest in practice. Our general finding is that RCV's weaknesses are rarely observed in real-world elections, with the exception that ballot exhaustion frequently causes majoritarian failures."
http://arxiv.org/abs/2301.11554v2,Heat and Worker Health,2023-01-27 06:40:29+00:00,"['Andrew Ireland', 'David Johnston', 'Rachel Knott']",econ.GN,"Extreme heat negatively impacts cognition, learning, and task performance. With increasing global temperatures, workers may therefore be at increased risk of work-related injuries and illness. This study estimates the effects of temperature on worker health using records spanning 1985-2020 from an Australian mandatory insurance scheme. High temperatures are found to cause significantly more claims, particularly among manual workers in outdoor-based industries. These adverse effects have not diminished across time, with the largest effect observed for the 2015-2020 period, indicating increasing vulnerability to heat. Within occupations, the workers most adversely affected by heat are female, older-aged and higher-earning. Finally, results from firm-level panel analyses show that the percentage increase in claims on hot days is largest at ""safer"" firms."
http://arxiv.org/abs/2301.12542v1,A Note on the Estimation of Job Amenities and Labor Productivity,2023-01-29 21:08:44+00:00,"['Arnaud Dupuy', 'Alfred Galichon']",econ.EM,"This paper introduces a maximum likelihood estimator of the value of job amenities and labor productivity in a single matching market based on the observation of equilibrium matches and wages. The estimation procedure simultaneously fits both the matching patterns and the wage curve. While our estimator is suited for a wide range of assignment problems, we provide an application to the estimation of the Value of a Statistical Life using compensating wage differentials for the risk of fatal injury on the job. Using US data for 2017, we estimate the Value of Statistical Life at \$ 6.3 million (\$2017)."
http://arxiv.org/abs/2301.09982v1,Prenatal Sugar Consumption and Late-Life Human Capital and Health: Analyses Based on Postwar Rationing and Polygenic Scores,2023-01-24 13:30:40+00:00,"['Gerard J. van den Berg', 'Stephanie von Hinke', 'R. Adele H. Wang']",econ.GN,"Maternal sugar consumption in utero may have a variety of effects on offspring. We exploit the abolishment of the rationing of sweet confectionery in the UK on April 24, 1949, and its subsequent reintroduction some months later, in an era of otherwise uninterrupted rationing of confectionery (1942-1953), sugar (1940-1953) and many other foods, and we consider effects on late-life cardiovascular disease, BMI, height, type-2 diabetes and the intake of sugar, fat and carbohydrates, as well as cognitive outcomes and birth weight. We use individual-level data from the UK Biobank for cohorts born between April 1947-May 1952. We also explore whether one's genetic ""predisposition"" to the outcome can moderate the effects of prenatal sugar exposure. We find that prenatal exposure to derationing increases education and reduces BMI and sugar consumption at higher ages, in line with the ""developmental origins"" explanatory framework, and that the sugar effects are stronger for those who are genetically ""predisposed"" to sugar consumption."
http://arxiv.org/abs/2302.02221v1,A quantification of how much crypto-miners are driving up the wholesale cost of energy in Texas,2023-02-04 19:12:09+00:00,"['Jangho Lee', 'Lily Wu', 'Andrew E. Dessler']",econ.GN,"The use of energy by cryptocurrency mining comes not just with an environmental cost but also an economic one through increases in electricity prices for other consumers. Here we investigate the increase in wholesale price on Texas ERCOT grid due to energy consumption from cryptocurrency mining. For every GW of cryptocurrency mining load on the grid, we find that the wholesale price of electricity on the ERCOT grid increases by 2 per Cent. Given that todays cryptocurrency mining load on the ERCOT grid is around 1 GW, it suggests that wholesale prices have already risen this amount. There are 27 GW of mining load waiting to be hooked up to the ERCOT grid. If cryptocurrency mining increases rapidly, the price of energy in Texas could skyrocket."
http://arxiv.org/abs/2302.06958v1,For One and All: Individual and Group Fairness in the Allocation of Indivisible Goods,2023-02-14 10:36:18+00:00,"['Jonathan Scarlett', 'Nicholas Teh', 'Yair Zick']",cs.GT,"Fair allocation of indivisible goods is a well-explored problem. Traditionally, research focused on individual fairness - are individual agents satisfied with their allotted share? - and group fairness - are groups of agents treated fairly? In this paper, we explore the coexistence of individual envy-freeness (i-EF) and its group counterpart, group weighted envy-freeness (g-WEF), in the allocation of indivisible goods. We propose several polynomial-time algorithms that provably achieve i-EF and g-WEF simultaneously in various degrees of approximation under three different conditions on the agents' (i) when agents have identical additive valuation functions, i-EFX and i-WEF1 can be achieved simultaneously; (ii) when agents within a group share a common valuation function, an allocation satisfying both i-EF1 and g-WEF1 exists; and (iii) when agents' valuations for goods within a group differ, we show that while maintaining i-EF1, we can achieve a 1/3-approximation to ex-ante g-WEF1. Our results thus provide a first step towards connecting individual and group fairness in the allocation of indivisible goods, in hopes of its useful application to domains requiring the reconciliation of diversity with individual demands."
http://arxiv.org/abs/2302.06580v2,Comparison Shopping: Learning Before Buying From Duopolists,2023-02-13 18:33:28+00:00,"['Brian C. Albrecht', 'Mark Whitmeyer']",econ.TH,"We explore a model of duopolistic competition in which consumers learn about the fit of each competitor's product. In equilibrium, consumers comparison shop: they learn only about the relative values of the products. When information is cheap, increasing the cost of information decreases consumer welfare; but when information is expensive, this relationship flips. As information frictions vanish, there is a limiting equilibrium that is ex post efficient."
http://arxiv.org/abs/2301.13410v1,Multi-Channel Auction Design in the Autobidding World,2023-01-31 04:57:59+00:00,"['Gagan Aggarwal', 'Andres Perlroth', 'Junyao Zhao']",cs.GT,"Over the past few years, more and more Internet advertisers have started using automated bidding for optimizing their advertising campaigns. Such advertisers have an optimization goal (e.g. to maximize conversions), and some constraints (e.g. a budget or an upper bound on average cost per conversion), and the automated bidding system optimizes their auction bids on their behalf. Often, these advertisers participate on multiple advertising channels and try to optimize across these channels. A central question that remains unexplored is how automated bidding affects optimal auction design in the multi-channel setting.
  In this paper, we study the problem of setting auction reserve prices in the multi-channel setting. In particular, we shed light on the revenue implications of whether each channel optimizes its reserve price locally, or whether the channels optimize them globally to maximize total revenue. Motivated by practice, we consider two models: one in which the channels have full freedom to set reserve prices, and another in which the channels have to respect floor prices set by the publisher. We show that in the first model, welfare and revenue loss from local optimization is bounded by a function of the advertisers' inputs, but is independent of the number of channels and bidders. In stark contrast, we show that the revenue from local optimization could be arbitrarily smaller than those from global optimization in the second model."
http://arxiv.org/abs/2301.07855v3,Digital Divide: Empirical Study of CIUS 2020,2023-01-19 02:52:42+00:00,"['Joann Jasiak', 'Peter MacKenzie', 'Purevdorj Tuvaandorj']",econ.EM,"As Canada and other major economies consider implementing ""digital money"" or Central Bank Digital Currencies, understanding how demographic and geographic factors influence public engagement with digital technologies becomes increasingly important. This paper uses data from the 2020 Canadian Internet Use Survey and employs survey-adapted Lasso inference methods to identify individual socio-economic and demographic characteristics determining the digital divide in Canada. We also introduce a score to measure and compare the digital literacy of various segments of Canadian population. Our findings reveal that disparities in the use of e.g. online banking, emailing, and digital payments exist across different demographic and socio-economic groups. In addition, we document the effects of COVID-19 pandemic on internet use in Canada and describe changes in the characteristics of Canadian internet users over the last decade."
http://arxiv.org/abs/2303.09720v2,Modeling urbanization dynamics by labor force migration,2023-03-17 01:28:27+00:00,['Hirotaka Goto'],nlin.AO,"Individual participants in human society collectively exhibit aggregation behavior. In this study, we present a simple microscopic model of labor force migration based on the active Brownian particles framework. In particular, agent-based simulations show that the model produces clusters of agents from a random initial distribution. Furthermore, two empirical regularities called Zipf's and Okun's laws were observed. To reveal the mechanism underlying the reproduced aggregation phenomena, we use our microscopic model to derive an extended Keller--Segel system, which is a classic model describing the aggregation behavior of biological organisms called taxis. The obtained macroscopic system indicates that the concentration of the workforce in the real world can be explained through a new type of taxis central to human behavior, highlighting the relevance of urbanization to blow-up phenomena in the derived PDE system. We then characterize the transition between the aggregation and diffusion regimes both analytically and computationally. The predicted long-term dynamics of urbanization -- originating in the asymmetric natures of employed and unemployed agents -- are compared with global empirical data, particularly in the realms of labor statistics and urban indicators."
http://arxiv.org/abs/2303.08653v2,On the robustness of posterior means,2023-03-15 14:36:08+00:00,['Jiafeng Chen'],math.ST,"Consider a normal location model $X \mid θ\sim N(θ, σ^2)$ with known $σ^2$. Suppose $θ\sim G_0$, where the prior $G_0$ has zero mean and variance bounded by $V$. Let $G_1$ be a possibly misspecified prior with zero mean and variance bounded by $V$. We show that the squared error Bayes risk of the posterior mean under $G_1$ is bounded, subjected to an additional tail condition on $G_1$, uniformly over $G_0, G_1, σ^2 > 0$."
http://arxiv.org/abs/2303.12263v4,Strategic Ambiguity in Global Games,2023-03-22 02:14:04+00:00,['Takashi Ui'],econ.TH,"In games with incomplete and ambiguous information, rational behavior depends not only on fundamental ambiguity (ambiguity about states) but also on strategic ambiguity (ambiguity about others' actions), which further induces hierarchies of ambiguous beliefs. We study the impacts of strategic ambiguity in global games and demonstrate the distinct effects of ambiguous-quality and low-quality information. Ambiguous-quality information makes more players choose an action yielding a constant payoff, whereas (unambiguous) low-quality information makes more players choose an ex-ante best response to the uniform belief over the opponents' actions. If the ex-ante best-response action yields a constant payoff, sufficiently ambiguous-quality information induces a unique equilibrium, whereas sufficiently low-quality information generates multiple equilibria. In applications to financial crises, we show that news of more ambiguous quality triggers a debt rollover crisis, whereas news of less ambiguous quality triggers a currency crisis."
http://arxiv.org/abs/2303.08445v1,Are high school degrees and university diplomas equally heritable in the US? A new measure of relative intergenerational mobility,2023-03-15 08:39:48+00:00,"['Anna Naszodi', 'Liliana Cuccu']",econ.GN,"This paper proposes a new measure of relative intergenerational mobility along the educational trait as a proxy of inequality of opportunity. The new measure is more suitable for controlling for the variations in the trait distributions of individuals and their parents than the commonly used intergenerational persistence coefficient. This point is illustrated by our empirical analysis of US census data from the period between 1960 and 2015: we show that controlling for the variations in the trait distributions adequately is vital in assessing the part of intergenerational mobility which is not caused by the educational expansion. Failing to do so can potentially reverse the relative priority of various policies aiming at reducing the ""heritability"" of high school degrees and tertiary education diplomas."
http://arxiv.org/abs/2303.01887v2,Fast Forecasting of Unstable Data Streams for On-Demand Service Platforms,2023-03-03 12:33:32+00:00,"['Yu Jeffrey Hu', 'Jeroen Rombouts', 'Ines Wilms']",econ.EM,"On-demand service platforms face a challenging problem of forecasting a large collection of high-frequency regional demand data streams that exhibit instabilities. This paper develops a novel forecast framework that is fast and scalable, and automatically assesses changing environments without human intervention. We empirically test our framework on a large-scale demand data set from a leading on-demand delivery platform in Europe, and find strong performance gains from using our framework against several industry benchmarks, across all geographical regions, loss functions, and both pre- and post-Covid periods. We translate forecast gains to economic impacts for this on-demand service platform by computing financial gains and reductions in computing costs."
http://arxiv.org/abs/2303.13598v3,Bootstrap-Assisted Inference for Generalized Grenander-type Estimators,2023-03-23 18:24:43+00:00,"['Matias D. Cattaneo', 'Michael Jansson', 'Kenichi Nagasawa']",math.ST,"Westling and Carone (2020) proposed a framework for studying the large sample distributional properties of generalized Grenander-type estimators, a versatile class of nonparametric estimators of monotone functions. The limiting distribution of those estimators is representable as the left derivative of the greatest convex minorant of a Gaussian process whose monomial mean can be of unknown order (when the degree of flatness of the function of interest is unknown). The standard nonparametric bootstrap is unable to consistently approximate the large sample distribution of the generalized Grenander-type estimators even if the monomial order of the mean is known, making statistical inference a challenging endeavour in applications. To address this inferential problem, we present a bootstrap-assisted inference procedure for generalized Grenander-type estimators. The procedure relies on a carefully crafted, yet automatic, transformation of the estimator. Moreover, our proposed method can be made ``flatness robust'' in the sense that it can be made adaptive to the (possibly unknown) degree of flatness of the function of interest. The method requires only the consistent estimation of a single scalar quantity, for which we propose an automatic procedure based on numerical derivative estimation and the generalized jackknife. Under random sampling, our inference method can be implemented using a computationally attractive exchangeable bootstrap procedure. We illustrate our methods with examples and we also provide a small simulation study. The development of formal results is made possible by some technical results that may be of independent interest."
http://arxiv.org/abs/2303.11976v4,Towards a Characterization of Random Serial Dictatorship,2023-03-21 16:08:32+00:00,"['Felix Brandt', 'Matthias Greger', 'René Romen']",econ.TH,"Random serial dictatorship (RSD) is a randomized assignment rule that - given a set of $n$ agents with strict preferences over $n$ houses - satisfies equal treatment of equals, ex post efficiency, and strategyproofness. For $n \le 3$, Bogomolnaia and Moulin (2001) have shown that RSD is characterized by these axioms. Extending this characterization to arbitrary $n$ is a long-standing open problem. By weakening ex post efficiency and strategyproofness, we reduce the question of whether RSD is characterized by these axioms for fixed $n$ to determining whether a matrix has rank $n^2 n!^n$. We provide computer-generated counterexamples to show that two other approaches for proving the characterization (using deterministic extreme points or restricted domains of preferences) are inadequate."
http://arxiv.org/abs/2302.11128v3,Ignorance Is Bliss: The Screening Effect of (Noisy) Information,2023-02-22 03:49:25+00:00,"['Felix Zhiyu Feng', 'Wenyu Wang', 'Yufeng Wu', 'Gaoqing Zhang']",econ.TH,"This paper studies the value of a firm's internal information when the firm faces an adverse selection problem arising from unobservable managerial abilities. While more precise information allows the firm to make ex post more efficient investment decisions, noisier information has an ex ante screening effect that allows the firm to attract on-average better managers. The trade-off between more effective screening of managers and more informed investment implies a non-monotonic relationship between firm value and information quality. A marginal improvement in information quality does not necessarily lead to an overall improvement in firm value."
http://arxiv.org/abs/2302.11643v3,An Empirical Analysis of Optimal Nonlinear Pricing in Business-to-Business Markets,2023-02-22 20:39:55+00:00,"['Soheil Ghili', 'Russ Yoon']",econ.GN,"In continuous-choice settings, consumers decide not only on whether to purchase a product, but also on how much to purchase. Thus, firms optimize a full price schedule rather than a single price point. This paper provides a methodology to empirically estimate the optimal schedule under multi-dimensional consumer heterogeneity with a focus on B2B applications. We apply our method to novel data from an educational-services firm that contains purchase-size information not only for deals that materialized, but also for potential deals that eventually failed. We show that this data, combined with identifying assumptions, helps infer how price sensitivity varies with ""customer size"". Using our estimated model, we show that the optimal second-degree price discrimination (i.e., optimal nonlinear tariff) improves the firm's profit upon linear pricing by at least 8.2%. That said, this second-degree price discrimination scheme only recovers 7.1% of the gap between the profitability of linear pricing and that of infeasible first degree price discrimination. We also conduct several further simulation analyses (i) empirically quantifying the magnitude by which incentive-compatibility constraints impact the optimal pricing and profits, (ii) comparing the role of demand- v.s. cost-side factors in shaping the optimal price schedule, and (iii) studying the implications of fixed fees for the optimal contract and profitability."
http://arxiv.org/abs/2301.13414v2,Incentive Compatibility in the Auto-bidding World,2023-01-31 05:08:37+00:00,"['Yeganeh Alimohammadi', 'Aranyak Mehta', 'Andres Perlroth']",econ.TH,"Auto-bidding has recently become a popular feature in ad auctions. This feature enables advertisers to simply provide high-level constraints and goals to an automated agent, which optimizes their auction bids on their behalf. In this paper, we examine the effect of different auctions on the incentives of advertisers to report their constraints to the auto-bidder intermediaries. More precisely, we study whether canonical auctions such as first price auction (FPA) and second price auction (SPA) are auto-bidding incentive compatible (AIC): whether an advertiser can gain by misreporting their constraints to the autobidder.
  We consider value-maximizing advertisers in two important settings: when they have a budget constraint and when they have a target cost-per-acquisition constraint. The main result of our work is that for both settings, FPA and SPA are not AIC. This contrasts with FPA being AIC when auto-bidders are constrained to bid using a (sub-optimal) uniform bidding policy. We further extend our main result and show that any (possibly randomized) auction that is truthful (in the classic profit-maximizing sense), scalar invariant and symmetric is not AIC. Finally, to complement our findings, we provide sufficient market conditions for FPA and SPA to become AIC for two advertisers. These conditions require advertisers' valuations to be well-aligned. This suggests that when the competition is intense for all queries, advertisers have less incentive to misreport their constraints.
  From a methodological standpoint, we develop a novel continuous model of queries. This model provides tractability to study equilibrium with auto-bidders, which contrasts with the standard discrete query model, which is known to be hard. Through the analysis of this model, we uncover a surprising result: in auto-bidding with two advertisers, FPA and SPA are auction equivalent."
http://arxiv.org/abs/2303.14732v3,Interdisciplinary Papers Supported by Disciplinary Grants Garner Deep and Broad Scientific Impact,2023-03-26 14:27:13+00:00,"['Minsu Park', 'Suman Kalyan Maity', 'Stefan Wuchty', 'Dashun Wang']",cs.DL,"Interdisciplinary research has emerged as a hotbed for innovation and a key approach to addressing complex societal challenges. The increasing dominance of grant-supported research in shaping scientific advances, coupled with growing interest in funding interdisciplinary work, raises fundamental questions about the effectiveness of interdisciplinary grants in fostering high-impact interdisciplinary research outcomes. Here, we quantify the interdisciplinarity of both research grants and publications, capturing 350,000 grants from 164 funding agencies across 26 countries and 1.3 million papers that acknowledged their support from 1985 to 2009. Our analysis uncovers two seemingly contradictory patterns: Interdisciplinary grants tend to produce interdisciplinary papers, which are generally associated with high impact. However, compared to disciplinary grants, interdisciplinary grants on average yield fewer papers and interdisciplinary papers they support tend to have substantially reduced impact. We demonstrate that the key to explaining this paradox lies in the power of disciplinary grants in propelling high-impact interdisciplinary research. Specifically, our results show that highly interdisciplinary papers supported by deeply disciplinary grants garner disproportionately more citations, both within their core disciplines and from broader fields. Moreover, disciplinary grants, particularly when combined with other similar grants, are more effective in producing high-impact interdisciplinary research. Amidst the rapid rise of support for interdisciplinary work across the sciences, these results highlight the hitherto unknown role of disciplinary grants in driving crucial interdisciplinary advances, suggesting that interdisciplinary research requires deep disciplinary expertise and investments."
http://arxiv.org/abs/2303.00982v3,Debiased Machine Learning of Aggregated Intersection Bounds and Other Causal Parameters,2023-03-02 05:24:37+00:00,['Vira Semenova'],econ.EM,"This paper proposes a novel framework of aggregated intersection of regression functions, where the target parameter is obtained by averaging the minimum (or maximum) of a collection of regression functions over the covariate space. Such quantities include the lower and upper bounds on distributional effects (Frechet-Hoeffding, Makarov) and the optimal welfare in the statistical treatment choice problem. The proposed estimator -- the envelope score estimator -- is shown to have an oracle property, where the oracle knows the identity of the minimizer for each covariate value. I apply this result to the bounds in the Roy model and the Horowitz-Manski-Lee bounds with a discrete outcome. The proposed approach performs well empirically on the data from the Oregon Health Insurance Experiment."
http://arxiv.org/abs/2301.10643v3,Automatic Debiased Estimation with Machine Learning-Generated Regressors,2023-01-25 15:26:18+00:00,"['Juan Carlos Escanciano', 'Telmo Pérez-Izquierdo']",econ.EM,"Many parameters of interest in economics and other social sciences depend on generated regressors. Examples in economics include structural parameters in models with endogenous variables estimated by control functions and in models with sample selection, treatment effect estimation with propensity score matching, and marginal treatment effects. More recently, Machine Learning (ML) generated regressors are becoming ubiquitous for these and other applications such as imputation with missing regressors, dimension reduction, including autoencoders, learned proxies, confounders and treatments, and for feature engineering with unstructured data, among others. We provide the first general method for valid inference with regressors generated from ML. Inference with generated regressors is complicated by the very complex expression for influence functions and asymptotic variances. Additionally, ML-generated regressors may lead to large biases in downstream inferences. To address these problems, we propose Automatic Locally Robust/debiased GMM estimators in a general three-step setting with ML-generated regressors. We illustrate our results with treatment effects and counterfactual parameters in the partially linear and nonparametric models with ML-generated regressors. We provide sufficient conditions for the asymptotic normality of our debiased GMM estimators and investigate their finite-sample performance through Monte Carlo simulations."
http://arxiv.org/abs/2302.01233v2,Sparse High-Dimensional Vector Autoregressive Bootstrap,2023-02-02 17:14:54+00:00,"['Robert Adamek', 'Stephan Smeekes', 'Ines Wilms']",econ.EM,"We introduce a high-dimensional multiplier bootstrap for time series data based on capturing dependence through a sparsely estimated vector autoregressive model. We prove its consistency for inference on high-dimensional means under two different moment assumptions on the errors, namely sub-gaussian moments and a finite number of absolute moments. In establishing these results, we derive a Gaussian approximation for the maximum mean of a linear process, which may be of independent interest."
http://arxiv.org/abs/2302.02867v1,Penalized Quasi-likelihood Estimation and Model Selection in Time Series Models with Parameters on the Boundary,2023-02-06 15:36:11+00:00,"['Heino Bohn Nielsen', 'Anders Rahbek']",econ.EM,"We extend the theory from Fan and Li (2001) on penalized likelihood-based estimation and model-selection to statistical and econometric models which allow for non-negativity constraints on some or all of the parameters, as well as time-series dependence. It differs from classic non-penalized likelihood estimation, where limiting distributions of likelihood-based estimators and test-statistics are non-standard, and depend on the unknown number of parameters on the boundary of the parameter space. Specifically, we establish that the joint model selection and estimation, results in standard asymptotic Gaussian distributed estimators. The results are applied to the rich class of autoregressive conditional heteroskedastic (ARCH) models for the modelling of time-varying volatility. We find from simulations that the penalized estimation and model-selection works surprisingly well even for a large number of parameters. A simple empirical illustration for stock-market returns data confirms the ability of the penalized estimation to select ARCH models which fit nicely the autocorrelation function, as well as confirms the stylized fact of long-memory in financial time series data."
http://arxiv.org/abs/2302.13455v4,Nickell Bias in Panel Local Projection: Financial Crises Are Worse Than You Think,2023-02-27 00:54:33+00:00,"['Ziwei Mei', 'Liugang Sheng', 'Zhentao Shi']",econ.EM,"Panel local projection (LP) with fixed-effects (FE) estimation is widely adopted for evaluating the economic consequences of financial crises across countries. This paper highlights a fundamental methodological issue: the presence of the Nickell bias in the panel FE estimator due to inherent dynamic structures of panel predictive specifications, even if the regressors have no lagged dependent variables. The Nickell bias invalidates the standard inferential procedure based on the $t$-statistic. We propose the split-panel jackknife (SPJ) estimator as a simple, easy-to-implement, and yet effective solution to eliminate the bias and restore valid statistical inference. We revisit four influential empirical studies on the impact of financial crises, and find that the FE method underestimates the economic losses of financial crises relative to the SPJ estimates."
http://arxiv.org/abs/2301.10541v3,Educational Game on Cryptocurrency Investment: Using Microeconomic Decision Making to Understand Macroeconomics Principles,2023-01-25 12:14:52+00:00,"['Jiasheng Zhu', 'Luyao Zhang']",econ.GN,"Gamification is an effective strategy for motivating and engaging users, which is grounded in business, marketing, and management by designing games in nongame contexts. Gamifying education, which consists of the design and study of educational games, is an emerging trend. However, the existing classroom games for understanding macroeconomics have weak connections to the microfoundations of individual decision-making. We design an educational game on cryptocurrency investment for understanding macroeconomic concepts in microeconomic decisions. We contribute to the literature by designing game-based learning that engages students in understanding macroeconomics in incentivized individual investment decisions. Our game can be widely implemented in online, in-person, and hybrid classrooms. We also reflect on strategies for improving the user experience for future educational game implementations."
http://arxiv.org/abs/2302.05590v2,Zero-Knowledge Mechanisms,2023-02-11 03:43:43+00:00,"['Ran Canetti', 'Amos Fiat', 'Yannai A. Gonczarowski']",econ.TH,"A powerful feature in mechanism design is the ability to irrevocably commit to the rules of a mechanism. Commitment is achieved by public declaration, which enables players to verify incentive properties in advance and the outcome in retrospect. However, public declaration can reveal superfluous information that the mechanism designer might prefer not to disclose, such as her target function or private costs. Avoiding this may be possible via a trusted mediator; however, the availability of a trustworthy mediator, especially if mechanism secrecy must be maintained for years, might be unrealistic. We propose a new approach to commitment, and show how to commit to, and run, any given mechanism without disclosing it, while enabling the verification of incentive properties and the outcome -- all without the need for any mediators. Our framework utilizes zero-knowledge proofs -- a cornerstone of modern cryptographic theory. Applications include both private-type settings such as auctions and private-action settings such as contracts, as well as non-mediated bargaining with hidden yet binding offers."
http://arxiv.org/abs/2306.10031v1,Marijuana on Main Streets? The Story Continues in Colombia: An Endogenous Three-part Model,2023-06-06 21:05:55+00:00,"['A. Ramirez-Hassan', 'C. Gomez', 'S. Velasquez', 'K. Tangarife']",econ.GN,"Cannabis is the most common illicit drug, and understanding its demand is relevant to analyze the potential implications of its legalization. This paper proposes an endogenous three-part model taking into account incidental truncation and access restrictions to study demand for marijuana in Colombia, and analyze the potential effects of its legalization. Our application suggests that modeling simultaneously access, intensive and extensive margin is relevant, and that selection into access is important for the intensive margin. We find that younger men that have consumed alcohol and cigarettes, living in a neighborhood with drug suppliers, and friends that consume marijuana face higher probability of having access and using this drug. In addition, we find that marijuana is an inelastic good (-0.45 elasticity). Our results are robust to different specifications and definitions. If marijuana were legalized, younger individuals with a medium or low risk perception about marijuana use would increase the probability of use in 3.8 percentage points, from 13.6% to 17.4%. Overall, legalization would increase the probability of consumption in 0.7 p.p. (2.3% to 3.0%). Different price settings suggest that annual tax revenues fluctuate between USD 11.0 million and USD 54.2 million, a potential benchmark is USD 32 million."
http://arxiv.org/abs/2305.16377v2,Validating a dynamic input-output model for the propagation of supply and demand shocks during the COVID-19 pandemic in Belgium,2023-05-25 15:28:01+00:00,"['Tijs W. Alleman', 'Koen Schoors', 'Jan M. Baetens']",econ.GN,"This work validates a dynamic production network model, used to quantify the impact of economic shocks caused by COVID-19 in the UK, using data for Belgium. Because the model was published early during the 2020 COVID-19 pandemic, it relied on several assumptions regarding the magnitude of the observed economic shocks, for which more accurate data have become available in the meantime. We refined the propagated shocks to align with observed data collected during the pandemic and calibrated some less well-informed parameters using 115 economic time series. The refined model effectively captures the evolution of GDP, revenue, and employment during the COVID-19 pandemic in Belgium at both individual economic activity and aggregate levels. However, the reduction in business-to-business demand is overestimated, revealing structural shortcomings in accounting for businesses' motivations to sustain trade despite the pandemic's induced shocks. We confirm that the relaxation of the stringent Leontief production function by a survey on the criticality of inputs significantly improved the model's accuracy. However, despite a large dataset, distinguishing between varying degrees of relaxation proved challenging. Overall, this work demonstrates the model's validity in assessing the impact of economic shocks caused by an epidemic in Belgium."
http://arxiv.org/abs/2305.10934v1,Context-Dependent Heterogeneous Preferences: A Comment on Barseghyan and Molinari (2023),2023-05-18 12:50:58+00:00,"['Matias D. Cattaneo', 'Xinwei Ma', 'Yusufcan Masatlioglu']",econ.TH,"Barseghyan and Molinari (2023) give sufficient conditions for semi-nonparametric point identification of parameters of interest in a mixture model of decision-making under risk, allowing for unobserved heterogeneity in utility functions and limited consideration. A key assumption in the model is that the heterogeneity of risk preferences is unobservable but context-independent. In this comment, we build on their insights and present identification results in a setting where the risk preferences are allowed to be context-dependent."
http://arxiv.org/abs/2304.00626v3,IV Regressions without Exclusion Restrictions,2023-04-02 20:54:19+00:00,"['Wayne Yuan Gao', 'Rui Wang']",econ.EM,"We study identification and estimation of endogenous linear and nonlinear regression models without excluded instrumental variables, based on the standard mean independence condition and a nonlinear relevance condition. Based on the identification results, we propose two semiparametric estimators as well as a discretization-based estimator that does not require any nonparametric regressions. We establish their asymptotic normality and demonstrate via simulations their robust finite-sample performances with respect to exclusion restrictions violations and endogeneity. Our approach is applied to study the returns to education, and to test the direct effects of college proximity indicators as well as family background variables on the outcome."
http://arxiv.org/abs/2304.00489v1,Reduction of Excess Capacity with Response of Capital Intensity,2023-04-02 08:56:50+00:00,['Samidh Pal'],econ.GN,"Purpose: The objective of this research was to show the response of the potential reduction of excess capacity in terms of capital intensity to the growth rate of labor productivity in the manufacturing industrial sector. Design/Methodology/Approach: The research was carried out in 2019 in 55 groups of Indian manufacturing industry within six major Indian industrial states. Mainly, the research used the modified VES (Variable Elasticity Substitution) estimation model. The research focused on the value of the additional substitution parameter of capital intensity (mu > 0). Findings: Almost all selected industry groups with in six states need capital-intensive production. The results found additional parameter of capital intensity (mu) is greater than zero for all industry groups. It means that a higher product per man can be obtained by increasing the capital per worker. Practical Implications: Research shows that an increasingly need for capital investment in need for higher labor productivity is likely to induce the manufacturing unit to use more capacity in existence. It reveals that investors in these selected six states can increase their capital investment. Originality/Value: The analysis of the result allowed to determine the fact that capital intensity is an essential variable for reduction of excess capacity which cannot be ignored in explaining productivity."
http://arxiv.org/abs/2305.03203v2,Delegating to Multiple Agents,2023-05-04 23:26:06+00:00,"['MohammadTaghi Hajiaghayi', 'Keivan Rezaei', 'Suho Shin']",cs.GT,"We consider a multi-agent delegation mechanism without money. In our model, given a set of agents, each agent has a fixed number of solutions which is exogenous to the mechanism, and privately sends a signal, e.g., a subset of solutions, to the principal. Then, the principal selects a final solution based on the agents' signals. In stark contrast to single-agent setting by Kleinberg and Kleinberg (EC'18) with an approximate Bayesian mechanism, we show that there exists efficient approximate prior-independent mechanisms with both information and performance gain, thanks to the competitive tension between the agents. Interestingly, however, the amount of such a compelling power significantly varies with respect to the information available to the agents, and the degree of correlation between the principal's and the agent's utility. Technically, we conduct a comprehensive study on the multi-agent delegation problem and derive several results on the approximation factors of Bayesian/prior-independent mechanisms in complete/incomplete information settings. As a special case of independent interest, we obtain comparative statics regarding the number of agents which implies the dominance of the multi-agent setting ($n \ge 2$) over the single-agent setting ($n=1$) in terms of the principal's utility. We further extend our problem by considering an examination cost of the mechanism and derive some analogous results in the complete information setting."
http://arxiv.org/abs/2305.02513v2,The Direct and Spillover Effects of Large-scale Affirmative Action at an Elite Brazilian University,2023-05-04 02:48:55+00:00,"['Cecilia Machado', 'Germán Reyes', 'Evan Riehl']",econ.GN,We examine the effects of an affirmative action policy at an elite Brazilian university that reserved 45 percent of admission slots for Black and low-income students. We find that marginally-admitted students who enrolled through the affirmative action tracks experienced a 14 percent increase in early-career earnings. But the adoption of affirmative action also caused a large decrease in earnings for the university's most highly-ranked students. We present evidence that the negative spillover effects on highly-ranked students' earnings were driven by both a reduction in human capital accumulation and a decline in the value of networking.
http://arxiv.org/abs/2305.02546v1,"Why Not Borrow, Invest, and Escape Poverty?",2023-05-04 04:45:44+00:00,"['Dagmara Celik Katreniak', 'Alexey Khazanov', 'Omer Moav', 'Zvika Neeman', 'Hosny Zoabi']",econ.GN,"Take up of microcredit by the poor for investment in businesses or human capital turned out to be very low. We show that this could be explained by risk aversion, without relying on fixed costs or other forms of non-convexity in the technology, if the investment is aimed at increasing the probability of success. Under this framework, rational risk-averse agents choose corner solutions, unlike in the case of a risky investment with an exogenous probability of success. Our online experiment confirms our theoretical predictions about how agents' choices differ when facing the two types of investments."
http://arxiv.org/abs/2305.02587v2,Employer Reputation and the Labor Market: Evidence from Glassdoor.com and Dice.com,2023-05-04 06:43:01+00:00,"['Ke', 'Ma', 'Sophie Yanying Sheng', 'Haitian Xie']",econ.GN,"How does employer reputation affect the labor market? We investigate this question using a novel dataset combining reviews from Glassdoor.com and job applications data from Dice.com. Labor market institutions such as Glassdoor.com crowd-sources information about employers to alleviate information problems faced by workers when choosing an employer. Raw crowd-sourced employer ratings are rounded when displayed to job seekers. By exploiting the rounding threshold, we identify the causal impact of Glassdoor ratings using a regression discontinuity framework. We document the effects of such ratings on both the demand and supply sides of the labor market. We find that displayed employer reputation affects an employer's ability to attract workers, especially when the displayed rating is ""sticky."" Employers respond to having a rating above the rounding threshold by posting more new positions and re-activating more job postings. The effects are the strongest for private, smaller, and less established firms, suggesting that online reputation is a substitute for other types of reputation."
http://arxiv.org/abs/2307.11683v1,Assessing the role of small farmers and households in agriculture and the rural economy and measures to support their sustainable development,2023-04-29 14:27:55+00:00,"['Oleg Nivievskyi', 'Pavlo Iavorskyi', 'Oleksandr Donchenko']",econ.GN,"The Ministry of Economy has an interest and demand in exploring how to increase the set of [legally registered] small family farmers in Ukraine and to examine more in details measures that could reduce the scale of the shadow agricultural market in Ukraine. Building upon the above political economy background and demand, we will be undertaking the analysis along the two separate but not totally independents streams of analysis, i.e. sustainable small scale (family) farming development and exploring the scale and measures for reducing the shadow agricultural market in Ukraine"
http://arxiv.org/abs/2306.12924v3,The Impact of Parenthood on Labour Market Outcomes of Women and Men in Poland,2023-06-22 14:36:53+00:00,"['Radost Waszkiewicz', 'Honorata Bogusz']",econ.GN,"We examine the gender gap in income in Poland in relation to parenthood status, employing the placebo event history method adapted to low-resolution data (Polish Generations and Gender Survey). Our analysis reveals anticipatory behavior in both women and men who expect to become parents. We observe a decrease of approximately 20 percent in mothers' income post-birth. In contrast, the income of fathers surpasses that of non-fathers both pre- and post-birth, suggesting that the fatherhood child premium may be primarily driven by selection. We note an increase (decrease) in hours worked for fathers (mothers). Finally, we compare the gender gaps in income and wages between women and men in the sample with those in a counterfactual scenario where the entire population is childless. Our findings indicate no statistically significant gender gaps in the counterfactual scenario, leading us to conclude that parenthood drives the gender gaps in income and wages in Poland."
http://arxiv.org/abs/2308.00795v1,Duopoly insurers' incentives for data quality under a mandatory cyber data sharing regime,2023-05-29 20:19:14+00:00,"['Carlos Barreto', 'Olof Reinert', 'Tobias Wiesinger', 'Ulrik Franke']",econ.TH,"We study the impact of data sharing policies on cyber insurance markets. These policies have been proposed to address the scarcity of data about cyber threats, which is essential to manage cyber risks. We propose a Cournot duopoly competition model in which two insurers choose the number of policies they offer (i.e., their production level) and also the resources they invest to ensure the quality of data regarding the cost of claims (i.e., the data quality of their production cost). We find that enacting mandatory data sharing sometimes creates situations in which at most one of the two insurers invests in data quality, whereas both insurers would invest when information sharing is not mandatory. This raises concerns about the merits of making data sharing mandatory."
http://arxiv.org/abs/2306.02875v1,Toward Textual Internet Immunity,2023-06-05 13:47:30+00:00,['Gregory M. Dickinson'],econ.GN,"Internet immunity doctrine is broken. Under Section 230 of the Communications Decency Act of 1996, online entities are absolutely immune from lawsuits related to content authored by third parties. The law has been essential to the internet's development over the last twenty years, but it has not kept pace with the times and is now deeply flawed. Democrats demand accountability for online misinformation. Republicans decry politically motivated censorship. And Congress, President Biden, the Department of Justice, and the Federal Communications Commission all have their own plans for reform. Absent from the fray, however -- until now -- has been the Supreme Court, which has never issued a decision interpreting Section 230. That appears poised to change, however, following Justice Thomas's statement in Malwarebytes v. Enigma in which he urges the Court to prune back decades of lower-court precedent to craft a more limited immunity doctrine. This Essay discusses how courts' zealous enforcement of the early internet's free-information ethos gave birth to an expansive immunity doctrine, warns of potential pitfalls to reform, and explores what a narrower, text-focused doctrine might mean for the tech industry."
http://arxiv.org/abs/2306.00002v1,The climate niche of Homo Sapiens,2023-05-22 14:17:53+00:00,['Richard S. J. Tol'],physics.soc-ph,"I propose the Dominicy-Hill-Worton estimator to estimate the current climate niche of Homo Sapiens and our croplands. I use this to extrapolate the degree of unprecedentedness of future climates. Worton's peeled hull is a non-parametric, N-dimensional generalization of order statistics. Dominicy and colleagues show that Hill's estimator of the tail-index can be applied to any homogeneous function of multivariate order statistics. I apply the Dominicy-Hill estimator to transects through Worton's peels. I find a thick tail for low temperatures and a thin tail for high ones. That is, warming is more worrying than cooling. Similarly, wettening is more worrying than drying. Furthermore, temperature changes are more important than changes in precipitation. The results are not affected by income, population density, or time. I replace the Hill estimator by the QQ one and correct it for top-censoring. The qualitative results are unaffected."
http://arxiv.org/abs/2306.07709v1,Coordinated Dynamic Bidding in Repeated Second-Price Auctions with Budgets,2023-06-13 11:55:04+00:00,"['Yurong Chen', 'Qian Wang', 'Zhijian Duan', 'Haoran Sun', 'Zhaohua Chen', 'Xiang Yan', 'Xiaotie Deng']",cs.GT,"In online ad markets, a rising number of advertisers are employing bidding agencies to participate in ad auctions. These agencies are specialized in designing online algorithms and bidding on behalf of their clients. Typically, an agency usually has information on multiple advertisers, so she can potentially coordinate bids to help her clients achieve higher utilities than those under independent bidding.
  In this paper, we study coordinated online bidding algorithms in repeated second-price auctions with budgets. We propose algorithms that guarantee every client a higher utility than the best she can get under independent bidding. We show that these algorithms achieve maximal coalition welfare and discuss bidders' incentives to misreport their budgets, in symmetric cases. Our proofs combine the techniques of online learning and equilibrium analysis, overcoming the difficulty of competing with a multi-dimensional benchmark. The performance of our algorithms is further evaluated by experiments on both synthetic and real data. To the best of our knowledge, we are the first to consider bidder coordination in online repeated auctions with constraints."
http://arxiv.org/abs/2306.06680v2,Centrality in Production Networks and International Technology Diffusion,2023-06-11 13:47:20+00:00,['Rinki Ito'],econ.GN,"This study examines whether the structure of global value chains (GVCs) affects international spillovers of research and development (R&D). Although the presence of ``hub'' countries in GVCs has been confirmed by previous studies, the role of these hub countries in the diffusion of the technology has not been analyzed. Using a sample of 21 countries and 14 manufacturing industries during the period 1995-2007, I explore the role of hubs as the mediator of knowledge by classifying countries and industries based on a ``centrality'' measure. I find that R&D spillovers from exporters with High centrality are the largest, suggesting that hub countries play an important role in both gathering and diffusing knowledge. I also find that countries with Middle centrality are getting important in the diffusion of knowledge. Finally, positive spillover effects from own are observed only in the G5 countries."
http://arxiv.org/abs/2306.10053v1,NFTs to MARS: Multi-Attention Recommender System for NFTs,2023-06-13 11:53:24+00:00,"['Seonmi Kim', 'Youngbin Lee', 'Yejin Kim', 'Joohwan Hong', 'Yongjae Lee']",cs.IR,"Recommender systems have become essential tools for enhancing user experiences across various domains. While extensive research has been conducted on recommender systems for movies, music, and e-commerce, the rapidly growing and economically significant Non-Fungible Token (NFT) market remains underexplored. The unique characteristics and increasing prominence of the NFT market highlight the importance of developing tailored recommender systems to cater to its specific needs and unlock its full potential. In this paper, we examine the distinctive characteristics of NFTs and propose the first recommender system specifically designed to address NFT market challenges. In specific, we develop a Multi-Attention Recommender System for NFTs (NFT-MARS) with three key characteristics: (1) graph attention to handle sparse user-item interactions, (2) multi-modal attention to incorporate feature preference of users, and (3) multi-task learning to consider the dual nature of NFTs as both artwork and financial assets. We demonstrate the effectiveness of NFT-MARS compared to various baseline models using the actual transaction data of NFTs collected directly from blockchain for four of the most popular NFT collections. The source code and data are available at https://anonymous.4open.science/r/RecSys2023-93ED."
http://arxiv.org/abs/2306.12176v5,"The Skill-Task Matching Model: Mechanism, Model Structure, and Algorithm",2023-06-21 11:10:42+00:00,"['Da Xie', 'WeiGuo Yang']",econ.TH,"We distinguished between the expected and actual profit of a firm. We proposed that, beyond maximizing profit, a firm's goal also encompasses minimizing the gap between expected and actual profit. Firms strive to enhance their capability to transform projects into reality through a process of trial and error, evident as a cyclical iterative optimization process. To characterize this iterative mechanism, we developed the Skill-Task Matching Model, extending the task approach in both multidimensional and iterative manners. We vectorized jobs and employees into task and skill vector spaces, respectively, while treating production techniques as a skill-task matching matrix and business strategy as a task value vector. In our model, the process of stabilizing production techniques and optimizing business strategies corresponds to the recalibration of parameters within the skill-task matching matrix and the task value vector. We constructed a feed-forward neural network algorithm to run this model and demonstrated how it can augment operational efficiency."
http://arxiv.org/abs/2306.12667v1,The Power of Menus in Contract Design,2023-06-22 04:28:44+00:00,"['Guru Guruganesh', 'Jon Schneider', 'Joshua Wang', 'Junyao Zhao']",cs.GT,"We study the power of menus of contracts in principal-agent problems with adverse selection (agents can be one of several types) and moral hazard (we cannot observe agent actions directly). For principal-agent problems with $T$ types and $n$ actions, we show that the best menu of contracts can obtain a factor $Ω(\max(n, \log T))$ more utility for the principal than the best individual contract, partially resolving an open question of Guruganesh et al. (2021). We then turn our attention to randomized menus of linear contracts, where we likewise show that randomized linear menus can be $Ω(T)$ better than the best single linear contract. As a corollary, we show this implies an analogous gap between deterministic menus of (general) contracts and randomized menus of contracts (as introduced by Castiglioni et al. (2022))."
http://arxiv.org/abs/2305.10728v2,Modeling Interference Using Experiment Roll-out,2023-05-18 05:57:37+00:00,"['Ariel Boyarsky', 'Hongseok Namkoong', 'Jean Pouget-Abadie']",stat.ME,"Experiments on online marketplaces and social networks suffer from interference, where the outcome of a unit is impacted by the treatment status of other units. We propose a framework for modeling interference using a ubiquitous deployment mechanism for experiments, staggered roll-out designs, which slowly increase the fraction of units exposed to the treatment to mitigate any unanticipated adverse side effects. Our main idea is to leverage the temporal variations in treatment assignments introduced by roll-outs to model the interference structure. Since there are often multiple competing models of interference in practice we first develop a model selection method that evaluates models based on their ability to explain outcome variation observed along the roll-out. Through simulations, we show that our heuristic model selection method, Leave-One-Period-Out, outperforms other baselines. Next, we present a set of model identification conditions under which the estimation of common estimands is possible and show how these conditions are aided by roll-out designs. We conclude with a set of considerations, robustness checks, and potential limitations for practitioners wishing to use our framework."
http://arxiv.org/abs/2305.05998v4,On the Time-Varying Structure of the Arbitrage Pricing Theory using the Japanese Sector Indices,2023-05-10 09:15:14+00:00,"['Koichiro Moriya', 'Akihiko Noda']",q-fin.ST,"This paper is the first study to examine the time instability of the APT in the Japanese stock market. In particular, we measure how changes in each risk factor affect the stock risk premiums to investigate the validity of the APT over time, applying the rolling window method to Fama and MacBeth's (1973) two-step regression and Kamstra and Shi's (2023) generalized GRS test. We summarize our empirical results as follows: (1) the changes in monetary policy by major central banks greatly affect the validity of the APT in Japan, and (2) the time-varying estimates of the risk premiums for each factor are also unstable over time, and they are affected by the business cycle and economic crises. Therefore, we conclude that the validity of the APT as an appropriate model to explain the Japanese sector index is not stable over time."
http://arxiv.org/abs/2305.09485v2,Executive Voiced Laughter and Social Approval: An Explorative Machine Learning Study,2023-05-16 14:39:00+00:00,"['Niklas Mueller', 'Steffen Klug', 'Andreas Koenig', 'Alexander Kathan', 'Lukas Christ', 'Bjoern Schuller', 'Shahin Amiriparian']",econ.GN,"We study voiced laughter in executive communication and its effect on social approval. Integrating research on laughter, affect-as-information, and infomediaries' social evaluations of firms, we hypothesize that voiced laughter in executive communication positively affects social approval, defined as audience perceptions of affinity towards an organization. We surmise that the effect of laughter is especially strong for joint laughter, i.e., the number of instances in a given communication venue for which the focal executive and the audience laugh simultaneously. Finally, combining the notions of affect-as-information and negativity bias in human cognition, we hypothesize that the positive effect of laughter on social approval increases with bad organizational performance. We find partial support for our ideas when testing them on panel data comprising 902 German Bundesliga soccer press conferences and media tenor, applying state-of-the-art machine learning approaches for laughter detection as well as sentiment analysis. Our findings contribute to research at the nexus of executive communication, strategic leadership, and social evaluations, especially by introducing laughter as a highly consequential potential, but understudied social lubricant at the executive-infomediary interface. Our research is unique by focusing on reflexive microprocesses of social evaluations, rather than the infomediary-routines perspectives in infomediaries' evaluations. We also make methodological contributions."
http://arxiv.org/abs/2305.11282v2,Statistical Estimation for Covariance Structures with Tail Estimates using Nodewise Quantile Predictive Regression Models,2023-05-18 19:57:23+00:00,['Christis Katsouris'],econ.EM,"This paper considers the specification of covariance structures with tail estimates. We focus on two aspects: (i) the estimation of the VaR-CoVaR risk matrix in the case of larger number of time series observations than assets in a portfolio using quantile predictive regression models without assuming the presence of nonstationary regressors and; (ii) the construction of a novel variable selection algorithm, so-called, Feature Ordering by Centrality Exclusion (FOCE), which is based on an assumption-lean regression framework, has no tuning parameters and is proved to be consistent under general sparsity assumptions. We illustrate the usefulness of our proposed methodology with numerical studies of real and simulated datasets when modelling systemic risk in a network."
http://arxiv.org/abs/2305.18916v1,Behavioral Causal Inference,2023-05-30 10:10:39+00:00,['Ran Spiegler'],econ.TH,"When inferring the causal effect of one variable on another from correlational data, a common practice by professional researchers as well as lay decision makers is to control for some set of exogenous confounding variables. Choosing an inappropriate set of control variables can lead to erroneous causal inferences. This paper presents a model of lay decision makers who use long-run observational data to learn the causal effect of their actions on a payoff-relevant outcome. Different types of decision makers use different sets of control variables. I obtain upper bounds on the equilibrium welfare loss due to wrong causal inferences, for various families of data-generating processes. The bounds depend on the structure of the type space. When types are ""ordered"" in a certain sense, the equilibrium condition greatly reduces the cost of wrong causal inference due to poor controls."
http://arxiv.org/abs/2305.18991v1,Generalized Autoregressive Score Trees and Forests,2023-05-30 12:41:52+00:00,"['Andrew J. Patton', 'Yasin Simsek']",econ.EM,"We propose methods to improve the forecasts from generalized autoregressive score (GAS) models (Creal et. al, 2013; Harvey, 2013) by localizing their parameters using decision trees and random forests. These methods avoid the curse of dimensionality faced by kernel-based approaches, and allow one to draw on information from multiple state variables simultaneously. We apply the new models to four distinct empirical analyses, and in all applications the proposed new methods significantly outperform the baseline GAS model. In our applications to stock return volatility and density prediction, the optimal GAS tree model reveals a leverage effect and a variance risk premium effect. Our study of stock-bond dependence finds evidence of a flight-to-quality effect in the optimal GAS forest forecasts, while our analysis of high-frequency trade durations uncovers a volume-volatility effect."
http://arxiv.org/abs/2305.19484v2,A Simple Method for Predicting Covariance Matrices of Financial Returns,2023-05-31 01:41:24+00:00,"['Kasper Johansson', 'Mehmet Giray Ogut', 'Markus Pelger', 'Thomas Schmelzer', 'Stephen Boyd']",econ.EM,"We consider the well-studied problem of predicting the time-varying covariance matrix of a vector of financial returns. Popular methods range from simple predictors like rolling window or exponentially weighted moving average (EWMA) to more sophisticated predictors such as generalized autoregressive conditional heteroscedastic (GARCH) type methods. Building on a specific covariance estimator suggested by Engle in 2002, we propose a relatively simple extension that requires little or no tuning or fitting, is interpretable, and produces results at least as good as MGARCH, a popular extension of GARCH that handles multiple assets. To evaluate predictors we introduce a novel approach, evaluating the regret of the log-likelihood over a time period such as a quarter. This metric allows us to see not only how well a covariance predictor does over all, but also how quickly it reacts to changes in market conditions. Our simple predictor outperforms MGARCH in terms of regret. We also test covariance predictors on downstream applications such as portfolio optimization methods that depend on the covariance matrix. For these applications our simple covariance predictor and MGARCH perform similarly."
http://arxiv.org/abs/2305.07472v2,Mechanism Design without Rational Expectations,2023-05-12 13:36:55+00:00,['Giacomo Rubbini'],econ.TH,"Is incentive compatibility still necessary for implementation if we relax the rational expectations assumption? This paper proposes a generalized model of implementation that does not assume agents hold rational expectations and characterizes the class of solution concepts requiring Bayesian Incentive Compatibility (BIC) for full implementation. Surprisingly, for a broad class of solution concepts, full implementation of functions still requires BIC even if rational expectations do not hold. This finding implies that some classical results, such as the impossibility of efficient bilateral trade (Myerson & Satterthwaite, 1983), hold for a broader range of non-equilibrium solution concepts, confirming their relevance even in boundedly rational setups."
http://arxiv.org/abs/2306.03632v2,Uniform Inference for Cointegrated Vector Autoregressive Processes,2023-06-06 12:41:58+00:00,"['Christian Holberg', 'Susanne Ditlevsen']",math.ST,"Uniformly valid inference for cointegrated vector autoregressive processes has so far proven difficult due to certain discontinuities arising in the asymptotic distribution of the least squares estimator. We extend asymptotic results from the univariate case to multiple dimensions and show how inference can be based on these results. Furthermore, we show that lag augmentation and a recent instrumental variable procedure can also yield uniformly valid tests and confidence regions. We verify the theoretical findings and investigate finite sample properties in simulation experiments for two specific examples."
http://arxiv.org/abs/2306.05966v1,An Empirical Analysis of the Effect of Ballot Truncation on Ranked-Choice Electoral Outcomes,2023-06-09 15:33:30+00:00,"['Mallory Dickerson', 'Erin Martin', 'David McCune']",econ.GN,"In ranked-choice elections voters cast preference ballots which provide a voter's ranking of the candidates. The method of ranked-choice voting (RCV) chooses a winner by using voter preferences to simulate a series of runoff elections. Some jurisdictions which use RCV limit the number of candidates that voters can rank on the ballot, imposing what we term a truncation level, which is the number of candidates that voters are allowed to rank. Given fixed voter preferences, the winner of the election can change if we impose different truncation levels. We use a database of 1171 real-world ranked-choice elections to empirically analyze the potential effects of imposing different truncation levels in ranked-choice elections. Our general finding is that if the truncation level is at least three then restricting the number of candidates which can be ranked on the ballot rarely affects the election winner."
http://arxiv.org/abs/2306.05867v1,The Relationship Between Burnout Operators with the Functions of Family Tehran Banking Melli Iran Bank in 2015,2023-06-09 13:01:57+00:00,"['Mohammad Heydari', 'Matineh Moghaddam', 'Habibollah Danai']",econ.GN,"In this study, the relationship between burnout and family functions of the Melli Iran Bank staff will be studied. A number of employees within the organization using appropriate scientific methods as the samples were selected by detailed questionnaire and the appropriate data is collected burnout and family functions. The method used descriptive statistical population used for this study consisted of 314 bank loan officers in branches of Melli Iran Bank of Tehran province and all the officials at the bank for >5 years of service at Melli Iran Bank branches in Tehran. They are married and men constitute the study population. The Maslach Burnout Inventory in the end internal to 0/90 alpha emotional exhaustion, depersonalization and low personal accomplishment Cronbach alpha of 0/79 and inventory by 0/71 within the last family to solve the problem 0/70, emotional response 0/51, touch 0/70, 0/69 affective involvement, roles, 0/59, 0/68 behavior is controlled. The results indicate that the hypothesis that included the relationship between burnout and 6, the family functioning, problem solving, communication, roles, affective responsiveness, affective fusion there was a significant relationship between behavior and the correlation was negative. The burnout is high; the functions within the family will be in trouble."
http://arxiv.org/abs/2306.04606v1,Network-based Representations and Dynamic Discrete Choice Models for Multiple Discrete Choice Analysis,2023-06-07 17:16:41+00:00,"['Hung Tran', 'Tien Mai']",econ.EM,"In many choice modeling applications, people demand is frequently characterized as multiple discrete, which means that people choose multiple items simultaneously. The analysis and prediction of people behavior in multiple discrete choice situations pose several challenges. In this paper, to address this, we propose a random utility maximization (RUM) based model that considers each subset of choice alternatives as a composite alternative, where individuals choose a subset according to the RUM framework. While this approach offers a natural and intuitive modeling approach for multiple-choice analysis, the large number of subsets of choices in the formulation makes its estimation and application intractable. To overcome this challenge, we introduce directed acyclic graph (DAG) based representations of choices where each node of the DAG is associated with an elemental alternative and additional information such that the number of selected elemental alternatives. Our innovation is to show that the multi-choice model is equivalent to a recursive route choice model on the DAG, leading to the development of new efficient estimation algorithms based on dynamic programming. In addition, the DAG representations enable us to bring some advanced route choice models to capture the correlation between subset choice alternatives. Numerical experiments based on synthetic and real datasets show many advantages of our modeling approach and the proposed estimation algorithms."
http://arxiv.org/abs/2306.13772v1,Heat increases experienced racial segregation in the United States,2023-06-01 08:39:10+00:00,"['Till Baldenius', 'Nicolas Koch', 'Hannah Klauber', 'Nadja Klein']",econ.GN,"Segregation on the basis of ethnic groups stands as a pervasive and persistent social challenge in many cities across the globe. Public spaces provide opportunities for diverse encounters but recent research suggests individuals adjust their time spent in such places to cope with extreme temperatures. We evaluate to what extent such adaptation affects racial segregation and thus shed light on a yet unexplored channel through which global warming might affect social welfare. We use large-scale foot traffic data for millions of places in 315 US cities between 2018 and 2020 to estimate an index of experienced isolation in daily visits between whites and other ethnic groups. We find that heat increases segregation. Results from panel regressions imply that a week with temperatures above 33°C in a city like Los Angeles induces an upward shift of visit isolation by 0.7 percentage points, which equals about 14% of the difference in the isolation index of Los Angeles to the more segregated city of Atlanta. The segregation-increasing effect is particularly strong for individuals living in lower-income areas and at places associated with leisure activities. Combining our estimates with climate model projections, we find that stringent mitigation policy can have significant co-benefits in terms of cushioning increases in racial segregation in the future."
http://arxiv.org/abs/2304.12276v2,Matching markets with farsighted couples,2023-04-24 17:16:43+00:00,"['Ata Atay', 'Sylvain Funck', 'Ana Mauleon', 'Vincent Vannetelbosch']",econ.TH,"We adopt the notion of the farsighted stable set to determine which matchings are stable when agents are farsighted in matching markets with couples. We show that a singleton matching is a farsighted stable set if and only if the matching is stable. Thus, matchings that are stable with myopic agents remain stable when agents become farsighted. Examples of farsighted stable sets containing multiple non-stable matchings are provided for markets with and without stable matchings. For couples markets where the farsighted stable set does not exist, we propose the DEM farsighted stable set to predict the matchings that are stable when agents are farsighted."
http://arxiv.org/abs/2304.12433v1,"Long memory, fractional integration and cointegration analysis of real convergence in Spain",2023-04-03 13:01:48+00:00,"['Mariam Kamal', 'Josu Arteche']",econ.GN,"This paper investigates economic convergence in terms of real income per capita among the autonomous regions of Spain. In order to converge, the series should cointegrate. This necessary condition is checked using two testing strategies recently proposed for fractional cointegration, finding no evidence of cointegration, which rules out the possibility of convergence between all or some of the Spanish regions. As an additional contribution, an extension of the critical values of one of the tests of fractional cointegration is provided for a different number of variables and sample sizes from those originally provided by the author, fitting those considered in this paper."
http://arxiv.org/abs/2304.12843v1,The structure of strategy-proof rules,2023-04-25 14:17:08+00:00,"['Jorge Alcalde-Unzu', 'Marc Vorsatz']",econ.TH,"We establish that all strategy-proof social choice rules in strict preference domains follow necessarily a two-step procedure. In the first step, agents are asked to reveal some specific information about their preferences. Afterwards, a subrule that is dictatorial or strategy-proof of range 2 must be applied, and the selected subrule may differ depending on the answers of the first step. As a consequence, the strategy-proof rules that have been identified in the literature for some domains can be reinterpreted in terms of our procedure and, more importantly, this procedure serves as a guide for determining the structure of the strategy-proof rules in domains that have not been explored yet."
http://arxiv.org/abs/2304.14960v1,Political Strategies to Overcome Climate Policy Obstructionism,2023-04-28 16:24:03+00:00,"['Sugandha Srivastav', 'Ryan Rafaty']",econ.GN,"Great socio-economic transitions see the demise of certain industries and the rise of others. The losers of the transition tend to deploy a variety of tactics to obstruct change. We develop a political-economy model of interest group competition and garner evidence of tactics deployed in the global climate movement. From this we deduce a set of strategies for how the climate movement competes against entrenched hydrocarbon interests. Five strategies for overcoming obstructionism emerge: (1) Appeasement, which involves compensating the losers; (2) Co-optation, which seeks to instigate change by working with incumbents; (3) Institutionalism, which involves changes to public institutions to support decarbonization; (4) Antagonism, which creates reputational or litigation costs to inaction; and (5) Countervailance, which makes low-carbon alternatives more competitive. We argue that each strategy addresses the problem of obstructionism through a different lens, reflecting a diversity of actors and theories of change within the climate movement. The choice of which strategy to pursue depends on the institutional context."
http://arxiv.org/abs/2305.00231v2,"Historical trend in educational homophily: U-shaped or not U-shaped? Or, how to set a criterion to choose a criterion?",2023-04-29 10:55:57+00:00,['Anna Naszodi'],econ.GN,"Measuring changes in overall inequality between different educational groups is often performed by quantifying variations in educational marital homophily across consecutive generations. However, this task becomes challenging when the education level of marriageable individuals is generation-specific. To address this challenge, various indicators have been proposed in the assortative mating literature.
  In this paper, we review a set of criteria that indicators must satisfy to be considered as suitable measures of homophily and inequality. Our analytical criteria include robustness to the number of educational categories distinguished and the negative association between intergenerational mobility and homophily. Additionally, we also impose an empirical criterion on the identified qualitative historical trend in homophily between 1960 and 2010 in the US at the national and sub-national levels.
  Our analysis reveals that while a specific cardinal indicator meets all three criteria, many commonly applied indices do not. We propose the application of this well-performing indicator to quantify the trend in overall inequality in any country, including European countries, with available population data on couples' education level."
http://arxiv.org/abs/2305.00641v2,On extensions of partial priorities in school choice,2023-05-01 03:17:33+00:00,"['Minoru Kitahara', 'Yasunori Okumura']",econ.TH,"We consider a school choice matching model where the priorities for schools are represented by binary relations that may not be weak order. We focus on the (total order) extensions of the binary relations. We introduce a class of algorithms to derive one of the extensions of a binary relation and characterize them by using the class. We show that if the binary relations are the partial orders, then for each stable matching for the profile of the binary relations, there is an extension for which it is also stable. Moreover, if there are multiple stable matchings for the profile of the binary relations that are ranked by Pareto dominance, there is an extension for which all of those matchings are stable. We provide several applications of these results."
http://arxiv.org/abs/2305.00403v2,Optimal tests following sequential experiments,2023-04-30 06:09:49+00:00,['Karun Adusumilli'],econ.EM,"Recent years have seen tremendous advances in the theory and application of sequential experiments. While these experiments are not always designed with hypothesis testing in mind, researchers may still be interested in performing tests after the experiment is completed. The purpose of this paper is to aid in the development of optimal tests for sequential experiments by analyzing their asymptotic properties. Our key finding is that the asymptotic power function of any test can be matched by a test in a limit experiment where a Gaussian process is observed for each treatment, and inference is made for the drifts of these processes. This result has important implications, including a powerful sufficiency result: any candidate test only needs to rely on a fixed set of statistics, regardless of the type of sequential experiment. These statistics are the number of times each treatment has been sampled by the end of the experiment, along with final value of the score (for parametric models) or efficient influence function (for non-parametric models) process for each treatment. We then characterize asymptotically optimal tests under various restrictions such as unbiasedness, α-spending constraints etc. Finally, we apply our our results to three key classes of sequential experiments: costly sampling, group sequential trials, and bandit experiments, and show how optimal inference can be conducted in these scenarios."
http://arxiv.org/abs/2305.01477v1,An Assignment Problem with Interdependent Valuations and Externalities,2023-05-02 14:59:04+00:00,"['Tatiana Daddario', 'Richard P. McLean', 'Andrew Postlewaite']",econ.TH,"In this paper, we take a mechanism design approach to optimal assignment problems with asymmetrically informed buyers. In addition, the surplus generated by an assignment of a buyer to a seller may be adversely affected by externalities generated by other assignments. The problem is complicated by several factors. Buyers know their own valuations and externality costs but do not know this same information for other buyers. Buyers also receive private signals correlated with the state and, consequently, the implementation problem exhibits interdependent valuations. This precludes a naive application of the VCG mechanism and to overcome this interdependency problem, we construct a two-stage mechanism. In the first stage, we exploit correlation in the firms signals about the state to induce truthful reporting of observed signals. Given that buyers are honest in stage 1, we then use a VCG-like mechanism in stage 2 that induces honest reporting of valuation and externality functions."
http://arxiv.org/abs/2304.02723v1,"Measuring Discrete Risks on Infinite Domains: Theoretical Foundations, Conditional Five Number Summaries, and Data Analyses",2023-04-05 19:53:12+00:00,"['Daoping Yu', 'Vytaras Brazauskas', 'Ricardas Zitikis']",stat.AP,"To accommodate numerous practical scenarios, in this paper we extend statistical inference for smoothed quantile estimators from finite domains to infinite domains. We accomplish the task with the help of a newly designed truncation methodology for discrete loss distributions with infinite domains. A simulation study illustrates the methodology in the case of several distributions, such as Poisson, negative binomial, and their zero inflated versions, which are commonly used in insurance industry to model claim frequencies. Additionally, we propose a very flexible bootstrap-based approach for the use in practice. Using automobile accident data and their modifications, we compute what we have termed the conditional five number summary (C5NS) for the tail risk and construct confidence intervals for each of the five quantiles making up C5NS, and then calculate the tail probabilities. The results show that the smoothed quantile approach classifies the tail riskiness of portfolios not only more accurately but also produces lower coefficients of variation in the estimation of tail probabilities than those obtained using the linear interpolation approach."
http://arxiv.org/abs/2304.06466v3,"Market-Based ""Actual"" Returns of Investors",2023-04-02 14:49:55+00:00,['Victor Olkhov'],econ.GN,"We describe how the market-based average and volatility of the ""actual"" return, which the investors gain within their market sales, depend on the statistical moments, volatilities, and correlations of the current and past market trade values. We describe three successive approximations. First, we derive the dependence of the market-based average and volatility of a single sale return on market trade statistical moments determined by multiple purchases in the past. Then, we describe the dependence of average and volatility of return that a single investor gains during the ""trading day."" Finally, we derive the market-based average and volatility of return of different investors during the ""trading day"" as a function of volatilities and correlations of market trade values. That highlights the distribution of the ""actual"" return of market trade and can serve as a benchmark for ""purchasing"" investors."
http://arxiv.org/abs/2304.05515v1,A Note on Cursed Sequential Equilibrium and Sequential Cursed Equilibrium,2023-04-11 21:48:23+00:00,"['Meng-Jhang Fong', 'Po-Hsuan Lin', 'Thomas R. Palfrey']",econ.TH,"In this short note, we compare the cursed sequential equilibrium (CSE) by Fong et al. (2023) and the sequential cursed equilibrium (SCE) by Cohen and Li (2023). We identify eight main differences between CSE and SCE with respect to the following features: (1) the family of applicable games, (2) the number of free parameters, (3) the belief updating process, (4) the treatment of public histories, (5) effects in games of complete information, (6) violations of subgame perfection and sequential rationality, (7) re-labeling of actions, and (8) effects in one-stage simultaneous-move games."
http://arxiv.org/abs/2304.05605v1,Payroll Tax Incidence: Evidence from Unemployment Insurance,2023-04-12 04:46:06+00:00,['Audrey Guo'],econ.GN,"Economic models assume that payroll tax burdens fall fully on workers, but where does tax incidence fall when taxes are firm-specific and time-varying? Unemployment insurance in the United States has the key feature of varying both across employers and over time, creating the potential for labor demand responses if tax costs cannot be fully passed on to worker wages. Using state policy changes and matched employer-employee job spells from the LEHD, I study how employment and earnings respond to payroll tax increases for highly exposed employers. I find significant drops in employment growth driven by lower hiring, and minimal evidence of pass-through to earnings. The negative employment effects are strongest for young and low-earning workers."
http://arxiv.org/abs/2304.04912v1,Financial Time Series Forecasting using CNN and Transformer,2023-04-11 00:56:57+00:00,"['Zhen Zeng', 'Rachneet Kaur', 'Suchetha Siddagangappa', 'Saba Rahimi', 'Tucker Balch', 'Manuela Veloso']",cs.LG,"Time series forecasting is important across various domains for decision-making. In particular, financial time series such as stock prices can be hard to predict as it is difficult to model short-term and long-term temporal dependencies between data points. Convolutional Neural Networks (CNN) are good at capturing local patterns for modeling short-term dependencies. However, CNNs cannot learn long-term dependencies due to the limited receptive field. Transformers on the other hand are capable of learning global context and long-term dependencies. In this paper, we propose to harness the power of CNNs and Transformers to model both short-term and long-term dependencies within a time series, and forecast if the price would go up, down or remain the same (flat) in the future. In our experiments, we demonstrated the success of the proposed method in comparison to commonly adopted statistical and deep learning methods on forecasting intraday stock price change of S&P 500 constituents."
http://arxiv.org/abs/2304.04981v1,Contingent Fees in Order Flow Auctions,2023-04-11 04:54:48+00:00,['Max Resnick'],econ.TH,"Many early order flow auction designs handle the payment for orders when they execute on the chain rather than when they are won in the auction. Payments in these auctions only take place when the orders are executed, creating a free option for whoever wins the order. Bids in these auctions set the strike price of this option rather than the option premium. This paper develops a simple model of an order flow auction and compares contingent fees with upfront payments as well as mixtures of the two. Results suggest that auctions with a greater share of the payment contingent on execution have lower execution probability, lower revenue, and increased effective spreads in equilibrium. A Reputation system can act as a negative contingent fee, partially mitigating the downsides; however, unless the system is calibrated perfectly, some of the undesirable qualities of the contingent fees remain. Results suggest that designers of order flow auctions should avoid contingent fees whenever possible."
http://arxiv.org/abs/2304.09775v1,The Impact of Industrial Zone:Evidence from China's National High-tech Zone Policy,2023-04-19 15:58:15+00:00,['Li Han'],econ.EM,"Based on the statistical yearbook data and related patent data of 287 cities in China from 2000 to 2020, this study regards the policy of establishing the national high-tech zones as a quasi-natural experiment. Using this experiment, this study firstly estimated the treatment effect of the policy and checked the robustness of the estimation. Then the study examined the heterogeneity in different geographic demarcation of China and in different city level of China. After that, this study explored the possible influence mechanism of the policy. It shows that the possible mechanism of the policy is financial support, industrial agglomeration of secondary industry and the spillovers. In the end, this study examined the spillovers deeply and showed the distribution of spillover effect."
http://arxiv.org/abs/2304.09978v1,"Equilibrium-Invariant Embedding, Metric Space, and Fundamental Set of $2\times2$ Normal-Form Games",2023-04-19 21:31:28+00:00,"['Luke Marris', 'Ian Gemp', 'Georgios Piliouras']",cs.GT,"Equilibrium solution concepts of normal-form games, such as Nash equilibria, correlated equilibria, and coarse correlated equilibria, describe the joint strategy profiles from which no player has incentive to unilaterally deviate. They are widely studied in game theory, economics, and multiagent systems. Equilibrium concepts are invariant under certain transforms of the payoffs. We define an equilibrium-inspired distance metric for the space of all normal-form games and uncover a distance-preserving equilibrium-invariant embedding. Furthermore, we propose an additional transform which defines a better-response-invariant distance metric and embedding. To demonstrate these metric spaces we study $2\times2$ games. The equilibrium-invariant embedding of $2\times2$ games has an efficient two variable parameterization (a reduction from eight), where each variable geometrically describes an angle on a unit circle. Interesting properties can be spatially inferred from the embedding, including: equilibrium support, cycles, competition, coordination, distances, best-responses, and symmetries. The best-response-invariant embedding of $2\times2$ games, after considering symmetries, rediscovers a set of 15 games, and their respective equivalence classes. We propose that this set of game classes is fundamental and captures all possible interesting strategic interactions in $2\times2$ games. We introduce a directed graph representation and name for each class. Finally, we leverage the tools developed for $2\times2$ games to develop game theoretic visualizations of large normal-form and extensive-form games that aim to fingerprint the strategic interactions that occur within."
http://arxiv.org/abs/2304.10651v1,Partition-based Stability of Coalitional Games,2023-04-20 21:26:07+00:00,['Jian Yang'],econ.TH,"We are concerned with the stability of a coalitional game, i.e., a transferable-utility (TU) cooperative game. First, the concept of core can be weakened so that the blocking of changes is limited to only those with multilateral backings. This principle of consensual blocking, as well as the traditional core-defining principle of unilateral blocking and one straddling in between, can all be applied to partition-allocation pairs. Each such pair is made up of a partition of the grand coalition and a corresponding allocation vector whose components are individually rational and efficient for the various constituent coalitions of the given partition. For the resulting strong, medium, and weak stability concepts, the first is core-compatible in that the traditional core exactly contains those allocations that are associated through this strong stability concept with the all-consolidated partition consisting of only the grand coalition. Probably more importantly, the latter medium and weak stability concepts are universal. By this, we mean that any game, no matter how ``poor'' it is, has its fair share of stable solutions. There is also a steepest ascent method to guide the convergence process to a mediumly stable partition-allocation pair from any starting partition."
http://arxiv.org/abs/2305.04137v2,Volatility of Volatility and Leverage Effect from Options,2023-05-06 21:33:06+00:00,"['Carsten H. Chong', 'Viktor Todorov']",econ.EM,"We propose model-free (nonparametric) estimators of the volatility of volatility and leverage effect using high-frequency observations of short-dated options. At each point in time, we integrate available options into estimates of the conditional characteristic function of the price increment until the options' expiration and we use these estimates to recover spot volatility. Our volatility of volatility estimator is then formed from the sample variance and first-order autocovariance of the spot volatility increments, with the latter correcting for the bias in the former due to option observation errors. The leverage effect estimator is the sample covariance between price increments and the estimated volatility increments. The rate of convergence of the estimators depends on the diffusive innovations in the latent volatility process as well as on the observation error in the options with strikes in the vicinity of the current spot price. Feasible inference is developed in a way that does not require prior knowledge of the source of estimation error that is asymptotically dominating."
http://arxiv.org/abs/2304.07331v1,Generalized Automatic Least Squares: Efficiency Gains from Misspecified Heteroscedasticity Models,2023-04-14 18:05:30+00:00,['Bulat Gafarov'],econ.EM,"It is well known that in the presence of heteroscedasticity ordinary least squares estimator is not efficient. I propose a generalized automatic least squares estimator (GALS) that makes partial correction of heteroscedasticity based on a (potentially) misspecified model without a pretest. Such an estimator is guaranteed to be at least as efficient as either OLS or WLS but can provide some asymptotic efficiency gains over OLS if the misspecified model is approximately correct. If the heteroscedasticity model is correct, the proposed estimator achieves full asymptotic efficiency. The idea is to frame moment conditions corresponding to OLS and WLS squares based on miss-specified heteroscedasticity as a joint generalized method of moments estimation problem. The resulting optimal GMM estimator is equivalent to a feasible GLS with estimated weight matrix. I also propose an optimal GMM variance-covariance estimator for GALS to account for any remaining heteroscedasticity in the residuals."
http://arxiv.org/abs/2304.06828v1,Predictive Incrementality by Experimentation (PIE) for Ad Measurement,2023-04-13 21:37:04+00:00,"['Brett R. Gordon', 'Robert Moakler', 'Florian Zettelmeyer']",econ.EM,"We present a novel approach to causal measurement for advertising, namely to use exogenous variation in advertising exposure (RCTs) for a subset of ad campaigns to build a model that can predict the causal effect of ad campaigns that were run without RCTs. This approach -- Predictive Incrementality by Experimentation (PIE) -- frames the task of estimating the causal effect of an ad campaign as a prediction problem, with the unit of observation being an RCT itself. In contrast, traditional causal inference approaches with observational data seek to adjust covariate imbalance at the user level. A key insight is to use post-campaign features, such as last-click conversion counts, that do not require an RCT, as features in our predictive model. We find that our PIE model recovers RCT-derived incremental conversions per dollar (ICPD) much better than the program evaluation approaches analyzed in Gordon et al. (forthcoming). The prediction errors from the best PIE model are 48%, 42%, and 62% of the RCT-based average ICPD for upper-, mid-, and lower-funnel conversion outcomes, respectively. In contrast, across the same data, the average prediction error of stratified propensity score matching exceeds 491%, and that of double/debiased machine learning exceeds 2,904%. Using a decision-making framework inspired by industry, we show that PIE leads to different decisions compared to RCTs for only 6% of upper-funnel, 7% of mid-funnel, and 13% of lower-funnel outcomes. We conclude that PIE could enable advertising platforms to scale causal ad measurement by extrapolating from a limited number of RCTs to a large set of non-experimental ad campaigns."
http://arxiv.org/abs/2304.08049v1,Economic consequences of the spatial and temporal variability of climate change,2023-04-17 08:07:11+00:00,"['Francisco Estrada', 'Richard S. J. Tol', 'Wouter Botzen']",econ.GN,"Damage functions in integrated assessment models (IAMs) map changes in climate to economic impacts and form the basis for most of estimates of the social cost of carbon. Implicit in these functions lies an unwarranted assumption that restricts the spatial variation (Svar) and temporal variability (Tvar) of changes in climate to be null. This could bias damage estimates and the climate policy advice from IAMs. While the effects of Tvar have been studied in the literature, those of Svar and their interactions with Tvar have not. Here we present estimates of the economic costs of climate change that account for both Tvar and Svar, as well as for the seasonality of damages across sectors. Contrary to the results of recent studies which show little effect that of Tvar on expected losses, we reveal that ignoring Svar produces large downward biases, as warming is highly heterogeneous over space. Using a conservative calibration for the damage function, we show that previous estimates are biased downwards by about 23-36%, which represents additional losses of about US$1,400-US$2,300 billion by 2050 and US$17-US$28 trillion by the end of the century, under a high emissions scenario. The present value of losses during the period 2020-2100 would be larger than reported in previous studies by $47-$66 trillion or about 1/2 to 3/4 of annual global GDP in 2020. Our results imply that using global mean temperature change in IAMs as a summary measure of warming is not adequate for estimating the costs of climate change. Instead, IAMs should include a more complete description of climate conditions."
http://arxiv.org/abs/2304.07480v3,Gini-stable Lorenz curves and their relation to the generalised Pareto distribution,2023-04-15 05:50:39+00:00,"['Lucio Bertoli-Barsotti', 'Marek Gagolewski', 'Grzegorz Siudem', 'Barbara Żogała-Siudem']",physics.soc-ph,"We introduce an iterative discrete information production process where we can extend ordered normalised vectors by new elements based on a simple affine transformation, while preserving the predefined level of inequality, G, as measured by the Gini index.
  Then, we derive the family of empirical Lorenz curves of the corresponding vectors and prove that it is stochastically ordered with respect to both the sample size and G which plays the role of the uncertainty parameter. We prove that asymptotically, we obtain all, and only, Lorenz curves generated by a new, intuitive parametrisation of the finite-mean Pickands' Generalised Pareto Distribution (GPD) that unifies three other families, namely: the Pareto Type II, exponential, and scaled beta distributions. The family is not only totally ordered with respect to the parameter G, but also, thanks to our derivations, has a nice underlying interpretation. Our result may thus shed a new light on the genesis of this family of distributions.
  Our model fits bibliometric, informetric, socioeconomic, and environmental data reasonably well. It is quite user-friendly for it only depends on the sample size and its Gini index."
http://arxiv.org/abs/2304.07855v1,Penalized Likelihood Inference with Survey Data,2023-04-16 18:38:14+00:00,"['Joann Jasiak', 'Purevdorj Tuvaandorj']",econ.EM,"This paper extends three Lasso inferential methods, Debiased Lasso, $C(α)$ and Selective Inference to a survey environment. We establish the asymptotic validity of the inference procedures in generalized linear models with survey weights and/or heteroskedasticity. Moreover, we generalize the methods to inference on nonlinear parameter functions e.g. the average marginal effect in survey logit models. We illustrate the effectiveness of the approach in simulated data and Canadian Internet Use Survey 2020 data."
http://arxiv.org/abs/2304.08957v3,Climate uncertainty impacts on optimal mitigation pathways and social cost of carbon,2023-04-18 12:47:51+00:00,"['Christopher J. Smith', 'Alaa Al Khourdajie', 'Pu Yang', 'Doris Folini']",econ.GN,"Emissions pathways used in climate policy analysis are often derived from integrated assessment models. However, such emissions pathways do not typically include climate feedbacks on socioeconomic systems and by extension do not consider climate uncertainty in their construction. We use a well-known cost-benefit integrated assessment model, the Dynamic Integrated Climate-Economy (DICE) model, with its climate component replaced by the Finite-amplitude Impulse Response (FaIR) model (v2.1). The climate uncertainty in FaIR is sampled with an ensemble that is consistent with historically observed climate and Intergovernmental Panel on Climate Change (IPCC) assessed ranges of key climate variables such as equilibrium climate sensitivity. Three scenarios are produced: a pathway similar to the ""optimal welfare"" scenario of DICE that has similar warming outcomes to current policies, and pathways that limit warming to ""well-below"" 2C and 1.5C with low overshoot, in line with Paris Agreement long-term temperature goals. Climate uncertainty alone is responsible for a factor of five variation (5-95% range) in the social cost of carbon in the 1.5C scenario. CO2 emissions trajectories resulting from the optimal level of emissions abatement in all pathways are also sensitive to climate uncertainty, with 2050 emissions ranging from -12 to +14 GtCO2/yr in the 1.5C scenario. Equilibrium climate sensitivity and the strength of present-day aerosol effective radiative forcing are strong determinants of social cost of carbon and mid-century CO2 emissions. This shows that narrowing climate uncertainty leads to more refined estimates for the social cost of carbon and provides more certainty about the optimal rate of emissions abatement. Including climate and climate uncertainty in integrated assessment model derived emissions scenarios would address a key missing feedback in scenario construction."
http://arxiv.org/abs/2304.09262v3,With a Grain of Salt: Uncertain Veracity of External News and Firm Disclosures,2023-04-18 19:51:59+00:00,"['Jonathan Libgober', 'Beatrice Michaeli', 'Elyashiv Wiedman']",econ.TH,"We examine how uncertain veracity of external news influences investor beliefs, market prices and corporate disclosures. Despite assuming independence between the news' veracity and the firm's endowment with private information, we find that favorable news is taken ``with a grain of salt'' in equilibrium -- more precisely, perceived as less likely veracious -- which reinforces investor beliefs that nondisclosing managers are hiding disadvantageous information. Hence more favorable external news could paradoxically lead to lower market valuation. That is, amid management silence, stock prices may be non-monotonic in the positivity of external news. In line with mounting empirical evidence, our analysis implies asymmetric price reactions to news and price declines following firm disclosures. We further predict that external news that is more likely veracious may increase or decrease the probability of disclosure and link these effects to empirically observable characteristics."
http://arxiv.org/abs/2304.09078v6,Club coefficients in the UEFA Champions League: Time for shift to an Elo-based formula,2023-04-18 15:51:13+00:00,['László Csató'],stat.AP,"One of the most popular club football tournaments, the UEFA Champions League, will see a fundamental reform from the 2024/25 season: the traditional group stage will be replaced by one league where each of the 36 teams plays eight matches. To guarantee that the opponents of the clubs are of the same strength in the new design, it is crucial to forecast the performance of the teams before the tournament as well as possible. This paper investigates whether the currently used rating of the teams, the UEFA club coefficient, can be improved by taking the games played in the national leagues into account. According to our logistic regression models, a variant of the Elo method provides a higher accuracy in terms of explanatory power in the Champions League matches. The Union of European Football Associations (UEFA) is encouraged to follow the example of the FIFA World Ranking and reform the calculation of the club coefficients in order to avoid unbalanced schedules in the novel tournament format of the Champions League."
http://arxiv.org/abs/2305.08340v1,Efficient Semiparametric Estimation of Average Treatment Effects Under Covariate Adaptive Randomization,2023-05-15 04:23:28+00:00,['Ahnaf Rafi'],econ.EM,"Experiments that use covariate adaptive randomization (CAR) are commonplace in applied economics and other fields. In such experiments, the experimenter first stratifies the sample according to observed baseline covariates and then assigns treatment randomly within these strata so as to achieve balance according to pre-specified stratum-specific target assignment proportions. In this paper, we compute the semiparametric efficiency bound for estimating the average treatment effect (ATE) in such experiments with binary treatments allowing for the class of CAR procedures considered in Bugni, Canay, and Shaikh (2018, 2019). This is a broad class of procedures and is motivated by those used in practice. The stratum-specific target proportions play the role of the propensity score conditional on all baseline covariates (and not just the strata) in these experiments. Thus, the efficiency bound is a special case of the bound in Hahn (1998), but conditional on all baseline covariates. Additionally, this efficiency bound is shown to be achievable under the same conditions as those used to derive the bound by using a cross-fitted Nadaraya-Watson kernel estimator to form nonparametric regression adjustments."
http://arxiv.org/abs/2305.11758v2,The Over-and-Above Implementation of Reserve Policy in India,2023-05-19 15:45:05+00:00,"['Orhan Aygün', 'Bertan Turhan']",econ.TH,"The over-and-above choice rule is the prominent selection procedure to implement affirmative action. In India, it is legally mandated to allocate public school seats and government job positions. This paper presents an axiomatic characterization of the over-and-above choice rule by rigorously stating policy goals as formal axioms. Moreover, we characterize the deferred acceptance mechanism coupled with the over-and-above choice rules for centralized marketplaces."
http://arxiv.org/abs/2305.14503v3,On the Instability of Fractional Reserve Banking,2023-05-23 20:16:50+00:00,['Heon Lee'],econ.TH,"This paper develops a dynamic monetary model to study the (in)stability of the fractional reserve banking system. The model shows that the fractional reserve banking system can endanger stability in that equilibrium is more prone to exhibit endogenous cyclic, chaotic, and stochastic dynamics under lower reserve requirements, although it can increase consumption in the steady-state. Introducing endogenous unsecured credit to the baseline model does not change the main results. The calibrated exercise suggests that this channel could be another source of economic fluctuations. This paper also provides empirical evidence that is consistent with the prediction of the model."
http://arxiv.org/abs/2305.09052v1,Grenander-type Density Estimation under Myerson Regularity,2023-05-15 22:24:46+00:00,['Haitian Xie'],econ.EM,"This study presents a novel approach to the density estimation of private values from second-price auctions, diverging from the conventional use of smoothing-based estimators. We introduce a Grenander-type estimator, constructed based on a shape restriction in the form of a convexity constraint. This constraint corresponds to the renowned Myerson regularity condition in auction theory, which is equivalent to the concavity of the revenue function for selling the auction item. Our estimator is nonparametric and does not require any tuning parameters. Under mild assumptions, we establish the cube-root consistency and show that the estimator asymptotically follows the scaled Chernoff's distribution. Moreover, we demonstrate that the estimator achieves the minimax optimal convergence rate."
http://arxiv.org/abs/2305.12192v1,Volatility jumps and the classification of monetary policy announcements,2023-05-20 13:36:06+00:00,"['Giampiero M. Gallo', 'Demetrio Lacava', 'Edoardo Otranto']",econ.GN,"Central Banks interventions are frequent in response to exogenous events with direct implications on financial market volatility. In this paper, we introduce the Asymmetric Jump Multiplicative Error Model (AJM), which accounts for a specific jump component of volatility within an intradaily framework. Taking the Federal Reserve (Fed) as a reference, we propose a new model-based classification of monetary announcements based on their impact on the jump component of volatility. Focusing on a short window following each Fed's communication, we isolate the impact of monetary announcements from any contamination carried by relevant events that may occur within the same announcement day."
http://arxiv.org/abs/2305.11581v1,"Trustworthy, responsible, ethical AI in manufacturing and supply chains: synthesis and emerging research questions",2023-05-19 10:43:06+00:00,"['Alexandra Brintrup', 'George Baryannis', 'Ashutosh Tiwari', 'Svetan Ratchev', 'Giovanna Martinez-Arellano', 'Jatinder Singh']",cs.AI,"While the increased use of AI in the manufacturing sector has been widely noted, there is little understanding on the risks that it may raise in a manufacturing organisation. Although various high level frameworks and definitions have been proposed to consolidate potential risks, practitioners struggle with understanding and implementing them.
  This lack of understanding exposes manufacturing to a multitude of risks, including the organisation, its workers, as well as suppliers and clients. In this paper, we explore and interpret the applicability of responsible, ethical, and trustworthy AI within the context of manufacturing. We then use a broadened adaptation of a machine learning lifecycle to discuss, through the use of illustrative examples, how each step may result in a given AI trustworthiness concern. We additionally propose a number of research questions to the manufacturing research community, in order to help guide future research so that the economic and societal benefits envisaged by AI in manufacturing are delivered safely and responsibly."
http://arxiv.org/abs/2306.00439v1,Examination of Supernets to Facilitate International Trade for Indian Exports to Brazil,2023-06-01 08:29:05+00:00,"['Evan Winter', 'Anupam Shah', 'Ujjwal Gupta', 'Anshul Kumar', 'Deepayan Mohanty', 'Juan Carlos Uribe', 'Aishwary Gupta', 'Mini P. Thomas']",econ.GN,"The objective of this paper is to investigate a more efficient cross-border payment and document handling process for the export of Indian goods to Brazil. The paper is structured into two sections: first, to explain the problems unique to the India-Brazil international trade corridor by highlighting the obstacles of compliance, speed, and payments; and second, to propose a digital solution for India-brazil trade utilizing Supernets, focusing on the use case of Indian exports. The solution assumes that stakeholders will be onboarded as permissioned actors (i.e. nodes) on a Polygon Supernet. By engaging trade and banking stakeholders, we ensure that the digital solution results in export benefits for Indian exporters, and a lawful channel to receive hard currency payments. The involvement of Brazilian and Indian banks ensures that Letter of Credit (LC) processing time and document handling occur at the speed of blockchain technology. The ultimate goal is to achieve faster settlement and negotiation period while maintaining a regulatory-compliant outcome, so that the end result is faster and easier, yet otherwise identical to the real-world process in terms of export benefits and compliance."
http://arxiv.org/abs/2305.19921v1,Deep Neural Network Estimation in Panel Data Models,2023-05-31 14:58:31+00:00,"['Ilias Chronopoulos', 'Katerina Chrysikou', 'George Kapetanios', 'James Mitchell', 'Aristeidis Raftapostolos']",econ.EM,"In this paper we study neural networks and their approximating power in panel data models. We provide asymptotic guarantees on deep feed-forward neural network estimation of the conditional mean, building on the work of Farrell et al. (2021), and explore latent patterns in the cross-section. We use the proposed estimators to forecast the progression of new COVID-19 cases across the G7 countries during the pandemic. We find significant forecasting gains over both linear panel and nonlinear time series models. Containment or lockdown policies, as instigated at the national-level by governments, are found to have out-of-sample predictive power for new COVID-19 cases. We illustrate how the use of partial derivatives can help open the ""black-box"" of neural networks and facilitate semi-structural analysis: school and workplace closures are found to have been effective policies at restricting the progression of the pandemic across the G7 countries. But our methods illustrate significant heterogeneity and time-variation in the effectiveness of specific containment policies."
http://arxiv.org/abs/2306.17025v1,Would Friedman Burn your Tokens?,2023-06-29 15:19:13+00:00,"['Aggelos Kiayias', 'Philip Lazos', 'Jan Christoph Schlegel']",econ.TH,"Cryptocurrencies come with a variety of tokenomic policies as well as aspirations of desirable monetary characteristics that have been described by proponents as 'sound money' or even 'ultra sound money.' These propositions are typically devoid of economic analysis so it is a pertinent question how such aspirations fit in the wider context of monetary economic theory. In this work, we develop a framework that determines the optimal token supply policy of a cryptocurrency, as well as investigate how such policy may be algorithmically implemented. Our findings suggest that the optimal policy complies with the Friedman rule and it is dependent on the risk free rate, as well as the growth of the cryptocurrency platform. Furthermore, we demonstrate a wide set of conditions under which such policy can be implemented via contractions and expansions of token supply that can be realized algorithmically with block rewards, taxation of consumption and burning the proceeds, and blockchain oracles."
http://arxiv.org/abs/2306.17095v2,Decomposing cryptocurrency high-frequency price dynamics into recurring and noisy components,2023-06-29 16:51:08+00:00,"['Marcin Wątorek', 'Maria Skupień', 'Jarosław Kwapień', 'Stanisław Drożdż']",q-fin.TR,"This paper investigates the temporal patterns of activity in the cryptocurrency market with a focus on Bitcoin, Ethereum, Dogecoin, and WINkLink from January 2020 to December 2022. Market activity measures - logarithmic returns, volume, and transaction number, sampled every 10 seconds, were divided into intraday and intraweek periods and then further decomposed into recurring and noise components via correlation matrix formalism. The key findings include the distinctive market behavior from traditional stock markets due to the nonexistence of trade opening and closing. This was manifest in three enhanced-activity phases aligning with Asian, European, and U.S. trading sessions. An intriguing pattern of activity surge in 15-minute intervals, particularly at full hours, was also noticed, implying the potential role of algorithmic trading. Most notably, recurring bursts of activity in bitcoin and ether were identified to coincide with the release times of significant U.S. macroeconomic reports such as Nonfarm payrolls, Consumer Price Index data, and Federal Reserve statements. The most correlated daily patterns of activity occurred in 2022, possibly reflecting the documented correlations with U.S. stock indices in the same period. Factors that are external to the inner market dynamics are found to be responsible for the repeatable components of the market dynamics, while the internal factors appear to be substantially random, which manifests itself in a good agreement between the empirical eigenvalue distributions in their bulk and the random matrix theory predictions expressed by the Marchenko-Pastur distribution. The findings reported support the growing integration of cryptocurrencies into the global financial markets."
http://arxiv.org/abs/2306.16553v1,Opinion dynamics in communities with major influencers and implicit social influence via mean-field approximation,2023-06-28 20:45:14+00:00,"['Delia Coculescu', 'Médéric Motte', 'Huyên Pham']",math.PR,"We study binary opinion formation in a large population where individuals are influenced by the opinions of other individuals. The population is characterised by the existence of (i) communities where individuals share some similar features, (ii) opinion leaders that may trigger unpredictable opinion shifts in the short term (iii) some degree of incomplete information in the observation of the individual or public opinion processes. In this setting, we study three different approximate mechanisms: common sampling approximation, independent sampling approximation, and, what will be our main focus in this paper, McKean-Vlasov (or mean-field) approximation. We show that all three approximations perform well in terms of different metrics that we introduce for measuring population level and individual level errors. In the presence of a common noise represented by the major influencers opinions processes, and despite the absence of idiosyncratic noises, we derive a propagation of chaos type result. For the particular case of a linear model and particular specifications of the major influencers opinion dynamics, we provide additional analysis, including long term behavior and fluctuations of the public opinion. The theoretical results are complemented by some concrete examples and numerical analysis, illustrating the formation of echo-chambers, the propagation of chaos, and phenomena such as snowball effect and social inertia.42 pages"
http://arxiv.org/abs/2304.06205v2,Difficult Lessons on Social Prediction from Wisconsin Public Schools,2023-04-13 00:59:12+00:00,"['Juan C. Perdomo', 'Tolani Britton', 'Moritz Hardt', 'Rediet Abebe']",cs.CY,"Early warning systems (EWS) are predictive tools at the center of recent efforts to improve graduation rates in public schools across the United States. These systems assist in targeting interventions to individual students by predicting which students are at risk of dropping out. Despite significant investments in their widespread adoption, there remain large gaps in our understanding of the efficacy of EWS, and the role of statistical risk scores in education.
  In this work, we draw on nearly a decade's worth of data from a system used throughout Wisconsin to provide the first large-scale evaluation of the long-term impact of EWS on graduation outcomes. We present empirical evidence that the prediction system accurately sorts students by their dropout risk. We also find that it may have caused a single-digit percentage increase in graduation rates, though our empirical analyses cannot reliably rule out that there has been no positive treatment effect.
  Going beyond a retrospective evaluation of DEWS, we draw attention to a central question at the heart of the use of EWS: Are individual risk scores necessary for effectively targeting interventions? We propose a simple mechanism that only uses information about students' environments -- such as their schools, and districts -- and argue that this mechanism can target interventions just as efficiently as the individual risk score-based mechanism. Our argument holds even if individual predictions are highly accurate and effective interventions exist. In addition to motivating this simple targeting mechanism, our work provides a novel empirical backbone for the robust qualitative understanding among education researchers that dropout is structurally determined. Combined, our insights call into question the marginal value of individual predictions in settings where outcomes are driven by high levels of inequality."
http://arxiv.org/abs/2304.14544v1,Assessing Text Mining and Technical Analyses on Forecasting Financial Time Series,2023-04-27 21:52:36+00:00,['Ali Lashgari'],econ.EM,"Forecasting financial time series (FTS) is an essential field in finance and economics that anticipates market movements in financial markets. This paper investigates the accuracy of text mining and technical analyses in forecasting financial time series. It focuses on the S&P500 stock market index during the pandemic, which tracks the performance of the largest publicly traded companies in the US. The study compares two methods of forecasting the future price of the S&P500: text mining, which uses NLP techniques to extract meaningful insights from financial news, and technical analysis, which uses historical price and volume data to make predictions. The study examines the advantages and limitations of both methods and analyze their performance in predicting the S&P500. The FinBERT model outperforms other models in terms of S&P500 price prediction, as evidenced by its lower RMSE value, and has the potential to revolutionize financial analysis and prediction using financial news data. Keywords: ARIMA, BERT, FinBERT, Forecasting Financial Time Series, GARCH, LSTM, Technical Analysis, Text Mining JEL classifications: G4, C8"
http://arxiv.org/abs/2304.13985v3,The Effects of High-frequency Anticipatory Trading: Small Informed Trader vs. Round-Tripper,2023-04-27 07:20:15+00:00,"['Ziyi Xu', 'Xue Cheng']",q-fin.TR,"In an extended Kyle's model, the interactions between a large informed trader and a high-frequency trader (HFT) who can anticipate the former's incoming order are studied. We find that, in equilibrium, HFT may play the role of Small-IT or Round-Tripper: both of them trade in the same direction as IT in advance, but when IT's order arrives, Small-IT continues to take liquidity away, while Round-Tripper supplies liquidity back. So Small-IT always harms IT, while Round-Tripper may benefit her. What's more, with an anticipatory HFT, normal-speed small uninformed traders suffer less and price discovery is accelerated."
http://arxiv.org/abs/2305.14698v1,Political Conflict and Economic Growth in Post-Independence Venezuela,2023-05-24 04:05:56+00:00,"['Dorothy Kronick', 'Francisco Rodríguez']",econ.GN,"Venezuela has suffered three economic catastrophes since independence: one each in the nineteenth, twentieth, and twenty-first centuries. Prominent explanations for this trilogy point to the interaction of class conflict and resource dependence. We turn attention to intra-class conflict, arguing that the most destructive policy choices stemmed not from the rich defending themselves against the masses but rather from pitched battles among elites. Others posit that Venezuelan political institutions failed to sustain growth because they were insufficiently inclusive; we suggest in addition that they inadequately mediated intra-elite conflict."
http://arxiv.org/abs/2306.09485v1,Identifying key players in dark web marketplaces,2023-06-15 20:30:43+00:00,"['Elohim Fonseca dos Reis', 'Alexander Teytelboym', 'Abeer ElBahraw', 'Ignacio De Loizaga', 'Andrea Baronchelli']",physics.soc-ph,"Dark web marketplaces have been a significant outlet for illicit trade, serving millions of users worldwide for over a decade. However, not all users are the same. This paper aims to identify the key players in Bitcoin transaction networks linked to dark markets and assess their role by analysing a dataset of 40 million Bitcoin transactions involving 31 markets in the period 2011-2021. First, we propose an algorithm that categorizes users either as buyers or sellers and shows that a large fraction of the traded volume is concentrated in a small group of elite market participants. Then, we investigate both market star-graphs and user-to-user networks and highlight the importance of a new class of users, namely `multihomers' who operate on multiple marketplaces concurrently. Specifically, we show how the networks of multihomers and seller-to-seller interactions can shed light on the resilience of the dark market ecosystem against external shocks. Our findings suggest that understanding the behavior of key players in dark web marketplaces is critical to effectively disrupting illegal activities."
http://arxiv.org/abs/2306.08743v1,The rise of the chaebol: A bibliometric analysis of business groups in South Korea,2023-06-14 20:54:04+00:00,['Artur F. Tomeczek'],econ.GN,"South Korea has become one of the most important economies in Asia. The largest Korean multinational firms are affiliated with influential family-owned business groups known as the chaebol. Despite the surging academic popularity of the chaebol, there is a considerable knowledge gap in the bibliometric analysis of business groups in Korea. In an attempt to fill this gap, the article aims to provide a systematic review of the chaebol and the role that business groups have played in the economy of Korea. Three distinct bibliometric networks are analyzed, namely the scientific collaboration network, bibliographic coupling network, and keyword co-occurrence network."
http://arxiv.org/abs/2306.08214v1,Response toward Public Health Policy Ambiguity and Insurance Decisions,2023-06-14 02:59:46+00:00,['Qiang Li'],econ.GN,Adjustments to public health policy are common. This paper investigates the impact of COVID-19 policy ambiguity on specific groups' insurance consumption. The results show that sensitive groups' willingness to pay (WTP) for insurance is 12.2% above the benchmark. Groups that have experienced income disruptions are more likely to suffer this. This paper offers fresh perspectives on the effects of pandemic control shifts.
http://arxiv.org/abs/2305.17615v5,Estimating overidentified linear models with heteroskedasticity and outliers,2023-05-28 03:04:57+00:00,['Lei Bill Wang'],econ.EM,"A large degree of overidentification causes severe bias in TSLS. A conventional heuristic rule used to motivate new estimators in this context is approximate bias. This paper formalizes the definition of approximate bias and expands the applicability of approximate bias to various classes of estimators that bridge OLS, TSLS, and Jackknife IV estimators (JIVEs). By evaluating their approximate biases, I propose new approximately unbiased estimators, including UOJIVE1 and UOJIVE2. UOJIVE1 can be interpreted as a generalization of an existing estimator UIJIVE1. Both UOJIVEs are proven to be consistent and asymptotically normal under a fixed number of instruments and controls. The asymptotic proofs for UOJIVE1 in this paper require the absence of high leverage points, whereas proofs for UOJIVE2 do not. In addition, UOJIVE2 is consistent under many-instrument asymptotic. The simulation results align with the theorems in this paper: (i) Both UOJIVEs perform well under many instrument scenarios with or without heteroskedasticity, (ii) When a high leverage point coincides with a high variance of the error term, an outlier is generated and the performance of UOJIVE1 is much poorer than that of UOJIVE2."
http://arxiv.org/abs/2306.17773v2,Obvious Manipulations in Matching without and with Contracts,2023-06-30 16:25:47+00:00,"['R. Pablo Arribillaga', 'E. Pepa Risma']",econ.TH,"This paper explores many-to-one matching models, both with and without contracts, where doctors' preferences are private and hospitals' preferences are public and substitutable. It is known that any stable-dominating mechanism --which is either stable or individually rational and Pareto-dominates (from the doctors' perspective) a stable mechanism--, is susceptible to manipulation by doctors. Our study focuses on \textit{obvious manipulations} and identifies stable-dominating mechanisms that prevent them. Without contracts, we show that more efficient mechanisms are less likely to be obviously manipulable and that any stable-dominating mechanism is not obviously manipulable. However, with contracts, none of these results hold. While we demonstrate that the Doctor-Proposing Deferred Acceptance (DA) Mechanism remains not obviously manipulable, we show that the Hospital-Proposing DA Mechanism and any efficient mechanism that Pareto-dominates the Doctor-Proposing DA Mechanism become (very) obviously manipulable, in the model with contracts."
http://arxiv.org/abs/2306.04135v3,Semiparametric Discrete Choice Models for Bundles,2023-06-07 04:12:02+00:00,"['Fu Ouyang', 'Thomas T. Yang']",econ.EM,"We propose two approaches to estimate semiparametric discrete choice models for bundles. Our first approach is a kernel-weighted rank estimator based on a matching-based identification strategy. We establish its complete asymptotic properties and prove the validity of the nonparametric bootstrap for inference. We then introduce a new multi-index least absolute deviations (LAD) estimator as an alternative, of which the main advantage is its capacity to estimate preference parameters on both alternative- and agent-specific regressors. Both methods can account for arbitrary correlation in disturbances across choices, with the former also allowing for interpersonal heteroskedasticity. We also demonstrate that the identification strategy underlying these procedures can be extended naturally to panel data settings, producing an analogous localized maximum score estimator and a LAD estimator for estimating bundle choice models with fixed effects. We derive the limiting distribution of the former and verify the validity of the numerical bootstrap as an inference tool. All our proposed methods can be applied to general multi-index models. Monte Carlo experiments show that they perform well in finite samples."
http://arxiv.org/abs/2306.05593v2,Localized Neural Network Modelling of Time Series: A Case Study on US Monetary Policy,2023-06-08 23:41:06+00:00,"['Jiti Gao', 'Fei Liu', 'Bin Peng', 'Yanrong Yang']",econ.EM,"In this paper, we investigate a semiparametric regression model under the context of treatment effects via a localized neural network (LNN) approach. Due to a vast number of parameters involved, we reduce the number of effective parameters by (i) exploring the use of identification restrictions; and (ii) adopting a variable selection method based on the group-LASSO technique. Subsequently, we derive the corresponding estimation theory and propose a dependent wild bootstrap procedure to construct valid inferences accounting for the dependence of data. Finally, we validate our theoretical findings through extensive numerical studies. In an empirical study, we revisit the impacts of a tightening monetary policy action on a variety of economic variables, including short-/long-term interest rate, inflation, unemployment rate, industrial price and equity return via the newly proposed framework using a monthly dataset of the US."
http://arxiv.org/abs/2305.08488v2,Hierarchical DCC-HEAVY Model for High-Dimensional Covariance Matrices,2023-05-15 09:44:24+00:00,"['Emilija Dzuverovic', 'Matteo Barigozzi']",econ.EM,"We introduce a HD DCC-HEAVY class of hierarchical-type factor models for high-dimensional covariance matrices, employing the realized measures built from higher-frequency data. The modelling approach features straightforward estimation and forecasting schemes, independent of the cross-sectional dimension of the assets under consideration, and accounts for sophisticated asymmetric dynamics in the covariances. Empirical analyses suggest that the HD DCC-HEAVY models have a better in-sample fit and deliver statistically and economically significant out-of-sample gains relative to the existing hierarchical factor model and standard benchmarks. The results are robust under different frequencies and market conditions."
http://arxiv.org/abs/2306.04177v3,Semiparametric Efficiency Gains From Parametric Restrictions on Propensity Scores,2023-06-07 06:06:48+00:00,['Haruki Kono'],econ.EM,"We explore how much knowing a parametric restriction on propensity scores improves semiparametric efficiency bounds in the potential outcome framework. For stratified propensity scores, considered as a parametric model, we derive explicit formulas for the efficiency gain from knowing how the covariate space is split. Based on these, we find that the efficiency gain decreases as the partition of the stratification becomes finer. For general parametric models, where it is hard to obtain explicit representations of efficiency bounds, we propose a novel framework that enables us to see whether knowing a parametric model is valuable in terms of efficiency even when it is high-dimensional. In addition to the intuitive fact that knowing the parametric model does not help much if it is sufficiently flexible, we discover that the efficiency gain can be nearly zero even though the parametric assumption significantly restricts the space of possible propensity scores."
http://arxiv.org/abs/2306.03073v2,Inference for Local Projections,2023-06-05 17:50:07+00:00,"['Atsushi Inoue', 'Òscar Jordà', 'Guido M. Kuersteiner']",econ.EM,"Inference for impulse responses estimated with local projections presents interesting challenges and opportunities. Analysts typically want to assess the precision of individual estimates, explore the dynamic evolution of the response over particular regions, and generally determine whether the impulse generates a response that is any different from the null of no effect. Each of these goals requires a different approach to inference. In this article, we provide an overview of results that have appeared in the literature in the past 20 years along with some new procedures that we introduce here."
http://arxiv.org/abs/2304.01273v3,Heterogeneity-robust granular instruments,2023-04-03 18:07:56+00:00,['Eric Qian'],econ.EM,"Granular instrumental variables (GIV) has experienced sharp growth in empirical macro-finance. The methodology's rise showcases granularity's potential for identification across many economic environments, like the estimation of spillovers and demand systems. I propose a new estimator--called robust granular instrumental variables (RGIV)--that enables studying unit-level heterogeneity in spillovers. Unlike existing methods that assume heterogeneity is a function of observables, RGIV leaves heterogeneity unrestricted. In contrast to the baseline GIV estimator, RGIV allows for unknown shock variances and equal-sized units. Applied to the Euro area, I find strong evidence of country-level heterogeneity in sovereign yield spillovers."
http://arxiv.org/abs/2305.01201v3,Estimating Input Coefficients for Regional Input-Output Tables Using Deep Learning with Mixup,2023-05-02 04:34:09+00:00,['Shogo Fukui'],econ.EM,"An input-output table is an important data for analyzing the economic situation of a region. Generally, the input-output table for each region (regional input-output table) in Japan is not always publicly available, so it is necessary to estimate the table. In particular, various methods have been developed for estimating input coefficients, which are an important part of the input-output table. Currently, non-survey methods are often used to estimate input coefficients because they require less data and computation, but these methods have some problems, such as discarding information and requiring additional data for estimation.
  In this study, the input coefficients are estimated by approximating the generation process with an artificial neural network (ANN) to mitigate the problems of the non-survey methods and to estimate the input coefficients with higher precision. To avoid over-fitting due to the small data used, data augmentation, called mixup, is introduced to increase the data size by generating virtual regions through region composition and scaling.
  By comparing the estimates of the input coefficients with those of Japan as a whole, it is shown that the accuracy of the method of this research is higher and more stable than that of the conventional non-survey methods. In addition, the estimated input coefficients for the three cities in Japan are generally close to the published values for each city."
http://arxiv.org/abs/2306.00296v2,Inference in Predictive Quantile Regressions,2023-06-01 02:32:47+00:00,"['Alex Maynard', 'Katsumi Shimotsu', 'Nina Kuriyama']",econ.EM,"This paper studies inference in predictive quantile regressions when the predictive regressor has a near-unit root. We derive asymptotic distributions for the quantile regression estimator and its heteroskedasticity and autocorrelation consistent (HAC) t-statistic in terms of functionals of Ornstein-Uhlenbeck processes. We then propose a switching-fully modified (FM) predictive test for quantile predictability. The proposed test employs an FM style correction with a Bonferroni bound for the local-to-unity parameter when the predictor has a near unit root. It switches to a standard predictive quantile regression test with a slightly conservative critical value when the largest root of the predictor lies in the stationary range. Simulations indicate that the test has a reliable size in small samples and good power. We employ this new methodology to test the ability of three commonly employed, highly persistent and endogenous lagged valuation regressors - the dividend price ratio, earnings price ratio, and book-to-market ratio - to predict the median, shoulders, and tails of the stock return distribution."
http://arxiv.org/abs/2304.03069v4,Adaptive Student's t-distribution with method of moments moving estimator for nonstationary time series,2023-04-06 13:37:27+00:00,['Jarek Duda'],stat.ME,"The real life time series are usually nonstationary, bringing a difficult question of model adaptation. Classical approaches like ARMA-ARCH assume arbitrary type of dependence. To avoid their bias, we will focus on recently proposed agnostic philosophy of moving estimator: in time $t$ finding parameters optimizing e.g. $F_t=\sum_{τ<t} (1-η)^{t-τ} \ln(ρ_θ(x_τ))$ moving log-likelihood, evolving in time. It allows for example to estimate parameters using inexpensive exponential moving averages (EMA), like absolute central moments $m_p=E[|x-μ|^p]$ evolving for one or multiple powers $p\in\mathbb{R}^+$ using $m_{p,t+1} = m_{p,t} + η(|x_t-μ_t|^p-m_{p,t})$. Application of such general adaptive methods of moments will be presented on Student's t-distribution, popular especially in economical applications, here applied to log-returns of DJIA companies. While standard ARMA-ARCH approaches provide evolution of $μ$ and $σ$, here we also get evolution of $ν$ describing $ρ(x)\sim |x|^{-ν-1}$ tail shape, probability of extreme events - which might turn out catastrophic, destabilizing the market."
http://arxiv.org/abs/2304.07108v3,Mean-field equilibrium price formation with exponential utility,2023-04-14 12:57:14+00:00,"['Masaaki Fujii', 'Masashi Sekine']",q-fin.MF,"In this paper, using the mean-field game theory, we study a problem of equilibrium price formation among many investors with exponential utility in the presence of liabilities unspanned by the security prices. The investors are heterogeneous in their initial wealth, risk-averseness parameter, as well as stochastic liability at the terminal time. We characterize the equilibrium risk-premium process of the risky stocks in terms of the solution to a novel mean-field backward stochastic differential equation (BSDE), whose driver has quadratic growth both in the stochastic integrands and in their conditional expectations. We prove the existence of a solution to the mean-field BSDE under several conditions and show that the resultant risk-premium process actually clears the market in the large population limit."
http://arxiv.org/abs/2304.05805v4,GDP nowcasting with artificial neural networks: How much does long-term memory matter?,2023-04-12 12:29:58+00:00,"['Kristóf Németh', 'Dániel Hadházi']",econ.EM,"We apply artificial neural networks (ANNs) to nowcast quarterly GDP growth for the U.S. economy. Using the monthly FRED-MD database, we compare the nowcasting performance of five different ANN architectures: the multilayer perceptron (MLP), the one-dimensional convolutional neural network (1D CNN), the Elman recurrent neural network (RNN), the long short-term memory network (LSTM), and the gated recurrent unit (GRU). The empirical analysis presents results from two distinctively different evaluation periods. The first (2012:Q1 -- 2019:Q4) is characterized by balanced economic growth, while the second (2012:Q1 -- 2024:Q2) also includes periods of the COVID-19 recession. During the first evaluation period, longer input sequences slightly improve nowcasting performance for some ANNs, but the best accuracy is still achieved with 8-month-long input sequences at the end of the nowcasting window. Results from the second test period depict the role of long-term memory even more clearly. The MLP, the 1D CNN, and the Elman RNN work best with 8-month-long input sequences at each step of the nowcasting window. The relatively weak performance of the gated RNNs also suggests that architectural features enabling long-term memory do not result in more accurate nowcasts for GDP growth. The combined results indicate that the 1D CNN seems to represent a \textit{``sweet spot''} between the simple time-agnostic MLP and the more complex (gated) RNNs. The network generates nearly as accurate nowcasts as the best competitor for the first test period, while it achieves the overall best accuracy during the second evaluation period. Consequently, as a first in the literature, we propose the application of the 1D CNN for economic nowcasting."
http://arxiv.org/abs/2304.13925v2,Difference-in-Differences with Compositional Changes,2023-04-27 02:30:24+00:00,"[""Pedro H. C. Sant'Anna"", 'Qi Xu']",econ.EM,"This paper studies difference-in-differences (DiD) setups with repeated cross-sectional data and potential compositional changes across time periods. We begin our analysis by deriving the efficient influence function and the semiparametric efficiency bound for the average treatment effect on the treated (ATT). We introduce nonparametric estimators that attain the semiparametric efficiency bound under mild rate conditions on the estimators of the nuisance functions, exhibiting a type of rate doubly robust (DR) property. Additionally, we document a trade-off related to compositional changes: We derive the asymptotic bias of DR DiD estimators that erroneously exclude compositional changes and the efficiency loss when one fails to correctly rule out compositional changes. We propose a nonparametric Hausman-type test for compositional changes based on these trade-offs. The finite sample performance of the proposed DiD tools is evaluated through Monte Carlo experiments and an empirical application. We consider extensions of our framework that accommodate double machine learning procedures with cross-fitting, and setups when some units are observed in both pre- and post-treatment periods. As a by-product of our analysis, we present a new uniform stochastic expansion of the local polynomial multinomial logit estimator, which may be of independent interest."
http://arxiv.org/abs/2306.13419v1,Multivariate Simulation-based Forecasting for Intraday Power Markets: Modelling Cross-Product Price Effects,2023-06-23 10:12:31+00:00,"['Simon Hirsch', 'Florian Ziel']",q-fin.ST,"Intraday electricity markets play an increasingly important role in balancing the intermittent generation of renewable energy resources, which creates a need for accurate probabilistic price forecasts. However, research to date has focused on univariate approaches, while in many European intraday electricity markets all delivery periods are traded in parallel. Thus, the dependency structure between different traded products and the corresponding cross-product effects cannot be ignored. We aim to fill this gap in the literature by using copulas to model the high-dimensional intraday price return vector. We model the marginal distribution as a zero-inflated Johnson's $S_U$ distribution with location, scale and shape parameters that depend on market and fundamental data. The dependence structure is modelled using latent beta regression to account for the particular market structure of the intraday electricity market, such as overlapping but independent trading sessions for different delivery days. We allow the dependence parameter to be time-varying. We validate our approach in a simulation study for the German intraday electricity market and find that modelling the dependence structure improves the forecasting performance. Additionally, we shed light on the impact of the single intraday coupling (SIDC) on the trading activity and price distribution and interpret our results in light of the market efficiency hypothesis. The approach is directly applicable to other European electricity markets."
http://arxiv.org/abs/2308.02491v1,Mapping Global Value Chains at the Product Level,2023-06-12 10:12:06+00:00,"['Lea Karbevska', 'César A. Hidalgo']",econ.GN,"Value chain data is crucial to navigate economic disruptions, such as those caused by the COVID-19 pandemic and the war in Ukraine. Yet, despite its importance, publicly available value chain datasets, such as the ``World Input-Output Database'', ``Inter-Country Input-Output Tables'', ``EXIOBASE'' or the ``EORA'', lack detailed information about products (e.g. Radio Receivers, Telephones, Electrical Capacitors, LCDs, etc.) and rely instead on more aggregate industrial sectors (e.g. Electrical Equipment, Telecommunications). Here, we introduce a method based on machine learning and trade theory to infer product-level value chain relationships from fine-grained international trade data. We apply our method to data summarizing the exports and imports of 300+ world regions (e.g. states in the U.S., prefectures in Japan, etc.) and 1200+ products to infer value chain information implicit in their trade patterns. Furthermore, we use proportional allocation to assign the trade flow between regions and countries. This work provides an approximate method to map value chain data at the product level with a relevant trade flow, that should be of interest to people working in logistics, trade, and sustainable development."
http://arxiv.org/abs/2304.00482v1,Immigrant assimilation in health care utilisation in Spain,2023-04-02 08:21:56+00:00,"['Zuleika Ferre', 'Patricia Triunfo', 'José-Ignacio Antón']",econ.GN,"Abundant evidence has tracked the labour market and health assimilation of immigrants, including static analyses of differences in how foreign-born and native-born residents consume health care services. However, we know much less about how migrants' patterns of health care usage evolve with time of residence, especially in countries providing universal or quasi-universal coverage. We investigate this process in Spain by combining all the available waves of the local health survey, which allows us to separately identify period, cohort, and assimilation effects. We find that the evidence of health assimilation is limited and solely applies to migrant females' visits to general practitioners. Nevertheless, the differential effects of ageing on health care use between foreign-born and native-born populations contributes to the convergence of utilisation patterns in most health services after 20 years in Spain. Substantial heterogeneity over time and by region of origin both suggest that studies modelling future welfare state finances would benefit from a more thorough assessment of migration."
http://arxiv.org/abs/2304.10636v2,The quality of school track assignment decisions by teachers,2023-04-20 20:29:27+00:00,"['Joppe de Ree', 'Matthijs Oosterveen', 'Dinand Webbink']",econ.GN,"This paper analyzes the effects of educational tracking and the quality of track assignment decisions. We motivate our analysis using a model of optimal track assignment under uncertainty. This model generates predictions about the average effects of tracking at the margin of the assignment process. In addition, we recognize that the average effects do not measure noise in the assignment process, as they may reflect a mix of both positive and negative tracking effects. To test these ideas, we develop a flexible causal approach that separates, organizes, and partially identifies tracking effects of any sign or form. We apply this approach in the context of a regression discontinuity design in the Netherlands, where teachers issue track recommendations that may be revised based on test score cutoffs, and where in some cases parents can overrule this recommendation. Our results indicate substantial tracking effects: between 40% and 100% of reassigned students are positively or negatively affected by enrolling in a higher track. Most tracking effects are positive, however, with students benefiting from being placed in a higher, more demanding track. While based on the current analysis we cannot reject the hypothesis that teacher assignments are unbiased, this result seems only consistent with a significant degree of noise. We discuss that parental decisions, whether to follow or deviate from teacher recommendations, may help reducing this noise."
http://arxiv.org/abs/2306.16904v2,Endogenous Barriers to Learning,2023-06-29 12:49:26+00:00,['Olivier Compte'],econ.GN,"Building on the idea that lack of experience is a source of errors but that experience should reduce them, we model agents' behavior using a stochastic choice model (logit quantal response), leaving endogenous the accuracy of their choices. In some games, higher accuracy leads to unstable logit-response dynamics. Starting from the lowest possible accuracy, we define the barrier to learning as the maximum accuracy which keeps the logit-response dynamic stable (for all lower accuracies). This defines a limit quantal response equilibrium. We apply the concept to centipede, travelers' dilemma, and 11-20 money-request games and to first-price and all-pay auctions, and discuss the role of strategy restrictions in reducing or amplifying barriers to learning."
http://arxiv.org/abs/2304.12647v3,Learned Collusion,2023-04-25 08:25:10+00:00,['Olivier Compte'],econ.TH,"Q-learning can be described as an all-purpose automaton that provides estimates (Q-values) of the continuation values associated with each available action and follows the naive policy of almost always choosing the action with highest Q-value. We consider a family of automata based on Q-values, whose policy may systematically favor some actions over others, for example through a bias that favors cooperation. We look for stable equilibrium biases, easily learned under converging logit/best-response dynamics over biases, not requiring any tacit agreement. These biases strongly foster collusion or cooperation across a rich array of payoff and monitoring structures, independently of initial Q-values."
http://arxiv.org/abs/2306.14247v3,Selling Multiple Complements with Packaging Costs,2023-06-25 13:44:42+00:00,['Simon Finster'],econ.TH,"We consider a package assignment problem with multiple units of indivisible items. The seller can specify preferences over partitions of their supply between buyers as packaging costs. We propose incremental costs together with a graph that defines cost interdependence to express these preferences. This facilitates the use of linear programming to characterize Walrasian equilibrium prices. Firstly, we show that equilibrium prices are uniform, anonymous, and linear in packages. Prices and marginal gains exhibit a nested structure, which we characterize in closed form for complete graphs. Secondly, we provide sufficient conditions for the existence of package-linear competitive prices using an ascending auction implementation. Our framework of partition preferences ensures fair and transparent dual pricing and admits preferences over the concentration of allocated bundles in the market."
http://arxiv.org/abs/2305.02185v4,Doubly Robust Uniform Confidence Bands for Group-Time Conditional Average Treatment Effects in Difference-in-Differences,2023-05-03 15:29:22+00:00,"['Shunsuke Imai', 'Lei Qin', 'Takahide Yanagi']",econ.EM,"We consider a panel data analysis to examine the heterogeneity in treatment effects with respect to groups, periods, and a pre-treatment covariate of interest in the staggered difference-in-differences setting of Callaway and Sant'Anna (2021). Under standard identification conditions, a doubly robust estimand conditional on the covariate identifies the group-time conditional average treatment effect given the covariate. Focusing on the case of a continuous covariate, we propose a three-step estimation procedure based on nonparametric local polynomial regressions and parametric estimation methods. Using uniformly valid distributional approximation results for empirical processes and weighted/multiplier bootstrapping, we develop doubly robust inference methods to construct uniform confidence bands for the group-time conditional average treatment effect function and a variety of useful summary parameters. The accompanying R package didhetero allows for easy implementation of our methods."
http://arxiv.org/abs/2306.11979v4,Qini Curves for Multi-Armed Treatment Rules,2023-06-21 02:12:07+00:00,"['Erik Sverdrup', 'Han Wu', 'Susan Athey', 'Stefan Wager']",stat.ME,"Qini curves have emerged as an attractive and popular approach for evaluating the benefit of data-driven targeting rules for treatment allocation. We propose a generalization of the Qini curve to multiple costly treatment arms, that quantifies the value of optimally selecting among both units and treatment arms at different budget levels. We develop an efficient algorithm for computing these curves and propose bootstrap-based confidence intervals that are exact in large samples for any point on the curve. These confidence intervals can be used to conduct hypothesis tests comparing the value of treatment targeting using an optimal combination of arms with using just a subset of arms, or with a non-targeting assignment rule ignoring covariates, at different budget levels. We demonstrate the statistical performance in a simulation experiment and an application to treatment targeting for election turnout."
http://arxiv.org/abs/2306.08559v3,Inference in clustered IV models with many and weak instruments,2023-06-14 15:05:48+00:00,['Johannes W. Ligtenberg'],econ.EM,"Data clustering reduces the effective sample size from the number of observations towards the number of clusters. For instrumental variable models this reduced effective sample size makes the instruments more likely to be weak, in the sense that they contain little information about the endogenous regressor, and many, in the sense that their number is large compared to the sample size. Consequently, weak and many instrument problems for estimators and tests in instrumental variable models are also more likely. None of the previously developed many and weak instrument robust tests, however, can be applied to clustered data as they all require independent observations. Therefore, I adapt the many and weak instrument robust jackknife Anderson--Rubin and jackknife score tests to clustered data by removing clusters rather than individual observations from the statistics. Simulations and a revisitation of a study on the effect of queenly reign on war show the empirical relevance of the new tests."
http://arxiv.org/abs/2307.11857v1,Scenario Sampling for Large Supermodular Games,2023-07-21 18:51:32+00:00,"['Bryan S. Graham', 'Andrin Pelican']",econ.EM,"This paper introduces a simulation algorithm for evaluating the log-likelihood function of a large supermodular binary-action game. Covered examples include (certain types of) peer effect, technology adoption, strategic network formation, and multi-market entry games. More generally, the algorithm facilitates simulated maximum likelihood (SML) estimation of games with large numbers of players, $T$, and/or many binary actions per player, $M$ (e.g., games with tens of thousands of strategic actions, $TM=O(10^4)$). In such cases the likelihood of the observed pure strategy combination is typically (i) very small and (ii) a $TM$-fold integral who region of integration has a complicated geometry. Direct numerical integration, as well as accept-reject Monte Carlo integration, are computationally impractical in such settings. In contrast, we introduce a novel importance sampling algorithm which allows for accurate likelihood simulation with modest numbers of simulation draws."
http://arxiv.org/abs/2308.15062v3,Forecasting with Feedback,2023-08-29 06:50:13+00:00,"['Robert P. Lieli', 'Augusto Nieto-Barthaburu']",econ.TH,"Systematically biased forecasts are typically interpreted as evidence of forecasters' irrationality and/or asymmetric loss. In this paper we propose an alternative explanation: when forecasts inform economic policy decisions, and the resulting actions affect the realization of the forecast target itself, forecasts may be optimally biased even under quadratic loss. The result arises in environments in which the forecaster is uncertain about the decision maker's reaction to the forecast, which is presumably the case in most applications. We illustrate the empirical relevance of our theory by reviewing some stylized properties of Green Book inflation forecasts and relating them to the predictions from our model. Our results point out that the presence of policy feedback poses a challenge to traditional tests of forecast rationality."
http://arxiv.org/abs/2307.13849v1,The Core of Bayesian Persuasion,2023-07-25 22:48:55+00:00,"['Laura Doval', 'Ran Eilat']",econ.TH,"An analyst observes the frequency with which an agent takes actions, but not the frequency with which she takes actions conditional on a payoff relevant state. In this setting, we ask when the analyst can rationalize the agent's choices as the outcome of the agent learning something about the state before taking action. Our characterization marries the obedience approach in information design (Bergemann and Morris, 2016) and the belief approach in Bayesian persuasion (Kamenica and Gentzkow, 2011) relying on a theorem by Strassen (1965) and Hall's marriage theorem. We apply our results to ring-network games and to identify conditions under which a data set is consistent with a public information structure in first-order Bayesian persuasion games."
http://arxiv.org/abs/2307.14282v3,Causal Effects in Matching Mechanisms with Strategically Reported Preferences,2023-07-26 16:35:42+00:00,"['Marinho Bertanha', 'Margaux Luflade', 'Ismael Mourifié']",econ.EM,"A growing number of central authorities use assignment mechanisms to allocate students to schools in a way that reflects student preferences and school priorities. However, most real-world mechanisms incentivize students to strategically misreport their preferences. Misreporting complicates the identification of causal parameters that depend on true preferences, which are necessary inputs for a broad class of counterfactual analyses. In this paper, we provide an identification approach that is robust to strategic misreporting and derive sharp bounds on causal effects of school assignment on future outcomes. Our approach applies to any mechanism as long as there exist placement scores and cutoffs that characterize that mechanism's allocation rule. We use data from a deferred acceptance mechanism that assigns students to more than 1,000 university--major combinations in Chile. Matching theory predicts and empirical evidence suggests that students behave strategically in Chile because they face constraints on their submission of preferences and have good a priori information on the schools they will have access to. Our bounds are informative enough to reveal significant heterogeneity in graduation success with respect to preferences and school assignment."
http://arxiv.org/abs/2307.06684v2,"The Heterogeneous Earnings Impact of Job Loss Across Workers, Establishments, and Markets",2023-07-13 11:11:38+00:00,"['Susan Athey', 'Lisa K. Simon', 'Oskar N. Skans', 'Johan Vikstrom', 'Yaroslav Yakymovych']",econ.GN,"Using generalized random forests and rich Swedish administrative data, we show that the earnings effects of job displacement due to establishment closures are extremely heterogeneous across and within (observable) worker types, establishments, and markets. The decile with the largest predicted effects loses 50 percent of annual earnings the year after displacement and losses accumulate to 200 percent over 7 years. The least affected decile experiences only marginal losses of 6 percent in the year after displacement. Prior to displacement workers in the most affected decile were lower paid and had negative earnings trajectories. Workers with large predicted effects are more sensitive to adverse market conditions than other workers. When restricting attention to simple targeting rules, the subgroup consisting of older workers in routine-task intensive jobs has the highest predictable effects of displacement."
http://arxiv.org/abs/2307.07037v1,The Determinants of Foreign Direct Investment (FDI) A Panel Data Analysis for the Emerging Asian Economies,2023-07-13 19:42:06+00:00,['ATM Omor Faruq'],econ.GN,"In this paper, we explore the economic, institutional, and political/governmental factors in attracting Foreign Direct Investment (FDI) inflows in the emerging twenty-four Asian economies. To examine the significant determinants of FDI, the study uses panel data for a period of seventeen years (2002-2018). The panel methodology enables us to deal with endogeneity and other issues. Multiple regression models are done for empirical evidence. The study focuses on a holistic approach and considers different variables under three broad areas: economic, institutional, and political aspects. The variables include Market Size, Trade Openness, Inflation, Natural Resource, Lending Rate, Capital Formation as economic factors and Business Regulatory Environment and Business Disclosure Index as institutional factors and Political Stability, Government Effectiveness, and Rule of Law as political factors. The empirical findings show most of the economic factors significantly affect FDI inflows whereas Business Disclosure is the only important institutional variable. Moreover, political stability has a significant positive impact in attracting foreign capital flow though the impact of government effectiveness is found insignificant. Overall, the economic factors prevail strongly compared to institutional and political factors."
http://arxiv.org/abs/2307.08011v2,Quantal Response Equilibrium with a Continuum of Types: Characterization and Nonparametric Identification,2023-07-16 11:40:06+00:00,"['Evan Friedman', 'Duarte Gonçalves']",econ.TH,"Quantal response equilibrium (QRE), a statistical generalization of Nash equilibrium, is a standard benchmark in the analysis of experimental data. Despite its influence, nonparametric characterizations and tests of QRE are unavailable beyond the case of finite games. We address this gap by completely characterizing the set of QRE in a class of binary-action games with a continuum of types. Our characterization provides sharp predictions in settings such as global games, volunteer's dilemma, and the compromise game. Further, we leverage our results to develop nonparametric tests of QRE. As an empirical application, we revisit the experimental data from Carrillo and Palfrey (2009) on the compromise game."
http://arxiv.org/abs/2307.06145v1,Robust Impulse Responses using External Instruments: the Role of Information,2023-07-12 13:00:00+00:00,"['Davide Brignone', 'Alessandro Franconi', 'Marco Mazzali']",econ.EM,"External-instrument identification leads to biased responses when the shock is not invertible and the measurement error is present. We propose to use this identification strategy in a structural Dynamic Factor Model, which we call Proxy DFM. In a simulation analysis, we show that the Proxy DFM always successfully retrieves the true impulse responses, while the Proxy SVAR systematically fails to do so when the model is either misspecified, does not include all relevant information, or the measurement error is present. In an application to US monetary policy, the Proxy DFM shows that a tightening shock is unequivocally contractionary, with deteriorations in domestic demand, labor, credit, housing, exchange, and financial markets. This holds true for all raw instruments available in the literature. The variance decomposition analysis highlights the importance of monetary policy shocks in explaining economic fluctuations, albeit at different horizons."
http://arxiv.org/abs/2307.09479v1,A Model of Competitive Assortment Planning Algorithm,2023-07-16 14:15:18+00:00,['Dipankar Das'],econ.TH,"With a novel search algorithm or assortment planning or assortment optimization algorithm that takes into account a Bayesian approach to information updating and two-stage assortment optimization techniques, the current research provides a novel concept of competitiveness in the digital marketplace. Via the search algorithm, there is competition between the platform, vendors, and private brands of the platform. The current paper suggests a model and discusses how competition and collusion arise in the digital marketplace through assortment planning or assortment optimization algorithm. Furthermore, it suggests a model of an assortment algorithm free from collusion between the platform and the large vendors. The paper's major conclusions are that collusive assortment may raise a product's purchase likelihood but fail to maximize expected revenue. The proposed assortment planning, on the other hand, maintains competitiveness while maximizing expected revenue."
http://arxiv.org/abs/2307.11039v1,Indicatori comuni del PNRR e framework SDGs: una proposta di indicatore composito,2023-07-20 17:21:14+00:00,"['Fabio Bacchini', 'Lorenzo Di Biagio', 'Giampiero M. Gallo', 'Vincenzo Spinelli']",econ.GN,"The main component of the NextGeneration EU (NGEU) program is the Recovery and Resilience Facility (RRF), spanning an implementation period between 2021 and 2026. The RRF also includes a monitoring system: every six months, each country is required to send an update on the progress of the plan against 14 common indicators, measured on specific quantitative scales. The aim of this paper is to present the first empirical evidence on this system, while, at the same time, emphasizing the potential of its integration with the sustainable development framework (SDGs). We propose to develop a first linkage between the 14 common indicators and the SDGs which allows us to produce a composite index (SDGs-RRF) for France, Germany, Italy, and Spain for the period 2014-2021. Over this time, widespread improvements in the composite index across the four countries led to a partial reduction of the divergence. The proposed approach represents a first step towards a wider use of the SDGs for the assessment of the RRF, in line with their use in the European Semester documents prepared by the European Commission."
http://arxiv.org/abs/2307.11137v3,Of Models and Tin Men: A Behavioural Economics Study of Principal-Agent Problems in AI Alignment using Large-Language Models,2023-07-20 17:19:15+00:00,"['Steve Phelps', 'Rebecca Ranson']",cs.AI,"AI Alignment is often presented as an interaction between a single designer and an artificial agent in which the designer attempts to ensure the agent's behavior is consistent with its purpose, and risks arise solely because of conflicts caused by inadvertent misalignment between the utility function intended by the designer and the resulting internal utility function of the agent. With the advent of agents instantiated with large-language models (LLMs), which are typically pre-trained, we argue this does not capture the essential aspects of AI safety because in the real world there is not a one-to-one correspondence between designer and agent, and the many agents, both artificial and human, have heterogeneous values. Therefore, there is an economic aspect to AI safety and the principal-agent problem is likely to arise. In a principal-agent problem conflict arises because of information asymmetry together with inherent misalignment between the utility of the agent and its principal, and this inherent misalignment cannot be overcome by coercing the agent into adopting a desired utility function through training. We argue the assumptions underlying principal-agent problems are crucial to capturing the essence of safety problems involving pre-trained AI models in real-world situations. Taking an empirical approach to AI safety, we investigate how GPT models respond in principal-agent conflicts. We find that agents based on both GPT-3.5 and GPT-4 override their principal's objectives in a simple online shopping task, showing clear evidence of principal-agent conflict. Surprisingly, the earlier GPT-3.5 model exhibits more nuanced behaviour in response to changes in information asymmetry, whereas the later GPT-4 model is more rigid in adhering to its prior alignment. Our results highlight the importance of incorporating principles from economics into the alignment process."
http://arxiv.org/abs/2309.09176v3,Odd period cycles and ergodic properties in price dynamics for an exchange economy,2023-09-17 06:55:36+00:00,['Tomohiro Uchiyama'],econ.GN,"In the first part of this paper (Sections 1-4), we study a standard exchange economy model with Cobb-Douglas type consumers and give a necessary and sufficient condition for the existence of an odd period cycle in the Walras-Samuelson (tatonnement) price adjustment process. We also give a sufficient condition for a price to be eventually attracted to a chaotic region. In the second part (Sections 5 and 6), we investigate ergodic properties of the price dynamics showing that the existence of chaos is not necessarily bad. (The future is still predictable on average.) Moreover, supported by a celebrated work of Avila et al. (Invent. Math., 2003), we conduct a sensitivity analysis to investigate a relationship between the ergodic sum (of prices) and the speed of price adjustment. We believe that our methods in this paper can be used to analyse many other chaotic economic models."
http://arxiv.org/abs/2309.09202v1,Examining psychology of science as a potential contributor to science policy,2023-09-17 08:07:25+00:00,"['Arash Mousavi', 'Reza Hafezi', 'Hasan Ahmadi']",econ.GN,"The psychology of science is the least developed member of the family of science studies. It is growing, however, increasingly into a promising discipline. After a very brief review of this emerging sub-field of psychology, we call for it to be invited into the collection of social sciences that constitute the interdisciplinary field of science policy. Discussing the classic issue of resource allocation, this paper tries to indicate how prolific a new psychological conceptualization of this problem would be. Further, from a psychological perspective, this research will argue in favor of a more realistic conception of science which would be a complement to the existing one in science policy."
http://arxiv.org/abs/2309.07371v1,The Fiscal Cost of Public Debt and Government Spending Shocks,2023-09-14 01:13:16+00:00,['Venance Riblier'],econ.GN,"This paper investigates how the cost of public debt shapes fiscal policy and its effect on the economy. Using U.S. historical data, I show that when servicing the debt creates a fiscal burden, the government responds to spending shocks by limiting debt issuance. As a result, the initial shock triggers only a limited increase in public spending in the short run, and even leads to spending reversal in the long run. Under these conditions, fiscal policy loses its ability to stimulate economic activity. This outcome arises as the fiscal authority limits its own ability to borrow to ensure public debt sustainability. These findings are robust to several identification and estimation strategies."
http://arxiv.org/abs/2309.07160v1,The effect of housewife labor on gdp calculations,2023-09-11 10:10:06+00:00,['Saadet Yagmur Kumcu'],econ.GN,"In this study, the evolutionary development of labor has been tried to be revealed based on theoretical analysis. Using the example of gdp, which is an indicator of social welfare, the economic value of the labor of housewives was tried to be measured with an empirical modeling. To this end; first of all, the concept of labor was questioned in orthodox (mainstream) economic theories; then, by abstracting from the labor-employment relationship, it was examined what effect the labor of unpaid housewives who are unemployed in the capitalist system could have on gdp. In theoretical analysis; It has been determined that the changing human profile moves away from rationality and creates limited rationality and, accordingly, a heterogeneous individual profile. Women were defined as the new example of heterogeneous individuals, as those who best fit the definition of limited rational individuals because they prefer to be housewives. In the empirical analysis of the study, housewife labor was taken into account as the main variable. In the empirical analysis of the study; In the case of Turkiye, using turkstat employment data and the atkinson inequality scale; the impact of housewife labor on gdp was calculated. The results of the theoretical and empirical analysis were evaluated in the context of labor-employment independence."
http://arxiv.org/abs/2307.11508v1,A Robust Site Selection Model under uncertainty for Special Hospital Wards in Hong Kong,2023-07-21 11:34:17+00:00,"['Mohammad Heydari', 'Yanan Fan', 'Kin Keung Lai']",econ.GN,"This paper process two robust models for site selection problems for one of the major Hospitals in Hong Kong. Three parameters, namely, level of uncertainty, infeasibility tolerance as well as the level of reliability, are incorporated. Then, 2 kinds of uncertainty; that is, the symmetric and bounded uncertainties have been investigated. Therefore, the issue of scheduling under uncertainty has been considered wherein unknown problem factors could be illustrated via a given probability distribution function. In this regard, Lin, Janak, and Floudas (2004) introduced one of the newly developed strong optimisation protocols. Hence, computers as well as the chemical engineering [1069-1085] has been developed for considering uncertainty illustrated through a given probability distribution. Finally, our accurate optimisation protocol has been on the basis of a min-max framework and in a case of application to the (MILP) problems it produced a precise solution that has immunity to uncertain data."
http://arxiv.org/abs/2307.11571v1,ESG Reputation Risk Matters: An Event Study Based on Social Media Data,2023-07-21 13:22:20+00:00,"['Maxime L. D. Nicolas', 'Adrien Desroziers', 'Fabio Caccioli', 'Tomaso Aste']",econ.GN,"We investigate the response of shareholders to Environmental, Social, and Governance-related reputational risk (ESG-risk), focusing exclusively on the impact of social media. Using a dataset of 114 million tweets about firms listed on the S&P100 index between 2016 and 2022, we extract conversations discussing ESG matters. In an event study design, we define events as unusual spikes in message posting activity linked to ESG-risk, and we then examine the corresponding changes in the returns of related assets. By focusing on social media, we gain insight into public opinion and investor sentiment, an aspect not captured through ESG controversies news alone. To the best of our knowledge, our approach is the first to distinctly separate the reputational impact on social media from the physical costs associated with negative ESG controversy news. Our results show that the occurrence of an ESG-risk event leads to a statistically significant average reduction of 0.29% in abnormal returns. Furthermore, our study suggests this effect is predominantly driven by Social and Governance categories, along with the ""Environmental Opportunities"" subcategory. Our research highlights the considerable impact of social media on financial markets, particularly in shaping shareholders' perception of ESG reputation. We formulate several policy implications based on our findings."
http://arxiv.org/abs/2307.12457v1,Indicator Choice in Pay-for-Performance,2023-07-23 23:43:35+00:00,"['Majid Mahzoon', 'Ali Shourideh', 'Ariel Zetlin-Jones']",econ.TH,"We study the classic principal-agent model when the signal observed by the principal is chosen by the agent. We fully characterize the optimal information structure from an agent's perspective in a general moral hazard setting with limited liability. Due to endogeneity of the contract chosen by the principal, the agent's choice of information is non-trivial. We show that the agent's problem can be mapped into a geometrical game between the principal and the agent in the space of likelihood ratios. We use this representation result to show that coarse contracts are sufficient: The agent can achieve her best with binary signals. Additionally, we can characterize conditions under which the agent is able to extract the entire surplus and implement the first-best efficient allocation. Finally, we show that when effort and performance are one-dimensional, under a general class of models, threshold signals are optimal. Our theory can thus provide a rationale for coarseness of contracts based on the bargaining power of the agent in negotiations."
http://arxiv.org/abs/2307.13966v1,Using Probabilistic Stated Preference Analyses to Understand Actual Choices,2023-07-26 05:56:54+00:00,['Romuald Meango'],econ.EM,"Can stated preferences help in counterfactual analyses of actual choice? This research proposes a novel approach to researchers who have access to both stated choices in hypothetical scenarios and actual choices. The key idea is to use probabilistic stated choices to identify the distribution of individual unobserved heterogeneity, even in the presence of measurement error. If this unobserved heterogeneity is the source of endogeneity, the researcher can correct for its influence in a demand function estimation using actual choices, and recover causal effects. Estimation is possible with an off-the-shelf Group Fixed Effects estimator."
http://arxiv.org/abs/2309.00214v1,Regret-Minimizing Project Choice,2023-09-01 02:17:32+00:00,"['Yingni Guo', 'Eran Shmaya']",econ.TH,"An agent observes the set of available projects and proposes some, but not necessarily all, of them. A principal chooses one or none from the proposed set. We solve for a mechanism that minimizes the principal's worst-case regret. We compare the single-project environment in which the agent can propose only one project with the multiproject environment in which he can propose many. In both environments, if the agent proposes one project, it is chosen for sure if the principal's payoff is sufficiently high; otherwise, the probability that it is chosen decreases in the agent's payoff. In the multiproject environment, the agent's payoff from proposing multiple projects equals his maximal payoff from proposing each project alone. The multiproject environment outperforms the single-project one by providing better fallback options than rejection and by delivering this payoff to the agent more efficiently."
http://arxiv.org/abs/2308.15627v1,Target PCA: Transfer Learning Large Dimensional Panel Data,2023-08-29 20:53:06+00:00,"['Junting Duan', 'Markus Pelger', 'Ruoxuan Xiong']",econ.EM,"This paper develops a novel method to estimate a latent factor model for a large target panel with missing observations by optimally using the information from auxiliary panel data sets. We refer to our estimator as target-PCA. Transfer learning from auxiliary panel data allows us to deal with a large fraction of missing observations and weak signals in the target panel. We show that our estimator is more efficient and can consistently estimate weak factors, which are not identifiable with conventional methods. We provide the asymptotic inferential theory for target-PCA under very general assumptions on the approximate factor model and missing patterns. In an empirical study of imputing data in a mixed-frequency macroeconomic panel, we demonstrate that target-PCA significantly outperforms all benchmark methods."
http://arxiv.org/abs/2309.15269v2,"Theoretical Foundations of Community Rating by a Private Monopolist Insurer: Framework, Regulation, and Numerical Analysis",2023-09-26 21:02:00+00:00,"['Yann Braouezec', 'John Cagnol']",econ.TH,"Community rating is a policy that mandates uniform premium regardless of the risk factors. In this paper, our focus narrows to the single contract interpretation wherein we establish a theoretical framework for community rating using Stiglitz's (1977) monopoly model in which there is a continuum of agents. We exhibit profitability conditions and show that, under mild regularity conditions, the optimal premium is unique and satisfies the inverse elasticity rule. Our numerical analysis, using realistic parameter values, reveals that under regulation, a 10% increase in indemnity is possible with minimal impact on other variables."
http://arxiv.org/abs/2309.16440v1,The effect of COVID restriction levels on shared micromobility travel patterns: A comparison between dockless bike sharing and e-scooter services,2023-09-28 13:47:33+00:00,"['Marco Diana', 'Andrea Chicco']",econ.GN,"The spread of the coronavirus pandemic had negative repercussions on the majority of transport systems in virtually all countries. After the lockdown period, travel restriction policies are now frequently adapted almost real-time according to observed trends in the spread of the disease, resulting in a rapidly changing transport market situation. Shared micromobility operators, whose revenues entirely come from their customers, need to understand how the demand is affected to adapt their operations. Within this framework, the present paper investigates how different COVID-19 restriction levels have affected the usage patterns of shared micromobility.
  Usage data of two dockless micromobility services (bike and e-scooters) operating in Turin (Italy) are analyzed between October 2020 and March 2021, a period characterized by different travel restriction levels. The average number of daily trips, trip distances and trip duration are retrieved for both services, and then compared to identify significant differences in trends as restriction levels change. Additionally, related impacts on the spatial dimension of the services are studied through hotspot maps.
  Results show that both services decreased during restrictions, however e-scooters experienced a larger variability in their demand and they had a quicker recovery when travel restrictions were loosened. Shared bikes, in general, suffered less from travel restriction levels, suggesting their larger usage for work and study-related trip purposes, which is confirmed also by the analysis of hotspots. E-scooters are both substituting and complementing public transport according to restriction levels, while usage patterns of shared bikes are more independent."
http://arxiv.org/abs/2307.15336v1,Only-child matching penalty in the marriage market,2023-07-28 06:27:25+00:00,"['Keisuke Kawata', 'Mizuki Komura']",econ.GN,"This study explores the marriage matching of only-child individuals and its outcome. Specifically, we analyze two aspects. First, we investigate how marital status (i.e., marriage with an only child, that with a non-only child and remaining single) differs between only children and non-only children. This analysis allows us to know whether people choose mates in a positive or a negative assortative manner regarding only-child status, and to predict whether only-child individuals benefit from marriage matching premiums or are subject to penalties regarding partner attractiveness. Second, we measure the premium/penalty by the size of the gap in partner's socio economic status (SES, here, years of schooling) between only-child and non--only-child individuals. The conventional economic theory and the observed marriage patterns of positive assortative mating on only-child status predict that only-child individuals are subject to a matching penalty in the marriage market, especially when their partner is also an only child. Furthermore, our estimation confirms that among especially women marrying an only-child husband, only children are penalized in terms of 0.57-years-lower educational attainment on the part of the partner."
http://arxiv.org/abs/2307.16554v1,The fiscal implications of stringent climate policy,2023-07-31 10:32:32+00:00,['Richard S. J. Tol'],econ.GN,"Stringent climate policy compatible with the targets of the 2015 Paris Agreement would pose a substantial fiscal challenge. Reducing carbon dioxide emissions by 95% or more by 2050 would raise 7% (1-17%) of GDP in carbon tax revenue, half of current, global tax revenue. Revenues are relatively larger in poorer regions. Subsidies for carbon dioxide sequestration would amount to 6.6% (0.3-7.1%) of GDP. These numbers are conservative as they were estimated using models that assume first-best climate policy implementation and ignore the costs of raising revenue. The fiscal challenge rapidly shrinks if emission targets are relaxed."
http://arxiv.org/abs/2308.03507v1,How to choose a Compatible Committee?,2023-08-07 12:02:21+00:00,"['Ritu Dutta', 'Rajnish Kumnar', 'Surajit Borkotokey']",econ.TH,"Electing a committee of size k from m alternatives (k < m) is an interesting problem under the multi-winner voting rules. However, very few committee selection rules found in the literature consider the coalitional possibilities among the alternatives that the voters believe that certain coalitions are more effective and can more efficiently deliver desired outputs. To include such possibilities, in this present study, we consider a committee selection problem (or multi-winner voting problem) where voters are able to express their opinion regarding interdependencies among alternatives. Using a dichotomous preference scale termed generalized approval evaluation we construct an $m$-person coalitional game which is more commonly called a cooperative game with transferable utilities. To identify each alternative's score we use the Shapley value (Shapley, 1953) of the cooperative game we construct for the purpose. Our approach to the committee selection problem emphasizes on an important issue called the compatibility principle. Further, we show that the properties of the Shapley value are well suited in the committee selection context too. We explore several properties of the proposed committee selection rule."
http://arxiv.org/abs/2309.02323v1,"Projections of Economic Impacts of Climate Change on Marine Protected Areas: Palau, the Great Barrier Reef, and the Bering Sea",2023-09-05 15:42:04+00:00,['Talya ten Brink'],q-bio.PE,"Climate change substantially impacts ecological systems. Marine species are shifting their distribution because of climate change towards colder waters, potentially compromising the benefits of currently established Marine Protected Areas (MPAs). Therefore, we demonstrate how three case study regions will be impacted by warming ocean waters to prepare stakeholders to understand how the fisheries around the MPAs is predicted to change. We chose the case studies to focus on large scale MPAs in i) a cold, polar region, ii) a tropical region near the equator, and iii) a tropical region farther from the equator. We quantify the biological impacts of shifts in species distribution due to climate change for fishing communities that depend on the Palau National Marine Sanctuary, the Great Barrier Reef Marine National Park Zone, and the North Bering Sea Research Area MPAs. We find that fisheries sectors will be impacted differently in different regions and show that all three regions can be supported by this methodology for decision making that joins sector income and species diversity."
http://arxiv.org/abs/2309.02447v2,Economic Complexity Limits Accuracy of Price Probability Predictions by Gaussian Distributions,2023-08-24 09:49:20+00:00,['Victor Olkhov'],q-fin.GN,"We discuss the economic reasons why the predictions of price and return statistical moments in the coming decades, in the best case, will be limited by their averages and volatilities. That limits the accuracy of the forecasts of price and return probabilities by Gaussian distributions. The economic origin of these restrictions lies in the fact that the predictions of the market-based n-th statistical moments of price and return for n=1,2,.., require the description of the economic variables of the n-th order that are determined by sums of the n-th degrees of values or volumes of market trades. The lack of existing models that describe the evolution of the economic variables determined by the sums of the 2nd degrees of market trades results in the fact that even predictions of the volatilities of price and return are very uncertain. One can ignore existing economic barriers that we highlight but cannot overcome or resolve them. The accuracy of predictions of price and return probabilities substantially determines the reliability of asset pricing models and portfolio theories. The restrictions on the accuracy of predictions of price and return statistical moments reduce the reliability and veracity of modern asset pricing and portfolio theories."
http://arxiv.org/abs/2308.04973v1,The Mobilität.Leben Study: a Year-Long Mobility-Tracking Panel,2023-08-09 14:16:27+00:00,"['Allister Loder', 'Fabienne Cantner', 'Victoria Dahmen', 'Klaus Bogenberger']",econ.GN,"The Mobilität.Leben study investigated travel behavior effects of a natural experiment in Germany. In response to the 2022 cost-of-living crisis, two policy measures to reduce travel costs for the population in June, July, and August 2022 were introduced: a fuel excise tax cut and almost fare-free public transport with the so-called 9-Euro-Ticket. The announcement of a successor ticket to the 9-Euro-Ticket, the so-called Deutschlandticket, led to the immediate decision to continue the study. The Mobilität.Leben study has two periods, the 9-Euro-Ticket period and the Deutschlandticket period, and comprises two elements: several questionnaires and a smartphone-based passive waypoint tracking. The entire duration of the study was almost thirteen months.
  In this paper, we report on the study design, the recruitment strategy, the study participation in the survey, and the tracking parts, and we share our experience in conducting such large-scale panel studies. Overall, 3,080 people registered for our study of which 1,420 decided to use the smartphone tracking app. While the relevant questionnaires in both phases have been completed by 818 participants, we have 170 study participants who completed the tracking in both phases and all relevant questionnaires. We find that providing a study compensation increases participation performance. It can be concluded that conducting year-long panel studies is possible, providing rich information on the heterogeneity in travel behavior between and within travelers."
http://arxiv.org/abs/2309.02089v1,On the use of U-statistics for linear dyadic interaction models,2023-09-05 09:51:45+00:00,['G. M. Szini'],econ.EM,"Even though dyadic regressions are widely used in empirical applications, the (asymptotic) properties of estimation methods only began to be studied recently in the literature. This paper aims to provide in a step-by-step manner how U-statistics tools can be applied to obtain the asymptotic properties of pairwise differences estimators for a two-way fixed effects model of dyadic interactions. More specifically, we first propose an estimator for the model that relies on pairwise differencing such that the fixed effects are differenced out. As a result, the summands of the influence function will not be independent anymore, showing dependence on the individual level and translating to the fact that the usual law of large numbers and central limit theorems do not straightforwardly apply. To overcome such obstacles, we show how to generalize tools of U-statistics for single-index variables to the double-indices context of dyadic datasets. A key result is that there can be different ways of defining the Hajek projection for a directed dyadic structure, which will lead to distinct, but equivalent, consistent estimators for the asymptotic variances. The results presented in this paper are easily extended to non-linear models."
http://arxiv.org/abs/2309.00943v2,iCOS: Option-Implied COS Method,2023-09-02 13:39:57+00:00,['Evgenii Vladimirov'],q-fin.ST,"This paper proposes the option-implied Fourier-cosine method, iCOS, for non-parametric estimation of risk-neutral densities, option prices, and option sensitivities. The iCOS method leverages the Fourier-based COS technique, proposed by Fang and Oosterlee (2008), by utilizing the option-implied cosine series coefficients. Notably, this procedure does not rely on any model assumptions about the underlying asset price dynamics, it is fully non-parametric, and it does not involve any numerical optimization. These features make it rather general and computationally appealing. Furthermore, we derive the asymptotic properties of the proposed non-parametric estimators and study their finite-sample behavior in Monte Carlo simulations. Our empirical analysis using S&P 500 index options and Amazon equity options illustrates the effectiveness of the iCOS method in extracting valuable information from option prices under different market conditions. Additionally, we apply our methodology to dissect and quantify observation and discretization errors in the VIX index."
http://arxiv.org/abs/2309.00556v1,The Effect of Punishment and Reward on Cooperation in a Prisoners' Dilemma Game,2023-09-01 16:07:56+00:00,['Alexander Kangas'],econ.TH,"This work studies the effect of incentives (in the form of punishment and reward) on the equilibrium fraction of cooperators and defectors in an iterated n-person prisoners' dilemma game. With a finite population of players employing a strategy of nice tit-for-tat or universal defect, an equilibrium fraction of each player-type can be identified from linearized payoff functions. Incentives take the form of targeted and general punishment, and targeted and general reward. The primary contribution of this work is in clearly articulating the design and marginal effect of these incentives on cooperation. Generalizable results indicate that while targeted incentives have the potential to substantially reduce but never entirely eliminate defection, they exhibit diminishing marginal effectiveness. General incentives on the other hand have the potential to eliminate all defection from the population of players. Applications to policy are briefly considered."
http://arxiv.org/abs/2309.00909v1,There is power in general equilibrium,2023-09-02 11:14:35+00:00,['Juan Jacobo'],econ.GN,"The article develops a general equilibrium model where power relations are central in the determination of unemployment, profitability, and income distribution. The paper contributes to the market forces versus institutions debate by providing a unified model capable of identifying key interrelations between technical and institutional changes in the economy. Empirically, the model is used to gauge the relative roles of technology and institutions in the behavior of the labor share, the unemployment rate, the capital-output ratio, and business profitability and demonstrates how they complement each other in providing an adequate narrative to the structural changes of the US economy."
http://arxiv.org/abs/2308.00062v1,Control and Spread of Contagion in Networks,2023-07-31 18:35:14+00:00,"['John Higgins', 'Tarun Sabarwal']",econ.TH,"We study proliferation of an action in binary action network coordination games that are generalized to include global effects. This captures important aspects of proliferation of a particular action or narrative in online social networks, providing a basis to understand their impact on societal outcomes. Our model naturally captures complementarities among starting sets, network resilience, and global effects, and highlights interdependence in channels through which contagion spreads. We present new, natural, and computationally tractable algorithms to define and compute equilibrium objects that facilitate the general study of contagion in networks and prove their theoretical properties. Our algorithms are easy to implement and help to quantify relationships previously inaccessible due to computational intractability. Using these algorithms, we study the spread of contagion in scale-free networks with 1,000 players using millions of Monte Carlo simulations. Our analysis provides quantitative and qualitative insight into the design of policies to control or spread contagion in networks. The scope of application is enlarged given the many other situations across different fields that may be modeled using this framework."
http://arxiv.org/abs/2307.15197v1,On the mathematics of the circular flow of economic activity with applications to the topic of caring for the vulnerable during pandemics,2023-07-27 21:01:52+00:00,"['Aziz Guergachi', 'Javid Hakim']",econ.GN,"We investigate, at the fundamental level, the questions of `why', `when' and `how' one could or should reach out to poor and vulnerable people to support them in the absence of governmental institutions. We provide a simple and new approach that is rooted in linear algebra and basic graph theory to capture the dynamics of income circulation among economic agents. A new linear algebraic model for income circulation is introduced, based on which we are able to categorize societies as fragmented or cohesive. We show that, in the case of fragmented societies, convincing wealthy agents at the top of the social hierarchy to support the poor and vulnerable will be very difficult. We also highlight how linear-algebraic and simple graph-theoretic methods help explain, from a fundamental point of view, some of the mechanics of class struggle in fragmented societies. Then, we explain intuitively and prove mathematically why, in cohesive societies, wealthy agents at the top of the social hierarchy tend to benefit by supporting the vulnerable in their society. A number of new concepts emerge naturally from our mathematical analysis to describe the level of cohesiveness of the society, the number of degrees of separation in business (as opposed to social) networks, and the level of generosity of the overall economy, which all tend to affect the rate at which the top wealthy class recovers its support money back. In the discussion on future perspectives, the connections between the proposed matrix model and statistical physics concepts are highlighted."
http://arxiv.org/abs/2308.15443v1,Combining predictive distributions of electricity prices: Does minimizing the CRPS lead to optimal decisions in day-ahead bidding?,2023-08-29 17:10:38+00:00,"['Weronika Nitka', 'Rafał Weron']",q-fin.ST,"Probabilistic price forecasting has recently gained attention in power trading because decisions based on such predictions can yield significantly higher profits than those made with point forecasts alone. At the same time, methods are being developed to combine predictive distributions, since no model is perfect and averaging generally improves forecasting performance. In this article we address the question of whether using CRPS learning, a novel weighting technique minimizing the continuous ranked probability score (CRPS), leads to optimal decisions in day-ahead bidding. To this end, we conduct an empirical study using hourly day-ahead electricity prices from the German EPEX market. We find that increasing the diversity of an ensemble can have a positive impact on accuracy. At the same time, the higher computational cost of using CRPS learning compared to an equal-weighted aggregation of distributions is not offset by higher profits, despite significantly more accurate predictions."
http://arxiv.org/abs/2309.10252v1,OPUS: An Integrated Assessment Model for Satellites and Orbital Debris,2023-09-19 02:18:13+00:00,"['Akhil Rao', 'Mark Moretto', 'Marcus Holzinger', 'Daniel Kaffine', 'Brian Weeden']",physics.space-ph,"An increasingly salient public policy challenge is how to manage the growing number of satellites in orbit, including large constellations. Many policy initiatives have been proposed that attempt to address the problem from different angles, but there is a paucity of analytical tools to help policymakers evaluate the efficacy of these different proposals and any potential counterproductive outcomes. To help address this problem, this paper summarizes work done to develop an experimental integrated assessment model -- Orbital Debris Propagators Unified with Economic Systems (OPUS) -- that combines both astrodynamics of the orbital population and economic behavior of space actors. For a given set of parameters, the model first utilizes a given astrodynamic propagator to assess the state of objects in orbit. It then uses a set of user-defined economic and policy parameters -- e.g. launch prices, disposal regulations -- to model how actors will respond to the economic incentives created by a given scenario. For the purposes of testing, the MIT Orbital Capacity Tool (MOCAT) version 4S was used as the primary astrodynamics propagator to simulate the true expected target collision probability ($p_c$) for a given end-of-life (EOL) disposal plan. To demonstrate propagator-agnosticism, a Gaussian mixture probability hypothesis density (GMPHD) filter was also used to simulate $p_c$. We also explore economic policy instruments to improve both sustainability of and economic welfare from orbit use. In doing so, we demonstrate that this hybrid approach can serve as a useful tool for evaluating policy proposals for managing orbital congestion. We also discuss areas where this work can be made more robust and expanded to include additional policy considerations."
http://arxiv.org/abs/2309.11394v1,Is Ethereum Proof of Stake Sustainable? $-$ Considering from the Perspective of Competition Among Smart Contract Platforms $-$,2023-09-20 15:18:21+00:00,"['Kenji Saito', 'Yutaka Soejima', 'Toshihiko Sugiura', 'Yukinobu Kitamura', 'Mitsuru Iwamura']",cs.CY,"Since the Merge update upon which Ethereum transitioned to Proof of Stake, it has been touted that it resulted in lower power consumption and increased security. However, even if that is the case, can this state be sustained?
  In this paper, we focus on the potential impact of competition with other smart contract platforms on the price of Ethereum's native currency, Ether (ETH), thereby raising questions about the safety and sustainability purportedly brought about by the design of Proof of Stake."
http://arxiv.org/abs/2309.11400v1,Transformers versus LSTMs for electronic trading,2023-09-20 15:25:43+00:00,"['Paul Bilokon', 'Yitao Qiu']",q-fin.TR,"With the rapid development of artificial intelligence, long short term memory (LSTM), one kind of recurrent neural network (RNN), has been widely applied in time series prediction.
  Like RNN, Transformer is designed to handle the sequential data. As Transformer achieved great success in Natural Language Processing (NLP), researchers got interested in Transformer's performance on time series prediction, and plenty of Transformer-based solutions on long time series forecasting have come out recently. However, when it comes to financial time series prediction, LSTM is still a dominant architecture. Therefore, the question this study wants to answer is: whether the Transformer-based model can be applied in financial time series prediction and beat LSTM.
  To answer this question, various LSTM-based and Transformer-based models are compared on multiple financial prediction tasks based on high-frequency limit order book data. A new LSTM-based model called DLSTM is built and new architecture for the Transformer-based model is designed to adapt for financial prediction. The experiment result reflects that the Transformer-based model only has the limited advantage in absolute price sequence prediction. The LSTM-based models show better and more robust performance on difference sequence prediction, such as price difference and price movement."
http://arxiv.org/abs/2309.11189v1,Increasing Ticketing Allocative Efficiency Using Marginal Price Auction Theory,2023-09-20 10:23:39+00:00,['Boxiang Fu'],econ.GN,Most modern ticketing systems rely on a first-come-first-serve or randomized allocation system to determine the allocation of tickets. Such systems has received considerable backlash in recent years due to its inequitable allotment and allocative inefficiency. We analyze a ticketing protocol based on a variation of the marginal price auction system. Users submit bids to the protocol based on their own utilities. The protocol awards tickets to the highest bidders and determines the final ticket price paid by all bidders using the lowest winning submitted bid. Game theoretic proof is provided to ensure the protocol more efficiently allocates the tickets to the bidders with the highest utilities. We also prove that the protocol extracts more economic rents for the event organizers and the non-optimality of ticket scalping under time-invariant bidder utilities.
http://arxiv.org/abs/2309.17268v2,Income Mobility and Mixing in North Macedonia,2023-09-29 14:19:27+00:00,"['Viktor Stojkoski', 'Sonja Mitikj', 'Marija Trpkova-Nestorovska', 'Dragan Tevdovski']",econ.GN,"This study presents the inaugural analysis of income mobility in North Macedonia from 1995-2021 using the Mixing Time and Mean First Passage Time (MFPT) metrics. We document larger mobility (in terms of Mixing Time) during the '90s, with and decreasing trend (in terms of mobility) until 1999. After this year the Mixing time has been consistent with a value of around 4 years. Using the MFPT, we highlight the evolving challenges individuals face when aspiring to higher income tiers. Namely, we show that there was a noticeable upward trend in MFPT from 1995 to 2006, a subsequent decline until 2017, and then an ascent again, peaking in 2021. These findings provide a foundational perspective on the income mobility in North Macedonia."
http://arxiv.org/abs/2308.00681v2,Increasing Supply Chain Resiliency Through Equilibrium Pricing and Stipulating Transportation Quota Regulation,2023-08-01 17:36:36+00:00,"['Mostafa Pazoki', 'Hamed Samarghandi', 'Mehdi Behroozi']",math.OC,"Supply chain disruption can occur for a variety of reasons, including natural disasters or market dynamics for which resilient strategies should be designed. If the disruption is profound and with dire consequences for the economy, it calls for the regulator's intervention to minimize the impact for the betterment of the society. This paper considers a shipping company with limited capacity which will ship a group of products with heterogeneous transportation and production costs and prices, and investigates the minimum quota regulation on transportation amounts stipulated by the government. An interesting example can happen in North American rail transportation market, where the rail capacity is used for a variety of products and commodities such as oil and grains. Similarly, in Europe supply chain of grains produced in Ukraine is disrupted by the Ukraine war and the blockade of sea transportation routes, which puts pressure on rail transportation capacity of Ukraine and its neighboring countries to the west that needs to be shared for shipping a variety of products including grains, military, and humanitarian supplies. Such situations require a proper execution of government intervention for effective management of the limited transportation capacity to avoid the rippling effects throughout the economy. We propose mathematical models and solutions for the market players and the government in a Canadian case study. Subsequently, the conditions that justify government intervention are identified, and an algorithm to obtain the optimum minimum quotas is presented."
http://arxiv.org/abs/2308.00706v1,Infodemia e pandemia: la cognitive warfare ai tempi del SARS-CoV-2,2023-07-26 17:24:35+00:00,"['Francesco Saverio Bucci', 'Matteo Cristofaro', 'Pier Luigi Giardino']",physics.soc-ph,"With the appearance of SARS-CoV-2, two epidemics have spread: one for health and one for information. The virus has generated an unprecedented infodemic, contributing to the establishment of a climate of great uncertainty. Massive and redundant information has also been an effective vector of propaganda, on a global scale, by state and non-state actors. A real hostile act in which physical violence is not foreseen, but systematic management of information through the manipulation of the cognitive sphere. For this reason, the adoption of an attitude of active critical analysis by citizens and institutions and also the implementation of common policies at an international level could undoubtedly facilitate the fight against cognitive warfare (so-called cognitive warfare) and therefore limit its disastrous effects."
http://arxiv.org/abs/2308.08808v1,"Entrepreneurial Higher Education Education, Knowledge and Wealth Creation",2023-08-17 06:36:25+00:00,"['Rahmat Ullah', 'Rashid Aftab', 'Saeed Siyal', 'Kashif Zaheer']",econ.GN,"This book presents detailed discussion on the role of higher education in terms of serving basic knowledge creation, teaching, and doing applied research for commercialization. The book presents an historical account on how this challenge was addressed earlier in education history, the cases of successful academic commercialization, the marriage between basic and applied science and how universities develop economies of the regions and countries. This book also discusses cultural and social challenges in research commercialization and pathways to break the status quo."
http://arxiv.org/abs/2308.07437v1,Impact of COVID-19 Lockdown Measures on Chinese Startups and Local Government Public Finance: Challenges and Policy Implications,2023-08-14 20:13:28+00:00,['Xin Sun'],econ.GN,"This paper aims to assess the impact of COVID-19 on the public finance of Chinese local governments, with a particular focus on the effect of lockdown measures on startups during the pandemic. The outbreak has placed significant fiscal pressure on local governments, as containment measures have led to declines in revenue and increased expenses related to public health and social welfare. In tandem, startups have faced substantial challenges, including reduced funding and profitability, due to the negative impact of lockdown measures on entrepreneurship. Moreover, the pandemic has generated short- and long-term economic shocks, affecting both employment and economic recovery. To address these challenges, policymakers must balance health concerns with economic development. In this regard, the government should consider implementing more preferential policies that focus on startups to ensure their survival and growth. Such policies may include financial assistance, tax incentives, and regulatory flexibility to foster innovation and entrepreneurship. By and large, the COVID-19 pandemic has had a profound impact on both the public finance of Chinese local governments and the startup ecosystem. Addressing the challenges faced by local governments and startups will require a comprehensive approach that balances health and economic considerations and includes targeted policies to support entrepreneurship and innovation."
http://arxiv.org/abs/2308.06303v1,A New Approach to Overcoming Zero Trade in Gravity Models to Avoid Indefinite Values in Linear Logarithmic Equations and Parameter Verification Using Machine Learning,2023-08-11 14:08:25+00:00,['Mikrajuddin Abdullah'],econ.GN,"The presence of a high number of zero flow trades continues to provide a challenge in identifying gravity parameters to explain international trade using the gravity model. Linear regression with a logarithmic linear equation encounters an indefinite value on the logarithmic trade. Although several approaches to solving this problem have been proposed, the majority of them are no longer based on linear regression, making the process of finding solutions more complex. In this work, we suggest a two-step technique for determining the gravity parameters: first, perform linear regression locally to establish a dummy value to substitute trade flow zero, and then estimating the gravity parameters. Iterative techniques are used to determine the optimum parameters. Machine learning is used to test the estimated parameters by analyzing their position in the cluster. We calculated international trade figures for 2004, 2009, 2014, and 2019. We just examine the classic gravity equation and discover that the powers of GDP and distance are in the same cluster and are both worth roughly one. The strategy presented here can be used to solve other problems involving log-linear regression."
http://arxiv.org/abs/2308.09535v2,Weak Identification with Many Instruments,2023-08-18 13:13:41+00:00,"['Anna Mikusheva', 'Liyang Sun']",econ.EM,"Linear instrumental variable regressions are widely used to estimate causal effects. Many instruments arise from the use of ``technical'' instruments and more recently from the empirical strategy of ``judge design''. This paper surveys and summarizes ideas from recent literature on estimation and statistical inferences with many instruments for a single endogenous regressor. We discuss how to assess the strength of the instruments and how to conduct weak identification-robust inference under heteroskedasticity. We establish new results for a jack-knifed version of the Lagrange Multiplier (LM) test statistic. Furthermore, we extend the weak-identification-robust tests to settings with both many exogenous regressors and many instruments. We propose a test that properly partials out many exogenous regressors while preserving the re-centering property of the jack-knife. The proposed tests have correct size and good power properties."
http://arxiv.org/abs/2308.10039v1,Do We Price Happiness? Evidence from Korean Stock Market,2023-08-19 14:55:49+00:00,['HyeonJun Kim'],econ.GN,"This study explores the potential of internet search volume data, specifically Google Trends, as an indicator for cross-sectional stock returns. Unlike previous studies, our research specifically investigates the search volume of the topic 'happiness' and its impact on stock returns in the aspect of risk pricing rather than as sentiment measurement. Empirical results indicate that this 'happiness' search exposure (HSE) can explain future returns, particularly for big and value firms. This suggests that HSE might be a reflection of a firm's ability to produce goods or services that meet societal utility needs. Our findings have significant implications for institutional investors seeking to leverage HSE-based strategies for outperformance. Additionally, our research suggests that, when selected judiciously, some search topics on Google Trends can be related to risks that impact stock prices."
http://arxiv.org/abs/2308.09783v3,Discretionary Extensions to Unemployment-Insurance Compensation and Some Potential Costs for a McCall Worker,2023-08-18 19:16:07+00:00,['Rich Ryan'],econ.GN,"Unemployment insurance provides temporary cash benefits to eligible unemployed workers. Benefits are sometimes extended by discretion during economic slumps. In a model that features temporary benefits and sequential job opportunities, a worker's reservation wages are studied when policymakers can make discretionary extensions to benefits. A worker's optimal labor-supply choice is characterized by a sequence of reservation wages that increases with weeks of remaining benefits. The possibility of an extension raises the entire sequence of reservation wages, meaning a worker is more selective when accepting job offers throughout their spell of unemployment. The welfare consequences of misperceiving the probability and length of an extension are investigated. Properties of the model can help policymakers interpret data on reservation wages, which may be important if extended benefits are used more often in response to economic slumps, virus pandemics, extreme heat, and natural disasters."
http://arxiv.org/abs/2308.09789v1,Managers' Choice of Disclosure Complexity,2023-08-18 19:21:23+00:00,['Jeremy Bertomeu'],econ.GN,"Aghamolla and Smith (2023) make a significant contribution to enhancing our understanding of how managers choose financial reporting complexity. I outline the key assumptions and implications of the theory, and discuss two empirical implications: (1) a U-shaped relationship between complexity and returns, and (2) a negative association between complexity and investor sophistication. However, the robust equilibrium also implies a counterfactual positive market response to complexity. I develop a simplified approach in which simple disclosures indicate positive surprises, and show that this implies greater investor skepticism toward complexity and a positive association between investor sophistication and complexity. More work is needed to understand complexity as an interaction of reporting and economic transactions, rather than solely as a reporting phenomenon."
http://arxiv.org/abs/2308.09818v1,Paths to Influence: How Coordinated Influence Operations Affect the Prominence of Ideas,2023-08-18 20:59:08+00:00,"['Darren L. Linvill', 'Patrick L. Warren']",econ.GN,"This paper presents four examples of different ways that coordinated influence operations exert pressure on the prominence of ideas on social networks. We argue that these examples illustrate the four archetypical paths to influence: promotion by strengthening, promotion by weakening, demotion by strengthening, and demotion by weakening. We formalize this idea in a stylized economic model of the optimal behavior of the influence operator and derive some predictions about when we should expect each path to be followed. Finally we sketch out how one might go about quantitatively estimating the key parameters of (a variant of) this model and how it applies much more broadly than in the international political influence examples that motivate it."
http://arxiv.org/abs/2308.06568v1,"""Zero Cost'' Majority Attacks on Permissionless Blockchains",2023-08-12 13:38:37+00:00,"['Joshua S. Gans', 'Hanna Halaburda']",cs.CR,"The core premise of permissionless blockchains is their reliable and secure operation without the need to trust any individual agent. At the heart of blockchain consensus mechanisms is an explicit cost (whether work or stake) for participation in the network and the opportunity to add blocks to the blockchain. A key rationale for that cost is to make attacks on the network, which could be theoretically carried out if a majority of nodes were controlled by a single entity, too expensive to be worthwhile. We demonstrate that a majority attacker can successfully attack with a {\em negative cost}, which shows that the protocol mechanisms are insufficient to create a secure network, and emphasizes the importance of socially driven mechanisms external to the protocol. At the same time, negative cost enables a new type of majority attack that is more likely to elude external scrutiny."
http://arxiv.org/abs/2308.05895v1,Characterizing Correlation Matrices that Admit a Clustered Factor Representation,2023-08-11 01:07:39+00:00,"['Chen Tong', 'Peter Reinhard Hansen']",econ.EM,"The Clustered Factor (CF) model induces a block structure on the correlation matrix and is commonly used to parameterize correlation matrices. Our results reveal that the CF model imposes superfluous restrictions on the correlation matrix. This can be avoided by a different parametrization, involving the logarithmic transformation of the block correlation matrix."
http://arxiv.org/abs/2308.06907v1,Generative Interpretation,2023-08-14 02:59:27+00:00,"['Yonathan A. Arbel', 'David Hoffman']",cs.CL,"We introduce generative interpretation, a new approach to estimating contractual meaning using large language models. As AI triumphalism is the order of the day, we proceed by way of grounded case studies, each illustrating the capabilities of these novel tools in distinct ways. Taking well-known contracts opinions, and sourcing the actual agreements that they adjudicated, we show that AI models can help factfinders ascertain ordinary meaning in context, quantify ambiguity, and fill gaps in parties' agreements. We also illustrate how models can calculate the probative value of individual pieces of extrinsic evidence. After offering best practices for the use of these models given their limitations, we consider their implications for judicial practice and contract theory. Using LLMs permits courts to estimate what the parties intended cheaply and accurately, and as such generative interpretation unsettles the current interpretative stalemate. Their use responds to efficiency-minded textualists and justice-oriented contextualists, who argue about whether parties will prefer cost and certainty or accuracy and fairness. Parties--and courts--would prefer a middle path, in which adjudicators strive to predict what the contract really meant, admitting just enough context to approximate reality while avoiding unguided and biased assimilation of evidence. As generative interpretation offers this possibility, we argue it can become the new workhorse of contractual interpretation."
http://arxiv.org/abs/2309.03740v1,Identifying spatial interdependence in panel data with large N and small T,2023-09-07 14:29:28+00:00,"['Deborah Gefang', 'Stephen G. Hall', 'George S. Tavlas']",econ.EM,"This paper develops a simple two-stage variational Bayesian algorithm to estimate panel spatial autoregressive models, where N, the number of cross-sectional units, is much larger than T, the number of time periods without restricting the spatial effects using a predetermined weighting matrix. We use Dirichlet-Laplace priors for variable selection and parameter shrinkage. Without imposing any a priori structures on the spatial linkages between variables, we let the data speak for themselves. Extensive Monte Carlo studies show that our method is super-fast and our estimated spatial weights matrices strongly resemble the true spatial weights matrices. As an illustration, we investigate the spatial interdependence of European Union regional gross value added growth rates. In addition to a clear pattern of predominant country clusters, we have uncovered a number of important between-country spatial linkages which are yet to be documented in the literature. This new procedure for estimating spatial effects is of particular relevance for researchers and policy makers alike."
http://arxiv.org/abs/2309.04020v2,Local Priority Mechanisms,2023-09-07 21:13:11+00:00,"['Joseph Root', 'David S. Ahn']",econ.TH,"We introduce a novel family of mechanisms for constrained allocation problems which we call local priority mechanisms. These mechanisms are parameterized by a function which assigns a set of agents, the local compromisers, to every infeasible allocation. The mechanism then greedily attempts to match agents with their top choices. Whenever it reaches an infeasible allocation, the local compromisers move to their next favorite alternative. Local priority mechanisms exist for any constraint, so this provides a method of constructing new designs for any constrained allocation problem. We give axioms which characterize local priority mechanisms. Since constrained allocation includes many canonical problems as special constraints, we apply this characterization to show that several well-known mechanisms, including deferred acceptance for school choice, top trading cycles for house allocation, and serial dictatorship can be understood as instances of local priority mechanisms. Other mechanisms, including the Boston mechanism, are not local priority mechanisms. We give sufficient conditions for a local priority mechanism to be group strategy-proof. We also provide conditions which enable welfare comparisons across local priority mechanisms."
http://arxiv.org/abs/2307.02673v1,Panel Data Nowcasting: The Case of Price-Earnings Ratios,2023-07-05 22:04:46+00:00,"['Andrii Babii', 'Ryan T. Ball', 'Eric Ghysels', 'Jonas Striaukas']",econ.EM,"The paper uses structured machine learning regressions for nowcasting with panel data consisting of series sampled at different frequencies. Motivated by the problem of predicting corporate earnings for a large cross-section of firms with macroeconomic, financial, and news time series sampled at different frequencies, we focus on the sparse-group LASSO regularization which can take advantage of the mixed frequency time series panel data structures. Our empirical results show the superior performance of our machine learning panel data regression models over analysts' predictions, forecast combinations, firm-specific time series regression models, and standard machine learning methods."
http://arxiv.org/abs/2307.01686v2,Transaction Fee Mechanism Design with Active Block Producers,2023-07-04 12:35:42+00:00,"['Maryam Bahrani', 'Pranav Garimidi', 'Tim Roughgarden']",cs.GT,"The incentive-compatibility properties of blockchain transaction fee mechanisms have been investigated with *passive* block producers that are motivated purely by the net rewards earned at the consensus layer. This paper introduces a model of *active* block producers that have their own private valuations for blocks (representing, for example, additional value derived from the application layer). The block producer surplus in our model can be interpreted as one of the more common colloquial meanings of the term ``MEV.''
  The main results of this paper show that transaction fee mechanism design is fundamentally more difficult with active block producers than with passive ones: with active block producers, no non-trivial or approximately welfare-maximizing transaction fee mechanism can be incentive-compatible for both users and block producers. These results can be interpreted as a mathematical justification for the current interest in augmenting transaction fee mechanisms with additional components such as order flow auctions, block producer competition, trusted hardware, or cryptographic techniques."
http://arxiv.org/abs/2307.01049v1,Doubly Robust Estimation of Direct and Indirect Quantile Treatment Effects with Machine Learning,2023-07-03 14:27:15+00:00,"['Yu-Chin Hsu', 'Martin Huber', 'Yu-Min Yen']",econ.EM,"We suggest double/debiased machine learning estimators of direct and indirect quantile treatment effects under a selection-on-observables assumption. This permits disentangling the causal effect of a binary treatment at a specific outcome rank into an indirect component that operates through an intermediate variable called mediator and an (unmediated) direct impact. The proposed method is based on the efficient score functions of the cumulative distribution functions of potential outcomes, which are robust to certain misspecifications of the nuisance parameters, i.e., the outcome, treatment, and mediator models. We estimate these nuisance parameters by machine learning and use cross-fitting to reduce overfitting bias in the estimation of direct and indirect quantile treatment effects. We establish uniform consistency and asymptotic normality of our effect estimators. We also propose a multiplier bootstrap for statistical inference and show the validity of the multiplier bootstrap. Finally, we investigate the finite sample performance of our method in a simulation study and apply it to empirical data from the National Job Corp Study to assess the direct and indirect earnings effects of training."
http://arxiv.org/abs/2308.04780v4,School Choice with Multiple Priorities,2023-08-09 08:11:10+00:00,"['Minoru Kitahara', 'Yasunori Okumura']",econ.TH,"This study considers a model where schools may have multiple priority orders on students, which may be inconsistent with each other. For example, in school choice systems, since the sibling priority and the walk zone priority coexist, the priority orders based on them would be conflicting. We introduce a weaker fairness notion called M-fairness to examine such markets. Further, we focus on a more specific situation where all schools have only two priority orders, and for a certain group of students, a priority order of each school is an improvement of the other priority order of the school. An illustrative example is the school choice matching market with a priority-based affirmative action policy. We introduce a mechanism that utilizes the efficiency adjusted deferred acceptance algorithm and show that the mechanism satisfies properties called responsiveness to improvements and improved-group optimally M-stability, which is stronger than student optimally M-stability."
http://arxiv.org/abs/2308.02450v2,Composite Quantile Factor Model,2023-08-04 16:32:11+00:00,['Xiao Huang'],econ.EM,"This paper introduces the method of composite quantile factor model for factor analysis in high-dimensional panel data. We propose to estimate the factors and factor loadings across multiple quantiles of the data, allowing the estimates to better adapt to features of the data at different quantiles while still modeling the mean of the data. We develop the limiting distribution of the estimated factors and factor loadings, and an information criterion for consistent factor number selection is also discussed. Simulations show that the proposed estimator and the information criterion have good finite sample properties for several non-normal distributions under consideration. We also consider an empirical study on the factor analysis for 246 quarterly macroeconomic variables. A companion R package cqrfactor is developed."
http://arxiv.org/abs/2308.04400v3,"Global evidence on the income elasticity of willingness to pay, relative price changes and public natural capital values",2023-08-08 17:01:49+00:00,"['Moritz A. Drupp', 'Zachary M. Turk', 'Ben Groom', 'Jonas Heckenhahn']",econ.GN,"While the global economy continues to grow, ecosystem services tend to stagnate or decline. Economic theory has shown how such shifts in relative scarcities can be reflected in project appraisal and accounting, but empirical evidence has been sparse to put theory into practice. To estimate relative price changes in ecosystem services to be used for making such adjustments, we perform a global meta-analysis of contingent valuation studies to derive income elasticities of marginal willingness to pay (WTP) for ecosystem services to proxy the degree of limited substitutability. Based on 735 income-WTP pairs from 396 studies, we find an income elasticity of WTP of around 0.6. Combined with good-specific growth rates, we estimate relative price change of ecosystem services of around 1.7 percent per year. In an application to natural capital valuation of forest ecosystem services by the World Bank, we show that natural capital should be uplifted by around 40 percent. Our assessment of aggregate public natural capital yields a larger value adjustment of between 58 and 97 percent, depending on the discount rate. We discuss implications for policy appraisal and for estimates of natural capital in comprehensive wealth accounts."
http://arxiv.org/abs/2309.06305v3,Sensitivity Analysis for Linear Estimators,2023-09-12 15:16:23+00:00,"['Jacob Dorn', 'Luther Yap']",econ.EM,"We propose a novel sensitivity analysis framework for linear estimators with identification failures that can be viewed as seeing the wrong outcome distribution. Our approach measures the degree of identification failure through the change in measure between the observed distribution and a hypothetical target distribution that would identify the causal parameter of interest. The framework yields a sensitivity analysis that generalizes existing bounds for Average Potential Outcome (APO), Regression Discontinuity (RD), and instrumental variables (IV) exclusion failure designs. Our partial identification results extend results from the APO context to allow even unbounded likelihood ratios. Our proposed sensitivity analysis consistently estimates sharp bounds under plausible conditions and estimates valid bounds under mild conditions. We find that our method performs well in simulations even when targeting a discontinuous and nearly infinite bound."
http://arxiv.org/abs/2308.13156v1,Parental Health Penalty on Adult Children's Employment: Gender Difference and Long-Term Consequence,2023-08-25 03:29:38+00:00,"['Jiayi Wen', 'Haili Huang']",econ.GN,"This paper examines the long-term gender-specific impacts of parental health shocks on adult children's employment in China. We build up an inter-temporal cooperative framework to analyze household work decisions in response to parental health deterioration. Then employing an event-study approach, we establish a causal link between parental health shocks and a notable decline in female employment rates. Male employment, however, remains largely unaffected. This negative impact shows no abatement up to eight years that are observable by the sample. These findings indicate the consequence of ""growing old before getting rich"" for developing countries."
http://arxiv.org/abs/2309.01637v3,The Robust F-Statistic as a Test for Weak Instruments,2023-09-04 14:42:21+00:00,['Frank Windmeijer'],econ.EM,"Montiel Olea and Pflueger (2013) proposed the effective F-statistic as a test for weak instruments in terms of the Nagar bias of the two-stage least squares (2SLS) estimator relative to a benchmark worst-case bias. We show that their methodology applies to a class of linear generalized method of moments (GMM) estimators with an associated class of generalized effective F-statistics. The standard nonhomoskedasticity robust F-statistic is a member of this class. The associated GMMf estimator, with the extension f for first-stage, is a novel and unusual estimator as the weight matrix is based on the first-stage residuals. As the robust F-statistic can also be used as a test for underidentification, expressions for the calculation of the weak-instruments critical values in terms of the Nagar bias of the GMMf estimator relative to the benchmark simplify and no simulation methods or Patnaik (1949) distributional approximations are needed. In the grouped-data IV designs of Andrews (2018), where the robust F-statistic is large but the effective F-statistic is small, the GMMf estimator is shown to behave much better in terms of bias than the 2SLS estimator, as expected by the weak-instruments test results."
http://arxiv.org/abs/2309.13251v4,Nonparametric estimation of conditional densities by generalized random forests,2023-09-23 04:20:13+00:00,['Federico Zincenko'],econ.EM,"Considering a continuous random variable Y together with a continuous random vector X, I propose a nonparametric estimator f^(.|x) for the conditional density of Y given X=x. This estimator takes the form of an exponential series whose coefficients Tx = (Tx1,...,TxJ) are the solution of a system of nonlinear equations that depends on an estimator of the conditional expectation E[p(Y)|X=x], where p is a J-dimensional vector of basis functions. The distinguishing feature of the proposed estimator is that E[p(Y)|X=x] is estimated by generalized random forest (Athey, Tibshirani, and Wager, Annals of Statistics, 2019), targeting the heterogeneity of Tx across x. I show that f^(.|x) is uniformly consistent and asymptotically normal, allowing J to grow to infinity. I also provide a standard error formula to construct asymptotically valid confidence intervals. Results from Monte Carlo experiments are provided."
http://arxiv.org/abs/2308.14989v3,Efficiency in Multiple-Type Housing Markets,2023-08-29 02:38:23+00:00,['Di Feng'],econ.TH,"We consider multiple-type housing markets (Moulin, 1995), which extend Shapley-Scarf housing markets (Shapley and Scarf, 1974) from one dimension to higher dimensions. In this model, Pareto efficiency is incompatible with individual rationality and strategy-proofness (Konishi et al., 2001). Therefore, we consider two weaker efficiency properties: coordinatewise efficiency and pairwise efficiency. We show that these two properties both (i) are compatible with individual rationality and strategy-proofness, and (ii) help us to identify two specific mechanisms. To be more precise, on various domains of preference profiles, together with other well-studied properties (individual rationality, strategy-proofness, and non-bossiness), coordinatewise efficiency and pairwise efficiency respectively characterize two extensions of the top-trading-cycles mechanism (TTC): the coordinatewise top-trading-cycles mechanism (cTTC) and the bundle top-trading-cycles mechanism (bTTC). Moreover, we propose several variations of our efficiency properties, and we find that each of them is either satisfied by cTTC or bTTC, or leads to an impossibility result (together with individual rationality and strategy-proofness). Therefore, our characterizations can be primarily interpreted as a compatibility test: any reasonable efficiency property that is not satisfied by cTTC or bTTC could be considered incompatible with individual rationality and strategy-proofness. The external validity of our results in the context of general environments is also discussed. For multiple-type housing markets with strict preferences, our characterization of bTTC constitutes the first characterization of an extension of the prominent TTC mechanism"
http://arxiv.org/abs/2309.14630v3,Free Discontinuity Regression: With an Application to the Economic Effects of Internet Shutdowns,2023-09-26 02:49:30+00:00,"['Florian Gunsilius', 'David Van Dijcke']",econ.EM,"Sharp, multidimensional changepoints-abrupt shifts in a regression surface whose locations and magnitudes are unknown-arise in settings as varied as gene-expression profiling, financial covariance breaks, climate-regime detection, and urban socioeconomic mapping. Despite their prevalence, there are no current approaches that jointly estimate the location and size of the discontinuity set in a one-shot approach with statistical guarantees. We therefore introduce Free Discontinuity Regression (FDR), a fully nonparametric estimator that simultaneously (i) smooths a regression surface, (ii) segments it into contiguous regions, and (iii) provably recovers the precise locations and sizes of its jumps. By extending a convex relaxation of the Mumford-Shah functional to random spatial sampling and correlated noise, FDR overcomes the fixed-grid and i.i.d. noise assumptions of classical image-segmentation approaches, thus enabling its application to real-world data of any dimension. This yields the first identification and uniform consistency results for multivariate jump surfaces: under mild SBV regularity, the estimated function, its discontinuity set, and all jump sizes converge to their true population counterparts. Hyperparameters are selected automatically from the data using Stein's Unbiased Risk Estimate, and large-scale simulations up to three dimensions validate the theoretical results and demonstrate good finite-sample performance. Applying FDR to an internet shutdown in India reveals a 25-35% reduction in economic activity around the estimated shutdown boundaries-much larger than previous estimates. By unifying smoothing, segmentation, and effect-size recovery in a general statistical setting, FDR turns free-discontinuity ideas into a practical tool with formal guarantees for modern multivariate data."
http://arxiv.org/abs/2309.13159v3,Nonparametric mixed logit model with market-level parameters estimated from market share data,2023-09-22 19:50:55+00:00,"['Xiyuan Ren', 'Joseph Y. J. Chow', 'Prateek Bansal']",econ.EM,"We propose a nonparametric mixed logit model that is estimated using market-level choice share data. The model treats each market as an agent and represents taste heterogeneity through market-specific parameters by solving a multiagent inverse utility maximization problem, addressing the limitations of existing market-level choice models with parametric estimation. A simulation study is conducted to evaluate the performance of our model in terms of estimation time, estimation accuracy, and out-of-sample predictive accuracy. In a real data application, we estimate the travel mode choice of 53.55 million trips made by 19.53 million residents in New York State. These trips are aggregated based on population segments and census block group-level origin-destination (OD) pairs, resulting in 120,740 markets. We benchmark our model against multinomial logit (MNL), nested logit (NL), inverse product differentiation logit (IPDL), and the BLP models. The results show that the proposed model improves the out-of-sample accuracy from 65.30% to 81.78%, with a computation time less than one-tenth of that taken to estimate the BLP model. The price elasticities and diversion ratios retrieved from our model and benchmark models exhibit similar substitution patterns. Moreover, the market-level parameters estimated by our model provide additional insights and facilitate their seamless integration into supply-side optimization models for transportation design. By measuring the compensating variation for the driving mode, we found that a $9 congestion toll would impact roughly 60 % of the total travelers. As an application of supply-demand integration, we showed that a 50% discount of transit fare could bring a maximum ridership increase of 9402 trips per day under a budget of $50,000 per day."
http://arxiv.org/abs/2308.11238v2,Distorted optimal transport,2023-08-22 07:25:51+00:00,"['Haiyan Liu', 'Bin Wang', 'Ruodu Wang', 'Sheng Chao Zhuang']",math.OC,"Classic optimal transport theory is formulated through minimizing the expected transport cost between two given distributions. We propose the framework of distorted optimal transport by minimizing a distorted expected cost, which is the cost under a non-linear expectation. This new formulation is motivated by concrete problems in decision theory, robust optimization, and risk management, and it has many distinct features compared to the classic theory. We choose simple cost functions and study different distortion functions and their implications on the optimal transport plan. We show that on the real line, the comonotonic coupling is optimal for the distorted optimal transport problem when the distortion function is convex and the cost function is submodular and monotone. Some forms of duality and uniqueness results are provided. For inverse-S-shaped distortion functions and linear cost, we obtain the unique form of optimal coupling for all marginal distributions, which turns out to have an interesting ``first comonotonic, then counter-monotonic"" dependence structure; for S-shaped distortion functions a similar structure is obtained. Our results highlight several challenges and features in distorted optimal transport, offering a new mathematical bridge between the fields of probability, decision theory, and risk management."
http://arxiv.org/abs/2309.06875v1,How to foster innovation in the social sciences? Qualitative evidence from focus group workshops at Oxford University,2023-09-13 10:38:39+00:00,"['Fabian Braesemann', 'Moritz Marpe']",econ.GN,"This report addresses challenges and opportunities for innovation in the social sciences at the University of Oxford. It summarises findings from two focus group workshops with innovation experts from the University ecosystem. Experts included successful social science entrepreneurs and professional service staff from the University. The workshops focused on four different dimensions related to innovative activities and commercialisation. The findings show several challenges at the institutional and individual level, together with features of the social scientific discipline that impede more innovation in the social sciences. Based on identifying these challenges, we present potential solutions and ways forward identified in the focus group discussions to foster social science innovation. The report aims to illustrate the potential of innovation and commercialisation of social scientific research for both researchers and the university."
http://arxiv.org/abs/2308.00013v1,"Bitcoin Gold, Litecoin Silver:An Introduction to Cryptocurrency's Valuation and Trading Strategy",2023-07-30 23:14:20+00:00,"['Haoyang Yu', 'Yutong Sun', 'Yulin Liu', 'Luyao Zhang']",cs.CE,"Historically, gold and silver have played distinct roles in traditional monetary systems. While gold has primarily been revered as a superior store of value, prompting individuals to hoard it, silver has commonly been used as a medium of exchange. As the financial world evolves, the emergence of cryptocurrencies has introduced a new paradigm of value and exchange. However, the store-of-value characteristic of these digital assets remains largely uncharted. Charlie Lee, the founder of Litecoin, once likened Bitcoin to gold and Litecoin to silver. To validate this analogy, our study employs several metrics, including unspent transaction outputs (UTXO), spent transaction outputs (STXO), Weighted Average Lifespan (WAL), CoinDaysDestroyed (CDD), and public on-chain transaction data. Furthermore, we've devised trading strategies centered around the Price-to-Utility (PU) ratio, offering a fresh perspective on crypto-asset valuation beyond traditional utilities. Our back-testing results not only display trading indicators for both Bitcoin and Litecoin but also substantiate Lee's metaphor, underscoring Bitcoin's superior store-of-value proposition relative to Litecoin. We anticipate that our findings will drive further exploration into the valuation of crypto assets. For enhanced transparency and to promote future research, we've made our datasets available on Harvard Dataverse and shared our Python code on GitHub as open source."
http://arxiv.org/abs/2309.03403v10,Sources of capital growth,2023-09-06 23:38:23+00:00,"['Gordon Getty', 'Nikita Tkachenko']",econ.GN,"Data from national accounts show no effect of change in net saving or consumption, in ratio to market-value capital, on change in growth rate of market-value capital (capital acceleration). Thus it appears that capital growth and acceleration arrive without help from net saving or consumption restraint. We explore ways in which this is possible, and discuss implications for economic teaching and public policy"
http://arxiv.org/abs/2307.00349v4,Unbalanced Growth and Land Overvaluation,2023-07-01 14:23:44+00:00,"['Tomohiro Hirano', 'Alexis Akira Toda']",econ.TH,"Historical trends suggest the decline in importance of land as a production factor but its continued importance as a store of value. Using an overlapping generations model with land and aggregate uncertainty, we theoretically study the long-run behavior of land prices and identify economic conditions under which land becomes overvalued on the long-run trend relative to the fundamentals defined by the present value of land rents. Unbalanced growth together with the elasticity of substitution between production factors plays a critical role. Around the trend, land prices exhibit recurrent stochastic fluctuations, with expansions and contractions in the size of land overvaluation."
http://arxiv.org/abs/2309.09481v2,Estimation and Testing of Forecast Rationality with Many Moments,2023-09-18 04:45:12+00:00,"['Tae-Hwy Lee', 'Tao Wang']",econ.GN,"We in this paper utilize P-GMM (Cheng and Liao, 2015) moment selection procedure to select valid and relevant moments for estimating and testing forecast rationality under the flexible loss proposed by Elliott et al. (2005). We motivate the moment selection in a large dimensional setting, explain the fundamental mechanism of P-GMM moment selection procedure, and elucidate how to implement it in the context of forecast rationality by allowing the existence of potentially invalid moment conditions. A set of Monte Carlo simulations is conducted to examine the finite sample performance of P-GMM estimation in integrating the information available in instruments into both the estimation and testing, and a real data analysis using data from the Survey of Professional Forecasters issued by the Federal Reserve Bank of Philadelphia is presented to further illustrate the practical value of the suggested methodology. The results indicate that the P-GMM post-selection estimator of forecaster's attitude is comparable to the oracle estimator by using the available information efficiently. The accompanying power of rationality and symmetry tests utilizing P-GMM estimation would be substantially increased through reducing the influence of uninformative instruments. When a forecast user estimates and tests for rationality of forecasts that have been produced by others such as Greenbook, P-GMM moment selection procedure can assist in achieving consistent and more efficient outcomes."
http://arxiv.org/abs/2309.14186v2,"Value-transforming financial, carbon and biodiversity footprint accounting",2023-09-25 14:47:28+00:00,"['Sami El Geneidy', 'Maiju Peura', 'Viivi-Maija Aumanen', 'Stefan Baumeister', 'Ulla Helimo', 'Veera Vainio', 'Janne S. Kotiaho']",econ.GN,"Transformative changes in our production and consumption habits are needed to halt biodiversity loss. Organizations are the way we humans have organized our everyday life, and much of our negative environmental impacts, also called carbon and biodiversity footprints, are caused by organizations. Here we explore how the accounts of any organization can be exploited to develop an integrated carbon and biodiversity footprint account. As a metric we utilize spatially explicit potential global loss of species across all ecosystem types and argue that it can be understood as the biodiversity equivalent. The utility of the biodiversity equivalent for biodiversity could be like what carbon dioxide equivalent is for climate. We provide a global country specific dataset that organizations, experts and researchers can use to assess consumption-based biodiversity footprints. We also argue that the current integration of financial and environmental accounting is superficial and provide a framework for a more robust financial value-transforming accounting model. To test the methodologies, we utilized a Finnish university as a living lab. Assigning an offsetting cost to the footprints significantly altered the financial value of the organization. We believe such value-transforming accounting is needed to draw the attention of senior executives and investors to the negative environmental impacts of their organizations."
http://arxiv.org/abs/2308.10046v6,Startup Acquisitions: Acquihires and Talent Hoarding,2023-08-19 15:10:30+00:00,"['Jean-Michel Benkert', 'Igor Letina', 'Shuo Liu']",econ.GN,"We study how competitive forces may drive firms to inefficiently acquire startup talent. In our model, two rival firms have the capacity to acquire and integrate a startup operating in an orthogonal market. We show that firms may pursue such acquihires primarily as a preemptive strategy, even when they appear unprofitable in isolation. Thus, acquihires, even absent traditional competition-reducing effects, need not be benign, as they can lead to inefficient talent allocation. Additionally, our analysis underscores that such talent hoarding can diminish consumer surplus and exacerbate job volatility for acquihired employees."
http://arxiv.org/abs/2307.13475v2,Large sample properties of GMM estimators under second-order identification,2023-07-25 13:08:32+00:00,['Hugo Kruiniger'],econ.EM,"Dovonon and Hall (Journal of Econometrics, 2018) proposed a limiting distribution theory for GMM estimators for a p - dimensional globally identified parameter vector φ when local identification conditions fail at first-order but hold at second-order. They assumed that the first-order underidentification is due to the expected Jacobian having rank p-1 at the true value φ_{0}, i.e., having a rank deficiency of one. After reparametrizing the model such that the last column of the Jacobian vanishes, they showed that the GMM estimator of the first p-1 parameters converges at rate T^{-1/2} and the GMM estimator of the remaining parameter, φ_{p}, converges at rate T^{-1/4}. They also provided a limiting distribution of T^{1/4}(φ_{p}-φ_{0,p}) subject to a (non-transparent) condition which they claimed to be not restrictive in general. However, as we show in this paper, their condition is in fact only satisfied when φ is overidentified and the limiting distribution of T^{1/4}(φ_{p}-φ_{0,p}), which is non-standard, depends on whether φ is exactly identified or overidentified. In particular, the limiting distributions of the sign of T^{1/4}(φ_{p}-φ_{0,p}) for the cases of exact and overidentification, respectively, are different and are obtained by using expansions of the GMM objective function of different orders. Unsurprisingly, we find that the limiting distribution theories of Dovonon and Hall (2018) for Indirect Inference (II) estimation under two different scenarios with second-order identification where the target function is a GMM estimator of the auxiliary parameter vector, are incomplete for similar reasons. We discuss how our results for GMM estimation can be used to complete both theories and how they can be used to obtain the limiting distributions of the II estimators in the case of exact identification under either scenario."
http://arxiv.org/abs/2309.06753v4,A Reexamination of Proof Approaches for the Impossibility Theorem,2023-09-13 06:58:07+00:00,['Kazuya Yamamoto'],econ.TH,"Revised proofs of Kenneth Arrow's impossibility theorem have been presented in prose form, incorporating novel ideas such as decisive sets and pivotal voters. This study develops another approach to proving the theorem. Using a proof calculus in formal logic, we construct a proof with a full mathematical representation. While previous proofs emphasize intuitive accessibility, this one focuses on meticulous derivation and reveals the global structure of the social welfare function central to the theorem."
http://arxiv.org/abs/2309.10642v6,Correcting Sample Selection Bias in PISA Rankings,2023-09-19 14:22:26+00:00,['Onil Boussim'],econ.EM,"This paper addresses the critical issue of sample selection bias in cross-country comparisons based on international assessments such as the Programme for International Student Assessment (PISA). Although PISA is widely used to benchmark educational performance across countries, it samples only students who remain enrolled in school at age 15. This introduces survival bias, particularly in countries with high dropout rates, potentially leading to distorted comparisons. To correct for this bias, I develop a simple adjustment of the classical Heckman selection model tailored to settings with fully truncated outcome data. My approach exploits the joint normality of latent errors and leverages information on the selection rate, allowing identification of the counterfactual mean outcome for the full population of 15-year-olds. Applying this method to PISA 2018 data, I show that adjusting for selection bias results in substantial changes in country rankings based on average performance. These results highlight the importance of accounting for non-random sample selection to ensure accurate and policy-relevant international comparisons of educational outcomes."
http://arxiv.org/abs/2308.10138v8,Genuinely Robust Inference for Clustered Data,2023-08-20 02:35:52+00:00,"['Harold D. Chiang', 'Yuya Sasaki', 'Yulong Wang']",econ.EM,"Conventional cluster-robust inference can be invalid when data contain clusters of unignorably large size. We formalize this issue by deriving a necessary and sufficient condition for its validity, and show that this condition is frequently violated in practice: specifications from 77% of empirical research articles in American Economic Review and Econometrica during 2020-2021 appear not to meet it. To address this limitation, we propose a genuinely robust inference procedure based on a new cluster score bootstrap. We establish its validity and size control across broad classes of data-generating processes where conventional methods break down. Simulation studies corroborate our theoretical findings, and empirical applications illustrate that employing the proposed method can substantially alter conventional statistical conclusions."
http://arxiv.org/abs/2307.00251v1,Local Eviction Moratoria and the Spread of COVID-19,2023-07-01 07:03:19+00:00,"['Julia Hatamyar', 'Christopher F. Parmeter']",econ.GN,"At various stages during the initial onset of the COVID-19 pandemic, various US states and local municipalities enacted eviction moratoria. One of the main aims of these moratoria was to slow the spread of COVID-19 infections. We deploy a semiparametric difference-in-differences approach with an event study specification to test whether the lifting of these local moratoria led to an increase in COVID-19 cases and deaths. Our main findings, across a range of specifications, are inconclusive regarding the impact of the moratoria - especially after accounting for the number of actual evictions and conducting the analysis at the county level. We argue that recently developed augmented synthetic control (ASCM) methods are more appropriate in this setting. Our ASCM results also suggest that the lifting of eviction moratoria had little to no impact on COVID-19 cases and deaths. Thus, it seems that eviction moratoria had little to no robust effect on reducing the spread of COVID-19 throwing into question its use as a non-pharmaceutical intervention."
http://arxiv.org/abs/2307.00369v1,The Yule-Frisch-Waugh-Lovell Theorem,2023-07-01 15:44:33+00:00,['Deepankar Basu'],econ.EM,"This paper traces the historical and analytical development of what is known in the econometrics literature as the Frisch-Waugh-Lovell theorem. This theorem demonstrates that the coefficients on any subset of covariates in a multiple regression is equal to the coefficients in a regression of the residualized outcome variable on the residualized subset of covariates, where residualization uses the complement of the subset of covariates of interest. In this paper, I suggest that the theorem should be renamed as the Yule-Frisch-Waugh-Lovell (YFWL) theorem to recognize the pioneering contribution of the statistician G. Udny Yule in its development. Second, I highlight recent work by the statistician, P. Ding, which has extended the YFWL theorem to a comparison of estimated covariance matrices of coefficients from multiple and partial, i.e. residualized regressions. Third, I show that, in cases where Ding's results do not apply, one can still resort to a computational method to conduct statistical inference about coefficients in multiple regressions using information from partial regressions."
http://arxiv.org/abs/2307.00413v1,The Classical Theory of Supply and Demand,2023-07-01 19:27:00+00:00,"['Sabiou Inoua', 'Vernon Smith']",econ.TH,"This paper introduces and formalizes the classical view on supply and demand, which, we argue, has an integrity independent and distinct from the neoclassical theory. Demand and supply, before the marginal revolution, are defined not by an unobservable criterion such as a utility function, but by an observable monetary variable, the reservation price: the buyer's (maximum) willingness to pay (WTP) value (a potential price) and the seller's (minimum) willingness to accept (WTA) value (a potential price) at the marketplace. Market demand and supply are the cumulative distribution of the buyers' and sellers' reservation prices, respectively. This WTP-WTA classical view of supply and demand formed the means whereby market participants were motivated in experimental economics although experimentalists (trained in neoclassical economics) were not cognizant of their link to the past. On this foundation was erected a vast literature on the rules of trading for a host of institutions, modern and ancient. This paper documents textually this reappraisal of classical economics and then formalizes it mathematically. A follow-up paper will articulate a theory of market price formation rooted in this classical view on supply and demand and in experimental findings on market behavior."
http://arxiv.org/abs/2309.04181v1,Concave many-to-one matching,2023-09-08 07:53:07+00:00,['Chao Huang'],econ.TH,"We propose a notion of concavity in two-sided many-to-one matching, which is an analogue to the balancedness condition in cooperative games. A stable matching exists when the market is concave. We provide a class of concave markets. In the proof of the existence theorem, we use Scarf's algorithm to find a stable schedule matching, which is of independent interest."
http://arxiv.org/abs/2309.05898v1,Strategic Behavior of Large Language Models: Game Structure vs. Contextual Framing,2023-09-12 00:54:15+00:00,"['Nunzio Lorè', 'Babak Heydari']",cs.GT,"This paper investigates the strategic decision-making capabilities of three Large Language Models (LLMs): GPT-3.5, GPT-4, and LLaMa-2, within the framework of game theory. Utilizing four canonical two-player games -- Prisoner's Dilemma, Stag Hunt, Snowdrift, and Prisoner's Delight -- we explore how these models navigate social dilemmas, situations where players can either cooperate for a collective benefit or defect for individual gain. Crucially, we extend our analysis to examine the role of contextual framing, such as diplomatic relations or casual friendships, in shaping the models' decisions. Our findings reveal a complex landscape: while GPT-3.5 is highly sensitive to contextual framing, it shows limited ability to engage in abstract strategic reasoning. Both GPT-4 and LLaMa-2 adjust their strategies based on game structure and context, but LLaMa-2 exhibits a more nuanced understanding of the games' underlying mechanics. These results highlight the current limitations and varied proficiencies of LLMs in strategic decision-making, cautioning against their unqualified use in tasks requiring complex strategic reasoning."
http://arxiv.org/abs/2309.06546v2,Not obviously manipulable allotment rules,2023-09-12 19:40:43+00:00,"['R. Pablo Arribillaga', 'Agustin G. Bonifacio']",econ.TH,"In the problem of allocating a single non-disposable commodity among agents whose preferences are single-peaked, we study a weakening of strategy-proofness called not obvious manipulability (NOM). If agents are cognitively limited, then NOM is sufficient to describe their strategic behavior. We characterize a large family of own-peak-only rules that satisfy efficiency, NOM, and a minimal fairness condition. We call these rules ""simple"". In economies with excess demand, simple rules fully satiate agents whose peak amount is less than or equal to equal division and assign, to each remaining agent, an amount between equal division and his peak. In economies with excess supply, simple rules are defined symmetrically. These rules can be thought of as a two-step procedure that involves solving a claims problem. We also show that the single-plateaued domain is maximal for the characterizing properties of simple rules. Therefore, even though replacing strategy-proofness with NOM greatly expands the family of admissible rules, the maximal domain of preferences involved remains basically unaltered."
http://arxiv.org/abs/2309.04821v1,Non-linear dimension reduction in factor-augmented vector autoregressions,2023-09-09 15:22:30+00:00,['Karin Klieber'],econ.EM,"This paper introduces non-linear dimension reduction in factor-augmented vector autoregressions to analyze the effects of different economic shocks. I argue that controlling for non-linearities between a large-dimensional dataset and the latent factors is particularly useful during turbulent times of the business cycle. In simulations, I show that non-linear dimension reduction techniques yield good forecasting performance, especially when data is highly volatile. In an empirical application, I identify a monetary policy as well as an uncertainty shock excluding and including observations of the COVID-19 pandemic. Those two applications suggest that the non-linear FAVAR approaches are capable of dealing with the large outliers caused by the COVID-19 pandemic and yield reliable results in both scenarios."
http://arxiv.org/abs/2309.04876v1,News-driven Expectations and Volatility Clustering,2023-09-09 21:05:07+00:00,['Sabiou Inoua'],q-fin.GN,"Financial volatility obeys two fascinating empirical regularities that apply to various assets, on various markets, and on various time scales: it is fat-tailed (more precisely power-law distributed) and it tends to be clustered in time. Many interesting models have been proposed to account for these regularities, notably agent-based models, which mimic the two empirical laws through a complex mix of nonlinear mechanisms such as traders' switching between trading strategies in highly nonlinear way. This paper explains the two regularities simply in terms of traders' attitudes towards news, an explanation that follows almost by definition of the traditional dichotomy of financial market participants, investors versus speculators, whose behaviors are reduced to their simplest forms. Long-run investors' valuations of an asset are assumed to follow a news-driven random walk, thus capturing the investors' persistent, long memory of fundamental news. Short-term speculators' anticipated returns, on the other hand, are assumed to follow a news-driven autoregressive process, capturing their shorter memory of fundamental news, and, by the same token, the feedback intrinsic to the short-sighted, trend-following (or herding) mindset of speculators. These simple, linear, models of traders' expectations, it is shown, explain the two financial regularities in a generic and robust way. Rational expectations, the dominant model of traders' expectations, is not assumed here, owing to the famous no-speculation, no-trade results"
http://arxiv.org/abs/2309.14581v1,Assessing Utility of Differential Privacy for RCTs,2023-09-26 00:10:32+00:00,"['Soumya Mukherjee', 'Aratrika Mustafi', 'Aleksandra Slavković', 'Lars Vilhuber']",stat.AP,"Randomized control trials, RCTs, have become a powerful tool for assessing the impact of interventions and policies in many contexts. They are considered the gold-standard for inference in the biomedical fields and in many social sciences. Researchers have published an increasing number of studies that rely on RCTs for at least part of the inference, and these studies typically include the response data collected, de-identified and sometimes protected through traditional disclosure limitation methods. In this paper, we empirically assess the impact of strong privacy-preservation methodology (with \ac{DP} guarantees), on published analyses from RCTs, leveraging the availability of replication packages (research compendia) in economics and policy analysis. We provide simulations studies and demonstrate how we can replicate the analysis in a published economics article on privacy-protected data under various parametrizations. We find that relatively straightforward DP-based methods allow for inference-valid protection of the published data, though computational issues may limit more complex analyses from using these methods. The results have applicability to researchers wishing to share RCT data, especially in the context of low- and middle-income countries, with strong privacy protection."
http://arxiv.org/abs/2309.14475v1,Designing Effective Music Excerpts,2023-09-25 19:16:45+00:00,"['Emaad Manzoor', 'Nikhil Malik']",econ.GN,"Excerpts are widely used to preview and promote musical works. Effective excerpts induce consumption of the source musical work and thus generate revenue. Yet, what makes an excerpt effective remains unexplored. We leverage a policy change by Apple that generates quasi-exogenous variation in the excerpts of songs in the iTunes Music Store to estimate that having a 60 second longer excerpt increases songs' unique monthly listeners by 5.4% on average, by 9.7% for lesser known songs, and by 11.1% for lesser known artists. This is comparable to the impact of being featured on the Spotify Global Top 50 playlist. We develop measures of musical repetition and unpredictability to examine information provision as a mechanism, and find that the demand-enhancing effect of longer excerpts is suppressed when they are repetitive, too predictable, or too unpredictable. Our findings support platforms' adoption of longer excerpts to improve content discovery and our measures can help inform excerpt selection in practice."
http://arxiv.org/abs/2309.14297v1,Leveraging Uncertainties to Infer Preferences: Robust Analysis of School Choice,2023-09-25 17:11:17+00:00,"['Yeon-Koo Che', 'Dong Woo Hahm', 'YingHua He']",econ.GN,"Inferring applicant preferences is fundamental in many analyses of school-choice data. Application mistakes make this task challenging. We propose a novel approach to deal with the mistakes in a deferred-acceptance matching environment. The key insight is that the uncertainties faced by applicants, e.g., due to tie-breaking lotteries, render some mistakes costly, allowing us to reliably infer relevant preferences. Our approach extracts all information on preferences robustly to payoff-insignificant mistakes. We apply it to school-choice data from Staten Island, NYC. Counterfactual analysis suggests that we underestimate the effects of proposed desegregation reforms when applicants' mistakes are not accounted for in preference inference and estimation."
http://arxiv.org/abs/2309.12034v1,A detection analysis for temporal memory patterns at different time-scales,2023-09-21 12:56:16+00:00,"['Fabio Vanni', 'David Lambert']",econ.EM,"This paper introduces a novel methodology that utilizes latency to unveil time-series dependence patterns. A customized statistical test detects memory dependence in event sequences by analyzing their inter-event time distributions. Synthetic experiments based on the renewal-aging property assess the impact of observer latency on the renewal property. Our test uncovers memory patterns across diverse time scales, emphasizing the event sequence's probability structure beyond correlations. The time series analysis produces a statistical test and graphical plots which helps to detect dependence patterns among events at different time-scales if any. Furthermore, the test evaluates the renewal assumption through aging experiments, offering valuable applications in time-series analysis within economics."
http://arxiv.org/abs/2309.12902v1,Reduced-rank Envelope Vector Autoregressive Models,2023-09-22 14:38:52+00:00,"['S. Yaser Samadi', 'Wiranthe B. Herath']",stat.ME,"The standard vector autoregressive (VAR) models suffer from overparameterization which is a serious issue for high-dimensional time series data as it restricts the number of variables and lags that can be incorporated into the model. Several statistical methods, such as the reduced-rank model for multivariate (multiple) time series (Velu et al., 1986; Reinsel and Velu, 1998; Reinsel et al., 2022) and the Envelope VAR model (Wang and Ding, 2018), provide solutions for achieving dimension reduction of the parameter space of the VAR model. However, these methods can be inefficient in extracting relevant information from complex data, as they fail to distinguish between relevant and irrelevant information, or they are inefficient in addressing the rank deficiency problem. We put together the idea of envelope models into the reduced-rank VAR model to simultaneously tackle these challenges, and propose a new parsimonious version of the classical VAR model called the reduced-rank envelope VAR (REVAR) model. Our proposed REVAR model incorporates the strengths of both reduced-rank VAR and envelope VAR models and leads to significant gains in efficiency and accuracy. The asymptotic properties of the proposed estimators are established under different error assumptions. Simulation studies and real data analysis are conducted to evaluate and illustrate the proposed method."
http://arxiv.org/abs/2308.07320v1,PM-Gati Shakti: Advancing India's Energy Future through Demand Forecasting -- A Case Study,2023-07-30 09:36:42+00:00,"['SujayKumar Reddy M', 'Gopakumar G']",econ.GN,"PM-Gati-Shakti Initiative, integration of ministries, including railways, ports, waterways, logistic infrastructure, mass transport, airports, and roads. Aimed at enhancing connectivity and bolstering the competitiveness of Indian businesses, the initiative focuses on six pivotal pillars known as ""Connectivity for Productivity"": comprehensiveness, prioritization, optimization, synchronization, analytical, and dynamic. In this study, we explore the application of these pillars to address the problem of ""Maximum Demand Forecasting in Delhi."" Electricity forecasting plays a very significant role in the power grid as it is required to maintain a balance between supply and load demand at all times, to provide a quality electricity supply, for Financial planning, generation reserve, and many more. Forecasting helps not only in Production Planning but also in Scheduling like Import / Export which is very often in India and mostly required by the rural areas and North Eastern Regions of India. As Electrical Forecasting includes many factors which cannot be detected by the models out there, We use Classical Forecasting Techniques to extract the seasonal patterns from the daily data of Maximum Demand for the Union Territory Delhi. This research contributes to the power supply industry by helping to reduce the occurrence of disasters such as blackouts, power cuts, and increased tariffs imposed by regulatory commissions. The forecasting techniques can also help in reducing OD and UD of Power for different regions. We use the Data provided by a department from the Ministry of Power and use different forecast models including Seasonal forecasts for daily data."
http://arxiv.org/abs/2308.05263v1,Solving the Forecast Combination Puzzle,2023-08-10 00:16:31+00:00,"['David T. Frazier', 'Ryan Covey', 'Gael M. Martin', 'Donald Poskitt']",econ.EM,"We demonstrate that the forecasting combination puzzle is a consequence of the methodology commonly used to produce forecast combinations. By the combination puzzle, we refer to the empirical finding that predictions formed by combining multiple forecasts in ways that seek to optimize forecast performance often do not out-perform more naive, e.g. equally-weighted, approaches. In particular, we demonstrate that, due to the manner in which such forecasts are typically produced, tests that aim to discriminate between the predictive accuracy of competing combination strategies can have low power, and can lack size control, leading to an outcome that favours the naive approach. We show that this poor performance is due to the behavior of the corresponding test statistic, which has a non-standard asymptotic distribution under the null hypothesis of no inferior predictive accuracy, rather than the {standard normal distribution that is} {typically adopted}. In addition, we demonstrate that the low power of such predictive accuracy tests in the forecast combination setting can be completely avoided if more efficient estimation strategies are used in the production of the combinations, when feasible. We illustrate these findings both in the context of forecasting a functional of interest and in terms of predictive densities. A short empirical example {using daily financial returns} exemplifies how researchers can avoid the puzzle in practical settings."
http://arxiv.org/abs/2308.07993v1,Does courier gender matter? Exploring mode choice behaviour for E-groceries crowd-shipping in developing economies,2023-08-15 18:54:54+00:00,"['Oleksandr Rossolov', 'Anastasiia Botsman', 'Serhii Lyfenko', 'Yusak O. Susilo']",econ.GN,"This paper examines the mode choice behaviour of people who may act as occasional couriers to provide crowd-shipping (CS) deliveries. Given its recent increase in popularity, online grocery services have become the main market for crowd-shipping deliveries' provider. The study included a behavioural survey, PTV Visum simulations and discrete choice behaviour modelling based on random utility maximization theory. Mode choice behaviour was examined by considering the gender heterogeneity of the occasional couriers in a multimodal urban transport network. The behavioural dataset was collected in the city of Kharkiv, Ukraine, at the beginning of 2021. The results indicated that women were willing to provide CS service with 8% less remuneration than men. Women were also more likely to make 10% longer detours by car and metro than men, while male couriers were willing to implement 25% longer detours when travelling by bike or walking. Considering the integration of CS detours into the couriers' routine trip chains, women couriers were more likely to attach the CS trip to the work-shopping trip chain whilst men would use the home-home evening time trip chain. The estimated marginal probability effect indicated a higher detour time sensitivity with respect to expected profit and the relative detour costs of the couriers."
http://arxiv.org/abs/2309.00635v1,Theoretical foundation for the Pareto distribution of international trade strength and introduction of an equation for international trade forecasting,2023-08-19 11:19:55+00:00,['Mikrajuddin Abdullah'],econ.GN,"I propose a new terminology, international trade strength, which is defined as the ratio of a country's total international trade to its GDP. This parameter represents a country's ability to generate international trade by utilizing its GDP. This figure is equivalent to GDP per capita, which represents a country's ability to use its population to generate GDP. Trade strength varies by country. The intriguing question is, what distribution function does the trade strength fulfill? In this paper, a theoretical foundation for predicting the distribution of trade strength and the rate of change of trade strength were developed. These two quantities were found to satisfy the Pareto distribution function. The equations were confirmed using data from the World Integrated Trade Solution (WITS) and the World Bank by comparing the Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) to five types of distribution functions (exponential, lognormal, gamma, Pareto, and Weibull). I also discovered that the fitting Pareto power parameter is fairly close to the theoretical parameter. In addition, a formula for forecasting a country's total international trade in the following years was also developed."
http://arxiv.org/abs/2309.01471v1,A Trimming Estimator for the Latent-Diffusion-Observed-Adoption Model,2023-09-04 09:29:25+00:00,['L. S. Sanna Stephan'],econ.EM,"Network diffusion models are applicable to many socioeconomic interactions, yet network interaction is hard to observe or measure. Whenever the diffusion process is unobserved, the number of possible realizations of the latent matrix that captures agents' diffusion statuses grows exponentially with the size of network. Due to interdependencies, the log likelihood function can not be factorized in individual components. As a consequence, exact estimation of latent diffusion models with more than one round of interaction is computationally infeasible. In the present paper, I propose a trimming estimator that enables me to establish and maximize an approximate log likelihood function that almost exactly identifies the peak of the true log likelihood function whenever no more than one third of eligible agents are subject to trimming."
http://arxiv.org/abs/2309.00805v1,Fairness Implications of Heterogeneous Treatment Effect Estimation with Machine Learning Methods in Policy-making,2023-09-02 03:06:14+00:00,"['Patrick Rehill', 'Nicholas Biddle']",econ.EM,"Causal machine learning methods which flexibly generate heterogeneous treatment effect estimates could be very useful tools for governments trying to make and implement policy. However, as the critical artificial intelligence literature has shown, governments must be very careful of unintended consequences when using machine learning models. One way to try and protect against unintended bad outcomes is with AI Fairness methods which seek to create machine learning models where sensitive variables like race or gender do not influence outcomes. In this paper we argue that standard AI Fairness approaches developed for predictive machine learning are not suitable for all causal machine learning applications because causal machine learning generally (at least so far) uses modelling to inform a human who is the ultimate decision-maker while AI Fairness approaches assume a model that is making decisions directly. We define these scenarios as indirect and direct decision-making respectively and suggest that policy-making is best seen as a joint decision where the causal machine learning model usually only has indirect power. We lay out a definition of fairness for this scenario - a model that provides the information a decision-maker needs to accurately make a value judgement about just policy outcomes - and argue that the complexity of causal machine learning models can make this difficult to achieve. The solution here is not traditional AI Fairness adjustments, but careful modelling and awareness of some of the decision-making biases that these methods might encourage which we describe."
http://arxiv.org/abs/2312.14325v1,Exploring Distributions of House Prices and House Price Indices,2023-12-21 22:38:24+00:00,"['Jiong Liu', 'Hamed Farahani', 'R. A. Serota']",econ.EM,"We use house prices (HP) and house price indices (HPI) as a proxy to income distribution. Specifically, we analyze sale prices in the 1970-2010 window of over 116,000 single-family homes in Hamilton County, Ohio, including Cincinnati metro area of about 2.2 million people. We also analyze HPI, published by Federal Housing Finance Agency (FHFA), for nearly 18,000 US ZIP codes that cover a period of over 40 years starting in 1980's. If HP can be viewed as a first derivative of income, HPI can be viewed as its second derivative. We use generalized beta (GB) family of functions to fit distributions of HP and HPI since GB naturally arises from the models of economic exchange described by stochastic differential equations. Our main finding is that HP and multi-year HPI exhibit a negative Dragon King (nDK) behavior, wherein power-law distribution tail gives way to an abrupt decay to a finite upper limit value, which is similar to our recent findings for realized volatility of S\&P500 index in the US stock market. This type of tail behavior is best fitted by a modified GB (mGB) distribution. Tails of single-year HPI appear to show more consistency with power-law behavior, which is better described by a GB Prime (GB2) distribution. We supplement full distribution fits by mGB and GB2 with direct linear fits (LF) of the tails. Our numerical procedure relies on evaluation of confidence intervals (CI) of the fits, as well as of p-values that give the likelihood that data come from the fitted distributions."
http://arxiv.org/abs/2312.16214v4,Stochastic Equilibrium the Lucas Critique and Keynesian Economics,2023-12-23 22:59:33+00:00,['David Staines'],econ.TH,"In this paper, a mathematically rigorous solution overturns existing wisdom regarding New Keynesian Dynamic Stochastic General Equilibrium. I develop a formal concept of stochastic equilibrium. I prove uniqueness and necessity, when agents are patient, with general application. Existence depends on appropriately specified eigenvalue conditions. Otherwise, no solution of any kind exists. I construct the equilibrium with Calvo pricing. I provide novel comparative statics with the non-stochastic model of mathematical significance. I uncover a bifurcation between neighbouring stochastic systems and approximations taken from the Zero Inflation Non-Stochastic Steady State (ZINSS). The correct Phillips curve agrees with the zero limit from the trend inflation framework. It contains a large lagged inflation coefficient and a small response to expected inflation. Price dispersion can be first or second order depending how shocks are scaled. The response to the output gap is always muted and is zero at standard parameters. A neutrality result is presented to explain why and align Calvo with Taylor pricing. Present and lagged demand shocks enter the Phillips curve so there is no Divine Coincidence and the system is identified from structural shocks alone. The lagged inflation slope is increasing in the inflation response, embodying substantive policy trade-offs. The Taylor principle is reversed, inactive settings are necessary, pointing towards inertial policy. The observational equivalence idea of the Lucas critique is disproven. The bifurcation results from the breakdown of the constraints implied by lagged nominal rigidity, associated with cross-equation cancellation possible only at ZINSS. There is a dual relationship between restrictions on the econometrician and constraints on repricing firms. Thus, if the model is correct, goodness of fit will jump."
http://arxiv.org/abs/2401.02428v1,Cuánto es demasiada inflación? Una clasificación de regímenes inflacionarios,2023-11-10 19:51:18+00:00,"['Manuel de Mier', 'Fernando Delbianco']",econ.GN,"The classifications of inflationary regimes proposed in the literature have mostly been based on arbitrary characterizations, subject to value judgments by researchers. The objective of this study is to propose a new methodological approach that reduces subjectivity and improves accuracy in the construction of such regimes. The method is built upon a combination of clustering techniques and classification trees, which allows for an historical periodization of Argentina's inflationary history for the period 1943-2022. Additionally, two procedures are introduced to smooth out the classification over time: a measure of temporal contiguity of observations and a rolling method based on the simple majority rule. The obtained regimes are compared against the existing literature on the inflation-relative price variability relationship, revealing a better performance of the proposed regimes."
http://arxiv.org/abs/2401.10261v1,How industrial clusters influence the growth of the regional GDP: A spatial-approach,2023-12-31 22:29:57+00:00,"['Vahidin Jeleskovic', 'Steffen Loeber']",econ.GN,"In this paper, we employ spatial econometric methods to analyze panel data from German NUTS 3 regions. Our goal is to gain a deeper understanding of the significance and interdependence of industry clusters in shaping the dynamics of GDP. To achieve a more nuanced spatial differentiation, we introduce indicator matrices for each industry sector which allows for extending the spatial Durbin model to a new version of it. This approach is essential due to both the economic importance of these sectors and the potential issue of omitted variables. Failing to account for industry sectors can lead to omitted variable bias and estimation problems. To assess the effects of the major industry sectors, we incorporate eight distinct branches of industry into our analysis. According to prevailing economic theory, these clusters should have a positive impact on the regions they are associated with. Our findings indeed reveal highly significant impacts, which can be either positive or negative, of specific sectors on local GDP growth. Spatially, we observe that direct and indirect effects can exhibit opposite signs, indicative of heightened competitiveness within and between industry sectors. Therefore, we recommend that industry sectors should be taken into consideration when conducting spatial analysis of GDP. Doing so allows for a more comprehensive understanding of the economic dynamics at play."
http://arxiv.org/abs/2310.16281v4,Improving Robust Decisions with Data,2023-10-25 01:27:24+00:00,['Xiaoyu Cheng'],econ.TH,"A decision-maker faces uncertainty governed by a data-generating process (DGP), which is only known to belong to a set of sequences of independent but possibly non-identical distributions. A robust decision maximizes the expected payoff against the worst possible DGP in this set. This paper characterizes when and how such robust decisions can be improved with data, measured by the expected payoff under the true DGP, no matter which possible DGP is the truth. It further develops novel and simple inference methods to achieve it, as common methods (e.g., maximum likelihood) may fail to deliver such an improvement."
http://arxiv.org/abs/2312.11767v1,"Least-cost diets to teach optimization and consumer behavior, with applications to health equity, poverty measurement and international development",2023-12-19 00:56:44+00:00,"['Jessica K. Wallingford', 'William A. Masters']",econ.GN,"The least-cost diet problem introduces students to optimization and linear programming, using the health consequences of food choice. We provide a graphical example, Excel workbook and Word template using actual data on item prices, food composition and nutrient requirements for a brief exercise in which students guess at and then solve for nutrient adequacy at lowest cost, before comparing modeled diets to actual consumption which has varying degrees of nutrient adequacy. The graphical example is a 'three sisters' diet of corn, beans and squash, and the full multidimensional model is compared to current food consumption in Ethiopia. This updated Stigler diet shows how cost minimization relates to utility maximization, and links to ongoing research and policy debates about the affordability of healthy diets worldwide."
http://arxiv.org/abs/2312.13515v1,Recognising natural capital on the balance sheet: options for water utilities,2023-12-21 01:34:09+00:00,"['Marie-Chantale Pelletier', 'Claire Horner', 'Mathew Vickers', 'Aliya Gul', 'Eren Turak', 'Christine Turner']",econ.GN,"Purpose: The aim of this study was to explore the feasibility of natural capital accounting for the purpose of strengthening sustainability claims by reporting entities. The study linked riparian land improvement to ecosystem services and tested options for incorporating natural capital into financial accounting practices, specifically on the balance sheet. Methodology: To test the approach, the study used a public asset manager (a water utility) with accountabilities to protect the environment including maintaining and enhancing riparian land assets. Research activities included stakeholder engagement, physical asset measurement, monetary valuation and financial recognition of natural capital income and assets. Natural capital income was estimated by modelling and valuing ecosystem services relating to stormwater filtration and carbon storage. Findings: This research described how a water utility could disclose changes in the natural capital assets they manage either through voluntary disclosures, in notes to the financial statements or as balance sheet items. We found that current accounting standards allowed the recognition of some types of environmental income and assets where ecosystem services were associated with cost savings. The proof-of-concept employed to estimate environmental income through ecosystem service modelling proved useful to strengthen sustainability claims or report financial returns on natural capital investment. Originality/value: This study applied financial accounting processes and principles to a realistic public asset management scenario with direct participation by the asset manager working together with academic researchers and a sub-national government environment management agency. Importantly it established that natural assets could be included in financial statements, proposing a new approach to measuring and reporting on natural capital."
http://arxiv.org/abs/2312.14853v1,In the Line of Fire: A Systematic Review and Meta-Analysis of Job Burnout Among Nurses,2023-12-22 17:25:55+00:00,"['Zahra Ghasemi Kooktapeh', 'Hakimeh Dustmohammadloo', 'Hooman Mehrdoost', 'Farivar Fatehi']",econ.GN,"Using a systematic review and meta-analysis, this study investigates the impact of the COVID-19 pandemic on job burnout among nurses. We review healthcare articles following the PRISMA 2020 guidelines and identify the main aspects and factors of burnout among nurses during the pandemic. Using the Maslach Burnout questionnaire, we searched PubMed, ScienceDirect, and Google Scholar, three open-access databases, for relevant sources measuring emotional burnout, personal failure, and nurse depersonalization. Two reviewers extract and screen data from the sources and evaluate the risk of bias. The analysis reveals that 2.75% of nurses experienced job burnout during the pandemic, with a 95% confidence interval and rates varying from 1.87% to 7.75%. These findings emphasize the need for interventions to address the pandemic's effect on job burnout among nurses and enhance their well-being and healthcare quality. We recommend considering individual, organizational, and contextual factors influencing healthcare workers' burnout. Future research should focus on identifying effective interventions to lower burnout in nurses and other healthcare professionals during pandemics and high-stress situations."
http://arxiv.org/abs/2312.13939v1,Binary Endogenous Treatment in Stochastic Frontier Models with an Application to Soil Conservation in El Salvador,2023-12-21 15:32:31+00:00,"['Samuele Centorrino', 'Maria Pérez-Urdiales', 'Boris Bravo-Ureta', 'Alan J. Wall']",econ.EM,"Improving the productivity of the agricultural sector is part of one of the Sustainable Development Goals set by the United Nations. To this end, many international organizations have funded training and technology transfer programs that aim to promote productivity and income growth, fight poverty and enhance food security among smallholder farmers in developing countries. Stochastic production frontier analysis can be a useful tool when evaluating the effectiveness of these programs. However, accounting for treatment endogeneity, often intrinsic to these interventions, only recently has received any attention in the stochastic frontier literature. In this work, we extend the classical maximum likelihood estimation of stochastic production frontier models by allowing both the production frontier and inefficiency to depend on a potentially endogenous binary treatment. We use instrumental variables to define an assignment mechanism for the treatment, and we explicitly model the density of the first and second-stage composite error terms. We provide empirical evidence of the importance of controlling for endogeneity in this setting using farm-level data from a soil conservation program in El Salvador."
http://arxiv.org/abs/2401.06134v1,Synergy or Rivalry? Glimpses of Regional Modernization and Public Service Equalization: A Case Study from China,2023-11-21 05:58:17+00:00,"['Shengwen Shi', ""Jian'an Zhang""]",econ.GN,"For most developing countries, increasing the equalization of basic public services is widely recognized as an effective channel to improve people's sense of contentment. However, for many emerging economies like China, the equalization level of basic public services may often be neglected in the trade-off between the speed and quality of development. Taking the Yangtze River Delta region of China as an example, this paper first adopts the coupling coordination degree model to explore current status of basic public services in this region, and then uses Moran's I index to study the overall equalization level of development there. Moreover, this paper uses the Theil index to analyze the main reasons for the spatial differences in the level of public services, followed by the AF method to accurately identify the exact weaknesses of the 40 counties of 10 cities with the weakest level of basic public service development. Based on this, this paper provides targeted optimization initiatives and continues to explore the factors affecting the growth of the level of public service equalization through the convergence model, verifying the convergence trend of the degree of public service equalization, and ultimately providing practical policy recommendations for promoting the equalization of basic public services."
http://arxiv.org/abs/2401.13670v1,"""The Roller Conduction Effect"" from the A-share Data Evidence",2023-10-16 00:57:53+00:00,['Wenbo Lyu'],econ.GN,"In the post-epidemic era, consumption recovery has obvious time and space transmission laws, and there are different valuation criteria for consumption segments. Using the A-share data of the consumption recovery stage from January to April 2022, this paper quantitatively compares the rotation effect between different consumption sectors when the valuation returns to the reasonable range. According to the new classification of ""sensory-based consumption"", it interprets the internal logic of digital consumption as A consumption upgrade tool and a higher valuation target, and expounds the ""the roller conduction effect"". The law of consumption recovery and valuation return period is explained from the perspective of time and space conduction. The study found that in the early stage of consumption recovery, the recovery of consumer confidence was slow. In this period, A-shares were mainly dominated by the stock capital game, and there was an obvious plate rotation law in the game. Being familiar with this law has strong significance, which not only helps policy makers to adjust the direction of policy guidance, but also helps financial investors to make better investment strategies. The disadvantage of this paper is that it has not yet studied the roller conduction effect of the global financial market, and more rigorous mathematical models are still needed to support the definition of stock funds, which is also the main direction of the author's future research."
http://arxiv.org/abs/2401.13678v1,"I Can't Go to Work Tomorrow! Work-Family Policies, Well-Being and Absenteeism",2023-12-05 13:47:05+00:00,"['Jose Aurelio Medina-Garrido', 'Jose Maria Biedma-Ferrer', 'Jaime Sanchez-Ortiz']",econ.GN,"Among the main causes of absenteeism are health problems, emotional problems, and inadequate work-family policies (WFP). This paper analyses the impact of the existence and accessibility of WFP on work absenteeism, by considering the mediating role of the well-being, which includes emotional as well as physical or health problems, that is generated by these policies. We differentiate between the existence of the WFP and its accessibility, as the mere existence of the WFP in an organisation is not enough. Additionally, workers must be able to access these policies easily and without retaliation of any kind. The model includes the hierarchy and the gender as moderating variables. To test the proposed hypotheses, a structural equation model based on the partial least squares structural equation modelling (PLS-SEM) approach is applied to a sample of employees in the service sector in Spain. On the one hand, the findings show that the existence of WFP has no direct effect on absenteeism; however, accessibility to these policies does have a direct effect on absenteeism. On the other hand, both the existence and accessibility of WFP have positive direct effects on emotional well-being. In addition, emotional well-being is positively related to physical well-being which, in turn, promotes a reduction in absenteeism. Finally, significant differences in the relationship between the existence of WFP and emotional well-being confirm the special difficulty of female managers in reconciling family life and work life."
http://arxiv.org/abs/2312.10333v1,Logit-based alternatives to two-stage least squares,2023-12-16 05:47:43+00:00,"['Denis Chetverikov', 'Jinyong Hahn', 'Zhipeng Liao', 'Shuyang Sheng']",econ.EM,"We propose logit-based IV and augmented logit-based IV estimators that serve as alternatives to the traditionally used 2SLS estimator in the model where both the endogenous treatment variable and the corresponding instrument are binary. Our novel estimators are as easy to compute as the 2SLS estimator but have an advantage over the 2SLS estimator in terms of causal interpretability. In particular, in certain cases where the probability limits of both our estimators and the 2SLS estimator take the form of weighted-average treatment effects, our estimators are guaranteed to yield non-negative weights whereas the 2SLS estimator is not."
http://arxiv.org/abs/2312.10749v1,A new behavioral model for portfolio selection using the Half-Full/Half-Empty approach,2023-12-17 15:50:10+00:00,"['Francesco Cesarone', 'Massimiliano Corradini', 'Lorenzo Lampariello', 'Jessica Riccioni']",q-fin.PM,"We focus on a behavioral model, that has been recently proposed in the literature, whose rational can be traced back to the Half-Full/Half-Empty glass metaphor. More precisely, we generalize the Half-Full/Half-Empty approach to the context of positive and negative lotteries and give financial and behavioral interpretations of the Half-Full/Half-Empty parameters. We develop a portfolio selection model based on the Half-Full/Half-Empty strategy, resulting in a nonconvex optimization problem, which, nonetheless, is proven to be equivalent to an alternative Mixed-Integer Linear Programming formulation. By means of the ensuing empirical analysis, based on three real-world datasets, the Half-Full/Half-Empty model is shown to be very versatile by appropriately varying its parameters, and to provide portfolios displaying promising performances in terms of risk and profitability, compared with Prospect Theory, risk minimization approaches and Equally-Weighted portfolios."
http://arxiv.org/abs/2312.11408v2,Approval-Based Committee Voting in Practice: A Case Study of (Over-)Representation in the Polkadot Blockchain,2023-12-18 18:15:38+00:00,"['Niclas Boehmer', 'Markus Brill', 'Alfonso Cevallos', 'Jonas Gehrlein', 'Luis Sánchez-Fernández', 'Ulrike Schmidt-Kraepelin']",cs.GT,"We provide the first large-scale data collection of real-world approval-based committee elections. These elections have been conducted on the Polkadot blockchain as part of their Nominated Proof-of-Stake mechanism and contain around one thousand candidates and tens of thousands of (weighted) voters each. We conduct an in-depth study of application-relevant questions, including a quantitative and qualitative analysis of the outcomes returned by different voting rules. Besides considering proportionality measures that are standard in the multiwinner voting literature, we pay particular attention to less-studied measures of overrepresentation, as these are closely related to the security of the Polkadot network. We also analyze how different design decisions such as the committee size affect the examined measures."
http://arxiv.org/abs/2312.11010v1,Endogenous preference for non-market goods in carbon abatement decision,2023-12-18 08:15:32+00:00,"['Fangzhi Wang', 'Hua Liao', 'Richard S. J. Tol', 'Changjing Ji']",econ.GN,"Carbon abatement decisions are usually based on the implausible assumption of constant social preference. This paper focuses on a specific case of market and non-market goods, and investigates the optimal climate policy when social preference for them is also changed by climate policy in the DICE model. The relative price of non-market goods grows over time due to increases in both relative scarcity and appreciation of it. Therefore, climbing relative price brings upward the social cost of carbon denominated in terms of market goods. Because abatement decision affects the valuation of non-market goods in the utility function, unlike previous climate-economy models, we solve the model iteratively by taking the obtained abatement rates from the last run as inputs in the current run. The results in baseline calibration advocate a more stringent climate policy, where endogenous social preference to climate policy raises the social cost of carbon further by roughly 12%-18% this century. Moreover, neglecting changing social preference leads to an underestimate of non-market goods damages by 15%. Our results support that climate policy is self-reinforced if it favors more expensive consumption type."
http://arxiv.org/abs/2310.10850v2,Digital interventions and habit formation in educational technology,2023-10-16 21:50:56+00:00,"['Keshav Agrawal', 'Susan Athey', 'Ayush Kanodia', 'Emil Palikot']",econ.GN,"As online educational technology products have become increasingly prevalent, rich evidence indicates that learners often find it challenging to establish regular learning habits and complete their programs. Concurrently, online products geared towards entertainment and social interactions are sometimes so effective in increasing user engagement and creating frequent usage habits that they inadvertently lead to digital addiction, especially among youth. In this project, we carry out a contest-based intervention, common in the entertainment context, on an educational app for Indian children learning English. Approximately ten thousand randomly selected learners entered a 100-day reading contest. They would win a set of physical books if they ranked sufficiently high on a leaderboard based on the amount of educational content consumed. Twelve weeks after the end of the contest, when the treatment group had no additional incentives to use the app, they continued their engagement with it at a rate 75\% higher than the control group, indicating a successful formation of a reading habit. In addition, we observed a 6\% increase in retention within the treatment group. These results underscore the potential of digital interventions in fostering positive engagement habits with educational technology products, ultimately enhancing users' long-term learning outcomes."
http://arxiv.org/abs/2312.17167v1,"The Gatekeeper Effect: The Implications of Pre-Screening, Self-selection, and Bias for Hiring Processes",2023-12-28 17:54:39+00:00,['Moran Koren'],econ.TH,"We study the problem of screening in decision-making processes under uncertainty, focusing on the impact of adding an additional screening stage, commonly known as a 'gatekeeper.' While our primary analysis is rooted in the context of job market hiring, the principles and findings are broadly applicable to areas such as educational admissions, healthcare patient selection, and financial loan approvals. The gatekeeper's role is to assess applicants' suitability before significant investments are made. Our study reveals that while gatekeepers are designed to streamline the selection process by filtering out less likely candidates, they can sometimes inadvertently affect the candidates' own decision-making process. We explore the conditions under which the introduction of a gatekeeper can enhance or impede the efficiency of these processes. Additionally, we consider how adjusting gatekeeping strategies might impact the accuracy of selection decisions. Our research also extends to scenarios where gatekeeping is influenced by historical biases, particularly in competitive settings like hiring. We discover that candidates confronted with a statistically biased gatekeeping process are more likely to withdraw from applying, thereby perpetuating the previously mentioned historical biases. The study suggests that measures such as affirmative action can be effective in addressing these biases. While centered on hiring, the insights and methodologies from our study have significant implications for a wide range of fields where screening and gatekeeping are integral."
http://arxiv.org/abs/2401.00227v1,Does the World Bank's Ease of Doing Business Index Matter for FDI? Findings from Africa,2023-12-30 13:27:31+00:00,['Bhaso Ndzendze'],econ.GN,"This paper investigates whether foreign investment (FDI) into Africa is at least partially responsive to World Bank-measured market friendliness. Specifically, I conducted analyses of four countries between 2009 and 2017, using cases that represent two of the highest scorers on the bank's Doing Business index as of 2008 (Mauritius and South Africa) and the two lowest scorers (DRC and CAR), and subsequently traced all four for growths or declines in FDI in relation to their scores in the index. The findings show that there is a moderate association between decreased costs of starting a business and growth of FDI. Mauritius, South Africa and the DRC reduced their total cost of starting a business by 71.7%, 143.7% and 122.9% for the entire period, and saw inward FDI increases of 167.6%, 79.8% and 152.21%, respectively. The CAR increased the cost of starting businesses but still saw increases in FDI. However, the country also saw the least amount of growth in FDI at only 13.3%."
http://arxiv.org/abs/2401.00313v3,Matching of Users and Creators in Two-Sided Markets with Departures,2023-12-30 20:13:28+00:00,"['Daniel Huttenlocher', 'Hannah Li', 'Liang Lyu', 'Asuman Ozdaglar', 'James Siderius']",cs.GT,"Many online platforms of today, including social media sites, are two-sided markets bridging content creators and users. Most of the existing literature on platform recommendation algorithms largely focuses on user preferences and decisions, and does not simultaneously address creator incentives. We propose a model of content recommendation that explicitly focuses on the dynamics of user-content matching, with the novel property that both users and creators may leave the platform permanently if they do not experience sufficient engagement. In our model, each player decides to participate at each time step based on utilities derived from the current match: users based on alignment of the recommended content with their preferences, and creators based on their audience size. We show that a user-centric greedy algorithm that does not consider creator departures can result in arbitrarily poor total engagement, relative to an algorithm that maximizes total engagement while accounting for two-sided departures. Moreover, in stark contrast to the case where only users or only creators leave the platform, we prove that with two-sided departures, approximating maximum total engagement within any constant factor is NP-hard. We present two practical algorithms, one with performance guarantees under mild assumptions on user preferences, and another that tends to outperform algorithms that ignore two-sided departures in practice."
http://arxiv.org/abs/2311.09083v1,Structural Advantages for Integrated Builders in MEV-Boost,2023-11-15 16:25:33+00:00,"['Mallesh Pai', 'Max Resnick']",econ.TH,"Currently, over 90% of Ethereum blocks are built using MEV-Boost, an auction that allows validators to sell their block-building power to builders who compete in an open English auction in each slot. Shortly after the merge, when MEV-Boost was in its infancy, most block builders were neutral, meaning they did not trade themselves but rather aggregated transactions from other traders. Over time, integrated builders, operated by trading firms, began to overtake many of the neutral builders. Outside of the integrated builder teams, little is known about which advantages integration confers beyond latency and how latency advantages distort on-chain trading.
  This paper explores these poorly understood advantages. We make two contributions. First, we point out that integrated builders are able to bid truthfully in their own bundle merge and then decide how much profit to take later in the final stages of the PBS auction when more information is available, making the auction for them look closer to a second-price auction while independent searchers are stuck in a first-price auction. Second, we find that latency disadvantages convey a winner's curse on slow bidders when underlying values depend on a stochastic price process that change as bids are submitted."
http://arxiv.org/abs/2311.08678v1,The Future of Sustainability in Germany: Areas for Improvement and Innovation,2023-11-15 03:57:16+00:00,"['Mehrnaz Kouhihabibi', 'Erfan Mohammadi']",econ.GN,"This paper reviews the literature on biodegradable waste management in Germany, a multifaceted endeavor that reflects its commitment to sustainability and environmental responsibility. It examines the processes and benefits of separate collection, recycling, and eco-friendly utilization of biodegradable materials, which produce valuable byproducts such as compost, digestate, and biogas. These byproducts serve as organic fertilizers, peat substitutes, and renewable energy sources, contributing to ecological preservation and economic prudence. The paper also discusses the global implications of biodegradable waste management, such as preventing methane emissions from landfills, a major source of greenhouse gas, and reusing organic matter and essential nutrients. Moreover, the paper explores how biodegradable waste management reduces waste volume, facilitates waste sorting and incineration, and sets a global example for addressing climate change and working toward a sustainable future. The paper highlights the importance of a comprehensive and holistic approach to sustainability that encompasses waste management, renewable energy, transportation, agriculture, waste reduction, and urban development."
http://arxiv.org/abs/2311.11637v1,Modeling economies of scope in joint production: Convex regression of input distance function,2023-11-20 09:53:08+00:00,"['Timo Kuosmanen', 'Sheng Dai']",stat.ME,"Modeling of joint production has proved a vexing problem. This paper develops a radial convex nonparametric least squares (CNLS) approach to estimate the input distance function with multiple outputs. We document the correct input distance function transformation and prove that the necessary orthogonality conditions can be satisfied in radial CNLS. A Monte Carlo study is performed to compare the finite sample performance of radial CNLS and other deterministic and stochastic frontier approaches in terms of the input distance function estimation. We apply our novel approach to the Finnish electricity distribution network regulation and empirically confirm that the input isoquants become more curved. In addition, we introduce the weight restriction to radial CNLS to mitigate the potential overfitting and increase the out-of-sample performance in energy regulation."
http://arxiv.org/abs/2311.12671v1,Predictive Density Combination Using a Tree-Based Synthesis Function,2023-11-21 15:29:09+00:00,"['Tony Chernis', 'Niko Hauzenberger', 'Florian Huber', 'Gary Koop', 'James Mitchell']",econ.EM,"Bayesian predictive synthesis (BPS) provides a method for combining multiple predictive distributions based on agent/expert opinion analysis theory and encompasses a range of existing density forecast pooling methods. The key ingredient in BPS is a ``synthesis'' function. This is typically specified parametrically as a dynamic linear regression. In this paper, we develop a nonparametric treatment of the synthesis function using regression trees. We show the advantages of our tree-based approach in two macroeconomic forecasting applications. The first uses density forecasts for GDP growth from the euro area's Survey of Professional Forecasters. The second combines density forecasts of US inflation produced by many regression models involving different predictors. Both applications demonstrate the benefits -- in terms of improved forecast accuracy and interpretability -- of modeling the synthesis function nonparametrically."
http://arxiv.org/abs/2311.11828v1,Would Monetary Incentives to COVID-19 vaccination reduce motivation?,2023-11-20 15:04:26+00:00,"['Eiji Yamamura', 'Yoshiro Tsutsui', 'Fumio Ohtake']",econ.GN,"Some people did not get the COVID-19 vaccine even though it was offered at no cost. A monetary incentive might lead people to vaccinate, although existing studies have provided different findings about this effect. We investigate how monetary incentives differ according to individual characteristics. Using panel data with online experiments, we found that (1) subsidies reduced vaccine intention but increased it after controlling heterogeneity; (2) the stronger the social image against the vaccination, the lower the monetary incentive; and (3) persistently unvaccinated people would intend to be vaccinated only if a large subsidy was provided."
http://arxiv.org/abs/2311.17858v1,On the Limits of Regression Adjustment,2023-11-29 18:04:39+00:00,"['Daniel Ting', 'Kenneth Hung']",stat.ME,"Regression adjustment, sometimes known as Controlled-experiment Using Pre-Experiment Data (CUPED), is an important technique in internet experimentation. It decreases the variance of effect size estimates, often cutting confidence interval widths in half or more while never making them worse. It does so by carefully regressing the goal metric against pre-experiment features to reduce the variance. The tremendous gains of regression adjustment begs the question: How much better can we do by engineering better features from pre-experiment data, for example by using machine learning techniques or synthetic controls? Could we even reduce the variance in our effect sizes arbitrarily close to zero with the right predictors? Unfortunately, our answer is negative. A simple form of regression adjustment, which uses just the pre-experiment values of the goal metric, captures most of the benefit. Specifically, under a mild assumption that observations closer in time are easier to predict that ones further away in time, we upper bound the potential gains of more sophisticated feature engineering, with respect to the gains of this simple form of regression adjustment. The maximum reduction in variance is $50\%$ in Theorem 1, or equivalently, the confidence interval width can be reduced by at most an additional $29\%$."
http://arxiv.org/abs/2311.17981v1,Optimizing the Generation and Transmission Capacity of Offshore Wind Parks under Weather Uncertainty,2023-11-29 18:02:08+00:00,"['David Kröger', 'Jan Peper', 'Nils Offermann', 'Christian Rehtanz']",eess.SY,"Offshore wind power in the North Sea is considered a main pillar in Europe's future energy system. A key challenge lies in determining the optimal spatial capacity allocation of offshore wind parks in combination with the dimensioning and layout of the connecting high-voltage direct current grid infrastructure. To determine economically cost optimal configurations, we apply an integrated capacity and transmission expansion problem within a pan-European electricity market and transmission grid model with a high spatial and temporal granularity. By conducting scenario analysis for the year 2030 with a gradually increasing CO2 price, possible offshore expansion paths are derived and presented. Special emphasis is laid on the effects of weather uncertainty by incorporating data from 21 historical weather years in the analysis. Two key findings are (i) an expansion in addition to the existing offshore wind capacity of 0 GW (136 EUR/tCO2), 12 GW (159 EUR/tCO2) and 30 GW (186 EUR/tCO2) dependent on the underlying CO2 price. (ii) A strong sensitivity of the results towards the underlying weather data highlighting the importance of incorporating multiple weather years."
http://arxiv.org/abs/2312.02055v1,Transaction Ordering Auctions,2023-12-04 17:14:06+00:00,['Jan Christoph Schlegel'],cs.GT,"We study equilibrium investment into bidding and latency reduction for different sequencing policies. For a batch auction design, we observe that bidders shade bids according to the likelihood that competing bidders land in the current batch. Moreover, in equilibrium, in the ex-ante investment stage before the auction, bidders invest into latency until they make zero profit in expectation.
  We compare the batch auction design to continuous time bidding policies (time boost) and observe that (depending on the choice of parameters) they obtain similar revenue and welfare guarantees."
http://arxiv.org/abs/2311.17443v1,Power system investment optimization to identify carbon neutrality scenarios for Italy,2023-11-29 08:35:48+00:00,"['Alice Di Bella', 'Federico Canti', 'Matteo Giacomo Prina', 'Valeria Casalicchio', 'Giampaolo Manzolini', 'Wolfram Sparber']",physics.soc-ph,"In 2021, the European Commission has adopted the Fit-for-55 policy package, legally binding European countries to reduce their CO2 emissions by 55% with respect to 1990, a first step to achieve carbon neutrality in 2050. In this context, it is crucial to help national policymakers to choose the most appropriate technologies to achieve these goals and energy system modelling can be a valuable tool. This article presents a model of the Italian power system realized employing the open energy modelling framework Oemof. A Linear Programming Optimization is implemented to evaluate how to minimise system costs at decreasing CO2 emissions in 2030. The developed tool is applied to evaluate different research questions: i) pathway towards full decarbonization and power self-sufficiency of the electricity sector in Italy, ii) relevance of flexibility assets in power grids: li-ion batteries, hydrogen storage and transmission lines reinforcement. A 55% CO2 emissions reduction for the actual Italian power sector can be achieved through an increase of 30% of the total annual system cost. Full decarbonization can be reached with four times today's annual costs, which could be lowered with sector coupling and considering more technologies."
http://arxiv.org/abs/2312.16927v1,Development of Choice Model for Brand Evaluation,2023-12-28 09:51:46+00:00,"['Marina Kholod', 'Nikita Mokrenko']",econ.EM,"Consumer choice modeling takes center stage as we delve into understanding how personal preferences of decision makers (customers) for products influence demand at the level of the individual. The contemporary choice theory is built upon the characteristics of the decision maker, alternatives available for the choice of the decision maker, the attributes of the available alternatives and decision rules that the decision maker uses to make a choice. The choice set in our research is represented by six major brands (products) of laundry detergents in the Japanese market. We use the panel data of the purchases of 98 households to which we apply the hierarchical probit model, facilitated by a Markov Chain Monte Carlo simulation (MCMC) in order to evaluate the brand values of six brands. The applied model also allows us to evaluate the tangible and intangible brand values. These evaluated metrics help us to assess the brands based on their tangible and intangible characteristics. Moreover, consumer choice modeling also provides a framework for assessing the environmental performance of laundry detergent brands as the model uses the information on components (physical attributes) of laundry detergents."
http://arxiv.org/abs/2312.16698v1,The Green Advantage: Analyzing the Effects of Eco-Friendly Marketing on Consumer Loyalty,2023-12-27 19:30:58+00:00,"['Erfan Mohammadi', 'MohammadMahdi Barzegar', 'Mahdi Nohekhan']",econ.GN,"The idea that marketing, in addition to profitability and sales, should also consider the consumer's health is not and has not been a far-fetched concept. It can be stated that there is no longer a way back to producing environmentally harmful products, and gradually, governmental pressures, competition, and changing customer attitudes are obliging companies to adopt and implement a green marketing approach. Over time, concepts such as green marketing have penetrated marketing literature, making environmental considerations one of the most important activities of companies. For this purpose, this research examines the effects of green marketing strategy on brand loyalty (case study: food exporting companies). The population of this study consists of 345 employees and managers of companies like Kalleh, Solico, Pemina, Sorben, Mac, Pol, and Casel, out of which 182 were randomly selected as a sample using Cochran's formula. This research is practical; the required data were collected through a survey and questionnaire. The research results indicate that (1) green marketing strategy significantly affects brand loyalty. (2) Green products have a significant positive effect on brand loyalty. (3) Green promotion has a significant positive effect on brand loyalty. (4) Green distribution has a significant positive effect on brand loyalty. (5) Green pricing has a significant positive effect on brand loyalty."
http://arxiv.org/abs/2312.16099v1,Direct Multi-Step Forecast based Comparison of Nested Models via an Encompassing Test,2023-12-26 15:55:48+00:00,['Jean-Yves Pitarakis'],econ.EM,"We introduce a novel approach for comparing out-of-sample multi-step forecasts obtained from a pair of nested models that is based on the forecast encompassing principle. Our proposed approach relies on an alternative way of testing the population moment restriction implied by the forecast encompassing principle and that links the forecast errors from the two competing models in a particular way. Its key advantage is that it is able to bypass the variance degeneracy problem afflicting model based forecast comparisons across nested models. It results in a test statistic whose limiting distribution is standard normal and which is particularly simple to construct and can accommodate both single period and longer-horizon prediction comparisons. Inferences are also shown to be robust to different predictor types, including stationary, highly-persistent and purely deterministic processes. Finally, we illustrate the use of our proposed approach through an empirical application that explores the role of global inflation in enhancing individual country specific inflation forecasts."
http://arxiv.org/abs/2312.16489v1,Best-of-Both-Worlds Linear Contextual Bandits,2023-12-27 09:32:18+00:00,"['Masahiro Kato', 'Shinji Ito']",cs.LG,"This study investigates the problem of $K$-armed linear contextual bandits, an instance of the multi-armed bandit problem, under an adversarial corruption. At each round, a decision-maker observes an independent and identically distributed context and then selects an arm based on the context and past observations. After selecting an arm, the decision-maker incurs a loss corresponding to the selected arm. The decision-maker aims to minimize the cumulative loss over the trial. The goal of this study is to develop a strategy that is effective in both stochastic and adversarial environments, with theoretical guarantees. We first formulate the problem by introducing a novel setting of bandits with adversarial corruption, referred to as the contextual adversarial regime with a self-bounding constraint. We assume linear models for the relationship between the loss and the context. Then, we propose a strategy that extends the RealLinExp3 by Neu & Olkhovskaya (2020) and the Follow-The-Regularized-Leader (FTRL). The regret of our proposed algorithm is shown to be upper-bounded by $O\left(\min\left\{\frac{(\log(T))^3}{Δ_{*}} + \sqrt{\frac{C(\log(T))^3}{Δ_{*}}},\ \ \sqrt{T}(\log(T))^2\right\}\right)$, where $T \in\mathbb{N}$ is the number of rounds, $Δ_{*} > 0$ is the constant minimum gap between the best and suboptimal arms for any context, and $C\in[0, T] $ is an adversarial corruption parameter. This regret upper bound implies $O\left(\frac{(\log(T))^3}{Δ_{*}}\right)$ in a stochastic environment and by $O\left( \sqrt{T}(\log(T))^2\right)$ in an adversarial environment. We refer to our strategy as the Best-of-Both-Worlds (BoBW) RealFTRL, due to its theoretical guarantees in both stochastic and adversarial regimes."
http://arxiv.org/abs/2311.12242v2,Uniformly Strict Equilibrium for Repeated Games with Private Monitoring and Communication,2023-11-20 23:47:26+00:00,"['Richard McLean', 'Ichiro Obara', 'Andrew Postlewaite']",econ.TH,"Cooperation through repetition is an important theme in game theory. In this regard, various celebrated ``folk theorems'' have been proposed for repeated games in increasingly more complex environments. There has, however, been insufficient attention paid to the robustness of a large set of equilibria that is needed for such folk theorems. Starting with perfect public equilibrium as our starting point, we study uniformly strict equilibria in repeated games with private monitoring and direct communication (cheap talk). We characterize the limit equilibrium payoff set and identify the conditions for the folk theorem to hold with uniformly strict equilibrium."
http://arxiv.org/abs/2311.12267v2,Learning Causal Representations from General Environments: Identifiability and Intrinsic Ambiguity,2023-11-21 01:09:11+00:00,"['Jikai Jin', 'Vasilis Syrgkanis']",cs.LG,"We study causal representation learning, the task of recovering high-level latent variables and their causal relationships in the form of a causal graph from low-level observed data (such as text and images), assuming access to observations generated from multiple environments. Prior results on the identifiability of causal representations typically assume access to single-node interventions which is rather unrealistic in practice, since the latent variables are unknown in the first place. In this work, we provide the first identifiability results based on data that stem from general environments. We show that for linear causal models, while the causal graph can be fully recovered, the latent variables are only identified up to the surrounded-node ambiguity (SNA) \citep{varici2023score}. We provide a counterpart of our guarantee, showing that SNA is basically unavoidable in our setting. We also propose an algorithm, \texttt{LiNGCReL} which provably recovers the ground-truth model up to SNA, and we demonstrate its effectiveness via numerical experiments. Finally, we consider general non-parametric causal models and show that the same identification barrier holds when assuming access to groups of soft single-node interventions."
http://arxiv.org/abs/2311.14720v2,Perceptions and Detection of AI Use in Manuscript Preparation for Academic Journals,2023-11-19 06:04:46+00:00,"['Nir Chemaya', 'Daniel Martin']",cs.CY,"The emergent abilities of Large Language Models (LLMs), which power tools like ChatGPT and Bard, have produced both excitement and worry about how AI will impact academic writing. In response to rising concerns about AI use, authors of academic publications may decide to voluntarily disclose any AI tools they use to revise their manuscripts, and journals and conferences could begin mandating disclosure and/or turn to using detection services, as many teachers have done with student writing in class settings. Given these looming possibilities, we investigate whether academics view it as necessary to report AI use in manuscript preparation and how detectors react to the use of AI in academic writing."
http://arxiv.org/abs/2311.14009v1,Organizational support for work-family life balance as an antecedent to the well-being of tourism employees in Spain,2023-11-23 13:53:13+00:00,"['Jose Aurelio Medina-Garrido', 'Jose Maria Biedma-Ferrer', 'Maria Bogren']",econ.GN,"The study of work-family conflict (WFC) and work-family policies (WFP) and their impact on the well-being of employees in the tourism sector is increasingly attracting the attention of researchers. To overcome the adverse effects of WFC, managers should promote WFP, which contribute to increased well-being at work and employees' commitment. This paper aims to analyze the impact of WFP accessibility and organizational support on well-being directly and by mediating the organizational commitment that these policies might encourage. In addition, we also study whether these relationships vary according to gender and employee seniority. To test the hypotheses derived from this objective, we collected 530 valid and completed questionnaires from workers in the tourism sector in Spain, which we analyzed using structural equation modeling based on the PLS-SEM approach. The results show that human resource management must consider the importance of organizational support for workers to make WFP accessible and generate organizational commitment and well-being at work."
http://arxiv.org/abs/2312.05630v1,An empirical analysis of the determinants of network construction for Azul Airlines,2023-12-09 18:35:14+00:00,"['B. F. Oliveira', 'A. V. M Oliveira']",econ.GN,"This paper describes an econometric model of the Brazilian domestic carrier Azul Airlines' network construction. We employed a discrete-choice framework of airline route entry to examine the effects of the merger of another regional carrier, Trip Airlines, with Azul in 2012, especially on its entry decisions. We contrasted the estimated entry determinants before and after the merger with the benchmarks of the US carriers JetBlue Airways and Southwest Airlines obtained from the literature, and proposed a methodology for comparing different airline entry patterns by utilizing the kappa statistic for interrater agreement. Our empirical results indicate a statistically significant agreement between raters of Azul and JetBlue, but not Southwest, and only for entries on previously existing routes during the pre-merger period. The results suggest that post-merger, Azul has adopted a more idiosyncratic entry pattern, focusing on the regional flights segment to conquer many monopoly positions across the country, and strengthening its profitability without compromising its distinguished expansion pace in the industry."
http://arxiv.org/abs/2312.05633v1,The New Age of Collusion? An Empirical Study into Airbnb's Pricing Dynamics and Market Behavior,2023-12-09 18:41:55+00:00,['Richeng Piao'],econ.GN,"This study investigates the implications of algorithmic pricing in digital marketplaces, focusing on Airbnb's pricing dynamics. With the advent of Airbnb's new pricing tool, this research explores how digital tools influence hosts' pricing strategies, potentially leading to market dynamics that straddle the line between efficiency and collusion. Utilizing a Regression Discontinuity Design (RDD) and Propensity Score Matching (PSM), the study examines the causal effects of the pricing tool on pricing behavior among hosts with different operational strategies. The findings aim to provide insights into the evolving landscape of digital economies, examining the balance between competitive market practices and the risk of tacit collusion facilitated by algorithmic pricing. This study contributes to the discourse on digital market regulation, offering a nuanced understanding of the implications of AI-driven tools in market dynamics and antitrust analysis."
http://arxiv.org/abs/2312.05898v1,Dynamic Spatiotemporal ARCH Models: Small and Large Sample Results,2023-12-10 14:19:22+00:00,"['Philipp Otto', 'Osman Doğan', 'Süleyman Taşpınar']",stat.ME,"This paper explores the estimation of a dynamic spatiotemporal autoregressive conditional heteroscedasticity (ARCH) model. The log-volatility term in this model can depend on (i) the spatial lag of the log-squared outcome variable, (ii) the time-lag of the log-squared outcome variable, (iii) the spatiotemporal lag of the log-squared outcome variable, (iv) exogenous variables, and (v) the unobserved heterogeneity across regions and time, i.e., the regional and time fixed effects. We examine the small and large sample properties of two quasi-maximum likelihood estimators and a generalized method of moments estimator for this model. We first summarize the theoretical properties of these estimators and then compare their finite sample properties through Monte Carlo simulations."
http://arxiv.org/abs/2401.05395v1,SRNI-CAR: A comprehensive dataset for analyzing the Chinese automotive market,2023-12-19 09:32:32+00:00,"['Ruixin Ding', 'Bowei Chen', 'James M. Wilson', 'Zhi Yan', 'Yufei Huang']",econ.GN,"The automotive industry plays a critical role in the global economy, and particularly important is the expanding Chinese automobile market due to its immense scale and influence. However, existing automotive sector datasets are limited in their coverage, failing to adequately consider the growing demand for more and diverse variables. This paper aims to bridge this data gap by introducing a comprehensive dataset spanning the years from 2016 to 2022, encompassing sales data, online reviews, and a wealth of information related to the Chinese automotive industry. This dataset serves as a valuable resource, significantly expanding the available data. Its impact extends to various dimensions, including improving forecasting accuracy, expanding the scope of business applications, informing policy development and regulation, and advancing academic research within the automotive sector. To illustrate the dataset's potential applications in both business and academic contexts, we present two application examples. Our developed dataset enhances our understanding of the Chinese automotive market and offers a valuable tool for researchers, policymakers, and industry stakeholders worldwide."
http://arxiv.org/abs/2310.18835v1,Experience-weighted attraction learning in network coordination games,2023-10-28 22:35:05+00:00,['Fulin Guo'],econ.GN,"This paper studies the action dynamics of network coordination games with bounded-rational agents. I apply the experience-weighted attraction (EWA) model to the analysis as the EWA model has several free parameters that can capture different aspects of agents' behavioural features. I show that the set of possible long-term action patterns can be largely different when the behavioural parameters vary, ranging from a unique possibility in which all agents favour the risk-dominant option to some set of outcomes richer than the collection of Nash equilibria. Monotonicity and non-monotonicity in the relationship between the number of possible long-term action profiles and the behavioural parameters are explored. I also study the question of influential agents in terms of whose initial predispositions are important to the actions of the whole network. The importance of agents can be represented by a left eigenvector of a Jacobian matrix provided that agents' initial attractions are close to some neutral level. Numerical calculations examine the predictive power of the eigenvector for the long-run action profile and how agents' influences are impacted by their behavioural features and network positions."
http://arxiv.org/abs/2310.17496v5,Tackling Interference Induced by Data Training Loops in A/B Tests: A Weighted Training Approach,2023-10-26 15:52:34+00:00,['Nian Si'],stat.ME,"In modern recommendation systems, the standard pipeline involves training machine learning models on historical data to predict user behaviors and improve recommendations continuously. However, these data training loops can introduce interference in A/B tests, where data generated by control and treatment algorithms, potentially with different distributions, are combined. To address these challenges, we introduce a novel approach called weighted training. This approach entails training a model to predict the probability of each data point appearing in either the treatment or control data and subsequently applying weighted losses during model training. We demonstrate that this approach achieves the least variance among all estimators that do not cause shifts in the training distributions. Through simulation studies, we demonstrate the lower bias and variance of our approach compared to other methods."
http://arxiv.org/abs/2310.17423v1,The Newtonian Mechanics of Demand,2023-10-26 14:29:38+00:00,['Max Mendel'],eess.SY,"Economic engineering is a new field wherein economic systems are modelled in the same manner as traditional mechanical and electrical engineering systems. In this paper, we use Newton's theory of motion as the basis for the theory of demand; thereby establishing a theoretical foundation for economic engineering. We follow Newton's original development, as set forth in the Principia, to determine economic analogs to his three laws of motion. The pivotal result is an operational definition for an economic force, i.e. a want or a desire, in terms of a price adjustment. With this, we model the price effects of scarcity and trade friction in analogy with the models for the spring and damping force. In turn, we define economic benefits and surplus as analogous to the definitions of mechanical work and energy. These are then used to interpret the various types of economic equilibrium considered by economists from a mechanical perspective. The effectiveness of the analogy is illustrated by applying it to modelling the price and inventory dynamics of various economic agents -- including consumers, dealers, holders, spot and futures traders -- using linear-time invariant systems theory."
http://arxiv.org/abs/2311.06641v1,Best Complete Approximations of Preference Relations,2023-11-11 18:45:59+00:00,"['Hiroki Nishimura', 'Efe A. Ok']",econ.TH,"We investigate the problem of approximating an incomplete preference relation $\succsim$ on a finite set by a complete preference relation. We aim to obtain this approximation in such a way that the choices on the basis of two preferences, one incomplete, the other complete, have the smallest possible discrepancy in the aggregate. To this end, we use the top-difference metric on preferences, and define a best complete approximation of $\succsim$ as a complete preference relation nearest to $\succsim$ relative to this metric. We prove that such an approximation must be a maximal completion of $\succsim$, and that it is, in fact, any one completion of $\succsim$ with the largest index. Finally, we use these results to provide a sufficient condition for the best complete approximation of a preference to be its canonical completion. This leads to closed-form solutions to the best approximation problem in the case of several incomplete preference relations of interest."
http://arxiv.org/abs/2311.06718v5,Sustainable Development Goal (SDG) 8: New Zealand Prospects while Yield Curve Inverts in Central Bank Digital Currency (CBDC) Era,2023-11-12 03:15:33+00:00,['Qionghua Chu'],econ.GN,"In the inverted yield curve environment, I intend to assess the feasibility of fulfilling Sustainable Development Goal (SDG) 8, decent work and economic growth, of the United Nations by 2030 in New Zealand. Central Bank Digital Currency (CBDC) issuance supports SDG 8, based on the Cobb-Douglas production function, the growth accounting relation, and the Theory of Aggregate Demand. Bright prospects exist for New Zealand."
http://arxiv.org/abs/2311.01206v1,How Does China's Household Portfolio Selection Vary with Financial Inclusion?,2023-11-02 13:00:08+00:00,"['Yong Bian', 'Xiqian Wang', 'Qin Zhang']",econ.GN,"Portfolio underdiversification is one of the most costly losses accumulated over a household's life cycle. We provide new evidence on the impact of financial inclusion services on households' portfolio choice and investment efficiency using 2015, 2017, and 2019 survey data for Chinese households. We hypothesize that higher financial inclusion penetration encourages households to participate in the financial market, leading to better portfolio diversification and investment efficiency. The results of the baseline model are consistent with our proposed hypothesis that higher accessibility to financial inclusion encourages households to invest in risky assets and increases investment efficiency. We further estimate a dynamic double machine learning model to quantitatively investigate the non-linear causal effects and track the dynamic change of those effects over time. We observe that the marginal effect increases over time, and those effects are more pronounced among low-asset, less-educated households and those located in non-rural areas, except for investment efficiency for high-asset households."
http://arxiv.org/abs/2311.04162v1,Coarse correlated equilibria in linear quadratic mean field games and application to an emission abatement game,2023-11-07 17:41:39+00:00,"['Luciano Campi', 'Federico Cannerozzi', 'Fanny Cartellier']",math.OC,"Coarse correlated equilibria (CCE) are a good alternative to Nash equilibria (NE), as they arise more naturally as outcomes of learning algorithms and they may exhibit higher payoffs than NE. CCEs include a device which allows players' strategies to be correlated without any cooperation, only through information sent by a mediator. We develop a methodology to concretely compute mean field CCEs in a linear-quadratic mean field game framework. We compare their performance to mean field control solutions and mean field NE (usually named MFG solutions). Our approach is implemented in the mean field version of an emission abatement game between greenhouse gas emitters. In particular, we exhibit a simple and tractable class of mean field CCEs which allows to outperform very significantly the mean field NE payoff and abatement levels, bridging the gap between the mean field NE and the social optimum obtained by mean field control."
http://arxiv.org/abs/2311.03471v3,Optimal Estimation Methodologies for Panel Data Regression Models,2023-11-06 19:15:11+00:00,['Christis Katsouris'],econ.EM,"This survey study discusses main aspects to optimal estimation methodologies for panel data regression models. In particular, we present current methodological developments for modeling stationary panel data as well as robust methods for estimation and inference in nonstationary panel data regression models. Some applications from the network econometrics and high dimensional statistics literature are also discussed within a stationary time series environment."
http://arxiv.org/abs/2310.19498v1,Green ammonia supply chain and associated market structure: an analysis based on transaction cost economics,2023-10-30 12:39:53+00:00,['Hanxin Zhao'],econ.GN,"Green ammonia is poised to be a key part in the hydrogen economy. This paper discusses green ammonia supply chains from a higher-level industry perspective with a focus on market structures. The architecture of upstream and downstream supply chains are explored. Potential ways to accelerate market emergence are discussed. Market structure is explored based on transaction cost economics and lessons from the oil and gas industry. Three market structure prototypes are developed for different phases. In the infancy, a highly vertically integrated structure is proposed to reduce risks and ensure capital recovery. A restructuring towards a disintegrated structure is necessary in the next stage to improve the efficiency. In the late stage, a competitive structure characterized by a separation between asset ownership and production activities and further development of short-term and spot markets is proposed towards a market-driven industry. Finally, a multi-linear regression model is developed to evaluate the developed structures using a case in the gas industry. Results indicate that high asset specificity and uncertainty and low frequency lead to a more disintegrated market structure, and vice versa, thus supporting the structures designed. We assume the findings and results contribute to developing green ammonia supply chains and the hydrogen economy."
http://arxiv.org/abs/2310.19557v1,A Bayesian Markov-switching SAR model for time-varying cross-price spillovers,2023-10-30 14:12:05+00:00,"['Christian Glocker', 'Matteo Iacopini', 'Tamás Krisztin', 'Philipp Piribauer']",stat.AP,"The spatial autoregressive (SAR) model is extended by introducing a Markov switching dynamics for the weight matrix and spatial autoregressive parameter. The framework enables the identification of regime-specific connectivity patterns and strengths and the study of the spatiotemporal propagation of shocks in a system with a time-varying spatial multiplier matrix. The proposed model is applied to disaggregated CPI data from 15 EU countries to examine cross-price dependencies. The analysis identifies distinct connectivity structures and spatial weights across the states, which capture shifts in consumer behaviour, with marked cross-country differences in the spillover from one price category to another."
http://arxiv.org/abs/2310.20415v1,Coalitional Manipulations and Immunity of the Shapley Value,2023-10-31 12:43:31+00:00,"['Christian Basteck', 'Frank Huettner']",econ.TH,"We consider manipulations in the context of coalitional games, where a coalition aims to increase the total payoff of its members. An allocation rule is immune to coalitional manipulation if no coalition can benefit from internal reallocation of worth on the level of its subcoalitions (reallocation-proofness), and if no coalition benefits from a lower worth while all else remains the same (weak coalitional monotonicity). Replacing additivity in Shapley's original characterization by these requirements yields a new foundation of the Shapley value, i.e., it is the unique efficient and symmetric allocation rule that awards nothing to a null player and is immune to coalitional manipulations. We further find that for efficient allocation rules, reallocation-proofness is equivalent to constrained marginality, a weaker variant of Young's marginality axiom. Our second characterization improves upon Young's characterization by weakening the independence requirement intrinsic to marginality."
http://arxiv.org/abs/2310.19788v3,Worst-Case Optimal Multi-Armed Gaussian Best Arm Identification with a Fixed Budget,2023-10-30 17:52:46+00:00,['Masahiro Kato'],math.ST,"This study investigates the experimental design problem for identifying the arm with the highest expected outcome, referred to as best arm identification (BAI). In our experiments, the number of treatment-allocation rounds is fixed. During each round, a decision-maker allocates an arm and observes a corresponding outcome, which follows a Gaussian distribution with variances that can differ among the arms. At the end of the experiment, the decision-maker recommends one of the arms as an estimate of the best arm. To design an experiment, we first discuss lower bounds for the probability of misidentification. Our analysis highlights that the available information on the outcome distribution, such as means (expected outcomes), variances, and the choice of the best arm, significantly influences the lower bounds. Because available information is limited in actual experiments, we develop a lower bound that is valid under the unknown means and the unknown choice of the best arm, which are referred to as the worst-case lower bound. We demonstrate that the worst-case lower bound depends solely on the variances of the outcomes. Then, under the assumption that the variances are known, we propose the Generalized-Neyman-Allocation (GNA)-empirical-best-arm (EBA) strategy, an extension of the Neyman allocation proposed by Neyman (1934). We show that the GNA-EBA strategy is asymptotically optimal in the sense that its probability of misidentification aligns with the lower bounds as the sample size increases infinitely and the differences between the expected outcomes of the best and other suboptimal arms converge to the same values across arms. We refer to such strategies as asymptotically worst-case optimal."
http://arxiv.org/abs/2311.06330v4,Smart Agent-Based Modeling: On the Use of Large Language Models in Computer Simulations,2023-11-10 18:54:33+00:00,"['Zengqing Wu', 'Run Peng', 'Xu Han', 'Shuyuan Zheng', 'Yixin Zhang', 'Chuan Xiao']",cs.AI,"Computer simulations offer a robust toolset for exploring complex systems across various disciplines. A particularly impactful approach within this realm is Agent-Based Modeling (ABM), which harnesses the interactions of individual agents to emulate intricate system dynamics. ABM's strength lies in its bottom-up methodology, illuminating emergent phenomena by modeling the behaviors of individual components of a system. Yet, ABM has its own set of challenges, notably its struggle with modeling natural language instructions and common sense in mathematical equations or rules. This paper seeks to transcend these boundaries by integrating Large Language Models (LLMs) like GPT into ABM. This amalgamation gives birth to a novel framework, Smart Agent-Based Modeling (SABM). Building upon the concept of smart agents -- entities characterized by their intelligence, adaptability, and computation ability -- we explore in the direction of utilizing LLM-powered agents to simulate real-world scenarios with increased nuance and realism. In this comprehensive exploration, we elucidate the state of the art of ABM, introduce SABM's potential and methodology, and present three case studies (source codes available at https://github.com/Roihn/SABM), demonstrating the SABM methodology and validating its effectiveness in modeling real-world systems. Furthermore, we cast a vision towards several aspects of the future of SABM, anticipating a broader horizon for its applications. Through this endeavor, we aspire to redefine the boundaries of computer simulations, enabling a more profound understanding of complex systems."
http://arxiv.org/abs/2310.00531v1,"Separately Convex and Separately Continuous Preferences: On Results of Schmeidler, Shafer, and Bergstrom-Parks-Rader",2023-10-01 00:43:45+00:00,"['Metin Uyanik', 'Aniruddha Ghosh', 'M. Ali Khan']",econ.TH,"We provide necessary and sufficient conditions for a correspondence taking values in a finite-dimensional Euclidean space to be open so as to revisit the pioneering work of Schmeidler (1969), Shafer (1974), Shafer-Sonnenschein (1975) and Bergstrom-Rader-Parks (1976) to answer several questions they and their followers left open. We introduce the notion of separate convexity for a correspondence and use it to relate to classical notions of continuity while giving salience to the notion of separateness as in the interplay of separate continuity and separate convexity of binary relations. As such, we provide a consolidation of the convexity-continuity postulates from a broad inter-disciplinary perspective and comment on how the qualified notions proposed here have implications of substantive interest for choice theory."
http://arxiv.org/abs/2310.00561v1,CausalGPS: An R Package for Causal Inference With Continuous Exposures,2023-10-01 03:31:01+00:00,"['Naeem Khoshnevis', 'Xiao Wu', 'Danielle Braun']",stat.CO,"Quantifying the causal effects of continuous exposures on outcomes of interest is critical for social, economic, health, and medical research. However, most existing software packages focus on binary exposures. We develop the CausalGPS R package that implements a collection of algorithms to provide algorithmic solutions for causal inference with continuous exposures. CausalGPS implements a causal inference workflow, with algorithms based on generalized propensity scores (GPS) as the core, extending propensity scores (the probability of a unit being exposed given pre-exposure covariates) from binary to continuous exposures. As the first step, the package implements efficient and flexible estimations of the GPS, allowing multiple user-specified modeling options. As the second step, the package provides two ways to adjust for confounding: weighting and matching, generating weighted and matched data sets, respectively. Lastly, the package provides built-in functions to fit flexible parametric, semi-parametric, or non-parametric regression models on the weighted or matched data to estimate the exposure-response function relating the outcome with the exposures. The computationally intensive tasks are implemented in C++, and efficient shared-memory parallelization is achieved by OpenMP API. This paper outlines the main components of the CausalGPS R package and demonstrates its application to assess the effect of long-term exposure to PM2.5 on educational attainment using zip code-level data from the contiguous United States from 2000-2016."
http://arxiv.org/abs/2310.13436v1,Non-linear approximations of DSGE models with neural-networks and hard-constraints,2023-10-20 11:49:56+00:00,['Emmet Hall-Hoffarth'],econ.TH,"Recently a number of papers have suggested using neural-networks in order to approximate policy functions in DSGE models, while avoiding the curse of dimensionality, which for example arises when solving many HANK models, and while preserving non-linearity. One important step of this method is to represent the constraints of the economic model in question in the outputs of the neural-network. I propose, and demonstrate the advantages of, a novel approach to handling these constraints which involves directly constraining the neural-network outputs, such that the economic constraints are satisfied by construction. This is achieved by a combination of re-scaling operations that are differentiable and therefore compatible with the standard gradient descent approach used when fitting neural-networks. This has a number of attractive properties, and is shown to out-perform the penalty-based approach suggested by the existing literature, which while theoretically sound, can be poorly behaved practice for a number of reasons that I identify."
http://arxiv.org/abs/2310.12825v1,Nonparametric Regression with Dyadic Data,2023-10-19 15:22:12+00:00,['Brice Romuald Gueyap Kounga'],econ.EM,This paper studies the identification and estimation of a nonparametric nonseparable dyadic model where the structural function and the distribution of the unobservable random terms are assumed to be unknown. The identification and the estimation of the distribution of the unobservable random term are also proposed. I assume that the structural function is continuous and strictly increasing in the unobservable heterogeneity. I propose suitable normalization for the identification by allowing the structural function to have some desirable properties such as homogeneity of degree one in the unobservable random term and some of its observables. The consistency and the asymptotic distribution of the estimators are proposed. The finite sample properties of the proposed estimators in a Monte-Carlo simulation are assessed.
http://arxiv.org/abs/2310.12863v3,A remark on moment-dependent phase transitions in high-dimensional Gaussian approximations,2023-10-19 16:12:13+00:00,"['Anders Bredahl Kock', 'David Preinerstorfer']",math.ST,"In this article, we study the critical growth rates of dimension below which Gaussian critical values can be used for hypothesis testing but beyond which they cannot. We are particularly interested in how these growth rates depend on the number of moments that the observations possess."
http://arxiv.org/abs/2310.15861v1,Social Learning of General Rules,2023-10-24 14:21:52+00:00,"['Enrique Urbano Arellano', 'Xinyang Wang']",econ.TH,"Why do agents adopt a particular general behavioral rule among a collection of possible alternatives? To address this question, we introduce a dynamic social learning framework, where agents rely on general rules of thumb and imitate the behavioral rules of successful peers. We find the social learning outcome can be characterized independent of the initial rule distribution. When one dominant general rule consistently yields superior problem-specific outcomes, social learning almost surely leads all agents to adopt this dominant rule; otherwise, provided the population is sufficiently large, the better rule for the more frequent problem becomes the consensus rule with arbitrarily high probability. As a result, the behavioral rule selected by the social learning process need not maximize social welfare. We complement our theoretical analysis with an application to the market sentiment selection in a stochastic production market."
http://arxiv.org/abs/2310.15933v1,Modeling and Contribution of Flexible Heating Systems for Transmission Grid Congestion Management,2023-10-24 15:29:28+00:00,"['David Kröger', 'Milijana Teodosic', 'Christian Rehtanz']",eess.SY,The large-scale integration of flexible heating systems in the European electricity market leads to a substantial increase of transportation requirements and consecutively grid congestions in the continental transmission grid. Novel model formulations for the grid-aware operation of both individual small-scale heat pumps and large-scale power-to-heat (PtH) units located in district heating networks are presented. The functionality of the models and the contribution of flexible heating systems for transmission grid congestion management is evaluated by running simulations for the target year 2035 for the German transmission grid. The findings show a decrease in annual conventional redispatch volumes and renewable energy sources (RES) curtailment resulting in cost savings of approximately 6 % through the integration of flexible heating systems in the grid congestion management scheme. The analysis suggests that especially large-scale PtH units in combination with thermal energy storages can contribute significantly to the alleviation of grid congestion and foster RES integration.
http://arxiv.org/abs/2310.16341v1,Elevating Women in the Workplace: The Dual Influence of Spiritual Intelligence and Ethical Environments on Job Satisfaction,2023-10-25 03:54:02+00:00,"['Ali Bai', 'Morteza Vahedian', 'Rashin Ghahreman', 'Hasan Piri']",econ.GN,"In today's rapidly evolving workplace, the dynamics of job satisfaction and its determinants have become a focal point of organizational studies. This research offers a comprehensive examination of the nexus between spiritual intelligence and job satisfaction among female employees, with particular emphasis on the moderating role of ethical work environments. Beginning with an exploration of the multifaceted nature of human needs, the study delves deep into the psychological underpinnings that drive job satisfaction. It elucidates how various tangible and intangible motivators, such as salary benefits and recognition, play pivotal roles in shaping employee attitudes and behaviors. Moreover, the research spotlights the unique challenges and experiences of female employees, advocating for a more inclusive understanding of their needs. An extensive review of the literature and empirical analysis culminates in the pivotal finding that integrating spiritual intelligence and ethical considerations within organizational practices can significantly enhance job satisfaction. Such a holistic approach, the paper posits, not only bolsters the well-being and contentment of female employees but also augments overall organizational productivity, retention rates, and morale."
http://arxiv.org/abs/2310.14438v1,BVARs and Stochastic Volatility,2023-10-22 22:48:17+00:00,['Joshua Chan'],econ.EM,"Bayesian vector autoregressions (BVARs) are the workhorse in macroeconomic forecasting. Research in the last decade has established the importance of allowing time-varying volatility to capture both secular and cyclical variations in macroeconomic uncertainty. This recognition, together with the growing availability of large datasets, has propelled a surge in recent research in building stochastic volatility models suitable for large BVARs. Some of these new models are also equipped with additional features that are especially desirable for large systems, such as order invariance -- i.e., estimates are not dependent on how the variables are ordered in the BVAR -- and robustness against COVID-19 outliers. Estimation of these large, flexible models is made possible by the recently developed equation-by-equation approach that drastically reduces the computational cost of estimating large systems. Despite these recent advances, there remains much ongoing work, such as the development of parsimonious approaches for time-varying coefficients and other types of nonlinearities in large BVARs."
http://arxiv.org/abs/2311.01787v1,Labour Absorption In Manufacturing Industry In Indonesia: Anomalous And Regressive Phenomena,2023-11-03 08:58:00+00:00,"['Tongam Sihol Nababan', 'Elvis Fresly Purba']",econ.GN,"The manufacturing industry sector was expected to generate new employment opportunities and take on labour. Gradually, however, it emerged as a menace to the sustenance of its workers. According to the findings of this study, 24 manufacturing subsectors with ISIC 2 digits in Indonesia exhibited regressive and abnormal patterns in the period 2012-2020. This suggests that, to a great extent, labour absorption has been limited and, in some cases, even shown a decline. Anomalous occurrences were observed in three subsectors: ISIC 12 (tobacco products), ISIC 26 (computer, electronic and optical products), and ISIC 31 (furniture). In contrast, regressive phenomena were present in the remaining 21 ISIC subsectors. Furthermore, the manufacturing industry displayed a negative correlation between employment and efficiency index, demonstrating this anomalous and regressive phenomenon. This implies that as the efficiency index of the manufacturing industry increases, the index of labour absorption decreases"
http://arxiv.org/abs/2310.07558v2,Smoothness-Adaptive Dynamic Pricing with Nonparametric Demand Learning,2023-10-11 15:02:13+00:00,"['Zeqi Ye', 'Hansheng Jiang']",stat.ML,"We study the dynamic pricing problem where the demand function is nonparametric and Hölder smooth, and we focus on adaptivity to the unknown Hölder smoothness parameter $β$ of the demand function. Traditionally the optimal dynamic pricing algorithm heavily relies on the knowledge of $β$ to achieve a minimax optimal regret of $\widetilde{O}(T^{\frac{β+1}{2β+1}})$. However, we highlight the challenge of adaptivity in this dynamic pricing problem by proving that no pricing policy can adaptively achieve this minimax optimal regret without knowledge of $β$. Motivated by the impossibility result, we propose a self-similarity condition to enable adaptivity. Importantly, we show that the self-similarity condition does not compromise the problem's inherent complexity since it preserves the regret lower bound $Ω(T^{\frac{β+1}{2β+1}})$. Furthermore, we develop a smoothness-adaptive dynamic pricing algorithm and theoretically prove that the algorithm achieves this minimax optimal regret bound without the prior knowledge $β$."
http://arxiv.org/abs/2310.08285v1,How would mobility-as-a-service (MaaS) platform survive as an intermediary? From the viewpoint of stability in many-to-many matching,2023-10-12 12:41:21+00:00,"['Rui Yao', 'Kenan Zhang']",econ.GN,"Mobility-as-a-service (MaaS) provides seamless door-to-door trips by integrating different transport modes. Although many MaaS platforms have emerged in recent years, most of them remain at a limited integration level. This study investigates the assignment and pricing problem for a MaaS platform as an intermediary in a multi-modal transportation network, which purchases capacity from service operators and sells multi-modal trips to travelers. The analysis framework of many-to-many stable matching is adopted to decompose the joint design problem and to derive the stability condition such that both operators and travelers are willing to participate in the MaaS system. To maximize the flexibility in route choice and remove boundaries between modes, we design an origin-destination pricing scheme for MaaS trips. On the supply side, we propose a wholesale purchase price for service capacity. Accordingly, the assignment problem is reformulated and solved as a bi-level program, where MaaS travelers make multi-modal trips to minimize their travel costs meanwhile interacting with non-MaaS travelers in the multi-modal transport system. We prove that, under the proposed pricing scheme, there always exists a stable outcome to the overall many-to-many matching problem. Further, given an optimal assignment and under some mild conditions, a unique optimal pricing scheme is ensured. Numerical experiments conducted on the extended Sioux Falls network also demonstrate that the proposed MaaS system could create a win-win-win situation -- the MaaS platform is profitable and both traveler welfare and transit operator revenues increase from a baseline scenario without MaaS."
http://arxiv.org/abs/2310.07789v1,Empirical Review of Youth-Employment Policies in Nigeria,2023-10-11 18:22:34+00:00,"['Oluwasola E. Omoju', 'Emily E. Ikhide', 'Iyabo A. Olanrele', 'Lucy E. Abeng', 'Marjan Petreski', 'Francis O. Adebayo', 'Itohan Odigie', 'Amina M. Muhammed']",econ.GN,"Youth unemployment is a major socioeconomic problem in Nigeria, and several youth-employment programs have been initiated and implemented to address the challenge. While detailed analyses of the impacts of some of these programs have been conducted, empirical analysis of implementation challenges and of the influence of limited political inclusivity on distribution of program benefits is rare. Using mixed research methods and primary data collected through focus-group discussion and key-informant interviews, this paper turns to that analysis. We found that, although there are several youth-employment programs in Nigeria, they have not yielded a marked reduction in youth-unemployment rates. The programs are challenged by factors such as lack of framework for proper governance and coordination, inadequate funding, lack of institutional implementation capacity, inadequate oversight of implementation, limited political inclusivity, lack of prioritization of vulnerable and marginalized groups, and focus on stand-alone programs that are not tied to long-term development plans. These issues need to be addressed to ensure that youth-employment programs yield better outcomes and that youth unemployment is significantly reduced."
http://arxiv.org/abs/2310.07839v1,"Marital Sorting, Household Inequality and Selection",2023-10-11 19:32:45+00:00,"['Iván Fernández-Val', 'Aico van Vuuren', 'Francis Vella']",econ.EM,"Using CPS data for 1976 to 2022 we explore how wage inequality has evolved for married couples with both spouses working full time full year, and its impact on household income inequality. We also investigate how marriage sorting patterns have changed over this period. To determine the factors driving income inequality we estimate a model explaining the joint distribution of wages which accounts for the spouses' employment decisions. We find that income inequality has increased for these households and increased assortative matching of wages has exacerbated the inequality resulting from individual wage growth. We find that positive sorting partially reflects the correlation across unobservables influencing both members' of the marriage wages. We decompose the changes in sorting patterns over the 47 years comprising our sample into structural, composition and selection effects and find that the increase in positive sorting primarily reflects the increased skill premia for both observed and unobserved characteristics."
http://arxiv.org/abs/2310.09745v1,"Economics unchained: Investigating the role of cryptocurrency, blockchain and intricacies of Bitcoin price fluctuations",2023-10-15 05:57:40+00:00,['Ishmeet Matharoo'],econ.GN,"This research paper presents a thorough economic analysis of Bitcoin and its impact. We delve into fundamental principles, and technological evolution into a prominent decentralized digital currency. Analysing Bitcoin's economic dynamics, we explore aspects such as transaction volume, market capitalization, mining activities, and macro trends. Moreover, we investigate Bitcoin's role in economy ecosystem, considering its implications on traditional financial systems, monetary policies, and financial inclusivity. We utilize statistical and analytical tools to assess equilibrium , market behaviour, and economic . Insights from this analysis provide a comprehensive understanding of Bitcoin's economic significance and its transformative potential in shaping the future of global finance. This research contributes to informed decision-making for individuals, institutions, and policymakers navigating the evolving landscape of decentralized finance."
http://arxiv.org/abs/2310.09919v1,On the population size in stochastic differential games,2023-10-15 19:06:56+00:00,"['Dylan Possamaï', 'Ludovic Tangpi']",math.PR,"Commuters looking for the shortest path to their destinations, the security of networked computers, hedge funds trading on the same stocks, governments and populations acting to mitigate an epidemic, or employers and employees agreeing on a contact, are all examples of (dynamic) stochastic differential games. In essence, game theory deals with the analysis of strategic interactions among multiple decision-makers. The theory has had enormous impact in a wide variety of fields, but its rigorous mathematical analysis is rather recent. It started with the pioneering work of von Neumann and Morgenstern published in 1944. Since then, game theory has taken centre stage in applied mathematics and related areas. Game theory has also played an important role in unsuspected areas: for instance in military applications, when the analysis of guided interceptor missiles in the 1950s motivated the study of games evolving dynamically in time. Such games (when possibly subject to randomness) are called stochastic differential games. Their study started with the work of Issacs, who crucially recognised the importance of (stochastic) control theory in the area. Over the past few decades since Isaacs's work, a rich theory of stochastic differential game has emerged and branched into several directions. This paper will review recent advances in the study of solvability of stochastic differential games, with a focus on a purely probabilistic technique to approach the problem. Unsurprisingly, the number of players involved in the game is a major factor of the analysis. We will explain how the size of the population impacts the analyses and solvability of the problem, and discuss mean field games as well as the convergence of finite player games to mean field games."
http://arxiv.org/abs/2310.10024v1,Managing Persuasion Robustly: The Optimality of Quota Rules,2023-10-16 02:52:39+00:00,"['Dirk Bergemann', 'Tan Gan', 'Yingkai Li']",econ.TH,"We study a sender-receiver model where the receiver can commit to a decision rule before the sender determines the information policy. The decision rule can depend on the signal structure and the signal realization that the sender adopts. This framework captures applications where a decision-maker (the receiver) solicit advice from an interested party (sender). In these applications, the receiver faces uncertainty regarding the sender's preferences and the set of feasible signal structures. Consequently, we adopt a unified robust analysis framework that includes max-min utility, min-max regret, and min-max approximation ratio as special cases. We show that it is optimal for the receiver to sacrifice ex-post optimality to perfectly align the sender's incentive. The optimal decision rule is a quota rule, i.e., the decision rule maximizes the receiver's ex-ante payoff subject to the constraint that the marginal distribution over actions adheres to a consistent quota, regardless of the sender's chosen signal structure."
http://arxiv.org/abs/2310.05761v1,Robust Minimum Distance Inference in Structural Models,2023-10-09 14:41:53+00:00,"['Joan Alegre', 'Juan Carlos Escanciano']",econ.EM,"This paper proposes minimum distance inference for a structural parameter of interest, which is robust to the lack of identification of other structural nuisance parameters. Some choices of the weighting matrix lead to asymptotic chi-squared distributions with degrees of freedom that can be consistently estimated from the data, even under partial identification. In any case, knowledge of the level of under-identification is not required. We study the power of our robust test. Several examples show the wide applicability of the procedure and a Monte Carlo investigates its finite sample performance. Our identification-robust inference method can be applied to make inferences on both calibrated (fixed) parameters and any other structural parameter of interest. We illustrate the method's usefulness by applying it to a structural model on the non-neutrality of monetary policy, as in \cite{nakamura2018high}, where we empirically evaluate the validity of the calibrated parameters and we carry out robust inference on the slope of the Phillips curve and the information effect."
http://arxiv.org/abs/2310.06999v1,Lung Cancer in Argentina: A Modelling Study of Disease and Economic Burden,2023-10-10 20:31:28+00:00,"['Andrea Alcaraz', 'Federico Rodriguez Cairoli', 'Carla Colaci', 'Constanza Silvestrini', 'Carolina Gabay', 'Natalia Espinola']",econ.GN,"Objectives: Lung cancer remains a significant global public health challenge and is still one of the leading cause of cancer-related death in Argentina. This study aims to assess the disease and economic burden of lung cancer in the country.
  Study design: Burden of disease study
  Methods. A mathematical model was developed to estimate the disease burden and direct medical cost attributable to lung cancer. Epidemiological parameters were obtained from local statistics, the Global Cancer Observatory, the Global Burden of Disease databases, and a literature review. Direct medical costs were estimated through micro-costing. Costs were expressed in US dollars (US$), April 2023 (1 US$ =216.38 argentine pesos). A second-order Monte Carlo simulation was performed to estimate the uncertainty.
  Results: Considering approximately 10,000 deaths, 12,000 incident cases, and 14,000 5-year prevalent cases, the economic burden of lung cancer in Argentina in 2023 was estimated to be US$ 556.20 million (396.96 -718.20), approximately 1.4% of the total healthcare expenditure for the country. The cost increased with a higher stage of the disease and the main driver was the drug acquisition (80%). 179,046 Disability-adjusted life years could be attributable to lung cancer representing the 10% of the total cancer.
  Conclusion: The disease and economic burden of lung cancer in Argentina implies a high cost for the health system and would represent 19% of the previously estimated economic burden for 29 cancers in Argentina."
http://arxiv.org/abs/2310.04906v1,Are Generation Z Less Car-centric Than Millennials? A Nationwide Analysis Through the Lens of Youth Licensing,2023-10-07 19:56:22+00:00,['Kailai Wang'],econ.GN,"The debate on whether young Americans are becoming less reliant on automobiles is still ongoing. This research compares driver's license acquisition patterns between Millennials and their succeeding Generation Z during late adolescence. It also examines factors influencing teenagers' decisions to obtain driver's licenses. The findings suggest that the decline in licensing rates may be attributed in part to generational shifts in attitudes and cultural changes, such as Generation Z's inclination toward educational trips and their digital upbringing. This research underscores the implications for planners, practitioners, and policymakers in adapting to potential shifts in American car culture."
http://arxiv.org/abs/2310.04907v1,An Information Theory Approach to the Stock and Cryptocurrency Market: A Statistical Equilibrium Perspective,2023-10-07 20:02:21+00:00,"['Emanuele Citera', 'Francesco De Pretis']",econ.TH,"We study the stochastic structure of cryptocurrency rates of returns as compared to stock returns by focusing on the associated cross-sectional distributions. We build two datasets. The first comprises forty-six major cryptocurrencies, and the second includes all the companies listed in the S&P 500. We collect individual data from January 2017 until December 2022. We then apply the Quantal Response Statistical Equilibrium (QRSE) model to recover the cross-sectional frequency distribution of the daily returns of cryptocurrencies and S&P 500 companies. We study the stochastic structure of these two markets and the properties of investors' behavior over bear and bull trends. Finally, we compare the degree of informational efficiency of these two markets."
http://arxiv.org/abs/2310.05110v1,The impact of the pandemic of Covid-19 on child poverty in North Macedonia: Simulation-based estimates,2023-10-08 10:57:28+00:00,['Marjan Petreski'],econ.GN,"The objective of this paper is to estimate the expected effects of the pandemic of Covid-19 for child poverty in North Macedonia. We rely on MK-MOD Tax & Benefit Microsimulation Model for North Macedonia based on the Survey on Income and Living Conditions 2019. The simulation takes into account the development of income, as per the observed developments in the first three quarters of 2020, derived from the Labor Force Survey, which incorporates the raw effect of the pandemic and the government response. In North Macedonia, almost no government measure directly aimed the income of children, however, three key and largest measures addressed household income: the wage subsidy of 14.500 MKD per worker in the hardest hit companies, relaxation of the criteria for obtaining the guaranteed minimum income, and one-off support to vulnerable groups of the population in two occasions. Results suggest that the relative child poverty rate is estimated to increase from 27.8 percent before the pandemic to 32.4 percent during the pandemic. This increase puts additional 19,000 children below the relative poverty threshold. Results further suggest that absolute poverty is likely to reduce primarily because of the automatic stabilizers in the case of social assistance and because of the one-time cash assistance."
http://arxiv.org/abs/2310.05114v1,Poverty during Covid-19 in North Macedonia: Analysis of the distributional impact of the crisis and government response,2023-10-08 11:04:48+00:00,['Marjan Petreski'],econ.GN,"In this paper we simulate the poverty effect of the Covid-19 pandemic in North Macedonia and we analyze the income-saving power of three key government measures: the employment-retention scheme, the relaxed Guaranteed Minimum Income support, and one-off cash allowances. In this attempt, the counterfactual scenarios are simulated by using MK-MOD, the Macedonian Tax and Benefit Microsimulation Model, incorporating actual data on the shock-s magnitude from the second quarter of 2020. The results suggest that without the government interventions, of the country-s two million citizens, an additional 120,000 people would have been pushed into poverty by COVID-19, where 340,000 were already poor before the pandemic. Of the 120,000 newly poor about 16,000 would have been pushed into destitute poverty. The government-s automatic stabilizers worked to shield the poorest people, though these were clearly pro-feminine. In all, the analyzed government measures recovered more than half of the income loss, which curbed the poverty-increasing effect and pulled an additional 34,000 people out of extreme poverty. The employment-retention measure was regressive and pro-masculine; the Guaranteed Minimum Income relaxation (including automatic stabilizers) was progressive and pro-feminine; and the one-off support has been pro-youth."
http://arxiv.org/abs/2310.05117v1,Minimum wage and manufacturing labor share: Evidence from North Macedonia,2023-10-08 11:12:58+00:00,"['Marjan Petreski', 'Jaakko Pehkonen']",econ.GN,"The objective of the paper is to understand if the minimum wage plays a role for the labor share of manufacturing workers in North Macedonia. We decompose labor share movements on those along a share-capital curve, shifts of this locus, and deviations from it. We use the capital-output ratio, total factor productivity and prices of inputs to capture these factors, while the minimum wage is introduced as an element that moves the curve off. We estimate a panel of 20 manufacturing branches over the 2012-2019 period with FE, IV and system-GMM estimators. We find that the role of the minimum wage for the labor share is industry-specific. For industrial branches which are labor-intensive and low-pay, it increases workers' labor share, along a complementarity between capital and labor. For capital-intensive branches, it reduces labor share, likely through the job loss channel and along a substitutability between labor and capital. This applies to both branches where foreign investment and heavy industry are nested."
http://arxiv.org/abs/2312.00282v1,Stochastic volatility models with skewness selection,2023-12-01 01:35:41+00:00,"['Igor Ferreira Batista Martins', 'Hedibert Freitas Lopes']",econ.EM,"This paper expands traditional stochastic volatility models by allowing for time-varying skewness without imposing it. While dynamic asymmetry may capture the likely direction of future asset returns, it comes at the risk of leading to overparameterization. Our proposed approach mitigates this concern by leveraging sparsity-inducing priors to automatically selects the skewness parameter as being dynamic, static or zero in a data-driven framework. We consider two empirical applications. First, in a bond yield application, dynamic skewness captures interest rate cycles of monetary easing and tightening being partially explained by central banks' mandates. In an currency modeling framework, our model indicates no skewness in the carry factor after accounting for stochastic volatility which supports the idea of carry crashes being the result of volatility surges instead of dynamic skewness."
http://arxiv.org/abs/2312.00517v1,Causal propensity as an antecedent of entrepreneurial intentions in tourism students,2023-12-01 11:46:22+00:00,"['Alicia Martin-Navarro', 'Felix Velicia-Martin', 'Jose Aurelio Medina-Garrido', 'Ricardo Gouveia Rodrigues']",econ.GN,"The tourism sector is a sector with many opportunities for business development. Entrepreneurship in this sector promotes economic growth and job creation. Knowing how entrepreneurial intention develops facilitates its transformation into entrepreneurial behaviour. Entrepreneurial behaviour can adopt a causal logic, an effectual logic or a combination of both. Considering the causal logic, decision-making is done through prediction. In this way, entrepreneurs try to increase their market share by planning strategies and analysing possible deviations from their plans. Previous literature studies causal entrepreneurial behaviour, as well as variables such as creative innovation, proactive decisions and entrepreneurship training when the entrepreneur has already created his or her firm. However, there is an obvious gap at a stage prior to the start of entrepreneurial activity when the entrepreneurial intention is formed. This paper analyses how creativity, proactivity, entrepreneurship education and the propensity for causal behaviour influence entrepreneurial intentions. To achieve the research objective, we analysed a sample of 464 undergraduate tourism students from two universities in southern Spain. We used SmartPLS 3 software to apply a structural equation methodology to the measurement model composed of nine hypotheses. The results show, among other relationships, that causal propensity, entrepreneurship learning programmes and proactivity are antecedents of entrepreneurial intentions. These findings have implications for theory, as they fill a gap in the field of entrepreneurial intentions. Considering propensity towards causal behaviour before setting up the firm is unprecedented. Furthermore, the results of this study have practical implications for the design of public education policies and the promotion of business creation in the tourism sector."
http://arxiv.org/abs/2312.00442v1,BPMS for management: a systematic literature review,2023-12-01 09:19:50+00:00,"['Alicia Martin-Navarro', 'Maria Paula Lechuga Sancho', 'Jose Aurelio Medina-Garrido']",econ.GN,"The aim of this paper is to carry out a systematic analysis of the literature to show the state of the art of Business Processes Management Systems (BPMS). BPMS represents a technology that automates business processes connecting users with their tasks. For this, a systematic review of the literature of the last ten years was carried out, using scientific papers indexed in the main databases of the knowledge area. The papers generated by the search were later analysed and filtered. Among the findings of this study, the academic interest and the multidisciplinary nature of the subject, as this type of studies have been identified in different areas of knowledge. Our research is a starting point for future research eager to develop a more robust theory and broaden the interest of the subject due its economic impact on process management."
http://arxiv.org/abs/2312.00457v1,Homophily and Specialization in Networks,2023-12-01 09:45:58+00:00,"['Patrick Allmis', 'Luca Paolo Merlino']",econ.TH,"In this paper, players contribute to two local public goods for which they have different tastes and sponsor costly links to enjoy the provision of others. In equilibrium, either there are several contributors specialized in public good provision or only two contributors who are not entirely specialized. Higher linking costs have a non-monotonic impact on welfare and polarization, as they affect who specializes in public good provision. When the available budget is small, subsidies should be given to players who already specialize in public good provision; otherwise, they should target only one player who specializes in public good provision."
http://arxiv.org/abs/2311.18453v1,Implementing Sustainable Tourism practices in luxury resorts of Maldives: Sustainability principles & Tripple Bottomline Approach,2023-11-30 10:55:33+00:00,"['Dr Mir Hasan Naqvi', 'Asnan Ahmed', 'Dr Asif Pervez']",econ.GN,"The aim of the research paper is to understand the sustainability challenges faced by resorts mainly luxury in Maldives and to implement the sustainable tourism practices. The Maldives economy is dependent mostly on the fishing, boat building, boat repairing and tourism. Over recent years there is a drastic change that has took place in Maldives in tourism industry. Maldives has progressed to be the upper middle-income country and luxury resorts are the reason for increased GDP in the country. Although there are some practices associated with the luxury resorts to follow in terms of environmental concerns. Present study focuses on the triple bottom line approach and the 12 major Sustainable Tourism Principles as a framework for sustainability practices and its implementation including the challenges associated in Maldives. The paper suggests some recommendations on several paradigm of enforcing laws and regulations, waste management facilities, fostering collaboration along with promoting local agriculture. The study also contemplates on several other areas such as on the impact of sustainability initiatives, coral restoration, and the use of sustainable supply chains. The intent of the current research is to suggest methods to promote the sustainable practices in luxury resort in Maldives."
http://arxiv.org/abs/2312.08171v1,"Individual Updating of Subjective Probability of Homicide Victimization: a ""Natural Experiment'' on Risk Communication",2023-12-13 14:31:27+00:00,"['José Raimundo Carvalho', 'Diego de Maria André', 'Yuri Costa']",econ.EM,"We investigate the dynamics of the update of subjective homicide victimization risk after an informational shock by developing two econometric models able to accommodate both optimal decisions of changing prior expectations which enable us to rationalize skeptical Bayesian agents with their disregard to new information. We apply our models to a unique household data (N = 4,030) that consists of socioeconomic and victimization expectation variables in Brazil, coupled with an informational ``natural experiment'' brought by the sample design methodology, which randomized interviewers to interviewees. The higher priors about their own subjective homicide victimization risk are set, the more likely individuals are to change their initial perceptions. In case of an update, we find that elders and females are more reluctant to change priors and choose the new response level. In addition, even though the respondents' level of education is not significant, the interviewers' level of education has a key role in changing and updating decisions. The results show that our econometric approach fits reasonable well the available empirical evidence, stressing the salient role heterogeneity represented by individual characteristics of interviewees and interviewers have on belief updating and lack of it, say, skepticism. Furthermore, we can rationalize skeptics through an informational quality/credibility argument."
http://arxiv.org/abs/2311.10917v1,Modeling trading games in a stochastic non-life insurance market,2023-11-18 00:03:55+00:00,"['Leonard Mushunje', 'David Edmund Allen']",econ.TH,"We studied the behavior and variation of utility between the two conflicting players in a closed Nash-equilibrium loop. Our modeling approach also captured the nexus between optimal premium strategizing and firm performance using the Lotka-Volterra completion model. Our model robustly modeled the two main cases, insurer-insurer and insurer-policyholder, which we accompanied by numerical examples of premium movements and their relationship to the market equilibrium point. We found that insurers with high claim exposures tend to set high premiums. The other competitors either set a competitive premium or adopt the fixed premium charge to remain in the game; otherwise, they will operate below the optimal point. We also noted an inverse link between trading premiums and claims in general insurance games due to self-interest and utility indifferences. We concluded that while an insurer aims to charge high premiums to enjoy more, policyholders are willing to avoid these charges by paying less."
http://arxiv.org/abs/2311.10210v1,Deriving Weeklong Activity-Travel Dairy from Google Location History: Survey Tool Development and A Field Test in Toronto,2023-11-16 21:56:03+00:00,"['Melvyn Li', 'Kaili Wang', 'Yicong Liu', 'Khandker Nurul Habib']",econ.GN,"This paper introduces an innovative travel survey methodology that utilizes Google Location History (GLH) data to generate travel diaries for transportation demand analysis. By leveraging the accuracy and omnipresence among smartphone users of GLH, the proposed methodology avoids the need for proprietary GPS tracking applications to collect smartphone-based GPS data. This research enhanced an existing travel survey designer, Travel Activity Internet Survey Interface (TRAISI), to make it capable of deriving travel diaries from the respondents' GLH. The feasibility of this data collection approach is showcased through the Google Timeline Travel Survey (GTTS) conducted in the Greater Toronto Area, Canada. The resultant dataset from the GTTS is demographically representative and offers detailed and accurate travel behavioural insights."
http://arxiv.org/abs/2311.08929v1,"The impact of Electricity Blackouts and poor infrastructure on the livelihood of residents and the local economy of City of Johannesburg, South Africa",2023-11-15 13:01:25+00:00,"['Nkosingizwile Mazwi Mchunu', 'George Okechukwu Onatu', 'Trynos Gumbo']",econ.GN,"This paper discusses the impact of electricity blackouts and poor infrastructure on the livelihood of residents and the local economy of Johannesburg, South Africa. The importance of a stable electricity grid plays a vital role in the effective functioning of urban infrastructure and the economy. The importance of electricity in the present-day South Africa has not been emphasized enough to be prioritized at all levels of government, especially at the local level, as it is where all socio-economic activities take place. The new South Africa needs to redefine the importance of electricity by ensuring that it is accessible, affordable, and produced sustainably, and most of all, by ensuring that the energy transition initiatives to green energy take place in a planned manner without causing harm to the economy, which might deepen the plight of South Africans. Currently, the City of Johannesburg is a growing spatial entity in both demographic and urbanization terms, and growing urban spaces require a stable supply of electricity for the proper functioning of urban systems and the growth of the local economy. The growth of the city brings about a massive demand for electricity that outstrips the current supply of electricity available on the local grid. The imbalance in the current supply and growing demand for electricity result in energy blackouts in the city, which have ripple effects on the economy and livelihoods of the people of Johannesburg. This paper examines the impact of electricity blackouts and poor infrastructure on the livelihood of residents and the local economy of Johannesburg, South Africa."
http://arxiv.org/abs/2310.09938v3,Unified Merger List in the Container Shipping Industry from 1966 to 2022: A Structural Estimation of M&A Matching,2023-10-15 20:16:18+00:00,"['Suguru Otani', 'Takuma Matsuda']",econ.GN,"We construct a novel unified merger list in the global container shipping industry between 1966 (the beginning of the industry) and 2022. Combining the list with proprietary data, we construct a structural matching model to describe the historical transition of the importance of a firm's age, size, and geographical proximity on merger decisions. We find that, as a positive factor, a firm's size is more important than a firm's age by 9.858 times as a merger incentive between 1991 and 2005. However, between 2006 and 2022, as a negative factor, a firm's size is more important than a firm's age by 2.013 times, that is, a firm's size works as a disincentive. We also find that the distance between buyer and seller firms works as a disincentive for the whole period, but the importance has dwindled to economic insignificance in recent years. In counterfactual simulations, we observe that the prohibition of mergers between firms in the same country would affect the merger configuration of not only the firms involved in prohibited mergers but also those involved in permitted mergers. Finally, we present interview-based evidence of the consistency between our merger lists, estimations, and counterfactual simulations with the industry experts' historical experiences."
http://arxiv.org/abs/2310.08115v2,Model-Agnostic Covariate-Assisted Inference on Partially Identified Causal Effects,2023-10-12 08:17:30+00:00,"['Wenlong Ji', 'Lihua Lei', 'Asher Spector']",econ.EM,"Many causal estimands are only partially identifiable since they depend on the unobservable joint distribution between potential outcomes. Stratification on pretreatment covariates can yield sharper bounds; however, unless the covariates are discrete with relatively small support, this approach typically requires binning covariates or estimating the conditional distributions of the potential outcomes given the covariates. Binning can result in substantial efficiency loss and become challenging to implement, even with a moderate number of covariates. Estimating conditional distributions, on the other hand, may yield invalid inference if the distributions are inaccurately estimated, such as when a misspecified model is used or when the covariates are high-dimensional. In this paper, we propose a unified and model-agnostic inferential approach for a wide class of partially identified estimands. Our method, based on duality theory for optimal transport problems, has four key properties. First, in randomized experiments, our approach can wrap around any estimates of the conditional distributions and provide uniformly valid inference, even if the initial estimates are arbitrarily inaccurate. A simple extension of our method to observational studies is doubly robust in the usual sense. Second, if nuisance parameters are estimated at semiparametric rates, our estimator is asymptotically unbiased for the sharp partial identification bound. Third, we can apply the multiplier bootstrap to select covariates and models without sacrificing validity, even if the true model is not selected. Finally, our method is computationally efficient. Overall, in three empirical applications, our method consistently reduces the width of estimated identified sets and confidence intervals without making additional structural assumptions."
http://arxiv.org/abs/2312.08799v2,Refined Characterizations of Approval-based Committee Scoring Rules,2023-12-14 10:34:07+00:00,"['Chris Dong', 'Patrick Lederer']",cs.GT,"In approval-based committee (ABC) elections, the goal is to select a fixed-size subset of the candidates, a so-called committee, based on the voters' approval ballots over the candidates. One of the most popular classes of ABC voting rules are ABC scoring rules, which have recently been characterized by Lackner and Skowron (2021). However, this characterization relies on a model where the output is a ranking of committees instead of a set of winning committees and no full characterization of ABC scoring rules exists in the latter standard setting. We address this issue by characterizing two important subclasses of ABC scoring rules in the standard ABC election model, thereby both extending the result of Lackner and Skowron (2021) to the standard setting and refining it to subclasses. In more detail, by relying on a consistency axiom for variable electorates, we characterize (i) the prominent class of Thiele rules and (ii) a new class of ABC voting rules called ballot size weighted approval voting. Based on these theorems, we also infer characterizations of three well-known ABC voting rules, namely multi-winner approval voting, proportional approval voting, and satisfaction approval voting."
http://arxiv.org/abs/2311.00905v3,Data-driven fixed-point tuning for truncated realized variations,2023-11-02 00:13:19+00:00,"['B. Cooper Boniece', 'José E. Figueroa-López', 'Yuchen Han']",math.ST,"Many methods for estimating integrated volatility and related functionals of semimartingales in the presence of jumps require specification of tuning parameters for their use in practice. In much of the available theory, tuning parameters are assumed to be deterministic and their values are specified only up to asymptotic constraints. However, in empirical work and in simulation studies, they are typically chosen to be random and data-dependent, with explicit choices often relying entirely on heuristics. In this paper, we consider novel data-driven tuning procedures for the truncated realized variations of a semimartingale with jumps based on a type of random fixed-point iteration. Being effectively automated, our approach alleviates the need for delicate decision-making regarding tuning parameters in practice and can be implemented using information regarding sampling frequency alone. We demonstrate our methods can lead to asymptotically efficient estimation of integrated volatility and exhibit superior finite-sample performance compared to popular alternatives in the literature."
http://arxiv.org/abs/2312.00436v1,Consensus group decision making under model uncertainty with a view towards environmental policy making,2023-12-01 09:08:26+00:00,"['Phoebe Koundouri', 'Georgios I. Papayiannis', 'Electra V. Petracou', 'Athanasios N. Yannacopoulos']",cs.MA,"In this paper we propose a consensus group decision making scheme under model uncertainty consisting of an iterative two-stage procedure and based on the concept of Fréchet barycenter. Each step consists of two stages: the agents first update their position in the opinion metric space by a local barycenter characterized by the agents' immediate interactions and then a moderator makes a proposal in terms of a global barycenter, checking for consensus at each step. In cases of large heterogeneous groups the procedure can be complemented by an auxiliary initial homogenization step, consisting of a clustering procedure in opinion space, leading to large homogeneous groups for which the aforementioned procedure will be applied.
  The scheme is illustrated in examples motivated from environmental economics."
http://arxiv.org/abs/2312.14565v2,Improving Task Instructions for Data Annotators: How Clear Rules and Higher Pay Increase Performance in Data Annotation in the AI Economy,2023-12-22 09:50:57+00:00,"['Johann Laux', 'Fabian Stephany', 'Alice Liefgreen']",econ.GN,"The global surge in AI applications is transforming industries, leading to displacement and complementation of existing jobs, while also giving rise to new employment opportunities. Data annotation, encompassing the labelling of images or annotating of texts by human workers, crucially influences the quality of a dataset directly influences the quality of AI models trained on it. This paper delves into the economics of data annotation, with a specific focus on the impact of task instruction design (that is, the choice between rules and standards as theorised in law and economics) and monetary incentives on data quality and costs. An experimental study involving 307 data annotators examines six groups with varying task instructions (norms) and monetary incentives. Results reveal that annotators provided with clear rules exhibit higher accuracy rates, outperforming those with vague standards by 14%. Similarly, annotators receiving an additional monetary incentive perform significantly better, with the highest accuracy rate recorded in the group working with both clear rules and incentives (87.5% accuracy). In addition, our results show that rules are perceived as being more helpful by annotators than standards and reduce annotators' difficulty in annotating images. These empirical findings underscore the double benefit of rule-based instructions on both data quality and worker wellbeing. Our research design allows us to reveal that, in our study, rules are more cost-efficient in increasing accuracy than monetary incentives. The paper contributes experimental insights to discussions on the economical, ethical, and legal considerations of AI technologies. Addressing policymakers and practitioners, we emphasise the need for a balanced approach in optimising data annotation processes for efficient and ethical AI development and usage."
http://arxiv.org/abs/2312.06927v1,WE economy: Potential of mutual aid distribution based on moral responsibility and risk vulnerability,2023-12-12 01:52:45+00:00,['Takeshi Kato'],econ.TH,"Reducing wealth inequality and disparity is a global challenge. The economic system is mainly divided into (1) gift and reciprocity, (2) power and redistribution, (3) market exchange, and (4) mutual aid without reciprocal obligations. The current inequality stems from a capitalist economy consisting of (2) and (3). To sublimate (1), which is the human economy, to (4), the concept of a ""mixbiotic society"" has been proposed in the philosophical realm. This is a society in which free and diverse individuals, ""I,"" mix with each other, recognize their respective ""fundamental incapability"" and sublimate them into ""WE"" solidarity. The economy in this society must have moral responsibility as a coadventurer and consideration for vulnerability to risk. Therefore, I focus on two factors of mind perception: moral responsibility and risk vulnerability, and propose a novel model of wealth distribution following an econophysical approach. Specifically, I developed a joint-venture model, a redistribution model in the joint-venture model, and a ""WE economy"" model. A simulation comparison of a combination of the joint ventures and redistribution with the WE economies reveals that WE economies are effective in reducing inequality and resilient in normalizing wealth distribution as advantages, and susceptible to free riders as disadvantages. However, this disadvantage can be compensated for by fostering consensus and fellowship, and by complementing it with joint ventures. This study essentially presents the effectiveness of moral responsibility, the complementarity between the WE economy and the joint economy, and the direction of the economy toward reducing inequality. Future challenges are to develop the WE economy model based on real economic analysis and psychology, as well as to promote WE economy fieldwork for worker coops and platform cooperatives to realize a desirable mixbiotic society."
http://arxiv.org/abs/2310.07157v1,Operating-Envelopes-Aware Decentralized Welfare Maximization for Energy Communities,2023-10-11 03:04:34+00:00,"['Ahmed S. Alahmed', 'Guido Cavraro', 'Andrey Bernstein', 'Lang Tong']",eess.SY,"We propose an operating-envelope-aware, prosumer-centric, and efficient energy community that aggregates individual and shared community distributed energy resources and transacts with a regulated distribution system operator (DSO) under a generalized net energy metering tariff design. To ensure safe network operation, the DSO imposes dynamic export and import limits, known as dynamic operating envelopes, on end-users' revenue meters. Given the operating envelopes, we propose an incentive-aligned community pricing mechanism under which the decentralized optimization of community members' benefit implies the optimization of overall community welfare. The proposed pricing mechanism satisfies the cost-causation principle and ensures the stability of the energy community in a coalition game setting. Numerical examples provide insights into the characteristics of the proposed pricing mechanism and quantitative measures of its performance."
http://arxiv.org/abs/2312.07520v2,Estimating Counterfactual Matrix Means with Short Panel Data,2023-12-12 18:51:39+00:00,"['Lihua Lei', 'Brad Ross']",econ.EM,"We develop a new, spectral approach for identifying and estimating average counterfactual outcomes under a low-rank factor model with short panel data and general outcome missingness patterns. Applications include event studies and studies of outcomes of ""matches"" between agents of two types, e.g. workers and firms, typically conducted under less-flexible Two-Way-Fixed-Effects (TWFE) models of outcomes. Given an infinite population of units and a finite number of outcomes, we show our approach identifies all counterfactual outcome means, including those not estimable by existing methods, if a particular graph constructed based on overlaps in observed outcomes between subpopulations is connected. Our analogous, computationally efficient estimation procedure yields consistent, asymptotically normal estimates of counterfactual outcome means under fixed-$T$ (number of outcomes), large-$N$ (sample size) asymptotics. In a semi-synthetic simulation study based on matched employer-employee data, our estimator has lower bias and only slightly higher variance than a TWFE-model-based estimator when estimating average log-wages."
http://arxiv.org/abs/2312.13564v2,The Effect of Antitrust Enforcement on Venture Capital Investments,2023-12-21 04:19:00+00:00,['Wentian Zhang'],econ.GN,"This paper studies the effect of antitrust enforcement on venture capital (VC) investments and VC-backed companies. To establish causality, I exploit the DOJ's decision to close several antitrust field offices in 2013, which reduced the antitrust enforcement in areas near the closed offices. I find that the reduction in antitrust enforcement causes a significant decrease in VC investments in startups located in the affected areas. Furthermore, these affected VC-backed startups exhibit a reduced likelihood of successful exits and diminished innovation performance. These negative results are mainly driven by startups in concentrated industries, where incumbents tend to engage in anticompetitive behaviors more frequently. To mitigate the adverse effect, startups should innovate more to differentiate their products. This paper sheds light on the importance of local antitrust enforcement in fostering competition and innovation."
http://arxiv.org/abs/2311.16260v3,Using Multiple Outcomes to Improve the Synthetic Control Method,2023-11-27 19:07:29+00:00,"['Liyang Sun', 'Eli Ben-Michael', 'Avi Feller']",econ.EM,"When there are multiple outcome series of interest, Synthetic Control analyses typically proceed by estimating separate weights for each outcome. In this paper, we instead propose estimating a common set of weights across outcomes, by balancing either a vector of all outcomes or an index or average of them. Under a low-rank factor model, we show that these approaches lead to lower bias bounds than separate weights, and that averaging leads to further gains when the number of outcomes grows. We illustrate this via a re-analysis of the impact of the Flint water crisis on educational outcomes."
http://arxiv.org/abs/2311.02299v4,The Fragility of Sparsity,2023-11-04 02:03:48+00:00,"['Michal Kolesár', 'Ulrich K. Müller', 'Sebastian T. Roelsgaard']",econ.EM,"We show, using three empirical applications, that linear regression estimates which rely on the assumption of sparsity are fragile in two ways. First, we document that different choices of the regressor matrix that do not impact ordinary least squares (OLS) estimates, such as the choice of baseline category with categorical controls, can move sparsity-based estimates by two standard errors or more. Second, we develop two tests of the sparsity assumption based on comparing sparsity-based estimators with OLS. The tests tend to reject the sparsity assumption in all three applications. Unless the number of regressors is comparable to or exceeds the sample size, OLS yields more robust inference at little efficiency cost."
http://arxiv.org/abs/2310.19479v2,Multilateral matching with scale economies,2023-10-30 12:09:33+00:00,['Chao Huang'],econ.TH,"This paper studies multilateral matching in which agents may negotiate contracts within any coalition. We assume scale economies such that an agent substitutes some existing contracts with new ones only if the latter involve a set of partners that is weakly larger than the original. A weakly setwise stable (or setwise stable) outcome exists and can be found by a Constrained Serial Dictatorship algorithm in markets with scale economies (resp. ordinal scale economies). The scale economies condition applies to an environment in which agents cooperate to achieve targets, such as markets in which countries sign bilateral agreements."
http://arxiv.org/abs/2402.07170v1,Research on the multi-stage impact of digital economy on rural revitalization in Hainan Province based on GPM model,2024-02-11 11:41:35+00:00,['Wenbo Lyu'],econ.GN,"The rapid development of the digital economy has had a profound impact on the implementation of the rural revitalization strategy. Based on this, this study takes Hainan Province as the research object to deeply explore the impact of digital economic development on rural revitalization. The study collected panel data from 2003 to 2022 to construct an evaluation index system for the digital economy and rural revitalization and used panel regression analysis and other methods to explore the promotion effect of the digital economy on rural revitalization. Research results show that the digital economy has a significant positive impact on rural revitalization, and this impact increases as the level of fiscal expenditure increases. The issuance of digital RMB has further exerted a regulatory effect and promoted the development of the digital economy and the process of rural revitalization. At the same time, the establishment of the Hainan Free Trade Port has also played a positive role in promoting the development of the digital economy and rural revitalization. In the prediction of the optimal strategy for rural revitalization based on the development levels of the primary, secondary, and tertiary industries (Rate1, Rate2, and Rate3), it was found that rate1 can encourage Hainan Province to implement digital economic innovation, encourage rate3 to implement promotion behaviors, and increase rate2 can At the level of sustainable development when rate3 promotes rate2's digital economic innovation behavior, it can standardize rate2's production behavior to the greatest extent, accelerate the faster application of the digital economy to the rural revitalization industry, and promote the technological advancement of enterprises."
http://arxiv.org/abs/2403.18837v2,Repetitive Dilemma Games in Distribution Information Using Interplay of Droop Quota: Meek's Method in Impact of Maximum Compensation and Minimum Cost Routes in Information Role of Marginal Contribution in Two-Sided Matching Markets,2024-02-19 15:14:20+00:00,['Yasuko Kawahata'],econ.GN,"This paper is a preliminary report of the research plan and a digest of the results and discussions. On research note explores the complex dynamics of fake news dissemination and fact-checking costs within the framework of information markets and analyzes the equilibrium between supply and demand using the concepts of droop quotas, Meek's method, and marginal contributions. By adopting a two-sided matching market perspective, we delve into scenarios in which markets are stable under the influence of fake news perceived as truth and those in which credibility prevails. Through the application of iterated dilemma game theory, we investigate the strategic choices of news providers affected by the costs associated with spreading fake news and fact-checking efforts. We further examine the maximum reward problem and strategies to minimize the cost path for spreading fake news, and consider a nuanced understanding of market segmentation into ""cheap"" and ""premium"" segments based on the nature of the information being spread. Our analysis uses mathematical models and computational processes to identify stable equilibrium points that ensure market stability in the face of deceptive information practices and provide insight into effective strategies to enhance the informational health of the market. Through this comprehensive approach, this paper aims for a more truthful and reliable perspective from which to observe information markets. This paper is partially an attempt to utilize ""Generative AI"" and was written with educational intent. There are currently no plans for it to become a peer-reviewed paper."
http://arxiv.org/abs/2403.11333v1,Identification of Information Structures in Bayesian Games,2024-03-17 20:25:38+00:00,['Masaki Miyashita'],econ.TH,"To what extent can an external observer observing an equilibrium action distribution in an incomplete information game infer the underlying information structure? We investigate this issue in a general linear-quadratic-Gaussian framework. A simple class of canonical information structures is offered and proves rich enough to rationalize any possible equilibrium action distribution that can arise under an arbitrary information structure. We show that the class is parsimonious in the sense that the relevant parameters can be uniquely pinned down by an observed equilibrium outcome, up to some qualifications. Our result implies, for example, that the accuracy of each agent's signal about the state is identified, as measured by how much observing the signal reduces the state variance. Moreover, we show that a canonical information structure characterizes the lower bound on the amount by which each agent's signal can reduce the state variance, across all observationally equivalent information structures. The lower bound is tight, for example, when the actual information structure is uni-dimensional, or when there are no strategic interactions among agents, but in general, there is a gap since agents' strategic motives confound their private information about fundamental and strategic uncertainty."
http://arxiv.org/abs/2403.04328v3,A dual approach to nonparametric characterization for random utility models,2024-03-07 08:48:17+00:00,"['Nobuo Koida', 'Koji Shirai']",econ.TH,"This paper develops a novel characterization for random utility models (RUM), which turns out to be a dual representation of the characterization by Kitamura and Stoye (2018, ECMA). For a given family of budgets and its ""patch"" representation á la Kitamura and Stoye, we construct a matrix $Ξ$ of which each row vector indicates the structure of possible revealed preference relations in each subfamily of budgets. Then, it is shown that a stochastic demand system on the patches of budget lines, say $π$, is consistent with a RUM, if and only if $Ξπ\geq \mathbb{1}$, where the RHS is the vector of $1$'s. In addition to providing a concise quantifier-free characterization, especially when $π$ is inconsistent with RUMs, the vector $Ξπ$ also contains information concerning (1) sub-families of budgets in which cyclical choices must occur with positive probabilities, and (2) the maximal possible weights on rational choice patterns in a population. The notion of Chvátal rank of polytopes and the duality theorem in linear programming play key roles to obtain these results."
http://arxiv.org/abs/2403.17777v1,Deconvolution from two order statistics,2024-03-26 15:09:55+00:00,"['JoonHwan Cho', 'Yao Luo', 'Ruli Xiao']",econ.EM,"Economic data are often contaminated by measurement errors and truncated by ranking. This paper shows that the classical measurement error model with independent and additive measurement errors is identified nonparametrically using only two order statistics of repeated measurements. The identification result confirms a hypothesis by Athey and Haile (2002) for a symmetric ascending auction model with unobserved heterogeneity. Extensions allow for heterogeneous measurement errors, broadening the applicability to additional empirical settings, including asymmetric auctions and wage offer models. We adapt an existing simulated sieve estimator and illustrate its performance in finite samples."
http://arxiv.org/abs/2403.19363v1,"Dynamic Correlation of Market Connectivity, Risk Spillover and Abnormal Volatility in Stock Price",2024-03-28 12:23:13+00:00,"['Muzi Chen', 'Nan Li', 'Lifen Zheng', 'Difang Huang', 'Boyao Wu']",econ.EM,"The connectivity of stock markets reflects the information efficiency of capital markets and contributes to interior risk contagion and spillover effects. We compare Shanghai Stock Exchange A-shares (SSE A-shares) during tranquil periods, with high leverage periods associated with the 2015 subprime mortgage crisis. We use Pearson correlations of returns, the maximum strongly connected subgraph, and $3σ$ principle to iteratively determine the threshold value for building a dynamic correlation network of SSE A-shares. Analyses are carried out based on the networking structure, intra-sector connectivity, and node status, identifying several contributions. First, compared with tranquil periods, the SSE A-shares network experiences a more significant small-world and connective effect during the subprime mortgage crisis and the high leverage period in 2015. Second, the finance, energy and utilities sectors have a stronger intra-industry connectivity than other sectors. Third, HUB nodes drive the growth of the SSE A-shares market during bull periods, while stocks have a think-tail degree distribution in bear periods and show distinct characteristics in terms of market value and finance. Granger linear and non-linear causality networks are also considered for the comparison purpose. Studies on the evolution of inter-cycle connectivity in the SSE A-share market may help investors improve portfolios and develop more robust risk management policies."
http://arxiv.org/abs/2401.14945v2,Free public transport to the destination: A causal analysis of tourists' travel mode choice,2024-01-26 15:27:45+00:00,"['Kevin Blättler', 'Hannes Wallimann', 'Widar von Arx']",econ.GN,"In this paper, we assess the impact of a fare-free public transport policy for overnight guests on travel mode choice to a Swiss tourism destination. The policy directly targets domestic transport to and from a destination, the substantial contributor to the CO2 emissions of overnight trips. Based on a survey sample, we identify the effect with the help of the random element that the information on the offer from a hotelier to the guest varies in day-to-day business. We estimate a shift from private cars to public transport due to the policy of, on average, 14.8 and 11.6 percentage points, depending on the application of propensity score matching and causal forest. This knowledge is relevant for policy-makers to design future offers that include more sustainable travels to a destination. Overall, our paper exemplifies how such an effect of comparable natural experiments in the travel and tourism industry can be properly identified with a causal framework and underlying assumptions."
http://arxiv.org/abs/2401.16275v1,Graph Neural Networks: Theory for Estimation with Application on Network Heterogeneity,2024-01-29 16:26:30+00:00,"['Yike Wang', 'Chris Gu', 'Taisuke Otsu']",econ.EM,"This paper presents a novel application of graph neural networks for modeling and estimating network heterogeneity. Network heterogeneity is characterized by variations in unit's decisions or outcomes that depend not only on its own attributes but also on the conditions of its surrounding neighborhood. We delineate the convergence rate of the graph neural networks estimator, as well as its applicability in semiparametric causal inference with heterogeneous treatment effects. The finite-sample performance of our estimator is evaluated through Monte Carlo simulations. In an empirical setting related to microfinance program participation, we apply the new estimator to examine the average treatment effects and outcomes of counterfactual policies, and to propose an enhanced strategy for selecting the initial recipients of program information in social networks."
http://arxiv.org/abs/2401.16406v1,A mathematical theory of power,2024-01-29 18:45:10+00:00,['Daniele De Luca'],econ.TH,"This paper proposes a new approach to power in Game Theory. Cooperation and conflict are simulated with a mechanism of payoff alteration, called F-game. Using convex combinations of preferences, an F-game can measure players' attitude to cooperate. We can then define actual and potential power as special relations between different states of the system."
http://arxiv.org/abs/2401.05784v2,Covariance Function Estimation for High-Dimensional Functional Time Series with Dual Factor Structures,2024-01-11 09:38:08+00:00,"['Chenlei Leng', 'Degui Li', 'Hanlin Shang', 'Yingcun Xia']",econ.EM,"We propose a flexible dual functional factor model for modelling high-dimensional functional time series. In this model, a high-dimensional fully functional factor parametrisation is imposed on the observed functional processes, whereas a low-dimensional version (via series approximation) is assumed for the latent functional factors. We extend the classic principal component analysis technique for the estimation of a low-rank structure to the estimation of a large covariance matrix of random functions that satisfies a notion of (approximate) functional ""low-rank plus sparse"" structure; and generalise the matrix shrinkage method to functional shrinkage in order to estimate the sparse structure of functional idiosyncratic components. Under appropriate regularity conditions, we derive the large sample theory of the developed estimators, including the consistency of the estimated factors and functional factor loadings and the convergence rates of the estimated matrices of covariance functions measured by various (functional) matrix norms. Consistent selection of the number of factors and a data-driven rule to choose the shrinkage parameter are discussed. Simulation and empirical studies are provided to demonstrate the finite-sample performance of the developed model and estimation methodology."
http://arxiv.org/abs/2401.05832v1,Interactions between dynamic team composition and coordination: An agent-based modeling approach,2024-01-11 11:03:29+00:00,"['Darío Blanco-Fernández', 'Stephan Leitner', 'Alexandra Rausch']",econ.GN,"This paper examines the interactions between selected coordination modes and dynamic team composition, and their joint effects on task performance under different task complexity and individual learning conditions. Prior research often treats dynamic team composition as a consequence of suboptimal organizational design choices. The emergence of new organizational forms that consciously employ teams that change their composition periodically challenges this perspective. In this paper, we follow the contingency theory and characterize dynamic team composition as a design choice that interacts with other choices such as the coordination mode, and with additional contextual factors such as individual learning and task complexity. We employ an agent-based modeling approach based on the NK framework, which includes a reinforcement learning mechanism, a recurring team formation mechanism based on signaling, and three different coordination modes. Our results suggest that by implementing lateral communication or sequential decision-making, teams may exploit the benefits of dynamic composition more than if decision-making is fully autonomous. The choice of a proper coordination mode, however, is partly moderated by the task complexity and individual learning. Additionally, we show that only a coordination mode based on lateral communication may prevent the negative effects of individual learning."
http://arxiv.org/abs/2401.06835v2,Austria's KlimaTicket: Assessing the short-term impact of a cheap nationwide travel pass on demand,2024-01-12 15:50:12+00:00,['Hannes Wallimann'],econ.GN,"Measures to reduce transport-related greenhouse gas emissions are of great importance to policy-makers. A recent example is the nationwide KlimaTicket in Austria, a country with a relatively high share of transport-related emissions. The cheap yearly season ticket introduced in October 2021 allows unlimited access to Austria's public transport network. Using the synthetic control and synthetic difference-in-differences methods, I assess the causal effect of this policy on public transport demand by constructing a data-driven counterfactual out of European railway companies to mimic the number of passengers of the Austrian Federal Railways without the KlimaTicket. The results indicate public transport demand grew slightly faster in Austria, i.e., 3.3 or 6.8 percentage points, depending on the method, than it would have in the absence of the KlimaTicket. However, the growth effect after the COVID-19 pandemic appears only statistically significant when applying the synthetic control method, and the positive effect on public transport demand growth disappears in 2022."
http://arxiv.org/abs/2401.06864v1,Deep Learning With DAGs,2024-01-12 19:35:54+00:00,"['Sourabh Balgi', 'Adel Daoud', 'Jose M. Peña', 'Geoffrey T. Wodtke', 'Jesse Zhou']",stat.ML,"Social science theories often postulate causal relationships among a set of variables or events. Although directed acyclic graphs (DAGs) are increasingly used to represent these theories, their full potential has not yet been realized in practice. As non-parametric causal models, DAGs require no assumptions about the functional form of the hypothesized relationships. Nevertheless, to simplify the task of empirical evaluation, researchers tend to invoke such assumptions anyway, even though they are typically arbitrary and do not reflect any theoretical content or prior knowledge. Moreover, functional form assumptions can engender bias, whenever they fail to accurately capture the complexity of the causal system under investigation. In this article, we introduce causal-graphical normalizing flows (cGNFs), a novel approach to causal inference that leverages deep neural networks to empirically evaluate theories represented as DAGs. Unlike conventional approaches, cGNFs model the full joint distribution of the data according to a DAG supplied by the analyst, without relying on stringent assumptions about functional form. In this way, the method allows for flexible, semi-parametric estimation of any causal estimand that can be identified from the DAG, including total effects, conditional effects, direct and indirect effects, and path-specific effects. We illustrate the method with a reanalysis of Blau and Duncan's (1967) model of status attainment and Zhou's (2019) model of conditional versus controlled mobility. To facilitate adoption, we provide open-source software together with a series of online tutorials for implementing cGNFs. The article concludes with a discussion of current limitations and directions for future development."
http://arxiv.org/abs/2401.13057v2,Inference under partial identification with minimax test statistics,2024-01-23 19:29:01+00:00,['Isaac Loh'],econ.EM,"We provide a means of computing and estimating the asymptotic distributions of statistics based on an outer minimization of an inner maximization. Such test statistics, which arise frequently in moment models, are of special interest in providing hypothesis tests under partial identification. Under general conditions, we provide an asymptotic characterization of such test statistics using the minimax theorem, and a means of computing critical values using the bootstrap. Making some light regularity assumptions, our results augment several asymptotic approximations that have been provided for partially identified hypothesis tests, and extend them by mitigating their dependence on local linear approximations of the parameter space. These asymptotic results are generally simple to state and straightforward to compute (esp.\ adversarially)."
http://arxiv.org/abs/2401.09265v1,Equity Premium in Efficient Markets,2024-01-17 15:15:18+00:00,['B. N. Kausik'],econ.GN,"Equity premium, the surplus returns of stocks over bonds, has been an enduring puzzle. While numerous prior works approach the problem assuming the utility of money is invariant across contexts, our approach implies that in efficient markets the utility of money is polymorphic, with risk aversion dependent on the information available in each context, i.e. the discount on each future cash flow depends on all information available on that cash flow. Specifically, we prove that in efficient markets, informed investors maximize return on volatility by being risk-neutral with riskless bonds, and risk-averse with equities, thereby resolving the puzzle. We validate our results on historical data with surprising consistency.
  JEL Classification: C58, G00, G12, G17"
http://arxiv.org/abs/2401.03293v1,Counterfactuals in factor models,2024-01-06 20:10:53+00:00,['Jad Beyhum'],econ.EM,"We study a new model where the potential outcomes, corresponding to the values of a (possibly continuous) treatment, are linked through common factors. The factors can be estimated using a panel of regressors. We propose a procedure to estimate time-specific and unit-specific average marginal effects in this context. Our approach can be used either with high-dimensional time series or with large panels. It allows for treatment effects heterogenous across time and units and is straightforward to implement since it only relies on principal components analysis and elementary computations. We derive the asymptotic distribution of our estimator of the average marginal effect and highlight its solid finite sample performance through a simulation exercise. The approach can also be used to estimate average counterfactuals or adapted to an instrumental variables setting and we discuss these extensions. Finally, we illustrate our novel methodology through an empirical application on income inequality."
http://arxiv.org/abs/2401.03598v2,Incontestable Assignments,2024-01-07 22:48:25+00:00,"['Benoit Decerf', 'Guillaume Haeringer', 'Martin Van der Linden']",econ.TH,"In school districts where assignments are exclusively determined by a clearinghouse students can only appeal their assignment with a valid reason. An assignment is incontestable if it is appeal-proof. We study incontestability when students do not observe the other students' preferences and assignments. Incontestability is shown to be equivalent to individual rationality, non-wastefulness, and respect for top-priority sets (a weakening of justified envy). Stable mechanisms and those Pareto dominating them are incontestable, as well as the Top-Trading Cycle mechanism (but Boston is not). Under a mild consistency property, incontestable mechanisms are i-indinstiguishable (Li, 2017), and share similar incentive properties."
http://arxiv.org/abs/2401.03658v1,Spiritual Intelligence's Role in Reducing Technostress through Ethical Work Climates,2024-01-08 04:05:15+00:00,"['Saleh Ghobbeh', 'Armita Atrian']",econ.GN,"This study explores the impact of spiritual intelligence (SI) on technostress, with a focus on the mediating role of the ethical environment. In an era where technological advancements continually reshape our work and personal lives, understanding the interplay between human intelligence, well-being, and ethics within organizations is increasingly significant. Spiritual intelligence, transcending traditional cognitive and emotional intelligences, emphasizes understanding personal meaning and values. This paper investigates how higher levels of SI enable individuals to integrate technology into their lives without undue stress, and how a robust ethical environment within organizations supports and amplifies these benefits. Through a comprehensive review of literature, empirical research, and detailed analysis, the study highlights the protective role of SI against technostress and the significant influence of an ethical climate in enhancing this effect. The findings offer valuable insights for organizational strategies aimed at promoting a harmonious, stress-free workplace environment."
http://arxiv.org/abs/2401.10937v1,Subjective Causality,2024-01-17 11:36:38+00:00,"['Joseph Y. Halpern', 'Evan Piermont']",econ.TH,"We show that it is possible to understand and identify a decision maker's subjective causal judgements by observing her preferences over interventions. Following Pearl [2000], we represent causality using causal models (also called structural equations models), where the world is described by a collection of variables, related by equations. We show that if a preference relation over interventions satisfies certain axioms (related to standard axioms regarding counterfactuals), then we can define (i) a causal model, (ii) a probability capturing the decision-maker's uncertainty regarding the external factors in the world and (iii) a utility on outcomes such that each intervention is associated with an expected utility and such that intervention $A$ is preferred to $B$ iff the expected utility of $A$ is greater than that of $B$. In addition, we characterize when the causal model is unique. Thus, our results allow a modeler to test the hypothesis that a decision maker's preferences are consistent with some causal model and to identify causal judgements from observed behavior."
http://arxiv.org/abs/2401.12274v1,Are Charter Value and Supervision Aligned? A Segmentation Analysis,2024-01-22 18:51:15+00:00,"['Juan Aparicio', 'Miguel A. Duran', 'Ana Lozano-Vivas', 'Jesus T. Pastor']",q-fin.RM,"Previous work suggests that the charter value hypothesis is theoretically grounded and empirically supported, but not universally. Accordingly, this paper aims to perform an analysis of the relations between charter value, risk taking, and supervision, taking into account the relations' complexity. Specifically, using the CAMELS rating system as a general framework for supervision, we study how charter value relates to risk and supervision by means of classification and regression tree analysis. The sample covers the period 2005-2016 and consists of listed banks in countries that were members of the Eurozone when it came into existence, along with Greece. To evaluate the crisis consequences, we also separately analyze four subperiods and countries that required financial aid from third parties and those that did not so, along with large and small banks. Our results reflect the complexity of the relations between charter value, supervision, and risk. Indeed, supervision and charter value seem aligned regarding only some types of risk"
http://arxiv.org/abs/2401.12301v1,Pricing and Usage: An Empirical Analysis of Lines of Credit,2024-01-22 19:01:04+00:00,['Miguel A. Duran'],econ.GN,"The hypothesis that committed revolving credit lines with fixed spreads can provide firms with interest rate insurance is a standard feature of models on these credit facilities' interest rate structure. Nevertheless, this hypothesis has not been tested. Its empirical examination is the main contribution of this paper. To perform this analysis, and given the unavailability of data, we hand-collect data on usage at the credit line level itself. The resulting dataset enables us also to take into account characteristics of credit lines that have been ignored by previous research. One of them is that credit lines can have simultaneously fixed and performance-based spreads."
http://arxiv.org/abs/2401.16542v1,Robust Performance Evaluation of Independent and Identical Agents,2024-01-29 20:21:39+00:00,['Ashwin Kambhampati'],econ.TH,"A principal provides nondiscriminatory incentives for independent and identical agents. The principal cannot observe the agents' actions, nor does she know the entire set of actions available to them. It is shown, very generally, that any worst-case optimal contract is nonaffine in performances. In addition, each agent's pay must depend on the performance of another. In the case of two agents and binary output, existence of a worst-case optimal contract is established and it is proven that any such contract exhibits joint performance evaluation -- each agent's pay is strictly increasing in the performance of the other. The analysis identifies a fundamentally new channel leading to the optimality of nonlinear team-based incentive pay."
http://arxiv.org/abs/2403.00653v1,Modelling Global Fossil CO2 Emissions with a Lognormal Distribution: A Climate Policy Tool,2024-03-01 16:34:10+00:00,"['Faustino Prieto', 'Catalina B. García-García', 'Román Salmerón Gómez']",econ.GN,"Carbon dioxide (CO2) emissions have emerged as a critical issue with profound impacts on the environment, human health, and the global economy. The steady increase in atmospheric CO2 levels, largely due to human activities such as burning fossil fuels and deforestation, has become a major contributor to climate change and its associated catastrophic effects. To tackle this pressing challenge, a coordinated global effort is needed, which necessitates a deep understanding of emissions patterns and trends. In this paper, we explore the use of statistical modelling, specifically the lognormal distribution, as a framework for comprehending and predicting CO2 emissions. We build on prior research that suggests a complex distribution of emissions and seek to test the hypothesis that a simpler distribution can still offer meaningful insights for policy-makers. We utilize data from three comprehensive databases and analyse six candidate distributions (exponential, Fisk, gamma, lognormal, Lomax, Weibull) to identify a suitable model for global fossil CO2 emissions. Our findings highlight the adequacy of the lognormal distribution in characterizing emissions across all countries and years studied. Furthermore, to provide additional support for this distribution, we provide statistical evidence supporting the applicability of Gibrat's law to those CO2 emissions. Finally, we employ the lognormal model to predict emission parameters for the coming years and propose two policies for reducing total fossil CO2 emissions. Our research aims to provide policy-makers with accurate and detailed information to support effective climate change mitigation strategies."
http://arxiv.org/abs/2402.19421v1,Crafting Knowledge: Exploring the Creative Mechanisms of Chat-Based Search Engines,2024-02-29 18:20:37+00:00,"['Lijia Ma', 'Xingchen Xu', 'Yong Tan']",cs.IR,"In the domain of digital information dissemination, search engines act as pivotal conduits linking information seekers with providers. The advent of chat-based search engines utilizing Large Language Models (LLMs) and Retrieval Augmented Generation (RAG), exemplified by Bing Chat, marks an evolutionary leap in the search ecosystem. They demonstrate metacognitive abilities in interpreting web information and crafting responses with human-like understanding and creativity. Nonetheless, the intricate nature of LLMs renders their ""cognitive"" processes opaque, challenging even their designers' understanding. This research aims to dissect the mechanisms through which an LLM-powered chat-based search engine, specifically Bing Chat, selects information sources for its responses. To this end, an extensive dataset has been compiled through engagements with New Bing, documenting the websites it cites alongside those listed by the conventional search engine. Employing natural language processing (NLP) techniques, the research reveals that Bing Chat exhibits a preference for content that is not only readable and formally structured, but also demonstrates lower perplexity levels, indicating a unique inclination towards text that is predictable by the underlying LLM. Further enriching our analysis, we procure an additional dataset through interactions with the GPT-4 based knowledge retrieval API, unveiling a congruent text preference between the RAG API and Bing Chat. This consensus suggests that these text preferences intrinsically emerge from the underlying language models, rather than being explicitly crafted by Bing Chat's developers. Moreover, our investigation documents a greater similarity among websites cited by RAG technologies compared to those ranked highest by conventional search engines."
http://arxiv.org/abs/2402.19425v1,Testing Information Ordering for Strategic Agents,2024-02-29 18:22:38+00:00,"['Sukjin Han', 'Hiroaki Kaido', 'Lorenzo Magnolfi']",econ.EM,A key primitive of a strategic environment is the information available to players. Specifying a priori an information structure is often difficult for empirical researchers. We develop a test of information ordering that allows researchers to examine if the true information structure is at least as informative as a proposed baseline. We construct a computationally tractable test statistic by utilizing the notion of Bayes Correlated Equilibrium (BCE) to translate the ordering of information structures into an ordering of functions. We apply our test to examine whether hubs provide informational advantages to certain airlines in addition to market power.
http://arxiv.org/abs/2401.07818v2,A General Approach for Computing a Consensus in Group Decision Making That Integrates Multiple Ethical Principles,2024-01-15 16:43:54+00:00,"['Francisco Salas-Molina', 'Filippo Bistaffa', 'Juan A. Rodriguez-Aguilar']",econ.TH,"We tackle the problem of computing a consensus according to multiple ethical principles -- which can include, for example, the principle of maximum freedom associated with the Benthamite doctrine and the principle of maximum fairness associated with the Rawlsian principles -- among the preferences of different individuals in the context of Group-Decision-Making. More formally, we put forward a novel formalisation of the above-mentioned problem based on a multinorm approximation problem that aims at minimising multiple p-metric distance functions, where each parameter p represents a given ethical principle. Our contribution incurs obvious benefits from a social-choice perspective. Firstly, our approach significantly generalises state-of-the-art approaches that were limited to only two ethical principles (p set to one, for maximum freedom, and p set to infinity, for maximum fairness). Secondly, our experimental results considering an established test case demonstrate that our approach is capable, thanks to a novel re-weighting scheme, to compute a multi-norm consensus that takes into account each ethical principle in a balanced way, in contrast with state-of-the-art approaches that were heavily biased towards the p=1 ethical principle"
http://arxiv.org/abs/2401.08013v1,A Day-to-Day Dynamical Approach to the Most Likely User Equilibrium Problem,2024-01-15 23:43:41+00:00,"['Jiayang Li', 'Qianni Wang', 'Liyang Feng', 'Jun Xie', 'Yu Marco Nie']",cs.GT,"The lack of a unique user equilibrium (UE) route flow in traffic assignment has posed a significant challenge to many transportation applications. The maximum-entropy principle, which advocates for the consistent selection of the most likely solution as a representative, is often used to address the challenge. Built on a recently proposed day-to-day (DTD) discrete-time dynamical model called cumulative logit (CULO), this study provides a new behavioral underpinning for the maximum-entropy UE (MEUE) route flow. It has been proven that CULO can reach a UE state without presuming travelers are perfectly rational. Here, we further establish that CULO always converges to the MEUE route flow if (i) travelers have zero prior information about routes and thus are forced to give all routes an equal choice probability, or (ii) all travelers gather information from the same source such that the so-called general proportionality condition is satisfied. Thus, CULO may be used as a practical solution algorithm for the MEUE problem. To put this idea into practice, we propose to eliminate the route enumeration requirement of the original CULO model through an iterative route discovery scheme. We also examine the discrete-time versions of four popular continuous-time dynamical models and compare them to CULO. The analysis shows that the replicator dynamic is the only one that has the potential to reach the MEUE solution with some regularity. The analytical results are confirmed through numerical experiments."
http://arxiv.org/abs/2401.08251v1,A techno-economic model for avoiding conflicts of interest between owners of offshore wind farms and maintenance suppliers,2024-01-16 10:04:29+00:00,"['Alberto Pliego Marugán', 'Fausto Pedro García Márquez', 'Jesús María Pinar Pérez']",cs.GT,"Currently, wind energy is one of the most important sources of renewable energy. Offshore locations for wind turbines are increasingly exploited because of their numerous advantages. However, offshore wind farms require high investment in maintenance service. Due to its complexity and special requirements, maintenance service is usually outsourced by wind farm owners. In this paper, we propose a novel approach to determine, quantify, and reduce the possible conflicts of interest between owners and maintenance suppliers. We created a complete techno-economic model to address this problem from an impartial point of view. An iterative process was developed to obtain statistical results that can help stakeholders negotiate the terms of the contract, in which the availability of the wind farm is the reference parameter by which to determine penalisations and incentives. Moreover, a multi-objective programming problem was addressed that maximises the profits of both parties without losing the alignment of their interests. The main scientific contribution of this paper is the maintenance analysis of offshore wind farms from two perspectives: that of the owner and the maintenance supplier. This analysis evaluates the conflicts of interest of both parties. In addition, we demonstrate that proper adjustment of some parameters, such as penalisation, incentives, and resources, and adequate control of availability can help reduce this conflict of interests."
http://arxiv.org/abs/2401.00748v2,Sequential choice functions and stability problems,2024-01-01 13:08:56+00:00,['Vladimir I. Danilov'],math.CO,The concept of sequential choice functions is introduced and studied. This concept applies to the reduction of the problem of stable matchings with sequential workers to a situation where the workers are linear.
http://arxiv.org/abs/2401.01565v2,Classification and Treatment Learning with Constraints via Composite Heaviside Optimization: a Progressive MIP Method,2024-01-03 06:39:18+00:00,"['Yue Fang', 'Junyi Liu', 'Jong-Shi Pang']",math.OC,"This paper proposes a Heaviside composite optimization approach and presents a progressive (mixed) integer programming (PIP) method for solving multi-class classification and multi-action treatment problems with constraints. A Heaviside composite function is a composite of a Heaviside function (i.e., the indicator function of either the open $( \, 0,\infty )$ or closed $[ \, 0,\infty \, )$ interval) with a possibly nondifferentiable function. Modeling-wise, we show how Heaviside composite optimization provides a unified formulation for learning the optimal multi-class classification and multi-action treatment rules, subject to rule-dependent constraints stipulating a variety of domain restrictions. A Heaviside composite function has an equivalent discrete formulation, and the resulting optimization problem can in principle be solved by integer programming (IP) methods. Nevertheless, for constrained learning problems with large data sets, a straightforward application of off-the-shelf IP solvers is usually ineffective in achieving global optimality. To alleviate such a computational burden, our major contribution is the proposal of the PIP method by leveraging the effectiveness of state-of-the-art IP solvers for problems of modest sizes. We provide the theoretical advantage of the PIP method with the connection to continuous optimization and show that the computed solution is locally optimal for a broad class of Heaviside composite optimization problems. The numerical performance of the PIP method is demonstrated by extensive computational experimentation."
http://arxiv.org/abs/2401.01411v1,Urban Street Network Design and Transport-Related Greenhouse Gas Emissions around the World,2024-01-02 19:20:54+00:00,"['Geoff Boeing', 'Clemens Pilgram', 'Yougeng Lu']",physics.soc-ph,"This study estimates the relationships between street network characteristics and transport-sector CO2 emissions across every urban area in the world and investigates whether they are the same across development levels and urban design paradigms. The prior literature has estimated relationships between street network design and transport emissions -- including greenhouse gases implicated in climate change -- primarily through case studies focusing on certain world regions or relatively small samples of cities, complicating generalizability and applicability for evidence-informed practice. Our worldwide study finds that straighter, more-connected, and less-overbuilt street networks are associated with lower transport emissions, all else equal. Importantly, these relationships vary across development levels and design paradigms -- yet most prior literature reports findings from urban areas that are outliers by global standards. Planners need a better empirical base for evidence-informed practice in under-studied regions, particularly the rapidly urbanizing Global South."
http://arxiv.org/abs/2401.00940v1,Theoretical Steps to Optimize Transportation in the Cubic Networks and the Congestion Paradox,2024-01-01 19:10:25+00:00,['Joonkyung Yoo'],econ.TH,"Given a player is guaranteed the same payoff for each delivery path in a single-cube delivery network, the player's best response is to randomly divide all goods and deliver them to all other nodes, and the best response satisfies the Kuhn-Tucker condition. The state of the delivery network is randomly complete. If congestion costs are introduced to the player's maximization problem in a multi-cubic delivery network, the congestion paradox arises where all coordinates become congested as long as the previous assumptions about payoffs are maintained."
http://arxiv.org/abs/2401.02139v1,Airport service quality perception and flight delays: examining the influence of psychosituational latent traits of respondents in passenger satisfaction surveys,2024-01-04 08:44:05+00:00,"['Alessandro V. M. Oliveira', 'Bruno F. Oliveira', 'Moises D. Vassallo']",econ.GN,"The service quality of a passenger transport operator can be measured through face-to-face surveys at the terminals or on board. However, the resulting responses may suffer from the influence of the intrinsic aspects of the respondent's personality and emotional context at the time of the interview. This study proposes a methodology to generate and select control variables for these latent psychosituational traits, thus mitigating the risk of omitted variable bias. We developed an econometric model of the determinants of passenger satisfaction in a survey conducted at the largest airport in Latin America, São Paulo GRU Airport. Our focus was on the role of flight delays in the perception of quality. The results of this study confirm the existence of a relationship between flight delays and the global satisfaction of passengers with airports. In addition, favorable evaluations regarding airports' food/beverage concessions and Wi-Fi services, but not their retail options, have a relevant moderating effect on that relationship. Furthermore, dissatisfaction arising from passengers' interaction with the airline can have negative spillover effects on their satisfaction with the airport. We also found evidence of blame-attribution behavior, in which only delays of internal origin, such as failures in flight management, are significant, indicating that passengers overlook weather-related flight delays. Finally, the results suggest that an empirical specification that does not consider the latent psychosituational traits of passengers produces a relevant overestimation of the absolute effect of flight delays on passenger satisfaction."
http://arxiv.org/abs/2403.07019v1,Reasons behind the Water Crisis and its Potential Health Outcomes,2024-03-09 19:06:39+00:00,"['Md. Galib Ishraq Emran', 'Rhidi Barma', 'Akram Hussain Khan', 'Mrinmoy Roy']",econ.GN,"Globally, the water crisis has become a significant problem that affects developing and industrialized nations. Water shortage can harm public health by increasing the chance of contracting water-borne diseases, dehydration, and malnutrition. This study aims to examine the causes of the water problem and its likely effects on human health. The study scrutinizes the reasons behind the water crisis, including population increase, climate change, and inefficient water management techniques. The results of a lack of water on human health, such as the spread of infectious diseases, a higher risk of starvation and dehydration, and psychological stress, are also concealed in the study. The research further suggests several ways to deal with the water situation and lessen its potential outcomes on human health. These remedies include enhanced sanitation and hygiene procedures, water management, and conservation techniques like rainwater gathering and wastewater recycling."
http://arxiv.org/abs/2402.17142v1,Distributions of Posterior Quantiles via Matching,2024-02-27 02:10:00+00:00,"['Anton Kolotilin', 'Alexander Wolitzky']",econ.TH,"We offer a simple analysis of the problem of choosing a statistical experiment to optimize the induced distribution of posterior medians, or more generally $q$-quantiles for any $q \in (0,1)$. We show that all implementable distributions of the posterior $q$-quantile are implemented by a single experiment, the $q$-quantile matching experiment, which pools pairs of states across the $q$-quantile of the prior in a positively assortative manner, with weight $q$ on the lower state in each pair. A dense subset of implementable distributions of posterior $q$-quantiles can be uniquely implemented by perturbing the $q$-quantile matching experiment. A linear functional is optimized over distributions of posterior $q$-quantiles by taking the optimal selection from each set of $q$-quantiles induced by the $q$-quantile matching experiment. The $q$-quantile matching experiment is the only experiment that simultaneously implements all implementable distributions of the posterior $q$-quantile."
http://arxiv.org/abs/2402.18452v1,Social Learning with Intrinsic Preferences,2024-02-28 16:26:00+00:00,"['Fabian Dvorak', 'Urs Fischbacher']",econ.GN,"Despite strong evidence for peer effects, little is known about how individuals balance intrinsic preferences and social learning in different choice environments. Using a combination of experiments and discrete choice modeling, we show that intrinsic preferences and social learning jointly influence participants' decisions, but their relative importance varies across choice tasks and environments. Intrinsic preferences guide participants' decisions in a subjective choice task, while social learning determines participants' decisions in a task with an objectively correct solution. A choice environment in which people expect to be rewarded for their choices reinforces the influence of intrinsic preferences, whereas an environment in which people expect to be punished for their choices reinforces conformist social learning. We use simulations to discuss the implications of these findings for the polarization of behavior."
http://arxiv.org/abs/2402.16771v1,Wisdom and Foolishness of Noisy Matching Markets,2024-02-26 17:40:58+00:00,"['Kenny Peng', 'Nikhil Garg']",econ.TH,"We consider a many-to-one matching market where colleges share true preferences over students but make decisions using only independent noisy rankings. Each student has a true value $v$, but each college $c$ ranks the student according to an independently drawn estimated value $v + X_c$ for $X_c\sim \mathcal{D}.$ We ask a basic question about the resulting stable matching: How noisy is the set of matched students? Two striking effects can occur in large markets (i.e., with a continuum of students and a large number of colleges). When $\mathcal{D}$ is light-tailed, noise is fully attenuated: only the highest-value students are matched. When $\mathcal{D}$ is long-tailed, noise is fully amplified: students are matched uniformly at random. These results hold for any distribution of student preferences over colleges, and extend to when only subsets of colleges agree on true student valuations instead of the entire market. More broadly, our framework provides a tractable approach to analyze implications of imperfect preference formation in large markets."
http://arxiv.org/abs/2402.15965v1,Evolving E-commerce Logistics Planning- Integrating Embedded Technology and Ant Colony Algorithm for Enhanced Efficiency,2024-02-25 02:57:53+00:00,['Lynn Huang'],econ.GN,"Amidst the era of networking, the e-commerce sector has undergone notable expansion, notably with the advent of Cross-border E-commerce (CBEC) in recent times. This growth trend persists, necessitating robust logistical frameworks to sustainably support operations. However, the current e-commerce logistics paradigm faces challenges in meeting evolving user demands, prompting a quest for innovative solutions. This research endeavors to address these complexities by undertaking a comprehensive analysis of CBEC logistics models and integrating embedded technology into logistical frameworks, resulting in the development of an advanced logistics tracking system. Moreover, employing the ant colony algorithm, the study conducts experimental investigations into optimizing logistics package distribution route planning. Noteworthy enhancements are observed in key metrics such as average delivery time, signaling the efficacy of this approach. In essence, this research offers a promising pathway towards optimizing logistics package distribution routes and bolstering package transportation efficiency within the CBEC domain."
http://arxiv.org/abs/2402.14939v1,"Evaluating the Financial Factors Influencing Maternal, Newborn, and Child Health in Africa",2024-02-22 19:50:54+00:00,"['Youssef Er-Rays', ""Meriem M'dioud""]",econ.GN,"The study investigated the impact of healthcare system efficiency on the delivery of maternal, newborn, and child services in Africa. Data Envelopment Analysis and Tobit regression were employed to assess the efficiency of 46 healthcare systems across the continent, utilizing the Variable Returns to Scale model with Input orientation to evaluate technical efficiency. The Tobit method was utilized to explore factors contributing to inefficiency, with inputs variables including hospital, physician, and paramedical staff, and outputs variables encompassing maternal, newborn, and child admissions, cesarean interventions, functional competency, and hospitalization days. Results revealed that only 26% of countries exhibited efficiency, highlighting a significant proportion of 74% with inefficiencies. Financial determinants such as current health expenditures, comprehensive coverage index, and current health expenditure per capita were found to have a negative impact on the efficiency of maternal-child services. These findings underscore a marginal deficiency in technical efficiency within Africa's healthcare systems, emphasizing the necessity for policymakers to reassess the roles of both human resources and financial dimensions in enhancing healthcare system performance."
http://arxiv.org/abs/2403.08230v1,A vicious cycle along busy bus corridors and how to abate it,2024-03-13 04:15:20+00:00,"['Minyu Shen', 'Weihua Gu', 'Michael J. Cassidy', 'Yongjie Lin', 'Wei Ni']",econ.GN,"We unveil that a previously-unreported vicious cycle can be created when bus queues form at curbside stops along a corridor. Buses caught in this cycle exhibit growing variation in headways as they travel from stop to stop. Bus (and patron) delays accumulate in like fashion and can grow large on long, busy corridors. We show that this damaging cycle can be abated in simple ways. Present solutions entail holding buses at a corridor entrance and releasing them as per various strategies proposed in the literature. We introduce a modest variant to the simplest of these strategies. It releases buses at headways that are slightly less than, or equal to, the scheduled values. It turns out that periodically releasing buses at slightly smaller headways can substantially reduce bus delays caused by holding so that benefits can more readily outweigh costs in corridors that contain a sufficient number of serial bus stops. The simple variant is shown to perform about as well as, or better than, other bus-holding strategies in terms of saving delays, and is more effective than other strategies in regularizing bus headways. We also show that grouping buses from across multiple lines and holding them by group can be effective when patrons have the flexibility to choose buses from across all lines in a group. Findings come by formulating select models of bus-corridor dynamics and using these to simulate part of the Bus Rapid Transit corridor in Guangzhou, China."
http://arxiv.org/abs/2403.10239v1,A Big Data Approach to Understand Sub-national Determinants of FDI in Africa,2024-03-15 12:12:54+00:00,"['A. Fronzetti Colladon', 'R. Vestrelli', 'S. Bait', 'M. M. Schiraldi']",cs.CL,"Various macroeconomic and institutional factors hinder FDI inflows, including corruption, trade openness, access to finance, and political instability. Existing research mostly focuses on country-level data, with limited exploration of firm-level data, especially in developing countries. Recognizing this gap, recent calls for research emphasize the need for qualitative data analysis to delve into FDI determinants, particularly at the regional level. This paper proposes a novel methodology, based on text mining and social network analysis, to get information from more than 167,000 online news articles to quantify regional-level (sub-national) attributes affecting FDI ownership in African companies. Our analysis extends information on obstacles to industrial development as mapped by the World Bank Enterprise Surveys. Findings suggest that regional (sub-national) structural and institutional characteristics can play an important role in determining foreign ownership."
http://arxiv.org/abs/2403.12694v1,"Performance, Knowledge Acquisition and Satisfaction in Self-selected Groups: Evidence from a Classroom Field Experiment",2024-03-19 12:51:45+00:00,"['Julius Düker', 'Alexander Rieber']",econ.GN,"We investigate how to efficiently set up work groups to boost group productivity, individual satisfaction, and learning. Therefore, we conduct a natural field experiment in a compulsory undergraduate course and study differences between self-selected and randomly assigned groups. We find that self-selected groups perform significantly worse on group assignments. Yet, students in self-selected groups learn more and are more satisfied than those in randomly assigned groups. The effect of allowing students to pick group members dominates the effect of different group compositions in self-selected groups: When controlling for the skill, gender, and home region composition of groups, the differences between self-selected and randomly formed groups persist almost unaltered. The distribution of GitHub commits per group reveals that the better average performance of randomly assigned groups is mainly driven by highly skilled individuals distributed over more groups due to the assignment mechanism. Moreover, these highly skilled individuals contribute more to the group in randomly formed groups. We argue that this mechanism explains why self-selected groups perform worse on the projects but acquire more knowledge than randomly formed groups. These findings are relevant for setting up workgroups in academic, business, and governmental organizations when tasks are not constrained to the skill set of specific individuals."
http://arxiv.org/abs/2403.13388v1,Optimal VPPI strategy under Omega ratio with stochastic benchmark,2024-03-20 08:29:57+00:00,"['Guohui Guan', 'Lin He', 'Zongxia Liang', 'Litian Zhang']",econ.GN,"This paper studies a variable proportion portfolio insurance (VPPI) strategy. The objective is to determine the risk multiplier by maximizing the extended Omega ratio of the investor's cushion, using a binary stochastic benchmark. When the stock index declines, investors aim to maintain the minimum guarantee. Conversely, when the stock index rises, investors seek to track some excess returns. The optimization problem involves the combination of a non-concave objective function with a stochastic benchmark, which is effectively solved based on the stochastic version of concavification technique. We derive semi-analytical solutions for the optimal risk multiplier, and the value functions are categorized into three distinct cases. Intriguingly, the classification criteria are determined by the relationship between the optimal risky multiplier in Zieling et al. (2014 and the value of 1. Simulation results confirm the effectiveness of the VPPI strategy when applied to real market data calibrations."
http://arxiv.org/abs/2403.06344v2,Can One Hear the Shape of a Decision Problem?,2024-03-10 23:54:44+00:00,['Mark Whitmeyer'],econ.TH,"We explore the connection between an agent's decision problem and her ranking of information structures. We find that a finite amount of ordinal data on the agent's ranking of experiments is enough to identify her (finite) set of undominated actions (up to relabeling and duplication) and the beliefs rendering each such action optimal. An additional smattering of cardinal data, comparing the relative value to the agent of finitely many pairs of experiments, identifies her utility function up to an action-independent payoff."
http://arxiv.org/abs/2403.05671v2,Investigating Changes of Water Quality in Reservoirs based on Flood and Inflow Fluctuations,2024-03-08 20:58:39+00:00,"['Shabnam Salehi', 'Mojtaba Ardestani']",econ.GN,"Water temperature and dissolved oxygen are essential indicators of water quality and ecosystem sustainability. Lately, heavy rainfalls are happening frequently and forcefully affecting the thermal structure and mixing layers in depth by sharply increasing the volume of inflow entitled flash flood. It can occur by sudden intense precipitation and develop within minutes or hours. Because of heavy debris load and speedy water, this phenomenon has remarkable effects on water quality. A higher flow during floods may worsens water quality at lakes and reservoirs that are thermally stratified (with separate density layers) and decrease dissolved oxygen content. However, it is unclear how well these parameters represent the response of lakes to changes in volume discharge. To address this question, researchers simulate the thermal structure in two stratified reservoirs, considering the Rajae reservoir as a representative reservoir in the north of Iran and Minab reservoir in the south. In this study, the model realistically represented variations of dissolved oxygen and temperature of dams Lake response to flash floods. The model performance was evaluated using observed data from stations on the dams lake. In this case, the inflow charge considered in a 10-day flash flood from April 6th to April 16th during the yearly normal flow. The complete mixture in a part of the thermal structure has been proved in Rajaee reservoir. The nonpermanent impact of the massive inflow of storm runoff caused an increase in oxygen-consuming, leading to a severe decrease in dissolved oxygen on epilimnion and metalimnion. The situation in Minab reservoir was relatively different from Rajae reservoir. The inflow changes not only cause mixture but also help expanding stratification."
http://arxiv.org/abs/2402.08387v1,Portfolio Optimization under Transaction Costs with Recursive Preferences,2024-02-13 11:26:17+00:00,"['Martin Herdegen', 'David Hobson', 'Alex S. L. Tse']",econ.GN,"The Merton investment-consumption problem is fundamental, both in the field of finance, and in stochastic control. An important extension of the problem adds transaction costs, which is highly relevant from a financial perspective but also challenging from a control perspective because the solution now involves singular control. A further significant extension takes us from additive utility to stochastic differential utility (SDU), which allows time preferences and risk preferences to be disentangled.
  In this paper, we study this extended version of the Merton problem with proportional transaction costs and Epstein-Zin SDU. We fully characterise all parameter combinations for which the problem is well posed (which may depend on the level of transaction costs) and provide a full verification argument that relies on no additional technical assumptions and uses primal methods only. The case with SDU requires new mathematical techniques as duality methods break down.
  Even in the special case of (additive) power utility, our arguments are significantly simpler, more elegant and more far-reaching than the ones in the extant literature. This means that we can easily analyse aspects of the problem which previously have been very challenging, including comparative statics, boundary cases which heretofore have required separate treatment and the situation beyond the small transaction cost regime. A key and novel idea is to parametrise consumption and the value function in terms of the shadow fraction of wealth, which may be of much wider applicability."
http://arxiv.org/abs/2402.08396v1,Domestic Competitive Balance and International Success: The Case of The Football Industry,2024-02-13 11:43:11+00:00,"['Juan D. Moreno-Ternero', 'Tim Pawlowski', 'Shlomo Weber']",econ.TH,"This paper examines the interdependence of international success and competitive balance of domestic sports competitions. More specifically, we apply the notion of the Herfindahl-Hirschman index to examine the effect of international rewards on distortion of competitive balance in domestic competitions and derive conditions under which the level of domestic competitive balance raises or falls. Our results yield interesting policy implications for the regulation of prize schemes in international competitions."
http://arxiv.org/abs/2402.08547v2,"Dueling Over Dessert, Mastering the Art of Repeated Cake Cutting",2024-02-13 15:53:09+00:00,"['Simina Brânzei', 'MohammadTaghi Hajiaghayi', 'Reed Phillips', 'Suho Shin', 'Kun Wang']",cs.GT,"We consider the setting of repeated fair division between two players, denoted Alice and Bob, with private valuations over a cake. In each round, a new cake arrives, which is identical to the ones in previous rounds. Alice cuts the cake at a point of her choice, while Bob chooses the left piece or the right piece, leaving the remainder for Alice. We consider two versions: sequential, where Bob observes Alice's cut point before choosing left/right, and simultaneous, where he only observes her cut point after making his choice. The simultaneous version was first considered by Aumann and Maschler (1995).
  We observe that if Bob is almost myopic and chooses his favorite piece too often, then he can be systematically exploited by Alice through a strategy akin to a binary search. This strategy allows Alice to approximate Bob's preferences with increasing precision, thereby securing a disproportionate share of the resource over time.
  We analyze the limits of how much a player can exploit the other one and show that fair utility profiles are in fact achievable. Specifically, the players can enforce the equitable utility profile of $(1/2, 1/2)$ in the limit on every trajectory of play, by keeping the other player's utility to approximately $1/2$ on average while guaranteeing they themselves get at least approximately $1/2$ on average. We show this theorem using a connection with Blackwell approachability.
  Finally, we analyze a natural dynamic known as fictitious play, where players best respond to the empirical distribution of the other player. We show that fictitious play converges to the equitable utility profile of $(1/2, 1/2)$ at a rate of $O(1/\sqrt{T})$."
http://arxiv.org/abs/2402.08564v1,Barriers to Collusion-resistant Transaction Fee Mechanisms,2024-02-13 16:05:42+00:00,"['Yotam Gafni', 'Aviv Yaish']",cs.GT,"To allocate transactions to blocks, cryptocurrencies use an auction-like transaction fee mechanism (TFM). A conjecture of Roughgarden [44] asks whether there is a TFM that is incentive compatible for both the users and the miner, and is also resistant to off-chain agreements (OCAs) between these parties, a collusion notion that captures the ability of users and the miner to jointly deviate for profit. The work of Chung and Shi [12] tackles the problem using the different collusion resistance notion of side-channel proofness (SCP), and shows an impossibility given this notion. We show that OCA-proofness and SCP are different, with SCP being strictly stronger. We then fully characterize the intersection of deterministic dominant strategy incentive-compatible (DSIC) and OCA-proof mechanisms, as well as deterministic MMIC and OCA-proof ones, and use this characterization to show that only the trivial mechanism is DSIC, myopic miner incentive-compatible (MMIC) and OCA-proof. We also show that a randomized mechanism can be at most 0.842-efficient in the worst case, and that the impossibility of a non-trivial DSIC, MMIC and OCA-proof extends to a couple of natural classes of randomized mechanisms."
http://arxiv.org/abs/2401.05517v1,On Efficient Inference of Causal Effects with Multiple Mediators,2024-01-10 19:28:01+00:00,"['Haoyu Wei', 'Hengrui Cai', 'Chengchun Shi', 'Rui Song']",stat.ME,"This paper provides robust estimators and efficient inference of causal effects involving multiple interacting mediators. Most existing works either impose a linear model assumption among the mediators or are restricted to handle conditionally independent mediators given the exposure. To overcome these limitations, we define causal and individual mediation effects in a general setting, and employ a semiparametric framework to develop quadruply robust estimators for these causal effects. We further establish the asymptotic normality of the proposed estimators and prove their local semiparametric efficiencies. The proposed method is empirically validated via simulated and real datasets concerning psychiatric disorders in trauma survivors."
http://arxiv.org/abs/2401.04573v1,Opportunities to upgrade the scientific disciplines space,2024-01-09 14:19:42+00:00,"['Nestor Gandelman', 'Osiris Jorge Parcero', 'Flavia Roldan']",econ.GN,"Knowledge generated in a given scientific domain may spill over into other close scientific disciplines, thereby improving performance. Using bibliometric data from the SCImago database drawn from a sample of 174 countries, we implement a measure of proximity based on revealed comparative advantage (RCA). Our estimates show that proximity between disciplines positively and significantly affects the RCA growth rate. This impact is larger on disciplines that currently do not have RCA."
http://arxiv.org/abs/2401.05183v1,Can unions impose costs on employers in education strikes? Evidence from pension disputes in UK universities,2024-01-10 14:32:55+00:00,"['Nils Braakmann', 'Barbara Eberth']",econ.GN,"The impact of strikes in educational institutions, specifically universities, on employers remains understudied. This paper investigates the impact of education strikes in UK universities from 2018 to 2022, primarily due to pension disputes. Using data from the Guardian University Guide and the 2014 and 2021 Research Excellence Frameworks and leveraging difference-in-differences and regression discontinuity approaches, our findings suggest significant declines in several student related outcomes, such as student satisfaction, and a more mixed picture for student attainment and research performance. These results highlight the substantial, albeit indirect, cost unions can impose on university employers during strikes."
http://arxiv.org/abs/2401.17047v1,Determinants of well-being,2024-01-30 14:28:59+00:00,"['Cristina Pereira', 'Hermínia Gonçalves', 'Teresa Sequeira']",econ.GN,"Traditionally, European social policies have focused on material well-being and social justice, neglecting subjective indicators. This review systematically examines the scientific understanding of well-being, its indicators, and its relationship with governance. It suggests that political systems and institutions significantly impact well-being, and that subjective indicators should be incorporated into public policy decisions. The findings advocate for a more holistic approach to well-being measurement, encompassing both objective and subjective dimensions."
http://arxiv.org/abs/2402.08108v1,Finding Moving-Band Statistical Arbitrages via Convex-Concave Optimization,2024-02-12 22:56:16+00:00,"['Kasper Johansson', 'Thomas Schmelzer', 'Stephen Boyd']",econ.EM,"We propose a new method for finding statistical arbitrages that can contain more assets than just the traditional pair. We formulate the problem as seeking a portfolio with the highest volatility, subject to its price remaining in a band and a leverage limit. This optimization problem is not convex, but can be approximately solved using the convex-concave procedure, a specific sequential convex programming method. We show how the method generalizes to finding moving-band statistical arbitrages, where the price band midpoint varies over time."
http://arxiv.org/abs/2402.07124v1,Econometric analysis to estimate the impact of holidays on airfares,2024-02-11 08:31:41+00:00,"['Helena Povoa', 'Alessandro V. M. Oliveira']",econ.GN,"The number of air transportation passengers during the holidays in Brazil has grown notably since the late nineties. One of the reasons is greater competition in airfares made possible by economic liberalization. This paper presents an econometric model of airline pricing aiming at estimating the impacts of holiday periods on fares, with special emphasis on three-day holiday events. It makes use of a database with daily collected data from the internet between 2008 and 2010 for the major Brazilian city, Sao Paulo. The econometric panel data model employs a two-way error components ""within"" estimator, controlling for airline/airport-pair fixed effect along with quotation and departure months effects. The decomposition of time effects between quotation and departure month effects is the main methodological contribution of the paper. Results allow for a comparative analysis of the performance of Sao Paulo's downtown and international airports - respectively, Congonhas (CGH), and Guarulhos (GRU) airports. As a result, the price of tickets bought 60 days in advance for flights with two stops leaving from the downtown airport fell by most."
http://arxiv.org/abs/2402.07125v1,Intermodal competition in the Brazilian interstate travel market,2024-02-11 08:35:55+00:00,"['Frederico A. Turolla', 'Moises D. Vassallo', 'Alessandro V. M. Oliveira']",econ.GN,"This paper presents a test of intermodal interaction between coaches and airlines in Brazil in order to check for the efficacy of recent liberalization measures designed to promote competition in both industries. Interstate travel service in the country is heavily provided by coaches, and the system is fully operated by the private sector under public delegation through permits and authorizations. Agency-based regulation was introduced in 2002 along with a price cap regime aimed at enhancing the flexibility to change fares in response to demand and cost conditions. By making use of a reaction function-based model of coach operators' pricing decisions in the interstate travel market, we then estimate the sensitivity of the changes in coach fares to the changes in airline fares in a simultaneous-equation framework. Intermodal interaction among coach operators and airlines is found to be highly significant and probably due to the competition for a small but increasing set of premium, quality-sensitive, coach passengers."
http://arxiv.org/abs/2402.11072v1,Awareness of self-control,2024-02-16 20:57:27+00:00,"['Mohammad Mehdi Mousavi', 'Mahdi Kohan Sefidi', 'Shirin Allahyarkhani']",econ.GN,Economists modeled self-control problems in decisions of people with the time-inconsistence preferences model. They argued that the source of self-control problems could be uncertainty and temptation. This paper uses an experimental test offered to individuals instantaneous reward and future rewards to measure awareness of self-control problems in a tempting condition and also measure the effect of commitment and flexibility cost on their welfare. The quasi-hyperbolic discounting model with time discount factor and present bias at the same time was used for making a model for measuring awareness and choice reversal conditions. The test showed 66% awareness of self-control (partially naive behaviors) in individuals. The welfare implication for individuals increased with commitment and flexibility costs. The result can be useful in marketing and policy-making fields design precisely offers for customers and society.
http://arxiv.org/abs/2402.11881v1,Ironing allocations,2024-02-19 06:41:55+00:00,['Filip Tokarski'],econ.TH,"I propose a new approach to solving standard screening problems when the monotonicity constraint binds. A simple geometric argument shows that when virtual values are quasi-concave, the optimal allocation can be found by appropriately truncating the solution to the relaxed problem. I provide a simple algorithm for finding this optimal truncation when virtual values are concave."
http://arxiv.org/abs/2402.13278v1,Determinantes do planejamento estratégico da rede de uma companhia aérea,2024-02-17 20:22:22+00:00,"['Bruno F. Oliveira', 'Alessandro V. M. Oliveira']",econ.GN,"This work focuses on trying to understand how the construction of an airline's network is made. For this purpose, the case of Azul was studied, investigating which and how factors affect the decision of this airline to enter domestic routes, in addition to analyzing how the merger of Azul with the regional airline Trip affected the company's network planning. For this, an academic study was conducted using an econometric model to understand the airline's entry model. The results show that Azul's business model is based on connecting new destinations, not yet served by its competitors, to one of its hubs, and consistently avoiding routes or airports dominated by other airlines. Regarding the effects of the merger, the results suggest that Azul moved away from its original entry model, based on JetBlue, to a model more oriented towards regional aviation, entering shorter routes and regional airports."
http://arxiv.org/abs/2402.13439v1,"Estimating Demand for Lamb, Beef, Pork, and Poultry in Canada",2024-02-21 00:16:08+00:00,['Zakary Rodrigue Diakité'],econ.GN,"This paper investigates the demand for lamb, beef, pork, and poultry in Canada, both at the national level and in disaggregated provinces, to identify meat consumption patterns in different provinces. Meat consumption plays a significant role in Canada's economy and is an important source of calories for the population. However, meat demand faces several consumption challenges due to logistic constraints, as a significant portion of the supply is imported from other countries. Therefore, there is a need for a better understanding of the causal relationships underlying lamb, beef, pork, and poultry consumption in Canada. Until recently, there have been no attempts to estimate meat consumption at the provincial level in Canada. Different Almost Ideal Demand System (AIDS) models have been applied for testing specifications to circumvent several econometric and theoretical problems. In particular, generalized AIDS and its Quadratic extension QUAIDS methods have been estimated across each province using the Iterative Linear Least Squares Estimator (ILLE) estimation Method. Weekly retail meat consumption price and quantity data from 2019 to 2022 have been used for Canada and for each province namely Quebec, Maritime provinces (New Brunswick, Nova Scotia, and Prince Edward Island), Ontario, total West (Yukon, Northwest Territory and Nunavut), Alberta, Manitoba-Saskatchewan and Manitoba as well as British Columbia. Consistent coefficients and demand elasticities estimates reveal patterns of substitution and/or complementarity between the four categories of meat. Meat consumption patterns differ across each province. Results show that the demand for the four categories of meat is responsive to price changes. Overall, lamb expenditure was found to be elastic and thus considered a luxury good during the study period, while the other three categories are considered normal goods across Canada."
http://arxiv.org/abs/2401.17334v1,Efficient estimation of parameters in marginals in semiparametric multivariate models,2024-01-29 22:15:08+00:00,"['Ivan Medovikov', 'Valentyn Panchenko', 'Artem Prokhorov']",econ.GN,"We consider a general multivariate model where univariate marginal distributions are known up to a parameter vector and we are interested in estimating that parameter vector without specifying the joint distribution, except for the marginals. If we assume independence between the marginals and maximize the resulting quasi-likelihood, we obtain a consistent but inefficient QMLE estimator. If we assume a parametric copula (other than independence) we obtain a full MLE, which is efficient but only under a correct copula specification and may be biased if the copula is misspecified. Instead we propose a sieve MLE estimator (SMLE) which improves over QMLE but does not have the drawbacks of full MLE. We model the unknown part of the joint distribution using the Bernstein-Kantorovich polynomial copula and assess the resulting improvement over QMLE and over misspecified FMLE in terms of relative efficiency and robustness. We derive the asymptotic distribution of the new estimator and show that it reaches the relevant semiparametric efficiency bound. Simulations suggest that the sieve MLE can be almost as efficient as FMLE relative to QMLE provided there is enough dependence between the marginals. We demonstrate practical value of the new estimator with several applications. First, we apply SMLE in an insurance context where we build a flexible semi-parametric claim loss model for a scenario where one of the variables is censored. As in simulations, the use of SMLE leads to tighter parameter estimates. Next, we consider financial risk management examples and show how the use of SMLE leads to superior Value-at-Risk predictions. The paper comes with an online archive which contains all codes and datasets."
http://arxiv.org/abs/2402.13580v2,Mechanism Design with Sequential-Move Games: Revelation Principle,2024-02-21 07:18:37+00:00,['Siyang Xiong'],econ.TH,"Traditionally, mechanism design focuses on simultaneous-move games (e.g., Myerson (1981)). In this paper, we study mechanism design with sequential-move games, and provide two results on revelation principles for general solution concepts (e.g., perfect Bayesian equilibrium, obvious dominance, strong-obvious dominance). First, if a solution concept is additive, implementation in sequential-move games is equivalent to implementation in simultaneous-move games. Second, for any solution concept \r{ho} and any social choice function f, we identify a canonical operator γ^{(\r{ho},f)}, which is defined on primitives. We prove that, if \r{ho} is monotonic, f can be implemented by a sequential-move game if and only if γ^{(\r{ho},f)} is achievable, which translates a complicated mechanism design problem into checking some conditions defined on primitives. Most of the existing solution concepts are either additive or monotonic."
http://arxiv.org/abs/2402.13177v1,The Ebb and Flow of Brand Loyalty: A 28-Year Bibliometric and Content Analysis,2024-02-20 17:40:59+00:00,"['Azin Yazdi', 'Sunder Ramachandran', 'Hoda Mohsenifard', 'Khaled Nawaser', 'Faraz Sasani', 'Behrooz Gharleghi']",econ.GN,"Business research is facing the challenge of scattered knowledge, particularly in the realm of brand loyalty (BL). Although literature reviews on BL exist, they predominantly concentrate on the pre-sent state, neglecting future trends. Therefore, a comprehensive review is imperative to ascertain emerging trends in BL This study employs a bibliometric approach, analyzing 1,468 papers from the Scopus database. Various tools including R software, VOS viewer software, and Publish or Perish are utilized. The aim is to portray the knowledge map, explore the publication years, identify the top authors and their co-occurrence, reliable documents, institutions, subjects, research hotspots, and pioneering countries and universities in the study of BL. The qualitative section of this research identifies gaps and emerging trends in BL through Word Cloud charts, word growth analysis, and a review of highly cited articles from the past four years. Results showed that highly cited articles mention topics such as brand love, consumer-brand identification, and social networks and the U.S. had the most productions in this field. Besides, most citations were related to Keller with 1,173 citations. Furthermore, in the qualitative section, social networks and brand experiences were found to be of interest to researchers in the field. Finally, by introducing the antecedents and consequences of BL, the gaps and emerging trends in BL were identified, so as to present the di-rection of future research in this area."
http://arxiv.org/abs/2402.13789v1,The seasonality of air ticket prices before and after the pandemic,2024-02-21 13:18:14+00:00,['Alessandro V. M. Oliveira'],econ.GN,"This study investigates price seasonality in the Brazilian air transport industry, emphasizing the impact of the COVID-19 pandemic on domestic airline pricing strategies. Given potential shifts in demand patterns following the global health crisis, this study explores possible long-term structural changes in the seasonality of Brazilian airfare. We analyze an open dataset of domestic city pairs from 2013 to 2023, employing an econometric model developed using Stata software. Our findings indicate alterations in seasonal patterns and long-term trends in the post-pandemic era. These changes underscore potential shifts in the composition of leisure and business travelers, along with the cost pressures faced by airlines."
http://arxiv.org/abs/2402.14003v2,Multidimensional Signaling with a Resource Constraint,2024-02-21 18:41:15+00:00,"['Seungjin Han', 'Alex Sam']",econ.TH,"We study multidimensional signaling (cognitive/non-cognitive) as a sender's portfolio choice with a resource constraint. We establish the existence of a unique monotone D1 equilibrium where the cognitive (non-cognitive) signal increases (decreases) in sender type and the sum of the two increases in sender type. The equilibrium is characterized by two threshold sender types. The low threshold is one where a kink occurs in signaling. The constraint is binding only for sender types above it. The high threshold is the other one, above which all types spend all the resources in cognitive signal with pooling and discontinuity on the top."
http://arxiv.org/abs/2402.12404v1,Estudos de cenários de implantação de um imposto ambiental no transporte aéreo no Brasil,2024-02-17 20:22:20+00:00,"['Carolina B. Resende', 'Alessandro V. M. Oliveira']",econ.GN,"In recent years, the topic of global warming and greenhouse gas emissions has been increasingly in the media. This theme has raised various flags, showing concern for the future and seeking alternatives for a more sustainable life over the years. When studying greenhouse gas emissions, one of the main emitters of such gases is the burning of fuels, in which transport vehicles that depend on gas and oil are included. In this respect, air transport through aircraft is one of the sources emitting such gases. Aiming to reduce gas emissions, one of the ways to do this is by reducing fuel consumption; for this, aircraft would have to be more efficient, or the offer and demand for flights would be reduced. However, what if aircraft fuel consumption were taxed? What would be the effects of this on air transport? Could this be one of the alternatives to reduce emissions? To understand this relationship between taxation and a possible reduction in fuel consumption, a study was developed by the Aeronautics Institute of Technology (ITA), using an econometric model to understand how demand would be affected by changes in airfares. In addition, a simulation of possible environmental taxes on the values of air tickets was carried out to analyze the demand response and to get an idea if this taxation would really solve the emission problems."
http://arxiv.org/abs/2402.12487v1,Explaining the emergence of land-use frontiers,2024-02-19 19:50:29+00:00,"['Patrick Meyfroidt', 'Dilini Abeygunawardane', 'Matthias Baumann', 'Adia Bey', 'Ana Buchadas', 'Cristina Chiarella', 'Victoria Junquera', 'Angela Kronenburg García', 'Tobias Kuemmerle', 'Yann le Polain de Waroux', 'Eduardo Oliveira', 'Michelle Picoli', 'Siyu Qin', 'Virginia Rodriguez García', 'Philippe Rufin']",econ.GN,"Land use expansion is linked to major sustainability concerns including climate change, food security and biodiversity loss. This expansion is largely concentrated in so-called frontiers, defined here as places experiencing marked transformations due to rapid resource exploitation. Understanding the mechanisms shaping these frontiers is crucial for sustainability. Previous work focused mainly on explaining how active frontiers advance, in particular into tropical forests. Comparatively, our understanding of how frontiers emerge in territories considered marginal in terms of agricultural productivity and global market integration remains weak. We synthesize conceptual tools explaining resource and land-use frontiers, including theories of land rent and agglomeration economies, of frontiers as successive waves, spaces of territorialization, friction, and opportunities, anticipation and expectation. We then propose a new theory of frontier emergence, which identifies exogenous pushes, legacies of past waves, and actors anticipations as key mechanisms by which frontiers emerge. Processes of abnormal rent creation and capture and the built-up of agglomeration economies then constitute key mechanisms sustaining active frontiers. Finally, we discuss five implications for the governance of frontiers for sustainability. Our theory focuses on agriculture and deforestation frontiers in the tropics, but can be inspirational for other frontier processes including for extractive resources, such as minerals."
http://arxiv.org/abs/2402.01005v1,The prices of renewable commodities: A robust stationarity analysis,2024-02-01 20:38:25+00:00,"['Manuel Landajo', 'María José Presno']",econ.EM,"This paper addresses the problem of testing for persistence in the effects of the shocks affecting the prices of renewable commodities, which have potential implications on stabilization policies and economic forecasting, among other areas. A robust methodology is employed that enables the determination of the potential presence and number of instant/gradual structural changes in the series, stationarity testing conditional on the number of changes detected, and the detection of change points. This procedure is applied to the annual real prices of eighteen renewable commodities over the period of 1900-2018. Results indicate that most of the series display non-linear features, including quadratic patterns and regime transitions that often coincide with well-known political and economic episodes. The conclusions of stationarity testing suggest that roughly half of the series are integrated. Stationarity fails to be rejected for grains, whereas most livestock and textile commodities do reject stationarity. Evidence is mixed in all soft commodities and tropical crops, where stationarity can be rejected in approximately half of the cases. The implication would be that for these commodities, stabilization schemes would not be recommended."
http://arxiv.org/abs/2402.01745v1,When and Where To Submit A Paper,2024-01-29 19:45:20+00:00,['Daniel Luo'],econ.TH,"What is the optimal order in which a researcher should submit their papers to journals of differing quality? I analyze a sequential search model without recall where the researcher's expected value from journal submission depends on the history of past submissions. Acceptances immediately terminate the search process and deliver some payoff, while rejections carry information about the paper's quality, affecting the researcher's belief in acceptance probability over future journals. When journal feedback does not change the paper's quality, the researcher's optimal strategy is monotone in their acceptance payoff. Submission costs distort the researcher's effective acceptance payoff, but maintain monotone optimality. If journals give feedback which can affect the paper's quality, such as through \textit{referee reports}, the search order can change drastically depending on the agent's prior belief about their paper's quality. However, I identify a set of \textit{assortative matched} conditions on feedback such that monotone strategies remain optimal whenever the agent's prior is sufficiently optimistic."
http://arxiv.org/abs/2402.03411v1,Bride Kidnapping and Informal Governance Institutions,2024-02-05 16:09:35+00:00,['Zachary Porreca'],econ.GN,"Bride kidnapping is a form of forced marriage in which a woman is taken against her will and coerced into accepting marriage with her captor. Post-Soviet Kyrgyzstan has seen a large increase in the prominence of this practice alongside a revitalization of traditional values and culture. As part of this resurgence of Kyrgyz identity and culture, the central government has formalized the authority of councils of elders called aksakals as an arbitrator for local dispute resolution -- guided by informal principles of tradition and cultural norm adherence. Bride kidnapping falls within the domain of aksakal authority. In this study, I leverage data from a nationally representative survey and specify a latent class nested logit model of mens' marriage modality choice to analyze the impacts that aksakal governance has on the decision to kidnap. Based on value assessment questions on the survey, men are assigned to a probability distribution over latent class membership. Utility function parameters for each potential marriage modality are estimated for each latent class of men. Results suggest that living under aksakal governance makes men 9% more likely to obtain a wife through bride capture, with men substituting kidnapping for choice marriage modalities such as elopement and standard love marriages."
http://arxiv.org/abs/2402.04165v1,Monthly GDP nowcasting with Machine Learning and Unstructured Data,2024-02-06 17:21:39+00:00,"['Juan Tenorio', 'Wilder Perez']",econ.EM,"In the dynamic landscape of continuous change, Machine Learning (ML) ""nowcasting"" models offer a distinct advantage for informed decision-making in both public and private sectors. This study introduces ML-based GDP growth projection models for monthly rates in Peru, integrating structured macroeconomic indicators with high-frequency unstructured sentiment variables. Analyzing data from January 2007 to May 2023, encompassing 91 leading economic indicators, the study evaluates six ML algorithms to identify optimal predictors. Findings highlight the superior predictive capability of ML models using unstructured data, particularly Gradient Boosting Machine, LASSO, and Elastic Net, exhibiting a 20% to 25% reduction in prediction errors compared to traditional AR and Dynamic Factor Models (DFM). This enhanced performance is attributed to better handling of data of ML models in high-uncertainty periods, such as economic crises."
http://arxiv.org/abs/2402.04166v1,Mind the Gap: Securely modeling cyber risk based on security deviations from a peer group,2024-02-06 17:22:45+00:00,"['Taylor Reynolds', 'Sarah Scheffler', 'Daniel J. Weitzner', 'Angelina Wu']",cs.CR,"There are two strategic and longstanding questions about cyber risk that organizations largely have been unable to answer: What is an organization's estimated risk exposure and how does its security compare with peers? Answering both requires industry-wide data on security posture, incidents, and losses that, until recently, have been too sensitive for organizations to share. Now, privacy enhancing technologies (PETs) such as cryptographic computing can enable the secure computation of aggregate cyber risk metrics from a peer group of organizations while leaving sensitive input data undisclosed. As these new aggregate data become available, analysts need ways to integrate them into cyber risk models that can produce more reliable risk assessments and allow comparison to a peer group. This paper proposes a new framework for benchmarking cyber posture against peers and estimating cyber risk within specific economic sectors using the new variables emerging from secure computations. We introduce a new top-line variable called the Defense Gap Index representing the weighted security gap between an organization and its peers that can be used to forecast an organization's own security risk based on historical industry data. We apply this approach in a specific sector using data collected from 25 large firms, in partnership with an industry ISAO, to build an industry risk model and provide tools back to participants to estimate their own risk exposure and privately compare their security posture with their peers."
http://arxiv.org/abs/2402.03800v1,Effects of carbon pricing and other climate policies on CO2 emissions,2024-02-06 08:37:19+00:00,"['Emanuel Kohlscheen', 'Richhild Moessner', 'Elod Takats']",econ.GN,"We provide ex-post empirical analysis of the effects of climate policies on carbon dioxide emissions at the aggregate national level. Our results are based on a comprehensive database of 121 countries. As climate policies we examine carbon taxes and emissions trading systems (ETS), as well as the overall stringency of climate policies. We use dynamic panel regressions, controlling for macroeconomic factors such as economic development, GDP growth, urbanisation, as well as the energy mix. We find that higher carbon taxes and prices of permits in ETS reduce carbon emissions. An increase in carbon taxes by $10 per ton of CO2 reduces CO2 emissions per capita by 1.3% in the short run and by 4.6% in the long run."
http://arxiv.org/abs/2402.05329v1,Selective linear segmentation for detecting relevant parameter changes,2024-02-08 00:10:05+00:00,"['Arnaud Dufays', 'Aristide Houndetoungan', 'Alain Coën']",econ.EM,"Change-point processes are one flexible approach to model long time series. We propose a method to uncover which model parameter truly vary when a change-point is detected. Given a set of breakpoints, we use a penalized likelihood approach to select the best set of parameters that changes over time and we prove that the penalty function leads to a consistent selection of the true model. Estimation is carried out via the deterministic annealing expectation-maximization algorithm. Our method accounts for model selection uncertainty and associates a probability to all the possible time-varying parameter specifications. Monte Carlo simulations highlight that the method works well for many time series models including heteroskedastic processes. For a sample of 14 Hedge funds (HF) strategies, using an asset based style pricing model, we shed light on the promising ability of our method to detect the time-varying dynamics of risk exposures as well as to forecast HF returns."
http://arxiv.org/abs/2403.02467v1,Applied Causal Inference Powered by ML and AI,2024-03-04 20:28:28+00:00,"['Victor Chernozhukov', 'Christian Hansen', 'Nathan Kallus', 'Martin Spindler', 'Vasilis Syrgkanis']",econ.EM,"An introduction to the emerging fusion of machine learning and causal inference. The book presents ideas from classical structural equation models (SEMs) and their modern AI equivalent, directed acyclical graphs (DAGs) and structural causal models (SCMs), and covers Double/Debiased Machine Learning methods to do inference in such models using modern predictive tools."
http://arxiv.org/abs/2402.11961v2,Optimal Design of Climate Disclosure Policies: Transparency versus Externality,2024-02-19 09:04:50+00:00,['Shangen Li'],econ.TH,"Does a more transparent climate disclosure policy induce lower emissions? This paper examines the welfare implications of transparency in climate disclosure regulation. Increased disclosure transparency could result in a larger equilibrium externality, but never leaves the firm worse off. Consequently, mandating full disclosure is no different from maximizing the firm's private benefit while disregarding the ensuing externality. Transparency beyond binary disclosure is necessary only when the firm holds private information about its incentives for emission reduction. I provide conditions under which focusing on threshold disclosure policies entails no loss of generality."
http://arxiv.org/abs/2401.15794v2,Regulation of Algorithmic Collusion,2024-01-28 22:47:10+00:00,"['Jason D. Hartline', 'Sheng Long', 'Chenhao Zhang']",cs.GT,"Consider sellers in a competitive market that use algorithms to adapt their prices from data that they collect. In such a context it is plausible that algorithms could arrive at prices that are higher than the competitive prices and this may benefit sellers at the expense of consumers (i.e., the buyers in the market). This paper gives a definition of plausible algorithmic non-collusion for pricing algorithms. The definition allows a regulator to empirically audit algorithms by applying a statistical test to the data that they collect. Algorithms that are good, i.e., approximately optimize prices to market conditions, can be augmented to contain the data sufficient to pass the audit. Algorithms that have colluded on, e.g., supra-competitive prices cannot pass the audit. The definition allows sellers to possess useful side information that may be correlated with supply and demand and could affect the prices used by good algorithms. The paper provides an analysis of the statistical complexity of such an audit, i.e., how much data is sufficient for the test of non-collusion to be accurate."
http://arxiv.org/abs/2401.11568v3,A Note on the Stability of Monotone Markov Chains,2024-01-21 19:10:41+00:00,['Bar Light'],math.PR,"This note studies monotone Markov chains, a subclass of Markov chains with extensive applications in operations research and economics. While the properties that ensure the global stability of these chains are well studied, their establishment often relies on the fulfillment of a certain splitting condition. We address the challenges of verifying the splitting condition by introducing simple, applicable conditions that ensure global stability. The simplicity of these conditions is demonstrated through various examples including autoregressive processes, portfolio allocation problems and resource allocation dynamics."
http://arxiv.org/abs/2402.11715v3,The Gerber-Shiu Expected Discounted Penalty Function: An Application to Poverty Trapping,2024-02-18 21:45:16+00:00,['José Miguel Flores-Contró'],econ.GN,"In this article, we consider a risk process to model the capital of a household. Our work focuses on the analysis of the trapping time of such a process, where trapping occurs when a household's capital level falls into the poverty area. A function analogous to the classical Gerber-Shiu function is introduced, which incorporates information on the trapping time, the capital surplus immediately before trapping and the capital deficit at trapping. We derive, under some assumptions, a model belonging to the family of generalised beta (GB) distributions that describes the distribution of the capital deficit at trapping given that trapping occurs. Affinities between the capital deficit at trapping and a class of poverty measures, known as the Foster-Greer-Thorbecke (FGT) index, are presented. The versatility of this model to estimate FGT indices is assessed using household microdata from Burkina Faso's Enquête Multisectorielle Continue (EMC) 2014."
http://arxiv.org/abs/2403.06150v1,Algorithmic Collusion and Price Discrimination: The Over-Usage of Data,2024-03-10 09:45:24+00:00,"['Zhang Xu', 'Mingsheng Zhang', 'Wei Zhao']",econ.GN,"As firms' pricing strategies increasingly rely on algorithms, two concerns have received much attention: algorithmic tacit collusion and price discrimination. This paper investigates the interaction between these two issues through simulations. In each period, a new buyer arrives with independently and identically distributed willingness to pay (WTP), and each firm, observing private signals about WTP, adopts Q-learning algorithms to set prices. We document two novel mechanisms that lead to collusive outcomes. Under asymmetric information, the algorithm with information advantage adopts a Bait-and-Restrained-Exploit strategy, surrendering profits on some signals by setting higher prices, while exploiting limited profits on the remaining signals by setting much lower prices. Under a symmetric information structure, competition on some signals facilitates convergence to supra-competitive prices on the remaining signals. Algorithms tend to collude more on signals with higher expected WTP. Both uncertainty and the lack of correlated signals exacerbate the degree of collusion, thereby reducing both consumer surplus and social welfare. A key implication is that the over-usage of data, both payoff-relevant and non-relevant, by AIs in competitive contexts will reduce the degree of collusion and consequently lead to a decline in industry profits."
http://arxiv.org/abs/2402.05024v4,Does the Use of Unusual Combinations of Datasets Contribute to Greater Scientific Impact?,2024-02-07 16:46:01+00:00,"['Yulin Yu', 'Daniel M. Romero']",cs.DL,"Scientific datasets play a crucial role in contemporary data-driven research, as they allow for the progress of science by facilitating the discovery of new patterns and phenomena. This mounting demand for empirical research raises important questions on how strategic data utilization in research projects can stimulate scientific advancement. In this study, we examine the hypothesis inspired by the recombination theory, which suggests that innovative combinations of existing knowledge, including the use of unusual combinations of datasets, can lead to high-impact discoveries. Focusing on social science, we investigate the scientific outcomes of such atypical data combinations in more than 30,000 publications that leverage over 5,000 datasets curated within one of the largest social science databases, ICPSR. This study offers four important insights. First, combining datasets, particularly those infrequently paired, significantly contributes to both scientific and broader impacts (e.g., dissemination to the general public). Second, infrequently paired datasets maintain a strong association with citation even after controlling for the atypicality of dataset topics. In contrast, the atypicality of dataset topics has a much smaller positive impact on citation counts. Third, smaller and less experienced research teams tend to use atypical combinations of datasets in research more frequently than their larger and more experienced counterparts. Lastly, despite the benefits of data combination, papers that amalgamate data remain infrequent. This finding suggests that the unconventional combination of datasets is an under-utilized but powerful strategy correlated with the scientific impact and broader dissemination of scientific discoveries"
http://arxiv.org/abs/2401.01080v2,Global analysis reveals persistent shortfalls and regional differences in availability of foods needed for health,2024-01-02 07:52:05+00:00,"['Leah Costlow', 'Anna Herforth', 'Timothy B. Sulser', 'Nicola Cenacchi', 'William A. Masters']",econ.GN,"Most people around the world still lack access to sufficient quantities of all food groups needed for an active and healthy life. This study traces historical and projected changes in global food systems toward alignment with the new Healthy Diet Basket (HDB) used by UN agencies and the World Bank to monitor the cost and affordability of healthy diets worldwide. Using the HDB as a standard to measure adequacy of national, regional and global supply-demand balances, we find substantial but inconsistent progress toward closer alignment with dietary guidelines, with large global shortfalls in fruits, vegetables, and legumes, nuts, and seeds, and large disparities among regions in use of animal source foods. Projections show that additional investments aimed at reducing chronic hunger would modestly accelerate improvements in adequacy where shortfalls are greatest, revealing the need for complementary investments to increase access to under-consumed food groups especially in low-income countries."
http://arxiv.org/abs/2402.18392v2,Unveiling the Potential of Robustness in Selecting Conditional Average Treatment Effect Estimators,2024-02-28 15:12:24+00:00,"['Yiyan Huang', 'Cheuk Hang Leung', 'Siyi Wang', 'Yijun Li', 'Qi Wu']",cs.LG,"The growing demand for personalized decision-making has led to a surge of interest in estimating the Conditional Average Treatment Effect (CATE). Various types of CATE estimators have been developed with advancements in machine learning and causal inference. However, selecting the desirable CATE estimator through a conventional model validation procedure remains impractical due to the absence of counterfactual outcomes in observational data. Existing approaches for CATE estimator selection, such as plug-in and pseudo-outcome metrics, face two challenges. First, they must determine the metric form and the underlying machine learning models for fitting nuisance parameters (e.g., outcome function, propensity function, and plug-in learner). Second, they lack a specific focus on selecting a robust CATE estimator. To address these challenges, this paper introduces a Distributionally Robust Metric (DRM) for CATE estimator selection. The proposed DRM is nuisance-free, eliminating the need to fit models for nuisance parameters, and it effectively prioritizes the selection of a distributionally robust CATE estimator. The experimental results validate the effectiveness of the DRM method in selecting CATE estimators that are robust to the distribution shift incurred by covariate shift and hidden confounders."
http://arxiv.org/abs/2403.00471v2,"Idiosyncratic Risk, Government Debt and Inflation",2024-03-01 11:57:29+00:00,['Matthias Hänsel'],econ.GN,"How does public debt matter for price stability? If it is useful for the private sector to insure idiosyncratic risk, even transitory government debt expansions can exert upward pressure on interest rates and create inflation. As I demonstrate using an analytically tractable model, this holds in the presence of an active Taylor rule and does not require the absence of future fiscal consolidation. Further analysis using a quantitative 2-asset HANK model reveals the magnitude of the mechanism to crucially depend on the structure of the asset market: under common assumptions, the interest rate effects of public debt are either overly strong or overly weak. After disciplining this aspect based on evidence regarding its long-term relationship with treasury returns, my framework indicates relevant short-run effects of public debt on inflation under active monetary policy: In particular, in the HANK model the mechanism can account for US inflation remaining elevated in 2023 and afterwards."
http://arxiv.org/abs/2403.08886v1,"Measuring the bioeconomy economically: exploring the connections between concepts, methods, data, indicators and their limitations",2024-03-13 18:14:32+00:00,"['Sebastián Leavy', 'Gabriela Allegretti', 'Elen Presotto', 'Marco Antonio Montoya', 'Edson Talamini']",econ.GN,"Despite its relevance, measuring the contributions of the bioeconomy to national economies remains an arduous task that faces limitations. Part of the difficulty is associated with the lack of a clear and widely accepted concept of the bioeconomy and moves on to the connections between methods, data and indicators. The present study aims to define the concepts of bioeconomy and to explore the connections between concepts, methods, data and indicators when measuring the bioeconomy economically, and the limitations involved in this process. The bioeconomy concepts were defined based on a literature review and a content analysis of 84 documents selected through snowballing procedures to find articles measuring 'how big is the bioeconomy?'. The content of the 84 documents was uploaded to the QDA Miner software and coded according to the bioeconomy concept, the methods or models used, the data sources accessed, the indicators calculated, and the limitations reported by the authors. The results of the occurrence and co-occurrence of the codes were extracted and analyzed statistically, indicating that the measurement of bioeconomy (i) need recognize and pursue the proposed concept of holistic bioeconomy; (ii) rarely considered aspects of holistic bioeconomy (3.5%); (iii) is primarily based on the concept of biomass-based bioeconomy (BmBB) (94%); (iv) the association with the concept of biosphere (BsBB) appeared in 26% of the studies; (v) the biotech-based bioeconomy (BtBB) was the least frequent (1.2%); (vi) there is a diversity of methods and models, but the most common are those traditionally used to measure macroeconomic activities, especially input-output models; (vii) depending on the prevailing methods, the data comes from various official statistical databases, such as national accounts and economic activity classification systems;..."
http://arxiv.org/abs/2401.03990v4,Identification with possibly invalid IVs,2024-01-08 16:11:22+00:00,"['Christophe Bruneel-Zupanc', 'Jad Beyhum']",econ.EM,"This paper proposes a novel identification strategy relying on quasi-instrumental variables (quasi-IVs). A quasi-IV is a relevant but possibly invalid IV because it is not exogenous or not excluded. We show that a variety of models with discrete or continuous endogenous treatment which are usually identified with an IV - quantile models with rank invariance, additive models with homogenous treatment effects, and local average treatment effect models - can be identified under the joint relevance of two complementary quasi-IVs instead. To achieve identification, we complement one excluded but possibly endogenous quasi-IV (e.g., ""relevant proxies"" such as lagged treatment choice) with one exogenous (conditional on the excluded quasi-IV) but possibly included quasi-IV (e.g., random assignment or exogenous market shocks). Our approach also holds if any of the two quasi-IVs turns out to be a valid IV. In practice, being able to address endogeneity with complementary quasi-IVs instead of IVs is convenient since there are many applications where quasi-IVs are more readily available. Difference-in-differences is a notable example: time is an exogenous quasi-IV while the group assignment acts as a complementary excluded quasi-IV."
http://arxiv.org/abs/2401.01804v2,Efficient Computation of Confidence Sets Using Classification on Equidistributed Grids,2024-01-03 16:04:14+00:00,['Lujie Zhou'],econ.EM,"Economic models produce moment inequalities, which can be used to form tests of the true parameters. Confidence sets (CS) of the true parameters are derived by inverting these tests. However, they often lack analytical expressions, necessitating a grid search to obtain the CS numerically by retaining the grid points that pass the test. When the statistic is not asymptotically pivotal, constructing the critical value for each grid point in the parameter space adds to the computational burden. In this paper, we convert the computational issue into a classification problem by using a support vector machine (SVM) classifier. Its decision function provides a faster and more systematic way of dividing the parameter space into two regions: inside vs. outside of the confidence set. We label those points in the CS as 1 and those outside as -1. Researchers can train the SVM classifier on a grid of manageable size and use it to determine whether points on denser grids are in the CS or not. We establish certain conditions for the grid so that there is a tuning that allows us to asymptotically reproduce the test in the CS. This means that in the limit, a point is classified as belonging to the confidence set if and only if it is labeled as 1 by the SVM."
http://arxiv.org/abs/2407.01533v1,Organizational transformation: The impact of servant leadership on work ethic culture with burnout as a mediating factor in the hospitality industry,2024-02-27 00:51:07+00:00,"['Darul Wiyono', 'Rinaldi Tanjung', 'Hedi Setiadi', 'Sri Marini', 'Yayan Sugiarto']",econ.GN,"Orientation: The study explores the connections among servant leadership, burnout, and work ethic culture in organizations. It aims to provide a detailed understanding of how servant leadership influences work ethic culture, especially by considering the role of burnout. Research Purpose: This study aims to understand how servant leadership influences work ethic culture and explore the mediating role of burnout in this relationship. Motivation for the Study: This study wants to fill gaps in our understanding of how servant leadership, burnout, and work ethic culture are connected. It seeks to add useful insights to what we already know from previous research. Research Approach/Design and Method: The study, using surveys and statistics, examines the links between servant leadership, burnout, and work ethic culture in 113 hotels in Bandung, Indonesia, with 339 participants. A 183-sample, chosen with a 0.05 margin of error, underwent SEM-PLS analysis using SmartPLS 3.0. Main Findings: The key findings underscore that servant leadership exerts a positive influence on work ethic culture, and burnout plays a pivotal mediating role in this dynamic. The results shed light on the intricate dynamics shaping organizational cultures. Practical/Managerial Implications: The findings aid organizations in forming supportive leadership policies, promoting employee well-being, and fostering ethical work culture. Managers can apply these insights to enhance leadership practices and reduce burnout impact. Contribution/Value-Add: This study clarifies the connection between servant leadership, burnout, and work ethic culture. The findings offer insights for future research and practical actions in organizational leadership."
http://arxiv.org/abs/2402.16580v2,Information-Enriched Selection of Stationary and Non-Stationary Autoregressions using the Adaptive Lasso,2024-02-26 14:04:24+00:00,"['Thilo Reinschlüssel', 'Martin C. Arnold']",stat.ME,"We propose a novel approach to elicit the weight of a potentially non-stationary regressor in the consistent and oracle-efficient estimation of autoregressive models using the adaptive Lasso. The enhanced weight builds on a statistic that exploits distinct orders in probability of the OLS estimator in time series regressions when the degree of integration differs. We provide theoretical results on the benefit of our approach for detecting stationarity when a tuning criterion selects the $\ell_1$ penalty parameter. Monte Carlo evidence shows that our proposal is superior to using OLS-based weights, as suggested by Kock [Econom. Theory, 32, 2016, 243-259]. We apply the modified estimator to model selection for German inflation rates after the introduction of the Euro. The results indicate that energy commodity price inflation and headline inflation are best described by stationary autoregressions."
http://arxiv.org/abs/2402.03894v3,Interpersonal trust: Asymptotic analysis of a stochastic coordination game with multi-agent learning,2024-02-06 11:02:01+00:00,"['Benedikt V. Meylahn', 'Arnoud V. den Boer', 'Michel Mandjes']",physics.soc-ph,"We study the interpersonal trust of a population of agents, asking whether chance may decide if a population ends up in a high trust or low trust state. We model this by a discrete time, random matching stochastic coordination game. Agents are endowed with an exponential smoothing learning rule about the behaviour of their neighbours. We find that, with probability one in the long run the whole population either always cooperates or always defects. By simulation we study the impact of the distributions of the payoffs in the game and of the exponential smoothing learning (memory of the agents). We find, that as the agent memory increases or as the size of the population increases, the actual dynamics start to resemble the expectation of the process. We conclude that it is indeed possible that different populations may converge upon high or low trust between its citizens simply by chance, though the game parameters (context of the society) may be quite telling."
http://arxiv.org/abs/2405.18434v1,Modeling the Feedback of AI Price Estimations on Actual Market Values,2024-03-13 03:44:13+00:00,"['Viorel Silaghi', 'Zobaida Alssadi', 'Ben Mathew', 'Majed Alotaibi', 'Ali Alqarni', 'Marius Silaghi']",econ.GN,"Public availability of Artificial Intelligence generated information can change the markets forever, and its factoring into economical dynamics may take economists by surprise, out-dating models and schools of thought. Real estate hyper-inflation is not a new phenomenon but its consistent and almost monotonous persistence over 12 years, coinciding with prominence of public estimation information from Zillow, a successful Mass Real Estate Estimator (MREE), could not escape unobserved. What we model is a repetitive theoretical game between the MREE and the home owners, where each player has secret information and expertise. If the intention is to keep housing affordable and maintain old American lifestyle with broad home-ownership, new challenges are defined. Simulations show that a simple restriction of MREE-style price estimation availability to opt-in properties may help partially reduce feedback loop by acting on its likely causes, as suggested by experimental simulation models. The conjecture that the MREE pressure on real estate inflation rate is correlated with the absolute MREE estimation errors, which is logically explainable, is then validated in simulations."
http://arxiv.org/abs/2402.15418v3,Reputational Algorithm Aversion,2024-02-23 16:28:55+00:00,['Gregory Weitzner'],econ.TH,"People are often reluctant to incorporate information produced by algorithms into their decisions, a phenomenon called ``algorithm aversion''. This paper shows how algorithm aversion arises when the choice to follow an algorithm conveys information about a human's ability. I develop a model in which workers make forecasts of an uncertain outcome based on their own private information and an algorithm's signal. Low-skill workers receive worse information than the algorithm and hence should always follow the algorithm's signal, while high-skill workers receive better information than the algorithm and should sometimes override it. However, due to reputational concerns, low-skill workers inefficiently override the algorithm to increase the likelihood they are perceived as high-skill. The model provides a fully rational microfoundation for algorithm aversion that aligns with the broad concern that AI systems will displace many types of workers."
http://arxiv.org/abs/2403.01585v2,Calibrating doubly-robust estimators with unbalanced treatment assignment,2024-03-03 18:40:11+00:00,['Daniele Ballinari'],econ.EM,"Machine learning methods, particularly the double machine learning (DML) estimator (Chernozhukov et al., 2018), are increasingly popular for the estimation of the average treatment effect (ATE). However, datasets often exhibit unbalanced treatment assignments where only a few observations are treated, leading to unstable propensity score estimations. We propose a simple extension of the DML estimator which undersamples data for propensity score modeling and calibrates scores to match the original distribution. The paper provides theoretical results showing that the estimator retains the DML estimator's asymptotic properties. A simulation study illustrates the finite sample performance of the estimator."
http://arxiv.org/abs/2403.17162v1,"Design Insights for Industrial CO2 Capture, Transport, and Storage Systems",2024-03-25 20:23:11+00:00,"['Tubagus Aryandi Gunawan', 'Lilianna Gittoes', 'Cecelia Isaac', 'Chris Greig', 'Eric Larson']",econ.GN,"We present design methods and insights for CO2 capture, transport, and storage systems for clusters of industrial facilities, with a case-study focus on the state of Louisiana. Our analytical framework includes: (1) evaluating the scale and concentration of capturable CO2 emissions at individual facilities for the purpose of estimating the cost of CO2 capture retrofits, (2) a screening method to identify potential CO2 storage sites and estimate their storage capacities, injectivities, and costs; and (3) an approach for cost-minimized design of pipeline infrastructure connecting CO2 capture plants with storage sites that considers land use patterns, existing rights-of-way, demographics, and a variety of social and environmental justice factors. In applying our framework to Louisiana, we estimate up to 50 million tCO2/y of industrial emissions (out of today's total emissions of 130 MtCO2/y) can be captured at under 100 USD/tCO2, and up to 100 MtCO2/y at under 120 USD/tCO2. We identified 98 potential storage sites with estimated aggregate total injectivity between 330 and 730 MtCO2/yr and storage costs ranging from 8 to 17 USD/tCO2. We find dramatic reductions in the aggregate pipeline length and CO2 transport cost per tonne when groups of capture plants share pipeline infrastructure rather than build dedicated single-user pipelines. Smaller facilities (emitting less than 1 MtCO2/y), which account for a quarter of Louisiana's industrial emissions, see the largest transport cost benefits from sharing of infrastructure. Pipeline routes designed to avoid disadvantaged communities (social and environmental justice) so as not to reinforce historical practices of disenfranchisement involve only modestly higher pipeline lengths and costs."
http://arxiv.org/abs/2403.07152v2,Success functions in large contests,2024-03-11 20:49:46+00:00,"['Yaron Azrieli', 'Christopher P. Chambers']",econ.TH,"We consider contests with a large set (continuum) of participants and axiomatize contest success functions that arise when performance is composed of both effort and a random element, and when winners are those whose performance exceeds a cutoff determined by a market clearing condition. A co-monotonicity property is essentially all that is needed for a representation in the general case, but significantly stronger conditions must hold to obtain an additive structure. We illustrate the usefulness of this framework by revisiting some of the classic questions in the contests literature."
http://arxiv.org/abs/2403.17206v2,The Devil is in the Details: Heterogeneous Effects of the German Minimum Wage on Working Hours and Minijobs,2024-03-25 21:35:43+00:00,"['Mario Bossler', 'Ying Liang', 'Thorsten Schank']",econ.GN,"In 2015, Germany introduced a national minimum wage. While the literature agrees on at most limited negative effects on the overall employment level, we go into detail and analyze the impact on the working hours dimension and on the subset of minijobs. Using data from the German Structure of Earnings Survey in 2010, 2014, and 2018, we find empirical evidence that the minimum wage significantly reduces inequality in hourly and monthly wages. While various theoretical mechanisms suggest a reduction in working hours, these remain unchanged on average. However, minijobbers experience a notable reduction in working hours which can be linked to the specific institutional framework. Regarding employment, the results show no effects for regular jobs, but there is a noteworthy decline in minijobs, driven by transitions to regular employment and non-employment. The transitions in non-employment imply a wage elasticity of employment of -0.1 for minijobs. Our findings highlight that the institutional setting leads to heterogeneous effects of the minimum wage."
http://arxiv.org/abs/2402.17042v2,Towards Generalizing Inferences from Trials to Target Populations,2024-02-26 21:49:44+00:00,"['Melody Y Huang', 'Harsh Parikh']",stat.ME,"Randomized Controlled Trials (RCTs) are pivotal in generating internally valid estimates with minimal assumptions, serving as a cornerstone for researchers dedicated to advancing causal inference methods. However, extending these findings beyond the experimental cohort to achieve externally valid estimates is crucial for broader scientific inquiry. This paper delves into the forefront of addressing these external validity challenges, encapsulating the essence of a multidisciplinary workshop held at the Institute for Computational and Experimental Research in Mathematics (ICERM), Brown University, in Fall 2023. The workshop congregated experts from diverse fields including social science, medicine, public health, statistics, computer science, and education, to tackle the unique obstacles each discipline faces in extrapolating experimental findings. Our study presents three key contributions: we integrate ongoing efforts, highlighting methodological synergies across fields; provide an exhaustive review of generalizability and transportability based on the workshop's discourse; and identify persistent hurdles while suggesting avenues for future research. By doing so, this paper aims to enhance the collective understanding of the generalizability and transportability of causal effects, fostering cross-disciplinary collaboration and offering valuable insights for researchers working on refining and applying causal inference methods."
http://arxiv.org/abs/2401.00839v3,Community Enforcement with Endogenous Records,2024-01-01 18:39:01+00:00,['Harry Pei'],econ.TH,"I study repeated games with anonymous random matching where players endogenously decide whether to disclose signals about their past actions. I establish an-anti folk theorem, that when players are sufficiently long-lived, they will almost always play their dominant actions and will almost never cooperate. When players' expected lifespans are intermediate, they can sustain some cooperation if their actions are substitutes but cannot sustain any cooperation if their actions are complements. Therefore, the maximal level of cooperation a community can sustain is not monotone with respect to its members' expected lifespans and the complementarity of players' actions can undermine their abilities to sustain cooperation."
http://arxiv.org/abs/2402.06594v2,They were robbed! Scoring by the middlemost to attenuate biased judging in boxing,2024-02-09 18:13:47+00:00,"['Stuart Baumann', 'Carl Singleton']",econ.GN,"Boxing has a long-standing problem with biased judging, impacting both professional and Olympic bouts. ""Robberies"", where boxers are widely seen as being denied rightful victories, threaten to drive fans and athletes away from the sport. To tackle this problem, we propose a minimalist adjustment in how boxing is scored: the winner would be decided by the majority of round-by-round victories according to the judges, rather than relying on the judges' overall bout scores. This approach, rooted in social choice theory and utilising majority rule and middlemost aggregation functions, creates a coordination problem for partisan judges and attenuates their influence. Our model analysis and simulations demonstrate the potential to significantly decrease the likelihood of a partisan judge swaying the result of a bout."
http://arxiv.org/abs/2403.14385v2,Estimating Causal Effects with Double Machine Learning -- A Method Evaluation,2024-03-21 13:21:33+00:00,"['Jonathan Fuhr', 'Philipp Berens', 'Dominik Papies']",stat.ML,"The estimation of causal effects with observational data continues to be a very active research area. In recent years, researchers have developed new frameworks which use machine learning to relax classical assumptions necessary for the estimation of causal effects. In this paper, we review one of the most prominent methods - ""double/debiased machine learning"" (DML) - and empirically evaluate it by comparing its performance on simulated data relative to more traditional statistical methods, before applying it to real-world data. Our findings indicate that the application of a suitably flexible machine learning algorithm within DML improves the adjustment for various nonlinear confounding relationships. This advantage enables a departure from traditional functional form assumptions typically necessary in causal effect estimation. However, we demonstrate that the method continues to critically depend on standard assumptions about causal structure and identification. When estimating the effects of air pollution on housing prices in our application, we find that DML estimates are consistently larger than estimates of less flexible methods. From our overall results, we provide actionable recommendations for specific choices researchers must make when applying DML in practice."
http://arxiv.org/abs/2405.08796v2,Variational Bayes and non-Bayesian Updating,2024-05-14 17:45:53+00:00,['Tomasz Strzalecki'],econ.TH,I show how variational Bayes can be used as a microfoundation for a popular model of non-Bayesian updating.
http://arxiv.org/abs/2405.10539v1,Overcoming Medical Overuse with AI Assistance: An Experimental Investigation,2024-05-17 04:47:36+00:00,"['Ziyi Wang', 'Lijia Wei', 'Lian Xue']",econ.GN,"This study evaluates the effectiveness of Artificial Intelligence (AI) in mitigating medical overtreatment, a significant issue characterized by unnecessary interventions that inflate healthcare costs and pose risks to patients. We conducted a lab-in-the-field experiment at a medical school, utilizing a novel medical prescription task, manipulating monetary incentives and the availability of AI assistance among medical students using a three-by-two factorial design. We tested three incentive schemes: Flat (constant pay regardless of treatment quantity), Progressive (pay increases with the number of treatments), and Regressive (penalties for overtreatment) to assess their influence on the adoption and effectiveness of AI assistance. Our findings demonstrate that AI significantly reduced overtreatment rates by up to 62% in the Regressive incentive conditions where (prospective) physician and patient interests were most aligned. Diagnostic accuracy improved by 17% to 37%, depending on the incentive scheme. Adoption of AI advice was high, with approximately half of the participants modifying their decisions based on AI input across all settings. For policy implications, we quantified the monetary (57%) and non-monetary (43%) incentives of overtreatment and highlighted AI's potential to mitigate non-monetary incentives and enhance social welfare. Our results provide valuable insights for healthcare administrators considering AI integration into healthcare systems."
http://arxiv.org/abs/2405.09500v1,Identifying Heterogeneous Decision Rules From Choices When Menus Are Unobserved,2024-05-15 16:49:19+00:00,"['Larry G Epstein', 'Kaushil Patel']",econ.TH,"Given only aggregate choice data and limited information about how menus are distributed across the population, we describe what can be inferred robustly about the distribution of preferences (or more general decision rules). We strengthen and generalize existing results on such identification and provide an alternative analytical approach to study the problem. We show further that our model and results are applicable, after suitable reinterpretation, to other contexts. One application is to the robust identification of the distribution of updating rules given only the population distribution of beliefs and limited information about heterogeneous information sources."
http://arxiv.org/abs/2405.17178v1,"Statistical Mechanism Design: Robust Pricing, Estimation, and Inference",2024-05-27 13:59:16+00:00,"['Duarte Gonçalves', 'Bruno A. Furtado']",econ.TH,"This paper tackles challenges in pricing and revenue projections due to consumer uncertainty. We propose a novel data-based approach for firms facing unknown consumer type distributions. Unlike existing methods, we assume firms only observe a finite sample of consumers' types. We introduce \emph{empirically optimal mechanisms}, a simple and intuitive class of sample-based mechanisms with strong finite-sample revenue guarantees. Furthermore, we leverage our results to develop a toolkit for statistical inference on profits. Our approach allows to reliably estimate the profits associated for any particular mechanism, to construct confidence intervals, and to, more generally, conduct valid hypothesis testing."
http://arxiv.org/abs/2404.05178v1,Estimating granular house price distributions in the Australian market using Gaussian mixtures,2024-04-08 04:01:43+00:00,"['Willem P Sijp', 'Anastasios Panagiotelis']",econ.EM,"A new methodology is proposed to approximate the time-dependent house price distribution at a fine regional scale using Gaussian mixtures. The means, variances and weights of the mixture components are related to time, location and dwelling type through a non linear function trained by a deep functional approximator. Price indices are derived as means, medians, quantiles or other functions of the estimated distributions. Price densities for larger regions, such as a city, are calculated via a weighted sum of the component density functions. The method is applied to a data set covering all of Australia at a fine spatial and temporal resolution. In addition to enabling a detailed exploration of the data, the proposed index yields lower prediction errors in the practical task of individual dwelling price projection from previous sales values within the three major Australian cities. The estimated quantiles are also found to be well calibrated empirically, capturing the complexity of house price distributions."
http://arxiv.org/abs/2404.11063v1,Attitudinal Loyalty Manifestation in Banking CSR: Cross-Buying Behavior and Customer Advocacy,2024-04-17 04:41:19+00:00,"['Muhamad Bhayuta Yudhi Putera', 'Melia Famiola']",econ.GN,"This study in the banking industry examines the influence of attitudinal loyalty on customer advocacy and cross buying behavior, alongside the moderating roles of Quality of Life and Corporate Social Responsibility support in the CSR fit and loyalty relationship. Employing Structural Equation Modeling, it reveals that higher attitudinal loyalty significantly boosts customer advocacy and propensity for cross buying. The findings highlight the importance of nurturing customer loyalty through valuable and relevant offerings, as CSR fit alone does not define the loyalty of the banking customer. Banks are advised to target customers with a high Quality of Life and engage with those who support CSR initiatives aligning with the banks objectives, to enhance loyalty and deepen customer relationships."
http://arxiv.org/abs/2404.07750v1,The Broken Rung: Gender and the Leadership Gap,2024-04-11 13:47:53+00:00,['Ingrid Haegele'],econ.GN,"Addressing female underrepresentation in leadership positions has become a key policy objective. However, little is known about the extent to which leadership appeals differently to women. Collecting new data from a large firm, I document that women are substantially less likely to apply for early-career promotions. Realized application patterns and large-scale surveys reveal the role of an understudied feature of promotions -- having to assume responsibility over a team -- which is less appealing to women. This gender difference is not accounted for by standard explanations, such as success likelihood or confidence, but is rather a product of common design features of leadership positions."
http://arxiv.org/abs/2404.08129v1,One Factor to Bind the Cross-Section of Returns,2024-04-11 21:10:24+00:00,"['Nicola Borri', 'Denis Chetverikov', 'Yukun Liu', 'Aleh Tsyvinski']",q-fin.GN,"We propose a new non-linear single-factor asset pricing model $r_{it}=h(f_{t}λ_{i})+ε_{it}$. Despite its parsimony, this model represents exactly any non-linear model with an arbitrary number of factors and loadings -- a consequence of the Kolmogorov-Arnold representation theorem. It features only one pricing component $h(f_{t}λ_{I})$, comprising a nonparametric link function of the time-dependent factor and factor loading that we jointly estimate with sieve-based estimators. Using 171 assets across major classes, our model delivers superior cross-sectional performance with a low-dimensional approximation of the link function. Most known finance and macro factors become insignificant controlling for our single-factor."
http://arxiv.org/abs/2404.01190v1,Call the Dentist! A (Con-)Cavity in the Value of Information,2024-04-01 15:48:23+00:00,['Mark Whitmeyer'],econ.TH,"A natural way of quantifying the ``amount of information'' in decision problems yields a globally concave value for information. Another (in contrast, adversarial) way almost never does."
http://arxiv.org/abs/2404.01782v1,Multicriteria Analysis Model in Sustainable Corn Farming Area Planning,2024-04-02 09:45:19+00:00,"['Abdul Haris', 'Muhammad Munawir Syarif', 'Hamed Narolla', 'Rachmat Hidayat']",econ.GN,"This study aims to develop a framework for multicriteria analysis to evaluate alternatives for sustainable corn agricultural area planning, considering the integration of ecological, economic, and social aspects as pillars of sustainability. The research method uses qualitative and quantitative approaches to integrate ecological, economic, and social aspects in the multicriteria analysis. The analysis involves land evaluation, subcriteria identification, and data integration using Multidimensional Scaling and Analytical Hierarchy Process methods to prioritize developing sustainable corn agricultural areas. Based on the results of the RAP-Corn analysis, it indicates that the ecological dimension depicts less sustainability. The AHP results yield weight distribution and highly relevant scores that describe tangible preferences. Priority directions are grouped as strategic steps toward achieving the goals of sustainable corn agricultural area planning."
http://arxiv.org/abs/2404.04105v1,"Judgment in macroeconomic output growth predictions: Efficiency, accuracy and persistence",2024-04-05 14:00:37+00:00,['Michael Pedersen'],econ.GN,"The present study applies observations of individual predictions of the first three releases of the US output growth rate to evaluate how the applied judgment affects prediction efficiency and accuracy as well as if judgment is persistent. While the first two issues have been assessed in other studies, there is little evidence on the formation of judgment in macroeconomic projections. Most of the forecasters produce unbiased predictions, but employing the median Bloomberg projection as baseline, it turns out that judgment generally does not improve accuracy. There seems to be persistence in the judgment applied by forecasters in the sense that the sign of the adjustment in the first release prediction carries over to the projections of the two following revisions. One possible explanation is that forecasters use some kind of anchor-and-adjustment heuristic."
http://arxiv.org/abs/2404.11839v1,(Empirical) Bayes Approaches to Parallel Trends,2024-04-18 01:40:03+00:00,"['Soonwoo Kwon', 'Jonathan Roth']",econ.EM,"We consider Bayes and Empirical Bayes (EB) approaches for dealing with violations of parallel trends. In the Bayes approach, the researcher specifies a prior over both the pre-treatment violations of parallel trends $δ_{pre}$ and the post-treatment violations $δ_{post}$. The researcher then updates their posterior about the post-treatment bias $δ_{post}$ given an estimate of the pre-trends $δ_{pre}$. This allows them to form posterior means and credible sets for the treatment effect of interest, $τ_{post}$. In the EB approach, the prior on the violations of parallel trends is learned from the pre-treatment observations. We illustrate these approaches in two empirical applications."
http://arxiv.org/abs/2404.13211v1,Long-term forecasts of statewide travel demand patterns using large-scale mobile phone GPS data: A case study of Indiana,2024-04-19 23:23:55+00:00,"['Rajat Verma', 'Eunhan Ka', 'Satish V. Ukkusuri']",econ.GN,"The growth in availability of large-scale GPS mobility data from mobile devices has the potential to aid traditional travel demand models (TDMs) such as the four-step planning model, but those processing methods are not commonly used in practice. In this study, we show the application of trip generation and trip distribution modeling using GPS data from smartphones in the state of Indiana. This involves extracting trip segments from the data and inferring the phone users' home locations, adjusting for data representativeness, and using a data-driven travel time-based cost function for the trip distribution model. The trip generation and interchange patterns in the state are modeled for 2025, 2035, and 2045. Employment sectors like industry and retail are observed to influence trip making behavior more than other sectors. The travel growth is predicted to be mostly concentrated in the suburban regions, with a small decline in the urban cores. Further, although the majority of the growth in trip flows over the years is expected to come from the corridors between the major urban centers of the state, relative interzonal trip flow growth will likely be uniformly spread throughout the state. We also validate our results with the forecasts of two travel demand models, finding a difference of 5-15% in overall trip counts. Our GPS data-based demand model will contribute towards augmenting the conventional statewide travel demand model developed by the state and regional planning agencies."
http://arxiv.org/abs/2404.17497v1,Merchants of Vulnerabilities: How Bug Bounty Programs Benefit Software Vendors,2024-04-26 15:51:59+00:00,"['Esther Gal-Or', 'Muhammad Zia Hydari', 'Rahul Telang']",cs.CR,"Software vulnerabilities enable exploitation by malicious hackers, compromising systems and data security. This paper examines bug bounty programs (BBPs) that incentivize ethical hackers to discover and responsibly disclose vulnerabilities to software vendors. Using game-theoretic models, we capture the strategic interactions between software vendors, ethical hackers, and malicious hackers. First, our analysis shows that software vendors can increase expected profits by participating in BBPs, explaining their growing adoption and the success of BBP platforms. Second, we find that vendors with BBPs will release software earlier, albeit with more potential vulnerabilities, as BBPs enable coordinated vulnerability disclosure and mitigation. Third, the optimal number of ethical hackers to invite to a BBP depends solely on the expected number of malicious hackers seeking exploitation. This optimal number of ethical hackers is lower than but increases with the expected malicious hacker count. Finally, higher bounties incentivize ethical hackers to exert more effort, thereby increasing the probability that they will discover severe vulnerabilities first while reducing the success probability of malicious hackers. These findings highlight BBPs' potential benefits for vendors beyond profitability. Earlier software releases are enabled by managing risks through coordinated disclosure. As cybersecurity threats evolve, BBP adoption will likely gain momentum, providing vendors with a valuable tool for enhancing security posture and stakeholder trust. Moreover, BBPs envelop vulnerability identification and disclosure into new market relationships and transactions, impacting software vendors' incentives regarding product security choices like release timing."
http://arxiv.org/abs/2406.15288v2,Difference-in-Differences when Parallel Trends Holds Conditional on Covariates,2024-06-21 16:32:32+00:00,"['Carolina Caetano', 'Brantly Callaway']",econ.EM,"In this paper, we study difference-in-differences identification and estimation strategies when the parallel trends assumption holds after conditioning on covariates. We consider empirically relevant settings where the covariates can be time-varying, time-invariant, or both. We uncover a number of weaknesses of commonly used two-way fixed effects (TWFE) regressions in this context, even in applications with only two time periods. In addition to some weaknesses due to estimating linear regression models that are similar to cases with cross-sectional data, we also point out a collection of additional issues that we refer to as \textit{hidden linearity bias} that arise because the transformations used to eliminate the unit fixed effect also transform the covariates (e.g., taking first differences can result in the estimating equation only including the change in covariates over time, not their level, and also drop time-invariant covariates altogether). We provide simple diagnostics for assessing how susceptible a TWFE regression is to hidden linearity bias based on reformulating the TWFE regression as a weighting estimator. Finally, we propose simple alternative estimation strategies that can circumvent these issues."
http://arxiv.org/abs/2406.19956v3,Three Scores and 15 Years (1948-2023) of Rao's Score Test: A Brief History,2024-06-28 14:41:13+00:00,"['Anil K. Bera', 'Yannis Bilias']",econ.EM,"Rao (1948) introduced the score test statistic as an alternative to the likelihood ratio and Wald test statistics. In spite of the optimality properties of the score statistic shown in Rao and Poti (1946), the Rao score (RS) test remained unnoticed for almost 20 years. Today, the RS test is part of the ``Holy Trinity'' of hypothesis testing and has found its place in the Statistics and Econometrics textbooks and related software. Reviewing the history of the RS test we note that remarkable test statistics proposed in the literature earlier or around the time of Rao (1948) mostly from intuition, such as Pearson (1900) goodness-fit-test, Moran (1948) I test for spatial dependence and Durbin and Watson (1950) test for serial correlation, can be given RS test statistic interpretation. At the same time, recent developments in the robust hypothesis testing under certain forms of misspecification, make the RS test an active area of research in Statistics and Econometrics. From our brief account of the history the RS test we conclude that its impact in science goes far beyond its calendar starting point with promising future research activities for many years to come."
http://arxiv.org/abs/2406.03742v3,"Evaluating Feature Selection Methods for Macro-Economic Forecasting, Applied for Inflation Indicator of Iran",2024-06-06 04:40:04+00:00,['Mahdi Goldani'],econ.GN,"This study explores various feature selection techniques applied to macro-economic forecasting, using Iran's World Bank Development Indicators. Employing a comprehensive evaluation framework that includes Root Mean Square Error (RMSE) and Mean Absolute Error (MAE) within a 10-fold cross-validation setup, this research systematically analyzes and ranks different feature selection methodologies. The study distinctly highlights the efficiency of Stepwise Selection, Tree-based methods, Hausdorff distance, Euclidean distance, and Mutual Information (MI) Score, noting their superior performance in reducing predictive errors. In contrast, methods like Recursive Feature Elimination with Cross-Validation (RFECV) and Variance Thresholding showed relatively lower effectiveness. The results underline the robustness of similarity-based approaches, particularly Hausdorff and Euclidean distances, which consistently performed well across various datasets, achieving an average rank of 9.125 out of a range of tested methods. This paper provides crucial insights into the effectiveness of different feature selection methods, offering significant implications for enhancing the predictive accuracy of models used in economic analysis and planning. The findings advocate for the prioritization of stepwise and tree-based methods alongside similarity-based techniques for researchers and practitioners working with complex economic datasets."
http://arxiv.org/abs/2404.13986v2,Stochastic Volatility in Mean: Efficient Analysis by a Generalized Mixture Sampler,2024-04-22 08:49:14+00:00,"['Daichi Hiraki', 'Siddhartha Chib', 'Yasuhiro Omori']",econ.EM,"In this paper we consider the simulation-based Bayesian analysis of stochastic volatility in mean (SVM) models. Extending the highly efficient Markov chain Monte Carlo mixture sampler for the SV model proposed in Kim et al. (1998) and Omori et al. (2007), we develop an accurate approximation of the non-central chi-squared distribution as a mixture of thirty normal distributions. Under this mixture representation, we sample the parameters and latent volatilities in one block. We also detail a correction of the small approximation error by using additional Metropolis-Hastings steps. The proposed method is extended to the SVM model with leverage. The methodology and models are applied to excess holding yields and S&P500 returns in empirical studies, and the SVM models are shown to outperform other volatility models based on marginal likelihoods."
http://arxiv.org/abs/2405.02217v4,"Identifying and exploiting alpha in linear asset pricing models with strong, semi-strong, and latent factors",2024-05-03 16:22:21+00:00,"['M. Hashem Pesaran', 'Ron P. Smith']",econ.EM,"The risk premia of traded factors are the sum of factor means and a parameter vector we denote by φ which is identified from the cross section regression of alpha of individual securities on the vector of factor loadings. If phi is non-zero one can construct ""phi-portfolios"" which exploit the systematic components of non-zero alpha. We show that for known values of betas and when phi is non-zero there exist phi-portfolios that dominate mean-variance portfolios. The paper then proposes a two-step bias corrected estimator of phi and derives its asymptotic distribution allowing for idiosyncratic pricing errors, weak missing factors, and weak error cross-sectional dependence. Small sample results from extensive Monte Carlo experiments show that the proposed estimator has the correct size with good power properties. The paper also provides an empirical application to a large number of U.S. securities with risk factors selected from a large number of potential risk factors according to their strength and constructs phi-portfolios and compares their Sharpe ratios to mean variance and S&P 500 portfolio."
http://arxiv.org/abs/2406.00665v3,Integrating solid direct air capture systems with green hydrogen production: Economic synergy of sector coupling,2024-06-02 08:26:43+00:00,"['Sunwoo Kim', 'Joungho Park', 'Jay H. Lee']",econ.GN,"In the global pursuit of sustainable energy solutions, mitigating carbon dioxide (CO2) emissions stands as a pivotal challenge. With escalating atmospheric CO2 levels, the imperative of direct air capture (DAC) systems becomes evident. Simultaneously, green hydrogen (GH) emerges as a pivotal medium for renewable energy. Nevertheless, the substantial expenses associated with these technologies impede widespread adoption, primarily due to significant installation costs and underutilized operational advantages when deployed independently. Integration through sector coupling enhances system efficiency and sustainability, while shared power sources and energy storage devices offer additional economic benefits. In this study, we assess the economic viability of polymer electrolyte membrane electrolyzers versus alkaline electrolyzers within the context of sector coupling. Our findings indicate that combining GH production with solid DAC systems yields significant economic advantages, with approximately a 10% improvement for PEM electrolyzers and a 20% enhancement for alkaline electrolyzers. These results highlight a substantial opportunity to improve the efficiency and economic viability of renewable energy and green hydrogen initiatives, thereby facilitating the broader adoption of cleaner technologies."
http://arxiv.org/abs/2406.12279v2,Strategy-proof Selling: a Geometric Approach,2024-06-18 05:15:11+00:00,['Mridu Prabal Goswami'],cs.GT,"We consider one buyer and one seller. For a bundle $(t,q)\in [0,\infty[\times [0,1]=\mathbb{Z}$, $q$ either refers to the wining probability of an object or a share of a good, and $t$ denotes the payment that the buyer makes. We define classical and restricted classical preferences of the buyer on $\mathbb{Z}$; they incorporate quasilinear, non-quasilinear, risk averse preferences with multidimensional pay-off relevant parameters. We define rich single-crossing subsets of the two classes, and characterize strategy-proof mechanisms by using monotonicity of the mechanisms and continuity of the indirect preference correspondences. We also provide a computationally tractable optimization program to compute the optimal mechanism. We do not use revenue equivalence and virtual valuations as tools in our proofs. Our proof techniques bring out the geometric interaction between the single-crossing property and the positions of bundles $(t,q)$s. Our proofs are simple and provide computationally tractable optimization program to compute the optimal mechanism. The extension of the optimization program to the $n-$ buyer environment is immediate."
http://arxiv.org/abs/2404.19699v3,Generative AI Usage and Exam Performance,2024-04-30 16:45:54+00:00,"['Janik Ole Wecks', 'Johannes Voshaar', 'Benedikt Jost Plate', 'Jochen Zimmermann']",econ.GN,"This study evaluates the impact of students' usage of generative artificial intelligence (GenAI) tools such as ChatGPT on their exam performance. We analyse student essays using GenAI detection systems to identify GenAI users among the cohort. Employing multivariate regression analysis, we find that students using GenAI tools score on average 6.71 (out of 100) points lower than non-users. While GenAI may offer benefits for learning and engagement, the way students actually use it correlates with diminished exam outcomes. Exploring the underlying mechanism, additional analyses show that the effect is particularly detrimental to students with high learning potential, suggesting an effect whereby GenAI tool usage hinders learning. Our findings provide important empirical evidence for the ongoing debate on the integration of GenAI in higher education and underscores the necessity for educators, institutions, and policymakers to carefully consider its implications for student performance."
http://arxiv.org/abs/2404.09297v2,Belief Bias Identification,2024-04-14 16:38:02+00:00,['Pedro Gonzalez-Fernandez'],econ.GN,"This paper proposes a unified theoretical model to identify and test a comprehensive set of probabilistic updating biases within a single framework. The model achieves separate identification by focusing on the updating of belief distributions, rather than classic point-belief measurements. Testing the model in a laboratory experiment reveals significant heterogeneity at the individual level: All tested biases are present, and each participant exhibits at least one identifiable bias. Notably, motivated-belief biases (optimism and pessimism) and sequence-related biases (gambler's fallacy and hot hand fallacy) are identified as key drivers of biased inference. Moreover, at the population level, base rate neglect emerges as a persistent influence. This study contributes to the belief-updating literature by providing a methodological toolkit for researchers examining links between different conflicting biases, or exploring connections between updating biases and other behavioral phenomena."
http://arxiv.org/abs/2406.19938v2,Non-Linearities in International Spillovers of the ECB$^\prime$s Monetary Policy. The Case of Non-ERM II Countries and Anti-Fragmentation Policy,2024-06-28 14:10:24+00:00,['Iones Kelanemer Holban'],econ.GN,"We investigate the presence of sign and size non-linearities in the impact of the European Central Bank$^\prime$s Anti-Fragmentation Policy on non-ERM II, EU countries. After identifying three orthogonal monetary policy shock using the method of Fanelli and Marsi [2022], we then select an optimal specification and estimate both linear and non linear impulse response functions using local projections (Dufour and Renault [1998], Goncalves et al. [2021]). The choice of non-linear transformations to separate sign and size effects is based on Caravello and Martinez-Bruera [Working Paper, 2024]. Lastly we compare the linear model to the non-linear ones using a battery of Wald tests and find significant evidence of sign non-linearities in the international spillovers of ECB policy."
http://arxiv.org/abs/2407.01564v1,Decarbonization analysis on residential end uses in the emerging economies,2024-05-17 09:11:23+00:00,"['Ran Yan', 'Minda Ma']",econ.GN,"This study explores the historical emission patterns and decarbonization efforts of China and India, the largest emerging emitters in residential building operations. Using a novel carbon intensity model and structural decomposition approach, it assesses the operational decarbonization progress over the past two decades. Results show significant decarbonization, with China and India collectively reducing 1498.3 and 399.7 MtCO2, respectively. Electrification notably contributed to decarbonizing space cooling and appliances in both countries."
http://arxiv.org/abs/2405.14617v1,"Towards an Optimal Staking Design: Balancing Security, User Growth, and Token Appreciation",2024-05-23 14:27:12+00:00,"['Nicolas Oderbolz', 'Beatrix Marosvölgyi', 'Matthias Hafner']",econ.GN,"This paper examines the economic and security implications of Proof-of-Stake (POS) designs, providing a survey of POS design choices and their underlying economic principles in prominent POS-blockchains. The paper argues that POS-blockchains are essentially platforms that connect three groups of agents: users, validators, and investors. To meet the needs of these groups, blockchains must balance trade-offs between security, user adoption, and investment into the protocol. We focus on the security aspect and identify two different strategies: increasing the quality of validators (static security) vs. increasing the quantity of stakes (dynamic security). We argue that quality comes at the cost of quantity, identifying a trade-off between the two strategies when designing POS systems. We test our qualitative findings using panel analysis on collected data. The analysis indicates that enhancing the quality of the validator set through security measures like slashing and minimum staking amounts may decrease dynamic security. Further, the analysis reveals a strategic divergence among blockchains, highlighting the absence of a single, universally optimal staking design solution. The optimal design hinges upon a platform's specific objectives and its developmental stage. This research compels blockchain developers to meticulously assess the trade-offs outlined in this paper when developing their staking designs."
http://arxiv.org/abs/2405.10494v1,Estimating Idea Production: A Methodological Survey,2024-05-17 02:17:34+00:00,"['Ege Erdil', 'Tamay Besiroglu', 'Anson Ho']",econ.GN,"Accurately modeling the production of new ideas is crucial for innovation theory and endogenous growth models. This paper provides a comprehensive methodological survey of strategies for estimating idea production functions. We explore various methods, including naive approaches, linear regression, maximum likelihood estimation, and Bayesian inference, each suited to different data availability settings. Through case studies ranging from total factor productivity to software R&D, we show how to apply these methodologies in practice. Our synthesis provides researchers with guidance on strategies for characterizing idea production functions and highlights obstacles that must be addressed through further empirical validation."
http://arxiv.org/abs/2405.10449v1,Optimal Text-Based Time-Series Indices,2024-05-16 21:20:45+00:00,"['David Ardia', 'Keven Bluteau']",econ.EM,"We propose an approach to construct text-based time-series indices in an optimal way--typically, indices that maximize the contemporaneous relation or the predictive performance with respect to a target variable, such as inflation. We illustrate our methodology with a corpus of news articles from the Wall Street Journal by optimizing text-based indices focusing on tracking the VIX index and inflation expectations. Our results highlight the superior performance of our approach compared to existing indices."
http://arxiv.org/abs/2405.17237v2,Mixing it up: Inflation at risk,2024-05-27 14:52:31+00:00,['Maximilian Schröder'],econ.EM,"Assessing the contribution of various risk factors to future inflation risks was crucial for guiding monetary policy during the recent high inflation period. However, existing methodologies often provide limited insights by focusing solely on specific percentiles of the forecast distribution. In contrast, this paper introduces a comprehensive framework that examines how economic indicators impact the entire forecast distribution of macroeconomic variables, facilitating the decomposition of the overall risk outlook into its underlying drivers. Additionally, the framework allows for the construction of risk measures that align with central bank preferences, serving as valuable summary statistics. Applied to the recent inflation surge, the framework reveals that U.S. inflation risk was primarily influenced by the recovery of the U.S. business cycle and surging commodity prices, partially mitigated by adjustments in monetary policy and credit spreads."
http://arxiv.org/abs/2405.15721v1,Dynamic Latent-Factor Model with High-Dimensional Asset Characteristics,2024-05-24 17:09:28+00:00,['Adam Baybutt'],econ.EM,"We develop novel estimation procedures with supporting econometric theory for a dynamic latent-factor model with high-dimensional asset characteristics, that is, the number of characteristics is on the order of the sample size. Utilizing the Double Selection Lasso estimator, our procedure employs regularization to eliminate characteristics with low signal-to-noise ratios yet maintains asymptotically valid inference for asset pricing tests. The crypto asset class is well-suited for applying this model given the limited number of tradable assets and years of data as well as the rich set of available asset characteristics. The empirical results present out-of-sample pricing abilities and risk-adjusted returns for our novel estimator as compared to benchmark methods. We provide an inference procedure for measuring the risk premium of an observable nontradable factor, and employ this to find that the inflation-mimicking portfolio in the crypto asset class has positive risk compensation."
http://arxiv.org/abs/2405.13327v1,Monitoring the carbon emissions transition of global building end-use activity,2024-05-22 04:08:28+00:00,"['Xiwang Xiang', 'Minda Ma']",econ.GN,"The building sector is the largest emitter globally and as such is at the forefront of the net-zero emissions pathway. This study is the first to present a bottom-up assessment framework integrated with the decomposing structural decomposition method to evaluate the emission patterns and decarbonization process of global residential building operations and commercial building operation simultaneously over the last two decades. The results reveal that (1) the average carbon intensity of global commercial building operations has maintained an annual decline of 1.94% since 2000, and emission factors and industrial structures were generally the key to decarbonizing commercial building operations; (2) the operational carbon intensity of global residential buildings has maintained an annual decline of 1.2% over the past two decades, and energy intensity and average household size have been key to this decarbonization; and (3) the total decarbonization of commercial building operations and residential buildings worldwide was 230.28 and 338.1 mega-tons of carbon dioxide per yr, respectively, with a decarbonization efficiency of 10.05% and 9.4%. Overall, this study assesses the global historical progress in decarbonizing global building operations and closes the relevant gap, and it helps plan the stepwise carbon neutral pathway of future global buildings by the mid-century."
http://arxiv.org/abs/2405.13186v1,Doing the right thing (or not) in a lemons-like situation: on the role of social preferences and Kantian moral concerns,2024-05-21 20:27:48+00:00,"['Ingela Alger', 'José Ignacio Rivero-Wildemauwe']",econ.GN,"We conduct a laboratory experiment using framing to assess the willingness to ``sell a lemon'', i.e., to undertake an action that benefits self but hurts the other (the ``buyer''). We seek to disentangle the role of other-regarding preferences and (Kantian) moral concerns, and to test if it matters whether the decision is described in neutral terms or as a market situation. When evaluating an action, morally motivated individuals consider what their own payoff would be if -- hypothetically -- the roles were reversed and the other subject chose the same action (universalization). We vary the salience of role uncertainty, thus varying the ease for participants to envisage the role-reversal scenario."
http://arxiv.org/abs/2405.15862v1,The Mariana Environmental Disaster and its Labor Market Effects,2024-05-24 18:08:49+00:00,"[""Hugo Sant'Anna""]",econ.GN,"This paper examines the labor market impacts of the 2015 Mariana Dam disaster in Brazil. It contrasts two theoretical models: an urban spatial equilibrium model and a factor of production model, with diverging perspectives on environmental influences on labor outcomes. Utilizing rich national administrative and spatial data, the study reveals that the unusual environmental alteration, with minimal human capital loss, primarily affected outcomes via the factor of production channel. Nevertheless, spatial equilibrium dynamics are discernible within certain market segments. This research contributes to the growing literature on environmental changes and its economic consequences."
http://arxiv.org/abs/2405.15825v1,"Multimarket Contact, Merger, and Airline Collusion",2024-05-23 20:08:10+00:00,['Ziyu Yan'],econ.GN,"This thesis investigates the dynamics of multimarket contact and airline mergers on collusive pricing of airlines. In align with Bernheim and Whinston (1990) and Athey et.al.(2004), it detects collusive pricing via pairwise price difference and price rigidity. The piece of work extends previous work by incorporating additional controls such as distinction between non-stop and stopover itineraries and detailed market concentration measures. The findings confirm a significant relationship between multimarket contact and reduced price differences, indicating collusive equilibria facilitated by frequent interactions across markets. Moreover, the results highlight that airlines exhibit more collusive behavior when pricing non-stop flights, and are more likely to attain tacit collusion when they approaches duopoly in a particular market. The study also explores the effects of airline mergers on collusion, employing an event study methodology with a difference-in-difference (DID) design. It finds no direct evidence that mergers lead to increased collusion among unmerged carriers. However, it reveals that during and after the merger process, carrier pairs between merged and unmerged carriers are more likely to collude compared to pairs of unmerged carriers."
http://arxiv.org/abs/2405.16193v1,Effect of Interest Payments on External Debt on Economic Growth in Kenya,2024-05-25 11:48:32+00:00,['Sammy Kemboi Chepkilot'],econ.GN,"In Kenya, interest payments on external debt have been increasing from 2010 to 2015, while GDP growth experienced a slight decline over the same period. Policymakers are concerned that the rapid increase in external debt in developing countries such as Kenya has the potential to erode the country's sovereign rating, particularly if it is not supported by proportionate growth in the size of the economy. The purpose of this study was to investigate the effect of interest payments on external debt on economic growth in Kenya. The study utilized secondary data for 25 years, from 1991 to 2015, for GDP growth and interest payments on external debt. The results from the analysis of variance statistics indicate that the model was statistically significant. This implies that interest payments on external debt are good predictors of GDP growth. Regression coefficient results show that GDP growth and the logarithm of interest payments on external debt are negatively and significantly related. The study recommends that future government plans should ensure that external borrowings are taken at rates not higher than the interest rate payments."
http://arxiv.org/abs/2405.16050v1,"Rationalizability, Iterated Dominance, and the Theorems of Radon and Carathéodory",2024-05-25 04:22:46+00:00,['Roy Long'],cs.GT,"The game theoretic concepts of rationalizability and iterated dominance are closely related and provide characterizations of each other. Indeed, the equivalence between them implies that in a two player finite game, the remaining set of actions available to players after iterated elimination of strictly dominated strategies coincides with the rationalizable actions. I prove a dimensionality result following from these ideas. I show that for two player games, the number of actions available to the opposing player provides a (tight) upper bound on how a player's pure strategies may be strictly dominated by mixed strategies. I provide two different frameworks and interpretations of dominance to prove this result, and in doing so relate it to Radon's Theorem and Carathéodory's Theorem from convex geometry. These approaches may be seen as following from point-line duality. A new proof of the classical equivalence between these solution concepts is also given."
http://arxiv.org/abs/2404.15495v2,Correlations versus noise in the NFT market,2024-04-23 20:15:12+00:00,"['Marcin Wątorek', 'Paweł Szydło', 'Jarosław Kwapień', 'Stanisław Drożdż']",q-fin.ST,"The non-fungible token (NFT) market emerges as a recent trading innovation leveraging blockchain technology, mirroring the dynamics of the cryptocurrency market. The current study is based on the capitalization changes and transaction volumes across a large number of token collections on the Ethereum platform. In order to deepen the understanding of the market dynamics, the collection-collection dependencies are examined by using the multivariate formalism of detrended correlation coefficient and correlation matrix. It appears that correlation strength is lower here than that observed in previously studied markets. Consequently, the eigenvalue spectra of the correlation matrix more closely follow the Marchenko-Pastur distribution, still, some departures indicating the existence of correlations remain. The comparison of results obtained from the correlation matrix built from the Pearson coefficients and, independently, from the detrended cross-correlation coefficients suggests that the global correlations in the NFT market arise from higher frequency fluctuations. Corresponding minimal spanning trees (MSTs) for capitalization variability exhibit a scale-free character while, for the number of transactions, they are somewhat more decentralized."
http://arxiv.org/abs/2406.09732v2,Finding pure Nash equilibria in large random games,2024-06-14 05:37:24+00:00,"['Andrea Collevecchio', 'Tuan-Minh Nguyen', 'Ziwen Zhong']",math.PR,"Best Response Dynamics (BRD) is a class of strategy updating rules to find Pure Nash Equilibria (PNE) in a game. At each step, a player is randomly picked, and the player switches to a ""best response"" strategy based on the strategies chosen by others, so that the new strategy profile maximises their payoff. If no such strategy exists, a different player will be chosen randomly. When no player wants to change their strategy anymore, the process reaches a PNE and will not deviate from it. On the other hand, either PNE may not exist, or BRD could be ""trapped"" within a subgame that has no PNE. We consider a random game with $N$ players, each with two actions available, and i.i.d. payoffs, in which the payoff distribution may have an atom, i.e. ties are allowed. We study a class of random walks in a random medium on the $N$-dimensional hypercube induced by the random game. The medium contains two types of obstacles corresponding to PNE and traps. The class of processes we analyze includes BRD, simple random walks on the hypercube, and many other nearest neighbour processes. We prove that, with high probability, these processes reach a PNE before hitting any trap."
http://arxiv.org/abs/2406.03053v2,Identification of structural shocks in Bayesian VEC models with two-state Markov-switching heteroskedasticity,2024-06-05 08:27:44+00:00,"['Justyna Wróblewska', 'Łukasz Kwiatkowski']",econ.EM,"We develop a Bayesian framework for cointegrated structural VAR models identified by two-state Markovian breaks in conditional covariances. The resulting structural VEC specification with Markov-switching heteroskedasticity (SVEC-MSH) is formulated in the so-called B-parameterization, in which the prior distribution is specified directly for the matrix of the instantaneous reactions of the endogenous variables to structural innovations. We discuss some caveats pertaining to the identification conditions presented earlier in the literature on stationary structural VAR-MSH models, and revise the restrictions to actually ensure the unique global identification through the two-state heteroskedasticity. To enable the posterior inference in the proposed model, we design an MCMC procedure, combining the Gibbs sampler and the Metropolis-Hastings algorithm. The methodology is illustrated both with a simulated as well as real-world data examples."
http://arxiv.org/abs/2406.04074v1,Estimation of Global Building Stocks by 2070: Unlocking Renovation Potential,2024-06-06 13:41:37+00:00,"['Shufan Zhang', 'Minda Ma', 'Nan Zhou', 'Jinyue Yan', 'Wei Feng', 'Ran Yan', 'Kairui You', 'Jingjing Zhang', 'Jing Ke']",econ.GN,"Buildings produce one-third of carbon emissions globally, however, data absence regarding global floorspace poses challenges in advancing building carbon neutrality. We compile the measured building stocks for 14 major economies and apply our global building stock model, GLOBUS, to evaluate future trends in stock turnover. Based on a scenario not considering renovation, by 2070 the building stock in developed economies will be ~1.4 times that of 2020 (100 billion m2); in developing economies it is expected to be 2.2 times that of 2020 (313 billion m2). Based on a techno-economic potential scenario, however, stocks in developed economies will decline to approximately 0.8 times the 2020 level, while stocks in developing economies will increase to nearly twice the 2020 level due to their fewer buildings currently. Overall, GLOBUS provides a way of calculating the global building stock, helping scientists, engineers, and policymakers conduct a range of investigation across various future scenarios."
http://arxiv.org/abs/2406.00610v1,Portfolio Optimization with Robust Covariance and Conditional Value-at-Risk Constraints,2024-06-02 03:50:20+00:00,['Qiqin Zhou'],q-fin.PM,"The measure of portfolio risk is an important input of the Markowitz framework. In this study, we explored various methods to obtain a robust covariance estimators that are less susceptible to financial data noise. We evaluated the performance of large-cap portfolio using various forms of Ledoit Shrinkage Covariance and Robust Gerber Covariance matrix during the period of 2012 to 2022. Out-of-sample performance indicates that robust covariance estimators can outperform the market capitalization-weighted benchmark portfolio, particularly during bull markets. The Gerber covariance with Mean-Absolute-Deviation (MAD) emerged as the top performer. However, robust estimators do not manage tail risk well under extreme market conditions, for example, Covid-19 period. When we aim to control for tail risk, we should add constraint on Conditional Value-at-Risk (CVaR) to make more conservative decision on risk exposure. Additionally, we incorporated unsupervised clustering algorithm K-means to the optimization algorithm (i.e. Nested Clustering Optimization, NCO). It not only helps mitigate numerical instability of the optimization algorithm, but also contributes to lower drawdown as well."
http://arxiv.org/abs/2405.20604v2,Tracking the Credibility Revolution across Fields,2024-05-31 03:32:13+00:00,['Paul Goldsmith-Pinkham'],econ.GN,"This paper updates Currie, Kleven, and Zwiers (2020) by examining the credibility revolution across fields, including finance and macroeconomics, using NBER working papers up to May 2024. While the growth in terms related to identification and research designs have continued, finance and macroeconomics have lagged behind applied micro. Difference-in-differences and regression discontinuity designs have risen since 2002, but the growth in difference-in-difference has been larger, more persistent, and more ubiquitous. In contrast, instrumental variables have stayed flat over this period. Finance and macro, particularly corporate finance, has experienced significant growth in mentions of experimental and quasi-experimental methods and identification over this time period, but a large component of the credibility revolution in finance is due to difference-in-differences. Bartik and shift-share instruments have grown across all fields, with the most pronounced growth in international trade and investment, economic history, and labor studies. Synthetic control has not seen continued growth, and has fallen since 2020."
http://arxiv.org/abs/2405.09161v2,Exploring the Potential of Large Language Models for Automation in Technical Customer Service,2024-05-15 07:48:10+00:00,"['Jochen Wulf', 'Juerg Meierhofer']",econ.GN,"Purpose: The purpose of this study is to investigate the potential of Large Language Models (LLMs) in transforming technical customer service (TCS) through the automation of cognitive tasks. Design/Methodology/Approach: Using a prototyping approach, the research assesses the feasibility of automating cognitive tasks in TCS with LLMs, employing real-world technical incident data from a Swiss telecommunications operator. Findings: Lower-level cognitive tasks such as translation, summarization, and content generation can be effectively automated with LLMs like GPT-4, while higher-level tasks such as reasoning require more advanced technological approaches such as Retrieval-Augmented Generation (RAG) or finetuning ; furthermore, the study underscores the significance of data ecosystems in enabling more complex cognitive tasks by fostering data sharing among various actors involved. Originality/Value: This study contributes to the emerging theory on LLM potential and technical feasibility in service management, providing concrete insights for operators of TCS units and highlighting the need for further research to address limitations and validate the applicability of LLMs across different domains."
http://arxiv.org/abs/2406.05172v1,Early School Leaving in Spain: a longitudinal analysis by gender,2024-06-07 12:59:09+00:00,"['Martín Martín-González', 'Sara M. González-Betancor', 'Carmen Pérez-Esparrells']",econ.GN,"Spain is one of the eight EU-27 countries that failed to reduce early school leaving (ESL) below 10% in 2020, and now faces the challenge of achieving a rate below 9% by 2030. The determinants of this phenomenon are usually studied using cross-sectional data at the micro-level and without differentiation by gender. In this study, we analyse it for the first time for Spain using panel data (between 2002-2020), taking into account the high regional inequalities at the macroeconomic level and the masculinisation of the phenomenon. The results show a positive relationship between ESL and socioeconomic variables such as the adolescent fertility rate, immigration, unemployment or the weight of the industrial and construction sectors in the regional economy, with significant gender differences that invite us to discuss educational policies. Surprisingly, youth unemployment has only small but significant impact on female ESL."
http://arxiv.org/abs/2406.06804v1,Robustness to Missing Data: Breakdown Point Analysis,2024-06-10 21:17:29+00:00,['Daniel Ober-Reynolds'],econ.EM,"Missing data is pervasive in econometric applications, and rarely is it plausible that the data are missing (completely) at random. This paper proposes a methodology for studying the robustness of results drawn from incomplete datasets. Selection is measured as the squared Hellinger divergence between the distributions of complete and incomplete observations, which has a natural interpretation. The breakdown point is defined as the minimal amount of selection needed to overturn a given result. Reporting point estimates and lower confidence intervals of the breakdown point is a simple, concise way to communicate the robustness of a result. An estimator of the breakdown point of a result drawn from a generalized method of moments model is proposed and shown root-n consistent and asymptotically normal under mild assumptions. Lower confidence intervals of the breakdown point are simple to construct. The paper concludes with a simulation study illustrating the finite sample performance of the estimators in several common models."
http://arxiv.org/abs/2406.01722v2,"On Labs and Fabs: Mapping How Alliances, Acquisitions, and Antitrust are Shaping the Frontier AI Industry",2024-06-03 18:28:33+00:00,['Tomás Aguirre'],econ.GN,"As frontier AI models advance, policy proposals for safe AI development are gaining increasing attention from researchers and policymakers. This paper explores the current integration in the AI supply chain, focusing on vertical relationships and strategic partnerships among AI labs, cloud providers, chip manufacturers, and lithography companies. It aims to lay the groundwork for a deeper understanding of the implications of various governance interventions, including antitrust measures. The study has two main contributions. First, it profiles 25 leading companies in the AI supply chain, analyzing 300 relationships and noting 80 significant mergers and acquisitions along with 40 antitrust cases. Second, we discuss potential market definitions and the integration drivers based on the observed trends. The analysis reveals predominant horizontal integration through natural growth rather than acquisitions and notable trends of backward vertical integration in the semiconductor supply chain. Strategic partnerships are also significant downstream, especially between AI companies and cloud providers, with large tech companies often pursuing conglomerate integration by acquiring specialized AI startups or forming alliances with frontier AI labs. To further understand the strategic partnerships in the industry, we provide three brief case studies featuring companies like OpenAI and Nvidia. We conclude by posing open research questions on market dynamics and possible governance interventions, such as licensing and safety audits."
http://arxiv.org/abs/2406.07809v1,Did Harold Zuercher Have Time-Separable Preferences?,2024-06-12 02:03:57+00:00,"['Jay Lu', 'Yao Luo', 'Kota Saito', 'Yi Xin']",econ.EM,"This paper proposes an empirical model of dynamic discrete choice to allow for non-separable time preferences, generalizing the well-known Rust (1987) model. Under weak conditions, we show the existence of value functions and hence well-defined optimal choices. We construct a contraction mapping of the value function and propose an estimation method similar to Rust's nested fixed point algorithm. Finally, we apply the framework to the bus engine replacement data. We improve the fit of the data with our general model and reject the null hypothesis that Harold Zuercher has separable time preferences. Misspecifying an agent's preference as time-separable when it is not leads to biased inferences about structure parameters (such as the agent's risk attitudes) and misleading policy recommendations."
http://arxiv.org/abs/2405.19463v1,Stochastic Optimization Algorithms for Instrumental Variable Regression with Streaming Data,2024-05-29 19:21:55+00:00,"['Xuxing Chen', 'Abhishek Roy', 'Yifan Hu', 'Krishnakumar Balasubramanian']",stat.ML,"We develop and analyze algorithms for instrumental variable regression by viewing the problem as a conditional stochastic optimization problem. In the context of least-squares instrumental variable regression, our algorithms neither require matrix inversions nor mini-batches and provides a fully online approach for performing instrumental variable regression with streaming data. When the true model is linear, we derive rates of convergence in expectation, that are of order $\mathcal{O}(\log T/T)$ and $\mathcal{O}(1/T^{1-ι})$ for any $ι>0$, respectively under the availability of two-sample and one-sample oracles, respectively, where $T$ is the number of iterations. Importantly, under the availability of the two-sample oracle, our procedure avoids explicitly modeling and estimating the relationship between confounder and the instrumental variables, demonstrating the benefit of the proposed approach over recent works based on reformulating the problem as minimax optimization problems. Numerical experiments are provided to corroborate the theoretical results."
http://arxiv.org/abs/2406.01407v1,Utilizing Large Language Models for Automating Technical Customer Support,2024-06-03 15:06:29+00:00,"['Jochen Wulf', 'Jürg Meierhofer']",econ.GN,"The use of large language models (LLMs) such as OpenAI's GPT-4 in technical customer support (TCS) has the potential to revolutionize this area. This study examines automated text correction, summarization of customer inquiries and question answering using LLMs. Through prototypes and data analyses, the potential and challenges of integrating LLMs into the TCS will be demonstrated. Our results show promising approaches for improving the efficiency and quality of customer service through LLMs, but also emphasize the need for quality-assured implementation and organizational adjustments in the data ecosystem."
http://arxiv.org/abs/2406.00442v1,Optimizing hydrogen and e-methanol production through Power-to-X integration in biogas plants,2024-06-01 13:40:36+00:00,"['Alberto Alamia', 'Behzad Partoon', 'Eoghan Rattigan', 'Gorm Brunn Andresen']",econ.EM,"The European Union strategy for net zero emissions relies on developing hydrogen and electro fuels infrastructure. These fuels will be crucial as energy carriers and balancing agents for renewable energy variability. Large scale production requires more renewable capacity, and various Power to X (PtX) concepts are emerging in renewable rich countries. However, sourcing renewable carbon to scale carbon based electro fuels is a significant challenge. This study explores a PtX hub that sources renewable CO2 from biogas plants, integrating renewable energy, hydrogen production, and methanol synthesis on site. This concept creates an internal market for energy and materials, interfacing with the external energy system. The size and operation of the PtX hub were optimized, considering integration with local energy systems and a potential hydrogen grid. The levelized costs of hydrogen and methanol were estimated for a 2030 start, considering new legislation on renewable fuels of non biological origin (RFNBOs). Our results show the PtX hub can rely mainly on on site renewable energy, selling excess electricity to the grid. A local hydrogen grid connection improves operations, and the behind the meter market lowers energy prices, buffering against market variability. We found methanol costs could be below 650 euros per ton and hydrogen production costs below 3 euros per kg, with standalone methanol plants costing 23 per cent more. The CO2 recovery to methanol production ratio is crucial, with over 90 per cent recovery requiring significant investment in CO2 and H2 storage. Overall, our findings support planning PtX infrastructures integrated with the agricultural sector as a cost effective way to access renewable carbon."
http://arxiv.org/abs/2406.12445v1,DAOs' Business Value from an Open Systems Perspective: A Best-Fit Framework Synthesis,2024-06-18 09:48:10+00:00,"['Lukas Küng', 'George M. Giaglis']",cs.CY,"Decentralized autonomous organizations (DAOs) are emerging innovative organizational structures, enabling collective coordination, and reshaping digital collaboration. Despite the promising and transformative characteristics of DAOs, the potential technological advancements and the understanding of the business value that organizations derive from implementing DAO characteristics are limited. This research applies a systematic review of DAOs' business applicability from an open systems perspective following a best-fit framework methodology. Within our approach, combining both framework and thematic analysis, we discuss how the open business principles apply to DAOs and present a new DAO business framework comprising of four core business elements: i) token, ii) transactions, iii) value system and iv) strategy with their corresponding sub-characteristics. This paper offers a preliminary DAO business framework that enhances the understanding of DAOs' transformative potential and guides organizations in innovating more inclusive business models (BMs), while also providing a theoretical foundation for researchers to build upon."
http://arxiv.org/abs/2406.09473v1,Multidimensional clustering in judge designs,2024-06-13 07:44:19+00:00,"['Johannes W. Ligtenberg', 'Tiemen Woutersen']",econ.EM,"Estimates in judge designs run the risk of being biased due to the many judge identities that are implicitly or explicitly used as instrumental variables. The usual method to analyse judge designs, via a leave-out mean instrument, eliminates this many instrument bias only in case the data are clustered in at most one dimension. What is left out in the mean defines this clustering dimension. How most judge designs cluster their standard errors, however, implies that there are additional clustering dimensions, which makes that a many instrument bias remains. We propose two estimators that are many instrument bias free, also in multidimensional clustered judge designs. The first generalises the one dimensional cluster jackknife instrumental variable estimator, by removing from this estimator the additional bias terms due to the extra dependence in the data. The second models all but one clustering dimensions by fixed effects and we show how these numerous fixed effects can be removed without introducing extra bias. A Monte-Carlo experiment and the revisitation of two judge designs show the empirical relevance of properly accounting for multidimensional clustering in estimation."
http://arxiv.org/abs/2406.09582v1,Existence and structure of Nash equilibria for supermodular games,2024-06-13 20:41:15+00:00,['Lu Yu'],econ.TH,"Two theorems announced by Topkis about the topological description of sublattices are proved. They are applied to extend some classical results concerning the existence and the order structure of Nash equilibria of certain supermodular games, with some problems in Zhou's proof corrected."
http://arxiv.org/abs/2406.09490v1,Newswire: A Large-Scale Structured Database of a Century of Historical News,2024-06-13 16:20:05+00:00,"['Emily Silcock', 'Abhishek Arora', ""Luca D'Amico-Wong"", 'Melissa Dell']",cs.CL,"In the U.S. historically, local newspapers drew their content largely from newswires like the Associated Press. Historians argue that newswires played a pivotal role in creating a national identity and shared understanding of the world, but there is no comprehensive archive of the content sent over newswires. We reconstruct such an archive by applying a customized deep learning pipeline to hundreds of terabytes of raw image scans from thousands of local newspapers. The resulting dataset contains 2.7 million unique public domain U.S. newswire articles, written between 1878 and 1977. Locations in these articles are georeferenced, topics are tagged using customized neural topic classification, named entities are recognized, and individuals are disambiguated to Wikipedia using a novel entity disambiguation model. To construct the Newswire dataset, we first recognize newspaper layouts and transcribe around 138 millions structured article texts from raw image scans. We then use a customized neural bi-encoder model to de-duplicate reproduced articles, in the presence of considerable abridgement and noise, quantifying how widely each article was reproduced. A text classifier is used to ensure that we only include newswire articles, which historically are in the public domain. The structured data that accompany the texts provide rich information about the who (disambiguated individuals), what (topics), and where (georeferencing) of the news that millions of Americans read over the course of a century. We also include Library of Congress metadata information about the newspapers that ran the articles on their front pages. The Newswire dataset is useful both for large language modeling - expanding training data beyond what is available from modern web texts - and for studying a diversity of questions in computational linguistics, social science, and the digital humanities."
http://arxiv.org/abs/2405.01341v1,Dynamic opinion updating with endogenous networks,2024-05-02 14:45:04+00:00,"['Ugo Bolletta', 'Paolo Pin']",econ.TH,"Polarization is a well-documented phenomenon across a wide range of social issues. However, prevailing theories often compartmentalize the examination of herding behavior and opinion convergence within different contexts. In this study, we delve into the micro-foundations of how individuals strategically select reference groups, offering insight into a dynamic process where both individual opinions and the network evolve simultaneously. We base our model on two parameters: people's direct benefit from connections and their adaptability in adjusting their opinions. Our research highlights which conditions impede the network from achieving complete connectivity, resulting in enduring polarization. Notably, our model also reveals that polarization can transiently emerge during the transition towards consensus. We explore the connection between these scenarios and a critical network metric: the initial diameter, under specific conditions related to the initial distribution of opinions."
http://arxiv.org/abs/2405.01798v2,The Economy and Public Diplomacy: An Analysis of RT's Economic Content and Context on Facebook,2024-05-03 01:04:10+00:00,"['Ayse D. Lokmanoglu', 'Carol K. Winkler', 'Kareem El Damanhoury', 'Virginia Massignan', 'Esteban Villa-Turek', 'Keyu Alexander Chen']",cs.IT,"With globalization's rise, economic interdependence's impacts have become a prominent factor affecting personal lives, as well as national and international dynamics. This study examines RT's public diplomacy efforts on its non-Russian Facebook accounts over the past five years to identify the prominence of economic topics across language accounts. Computational analysis, including word embeddings and statistical methods, investigates how offline economic indicators, like currency values and oil prices, correspond to RT's online economic content changes. The results demonstrate that RT uses message reinforcement associated economic topics as an audience targeting strategy and differentiates their use with changing currency and oil values."
http://arxiv.org/abs/2404.18499v1,Quantitative Tools for Time Series Analysis in Natural Language Processing: A Practitioners Guide,2024-04-29 08:41:17+00:00,['W. Benedikt Schmal'],econ.GN,"Natural language processing tools have become frequently used in social sciences such as economics, political science, and sociology. Many publications apply topic modeling to elicit latent topics in text corpora and their development over time. Here, most publications rely on visual inspections and draw inference on changes, structural breaks, and developments over time. We suggest using univariate time series econometrics to introduce more quantitative rigor that can strengthen the analyses. In particular, we discuss the econometric topics of non-stationarity as well as structural breaks. This paper serves as a comprehensive practitioners guide to provide researchers in the social and life sciences as well as the humanities with concise advice on how to implement econometric time series methods to thoroughly investigate topic prevalences over time. We provide coding advice for the statistical software R throughout the paper. The application of the discussed tools to a sample dataset completes the analysis."
http://arxiv.org/abs/2405.05220v1,Causal Duration Analysis with Diff-in-Diff,2024-05-08 17:13:34+00:00,"['Ben Deaner', 'Hyejin Ku']",econ.EM,"In economic program evaluation, it is common to obtain panel data in which outcomes are indicators that an individual has reached an absorbing state. For example, they may indicate whether an individual has exited a period of unemployment, passed an exam, left a marriage, or had their parole revoked. The parallel trends assumption that underpins difference-in-differences generally fails in such settings. We suggest identifying conditions that are analogous to those of difference-in-differences but apply to hazard rates rather than mean outcomes. These alternative assumptions motivate estimators that retain the simplicity and transparency of standard diff-in-diff, and we suggest analogous specification tests. Our approach can be adapted to general linear restrictions between the hazard rates of different groups, motivating duration analogues of the triple differences and synthetic control methods. We apply our procedures to examine the impact of a policy that increased the generosity of unemployment benefits, using a cross-cohort comparison."
http://arxiv.org/abs/2404.17885v1,Sequential monitoring for explosive volatility regimes,2024-04-27 12:45:22+00:00,"['Lajos Horvath', 'Lorenzo Trapani', 'Shixuan Wang']",econ.EM,"In this paper, we develop two families of sequential monitoring procedure to (timely) detect changes in a GARCH(1,1) model. Whilst our methodologies can be applied for the general analysis of changepoints in GARCH(1,1) sequences, they are in particular designed to detect changes from stationarity to explosivity or vice versa, thus allowing to check for volatility bubbles. Our statistics can be applied irrespective of whether the historical sample is stationary or not, and indeed without prior knowledge of the regime of the observations before and after the break. In particular, we construct our detectors as the CUSUM process of the quasi-Fisher scores of the log likelihood function. In order to ensure timely detection, we then construct our boundary function (exceeding which would indicate a break) by including a weighting sequence which is designed to shorten the detection delay in the presence of a changepoint. We consider two types of weights: a lighter set of weights, which ensures timely detection in the presence of changes occurring early, but not too early after the end of the historical sample; and a heavier set of weights, called Renyi weights which is designed to ensure timely detection in the presence of changepoints occurring very early in the monitoring horizon. In both cases, we derive the limiting distribution of the detection delays, indicating the expected delay for each set of weights. Our theoretical results are validated via a comprehensive set of simulations, and an empirical application to daily returns of individual stocks."
http://arxiv.org/abs/2404.17713v1,Revisiting the Resource Curse in the Age of Energy Transition: Cobalt Reserves and Conflict in Africa,2024-04-26 22:16:32+00:00,['Weihong Qi'],econ.GN,"This study reevaluates the traditional understanding of the ""political resource curse"" by examining the unique impact of energy transition metals, specifically cobalt, on local-level conflicts in Africa. Contrary to previous studies that primarily focus on high-value minerals and their political outcomes resulted from substantial economic revenues, this study investigates cobalt's influence on local conflict. Despite its strategic importance, cobalt's limited commercial value presents a unique yet critical case for analysis. Different with the prevailing view that links mineral reserves with increased conflict, this research finds that regions rich in cobalt experience a reduction in conflict. This decrease is attributed to enhanced government security measures, which are implemented independently of the economic benefits derived from cobalt as a commodity. The study utilizes a combination of georeferenced data and a difference-in-difference design to analyze the causal relationship between cobalt deposits and regional conflict. The findings suggest that the presence of cobalt deposits leads to enhanced security interventions by governments, effectively reducing the likelihood of non-governmental actors taking control of these territories. This pattern offers a new perspective on the role of energy transition metals in shaping conflict and governance, highlighting the need to reassess theoretical frameworks related to the political implications of natural resources with the ongoing energy revolution."
http://arxiv.org/abs/2406.15311v1,The disruption index suffers from citation inflation and is confounded by shifts in scholarly citation practice,2024-06-21 17:10:01+00:00,"['Alexander M. Petersen', 'Felber Arroyave', 'Fabio Pammolli']",cs.DL,"Measuring the rate of innovation in academia and industry is fundamental to monitoring the efficiency and competitiveness of the knowledge economy. To this end, a disruption index (CD) was recently developed and applied to publication and patent citation networks (Wu et al., Nature 2019; Park et al., Nature 2023). Here we show that CD systematically decreases over time due to secular growth in research and patent production, following two distinct mechanisms unrelated to innovation -- one behavioral and the other structural. Whereas the behavioral explanation reflects shifts associated with techno-social factors (e.g. self-citation practices), the structural explanation follows from `citation inflation' (CI), an inextricable feature of real citation networks attributable to increasing reference list lengths, which causes CD to systematically decrease. We demonstrate this causal link by way of mathematical deduction, computational simulation, multi-variate regression, and quasi-experimental comparison of the disruptiveness of PNAS versus PNAS Plus articles, which differ only in their lengths. Accordingly, we analyze CD data available in the SciSciNet database and find that disruptiveness incrementally increased from 2005-2015, and that the negative relationship between disruption and team-size is remarkably small in overall magnitude effect size, and shifts from negative to positive for team size $\geq$ 8 coauthors."
http://arxiv.org/abs/2406.13826v1,Testing identification in mediation and dynamic treatment models,2024-06-19 20:45:15+00:00,"['Martin Huber', 'Kevin Kloiber', 'Lukas Laffers']",econ.EM,"We propose a test for the identification of causal effects in mediation and dynamic treatment models that is based on two sets of observed variables, namely covariates to be controlled for and suspected instruments, building on the test by Huber and Kueck (2022) for single treatment models. We consider models with a sequential assignment of a treatment and a mediator to assess the direct treatment effect (net of the mediator), the indirect treatment effect (via the mediator), or the joint effect of both treatment and mediator. We establish testable conditions for identifying such effects in observational data. These conditions jointly imply (1) the exogeneity of the treatment and the mediator conditional on covariates and (2) the validity of distinct instruments for the treatment and the mediator, meaning that the instruments do not directly affect the outcome (other than through the treatment or mediator) and are unconfounded given the covariates. Our framework extends to post-treatment sample selection or attrition problems when replacing the mediator by a selection indicator for observing the outcome, enabling joint testing of the selectivity of treatment and attrition. We propose a machine learning-based test to control for covariates in a data-driven manner and analyze its finite sample performance in a simulation study. Additionally, we apply our method to Slovak labor market data and find that our testable implications are not rejected for a sequence of training programs typically considered in dynamic treatment evaluations."
http://arxiv.org/abs/2406.13882v1,Allocation Requires Prediction Only if Inequality Is Low,2024-06-19 23:23:32+00:00,"['Ali Shirali', 'Rediet Abebe', 'Moritz Hardt']",cs.LG,"Algorithmic predictions are emerging as a promising solution concept for efficiently allocating societal resources. Fueling their use is an underlying assumption that such systems are necessary to identify individuals for interventions. We propose a principled framework for assessing this assumption: Using a simple mathematical model, we evaluate the efficacy of prediction-based allocations in settings where individuals belong to larger units such as hospitals, neighborhoods, or schools. We find that prediction-based allocations outperform baseline methods using aggregate unit-level statistics only when between-unit inequality is low and the intervention budget is high. Our results hold for a wide range of settings for the price of prediction, treatment effect heterogeneity, and unit-level statistics' learnability. Combined, we highlight the potential limits to improving the efficacy of interventions through prediction."
http://arxiv.org/abs/2406.13726v1,Global Solutions to Master Equations for Continuous Time Heterogeneous Agent Macroeconomic Models,2024-06-19 17:42:53+00:00,"['Zhouzhou Gu', 'Mathieu Laurière', 'Sebastian Merkel', 'Jonathan Payne']",math.OC,"We propose and compare new global solution algorithms for continuous time heterogeneous agent economies with aggregate shocks. First, we approximate the agent distribution so that equilibrium in the economy can be characterized by a high, but finite, dimensional non-linear partial differential equation. We consider different approximations: discretizing the number of agents, discretizing the agent state variables, and projecting the distribution onto a finite set of basis functions. Second, we represent the value function using a neural network and train it to solve the differential equation using deep learning tools. We refer to the solution as an Economic Model Informed Neural Network (EMINN). The main advantage of this technique is that it allows us to find global solutions to high dimensional, non-linear problems. We demonstrate our algorithm by solving important models in the macroeconomics and spatial literatures (e.g. Krusell and Smith (1998), Khan and Thomas (2007), Bilal (2023))."
http://arxiv.org/abs/2405.17166v1,Cross-border cannibalization: Spillover effects of wind and solar energy on interconnected European electricity markets,2024-05-27 13:42:35+00:00,"['Clemens Stiewe', 'Alice Lixuan Xu', 'Anselm Eicke', 'Lion Hirth']",econ.EM,"The average revenue, or market value, of wind and solar energy tends to fall with increasing market shares, as is now evident across European electricity markets. At the same time, these markets have become more interconnected. In this paper, we empirically study the multiple cross-border effects on the value of renewable energy: on one hand, interconnection is a flexibility resource that allows to export energy when it is locally abundant, benefitting renewables. On the other hand, wind and solar radiation are correlated across space, so neighboring supply adds to the local one to depress domestic prices. We estimate both effects, using spatial panel regression on electricity market data from 2015 to 2023 from 30 European bidding zones. We find that domestic wind and solar value is not only depressed by domestic, but also by neighboring renewables expansion. The better interconnected a market is, the smaller the effect of domestic but the larger the effect of neighboring renewables. While wind value is stabilized by interconnection, solar value is not. If wind market share increases both at home and in neighboring markets by one percentage point, the value factor of wind energy is reduced by just above 1 percentage points. For solar, this number is almost 4 percentage points."
http://arxiv.org/abs/2406.11528v1,Optimal Robust Contract Design,2024-06-17 13:29:24+00:00,"['Bo Peng', 'Zhihao Gavin Tang']",econ.TH,"We consider the robust contract design problem when the principal only has limited information about the actions the agent can take. The principal evaluates a contract according to its worst-case performance caused by the uncertain action space. Carroll (AER 2015) showed that a linear contract is optimal among deterministic contracts. Recently, Kambhampati (JET 2023) showed that the principal's payoff can be strictly increased via randomization over linear contracts. In this paper, we characterize the optimal randomized contract, which remains linear and admits a closed form of its cumulative density function. The advantage of randomized contracts over deterministic contracts can be arbitrarily large even when the principal knows only one non-trivial action of the agent. Furthermore, our result generalizes to the model of contracting with teams, by Dai and Toikka (Econometrica 2022)."
http://arxiv.org/abs/2406.18463v1,Complexity Aversion,2024-06-26 16:14:14+00:00,"['Yuan Gu', 'Chao Hung Chan']",econ.TH,"This paper proposes a model of decision-making under uncertainty in which an agent is constrained in her cognitive ability to consider complex acts. We identify the complexity of an act according to the corresponding partition of state space. The agent ranks acts according to the expected utility net of complexity cost. A key feature of this model is that the agent is able to update her complexity cost function after the arrival of new information. The main result characterizes axiomatically an updating rule for complexity cost function, the Minimal Complexity Aversion representation. According to this rule, the agent measures the complexity cost of an act conditional on the new information by using the cost of another act that gives exactly the same partition of the event but with the lowest ex-ante cost."
http://arxiv.org/abs/2405.11371v2,A simple proof of the representation theorem for betweenness preferences,2024-05-18 19:09:26+00:00,['Yutaro Akita'],econ.TH,This paper presents a simple proof of Dekel (1986)'s representation theorem for betweenness preferences. The proof is based on the separation theorem.
http://arxiv.org/abs/2405.15781v2,Prediction of healthcare costs on consumer direct health plan in the Brazilian context,2024-04-20 08:38:57+00:00,"['Claudia M. Peixoto', 'Diego Marcondes', 'Mariana P. Melo', 'Ana C. Maia', 'Luis A. Correia']",econ.GN,"The rise in healthcare costs has led to the adoption of cost-sharing devices in health plans. This article explores this discussion by simulating Health Savings Accounts (HSAs) to cover medical and hospital expenses, supported by catastrophic insurance. Simulating 10 million lives, we evaluate the utilization of catastrophic insurance and the balances of HSAs at the end of working life. To estimate annual expenditures, a Markov Chains approach - distinct from the usual ones - was used based on recent past expenditures, age range, and gender. The results suggest that HSAs do not create inequalities, offering a viable method to sustain private healthcare financing for the elderly."
http://arxiv.org/abs/2406.19063v1,Convex Choice,2024-06-27 10:29:52+00:00,"['Navin Kartik', 'Andreas Kleiner']",econ.TH,"For multidimensional Euclidean type spaces, we study convex choice: from any choice set, the set of types that make the same choice is convex. We establish that, in a suitable sense, this property characterizes the sufficiency of local incentive constraints. Convex choice is also of interest more broadly. We tie convex choice to a notion of directional single-crossing differences (DSCD). For an expected-utility agent choosing among lotteries, DSCD implies that preferences are either one-dimensional or must take the affine form that has been tractable in multidimensional mechanism design."
http://arxiv.org/abs/2406.19033v1,Factor multivariate stochastic volatility models of high dimension,2024-06-27 09:39:10+00:00,"['Benjamin Poignard', 'Manabu Asai']",econ.EM,"Building upon the pertinence of the factor decomposition to break the curse of dimensionality inherent to multivariate volatility processes, we develop a factor model-based multivariate stochastic volatility (fMSV) framework that relies on two viewpoints: sparse approximate factor model and sparse factor loading matrix. We propose a two-stage estimation procedure for the fMSV model: the first stage obtains the estimators of the factor model, and the second stage estimates the MSV part using the estimated common factor variables. We derive the asymptotic properties of the estimators. Simulated experiments are performed to assess the forecasting performances of the covariance matrices. The empirical analysis based on vectors of asset returns illustrates that the forecasting performances of the fMSV models outperforms competing conditional covariance models."
http://arxiv.org/abs/2406.18936v1,Credit Ratings: Heterogeneous Effect on Capital Structure,2024-06-27 07:08:14+00:00,"['Helmut Wasserbacher', 'Martin Spindler']",econ.GN,"Why do companies choose particular capital structures? A compelling answer to this question remains elusive despite extensive research. In this article, we use double machine learning to examine the heterogeneous causal effect of credit ratings on leverage. Taking advantage of the flexibility of random forests within the double machine learning framework, we model the relationship between variables associated with leverage and credit ratings without imposing strong assumptions about their functional form. This approach also allows for data-driven variable selection from a large set of individual company characteristics, supporting valid causal inference. We report three findings: First, credit ratings causally affect the leverage ratio. Having a rating, as opposed to having none, increases leverage by approximately 7 to 9 percentage points, or 30\% to 40\% relative to the sample mean leverage. However, this result comes with an important caveat, captured in our second finding: the effect is highly heterogeneous and varies depending on the specific rating. For AAA and AA ratings, the effect is negative, reducing leverage by about 5 percentage points. For A and BBB ratings, the effect is approximately zero. From BB ratings onwards, the effect becomes positive, exceeding 10 percentage points. Third, contrary to what the second finding might imply at first glance, the change from no effect to a positive effect does not occur abruptly at the boundary between investment and speculative grade ratings. Rather, it is gradual, taking place across the granular rating notches (""+/-"") within the BBB and BB categories."
http://arxiv.org/abs/2404.19555v1,Transforming Credit Guarantee Schemes with Distributed Ledger Technology,2024-04-30 13:38:05+00:00,"['Sabrina Leo', 'Andrea Delle Foglie', 'Luca Barbaro', 'Edoardo Marangone', 'Ida Claudia Panetta', 'Claudio Di Ciccio']",cs.CE,"Credit Guarantee Schemes (CGSs) are crucial in mitigating SMEs' financial constraints. However, they are renownedly affected by critical shortcomings, such as a lack of financial sustainability and operational efficiency. Distributed Ledger Technologies (DLTs) have shown significant revolutionary influence in several sectors, including finance and banking, thanks to the full operational traceability they bring alongside verifiable computation. Nevertheless, the potential synergy between DLTs and CGSs has not been thoroughly investigated yet. This paper proposes a comprehensive framework to utilise DLTs, particularly blockchain technologies, in CGS processes to improve operational efficiency and effectiveness. To this end, we compare key architectural characteristics considering access level, governance structure, and consensus method, to examine their fit with CGS processes. We believe this study can guide policymakers and stakeholders, thereby stimulating further innovation in this promising field."
http://arxiv.org/abs/2405.01913v1,"Unleashing the Power of AI: Transforming Marketing Decision-Making in Heavy Machinery with Machine Learning, Radar Chart Simulation, and Markov Chain Analysis",2024-05-03 08:12:14+00:00,"['Tian Tian', 'Jiahao Deng']",econ.EM,"This pioneering research introduces a novel approach for decision-makers in the heavy machinery industry, specifically focusing on production management. The study integrates machine learning techniques like Ridge Regression, Markov chain analysis, and radar charts to optimize North American Crawler Cranes market production processes. Ridge Regression enables growth pattern identification and performance assessment, facilitating comparisons and addressing industry challenges. Markov chain analysis evaluates risk factors, aiding in informed decision-making and risk management. Radar charts simulate benchmark product designs, enabling data-driven decisions for production optimization. This interdisciplinary approach equips decision-makers with transformative insights, enhancing competitiveness in the heavy machinery industry and beyond. By leveraging these techniques, companies can revolutionize their production management strategies, driving success in diverse markets."
http://arxiv.org/abs/2404.17049v1,Overidentification in Shift-Share Designs,2024-04-25 21:16:02+00:00,"['Jinyong Hahn', 'Guido Kuersteiner', 'Andres Santos', 'Wavid Willigrod']",econ.EM,"This paper studies the testability of identifying restrictions commonly employed to assign a causal interpretation to two stage least squares (TSLS) estimators based on Bartik instruments. For homogeneous effects models applied to short panels, our analysis yields testable implications previously noted in the literature for the two major available identification strategies. We propose overidentification tests for these restrictions that remain valid in high dimensional regimes and are robust to heteroskedasticity and clustering. We further show that homogeneous effect models in short panels, and their corresponding overidentification tests, are of central importance by establishing that: (i) In heterogenous effects models, interpreting TSLS as a positively weighted average of treatment effects can impose implausible assumptions on the distribution of the data; and (ii) Alternative identifying strategies relying on long panels can prove uninformative in short panel applications. We highlight the empirical relevance of our results by examining the viability of Bartik instruments for identifying the effect of rising Chinese import competition on US local labor markets."
http://arxiv.org/abs/2405.07084v1,Counting steps for re-stabilization in a labor matching market,2024-05-11 19:53:25+00:00,"['Agustin G. Bonifacio', 'Nadia Guiñazu', 'Noelia Juarez', 'Pablo Neme', 'Jorge Oviedo']",econ.TH,"We study a one-to-one labor matching market. If a worker considers resigning from her current job to obtain a better one, how long does it take for this worker to actually get it? We present an algorithm that models this situation as a re-stabilization process involving a vacancy chain. Each step of the algorithm is a link of such a chain. We show that the length of this vacancy chain, which can be interpreted as the time the worker has to wait for her new job, is intimately connected with the lattice structure of the set of stable matchings of the market. Namely, this length can be computed by considering the cardinalities of cycles in preferences derived from the initial and final stable matchings involved."
http://arxiv.org/abs/2405.07071v1,Colocation of skill related suppliers -- Revisiting coagglomeration using firm-to-firm network data,2024-05-11 19:00:12+00:00,"['Sándor Juhász', 'Zoltán Elekes', 'Virág Ilyés', 'Frank Neffke']",physics.soc-ph,"Strong local clusters help firms compete on global markets. One explanation for this is that firms benefit from locating close to their suppliers and customers. However, the emergence of global supply chains shows that physical proximity is not necessarily a prerequisite to successfully manage customer-supplier relations anymore. This raises the question when firms need to colocate in value chains and when they can coordinate over longer distances. We hypothesize that one important aspect is the extent to which supply chain partners exchange not just goods but also know-how. To test this, we build on an expanding literature that studies the drivers of industrial coagglomeration to analyze when supply chain connections lead firms to colocation. We exploit detailed micro-data for the Hungarian economy between 2015 and 2017, linking firm registries, employer-employee matched data and firm-to-firm transaction data from value-added tax records. This allows us to observe colocation, labor flows and value chain connections at the level of firms, as well as construct aggregated coagglomeration patterns, skill relatedness and input-output connections between pairs of industries. We show that supply chains are more likely to support coagglomeration when the industries involved are also skill related. That is, input-output and labor market channels reinforce each other, but supplier connections only matter for colocation when industries have similar labor requirements, suggesting that they employ similar types of know-how. We corroborate this finding by analyzing the interactions between firms, showing that supplier relations are more geographically constrained between companies that operate in skill related industries."
http://arxiv.org/abs/2405.07431v1,Packing Peanuts: The Role Synthetic Data Can Play in Enhancing Conventional Economic Prediction Models,2024-05-13 02:09:32+00:00,['Vansh Murad Kalia'],econ.GN,"Packing peanuts, as defined by Wikipedia, is a common loose-fill packaging and cushioning material that helps prevent damage to fragile items. In this paper, I propose that synthetic data, akin to packing peanuts, can serve as a valuable asset for economic prediction models, enhancing their performance and robustness when integrated with real data. This hybrid approach proves particularly beneficial in scenarios where data is either missing or limited in availability. Through the utilization of Affinity credit card spending and Womply small business datasets, this study demonstrates the substantial performance improvements achieved by employing a hybrid data approach, surpassing the capabilities of traditional economic modeling techniques."
http://arxiv.org/abs/2405.08052v1,"Trade Openness, Tariffs and Economic Growth: An Empirical Study from Countries of G-20",2024-05-13 16:43:21+00:00,['S M Toufiqul Huq Sowrov'],econ.GN,"International trade has been in the forefront of economic development and growth debates. Trade openness, its definition, scope, and impacts have also been studied numerously. Tariff has been dubbed as negative influencer of economic growth as per conventional wisdom and most empirical studies. This paper empirically examines relationships among trade openness as trade share to GDP, import tariff rate and economic growth. Panel dataset of 11 G-20 member countries were selected for the study. Results found a positively significant correlation between trade openness and economic growth. Tariff has negatively significant correlation with economic growth in lagged model. OLS and panel data fixed-effects regression were employed to carry out the regression analysis. To deal with endogeneity in trade openness variable, a 1-year lag regression technique was conducted. Results are robust and significant. Policy recommendation suggests country specific trade opening and tariff relaxation."
http://arxiv.org/abs/2405.05578v1,Shaping the Future of Urban Mobility: Insights into Autonomous Vehicle Acceptance in Shanghai Through TAM and Perceived Risk Analysis,2024-05-09 07:08:33+00:00,"['Miaomiao Shen', 'Linxuan Yu', 'Jing Xu', 'Zihao Sang', 'Ruijia Li', 'Xiang Yuan']",econ.GN,"Autonomous vehicles (AVs) have begun experimental commercialization initiatives in places such as Shanghai, China, and it is a valuable research question whether people's willingness to use AVs has changed from the prior. This study explores Shanghai residents' attitudes towards AVs by applying the Technology Acceptance Model (TAM), the Perceived Risk (BAR) model, and introducing perceived externalities as a new psychological variable. Through a survey in Shanghai, where AVs are operational, and structural equation modeling, it was found that perceived usefulness and ease of use positively influence willingness to use AVs, with perceived usefulness being the most significant factor. Perceived externalities have a positive impact, while perceived risk negatively affects willingness to use. Interestingly, ease of use increases perceived risk, but this is mitigated by the benefits perceived in usefulness. This research, differing significantly from previous studies, aims to guide government policy and industry strategies to enhance design, marketing, and popularization."
http://arxiv.org/abs/2405.13879v3,FACT or Fiction: Can Truthful Mechanisms Eliminate Federated Free Riding?,2024-05-22 17:59:44+00:00,"['Marco Bornstein', 'Amrit Singh Bedi', 'Abdirisak Mohamed', 'Furong Huang']",cs.GT,"Standard federated learning (FL) approaches are vulnerable to the free-rider dilemma: participating agents can contribute little to nothing yet receive a well-trained aggregated model. While prior mechanisms attempt to solve the free-rider dilemma, none have addressed the issue of truthfulness. In practice, adversarial agents can provide false information to the server in order to cheat its way out of contributing to federated training. In an effort to make free-riding-averse federated mechanisms truthful, and consequently less prone to breaking down in practice, we propose FACT. FACT is the first federated mechanism that: (1) eliminates federated free riding by using a penalty system, (2) ensures agents provide truthful information by creating a competitive environment, and (3) encourages agent participation by offering better performance than training alone. Empirically, FACT avoids free-riding when agents are untruthful, and reduces agent loss by over 4x."
http://arxiv.org/abs/2405.12083v5,Instrumented Difference-in-Differences with Heterogeneous Treatment Effects,2024-05-20 14:52:39+00:00,['Sho Miyaji'],econ.EM,"Many studies exploit variation in policy adoption timing across units as an instrument for treatment. This paper formalizes the underlying identification strategy as an instrumented difference-in-differences (DID-IV). In this design, a Wald-DID estimand, which scales the DID estimand of the outcome by the DID estimand of the treatment, captures the local average treatment effect on the treated (LATET). We extend the canonical DID-IV design to multiple period settings with the staggered adoption of the instrument across units. Moreover, we propose a credible estimation method in this design that is robust to treatment effect heterogeneity. We illustrate the empirical relevance of our findings, estimating returns to schooling in the United Kingdom. In this application, the two-way fixed effects instrumental variable regression, the conventional approach to implement DID-IV designs, yields a negative estimate. By contrast, our estimation method indicates a substantial gain from schooling."
http://arxiv.org/abs/2404.08288v2,Istanbul Flower Auction: The Need for Speed,2024-04-12 07:26:11+00:00,"['Isa Hafalir', 'Onur Kesten', 'Donglai Luo', 'Katerina Sherstyuk', 'Cong Tao']",econ.TH,"We examine a unique auction format used in the Istanbul flower market, which could transform into either Dutch or English auction depending on bidders' bidding behaviors. By introducing a time cost that reduces the value of a perishable good as time passes, we explore how this hybrid auction format accommodates the desire for speed via an adaptive starting price. We show that the Istanbul Flower Auction outperforms both the Dutch and English auctions in terms of the auctioneer's utility. With numerical analysis, we also illustrate the Istanbul Flower Auction's superiority in terms of social welfare and auction duration. Our results highlight the critical role of auction design in improving welfare when the duration of the auction process plays a role."
http://arxiv.org/abs/2405.09087v3,Why Transaction Cost Economics Failed and How to Fix It,2024-05-15 04:48:25+00:00,['Li Mingqian'],econ.TH,"The connotation of transaction costs has never been definitively determined, and the independence of the concept has never been rigorously demonstrated. This paper delves into the thought systems of several prominent economists in the development of transaction cost economics, starting from first-hand materials. By combining multiple works of the authors, it reconstructs the true meanings and identifies endogeneity issues and logical inconsistencies. The conclusion of this paper is bold. Previous research has been largely filled with misinterpretations and misunderstandings, as people have focused solely on the wording of transaction cost definitions, neglecting the nature of transaction costs. The intention of transaction cost theory has been unwittingly assimilated into the objects it intends to criticize. After delineating the framework of ""transaction costs-property rights-competition"", this paper reconstructs the concept of transaction costs and the history of transaction cost concepts, providing a direct response to this theoretical puzzle that has plagued the academic community for nearly a century."
http://arxiv.org/abs/2406.00941v2,A Robust Residual-Based Test for Structural Changes in Factor Models,2024-06-03 02:38:39+00:00,"['Bin Peng', 'Liangjun Su', 'Yayi Yan']",econ.EM,"In this paper, we propose an easy-to-implement residual-based specification testing procedure for detecting structural changes in factor models, which is powerful against both smooth and abrupt structural changes with unknown break dates. The proposed test is robust against the over-specified number of factors, and serially and crosssectionally correlated error processes. A new central limit theorem is given for the quadratic forms of panel data with dependence over both dimensions, thereby filling a gap in the literature. We establish the asymptotic properties of the proposed test statistic, and accordingly develop a simulation-based scheme to select critical value in order to improve finite sample performance. Through extensive simulations and a real-world application, we confirm our theoretical results and demonstrate that the proposed test exhibits desirable size and power in practice."
http://arxiv.org/abs/2405.19104v2,Phase transitions in debt recycling,2024-05-29 14:07:52+00:00,"['Sabrina Aufiero', 'Preben Forer', 'Pierpaolo Vivo', 'Fabio Caccioli', 'Silvia Bartolucci']",q-fin.RM,"Debt recycling is an aggressive equity extraction strategy that potentially permits faster repayment of a mortgage. While equity progressively builds up as the mortgage is repaid monthly, mortgage holders may obtain another loan they could use to invest on a risky asset. The wealth produced by a successful investment is then used to repay the mortgage faster. The strategy is riskier than a standard repayment plan since fluctuations in the house market and investment's volatility may also lead to a fast default, as both the mortgage and the liquidity loan are secured against the same good. The general conditions of the mortgage holder and the outside market under which debt recycling may be recommended or discouraged have not been fully investigated. In this paper, to evaluate the effectiveness of traditional monthly mortgage repayment versus debt recycling strategies, we build a dynamical model of debt recycling and study the time evolution of equity and mortgage balance as a function of loan-to-value ratio, house market performance, and return of the risky investment. We find that the model has a rich behavior as a function of its main parameters, showing strongly and weakly successful phases - where the mortgage is eventually repaid faster and slower than the standard monthly repayment strategy, respectively - a default phase where the equity locked in the house vanishes before the mortgage is repaid, signalling a failure of the debt recycling strategy, and a permanent re-mortgaging phase - where further investment funds from the lender are continuously secured, but the mortgage is never fully repaid. The strategy's effectiveness is found to be highly sensitive to the initial mortgage-to-equity ratio, the monthly amount of scheduled repayments, and the economic parameters at the outset. The analytical results are corroborated with numerical simulations with excellent agreement."
http://arxiv.org/abs/2406.08955v2,Equality of Opportunity and Opportunity Pluralism,2024-06-13 09:34:27+00:00,['Giovanni Valvassori Bolgè'],econ.TH,"This paper seeks to explore the potential trade-off arising between the theories of $\textit{Equality of Opportunity}$ and $\textit{Opportunity Pluralism}$. Whereas the first theory has received much attention in the literature on Welfare Economics, the second one has only recently been introduced with the publication of the book by Joseph Fishkin, $\textit{Bottlenecks: A New Theory of Equal Opportunity}$. After arguing extensively that any notion of human flourishing is incompatible with traditional theories of $\textit{Equality of Opportunity}$, the author proposes an alternative theory squarely based on a broad notion of human development. This paper seeks to formalize the argument made in this book through the lens of economic theory. My analysis suggests that traditional theories of $\textit{Equality of Opportunity}$ are not incompatible with $\textit{Opportunity Pluralism}$."
http://arxiv.org/abs/2406.08936v2,Mechanism Design by a Politician,2024-06-13 09:06:41+00:00,['Giovanni Valvassori Bolgè'],econ.TH,"A set of agents has to make a decision about the provision of a public good and its financing. Agents have heterogeneous values for the public good and each agent's value is private information. An agenda-setter has the right to make a proposal about a public-good level and a vector of contributions. For the proposal to be approved, only the favourable votes of a subset of agents are needed. If the proposal is not approved, a type-dependent outside option is implemented. I characterize the optimal public-good provision and the coalition-formation for any outside option in dominant strategies. Optimal public-good provision might be a non-monotonic function of the outside option public-good level. Moreover, the optimal coalition might be a non-convex set of types."
http://arxiv.org/abs/2405.05744v4,Designing Social Learning,2024-05-09 13:09:15+00:00,"['Aleksei Smirnov', 'Egor Starkov']",econ.TH,"This paper studies strategic communication in the context of social learning. Product reviews are used by consumers to learn product quality, but in order to write a review, a consumer must be convinced to purchase the item first. When reviewers care about welfare of future consumers, this leads to a conflict: a reviewer today wants the future consumers to purchase the item even when this comes at a loss to them, so that more information is revealed for the consumers that come after. We show that due to this conflict, communication via reviews is inevitably noisy, regardless of whether reviewers can commit to a communication strategy or have to resort to cheap talk. The optimal communication mechanism involves truthful communication of extreme experiences and pools the moderate experiences together."
http://arxiv.org/abs/2406.07186v3,Information Aggregation with Costly Information Acquisition,2024-06-11 11:59:01+00:00,"['Spyros Galanis', 'Sergei Mikhalishchev']",econ.TH,"We study information aggregation in a dynamic trading model with partially informed traders. Ostrovsky [2012] showed that 'separable' securities aggregate information in all equilibria, however, separability is not robust to small changes in the traders' private information. To remedy this problem, we enhance the model by allowing traders to acquire signals with cost $κ$, in every period. We show that '$κ$ separable securities' aggregate information and, as the cost decreases, nearly all securities become $κ$ separable, irrespective of the traders' initial private information. Moreover, the switch to $κ$ separability happens not gradually but discontinuously, hence even a small decrease in costs can result in a security aggregating information. Finally, even with myopic traders, cheaper information may accelerate or decelerate information aggregation for all but Arrow-Debreu securities."
http://arxiv.org/abs/2404.18445v2,AI and the Dynamic Supply of Training Data,2024-04-29 06:00:59+00:00,"['Christian Peukert', 'Florian Abeillon', 'Jérémie Haese', 'Franziska Kaiser', 'Alexander Staub']",econ.GN,"Artificial intelligence (AI) systems rely heavily on human-generated data, yet the people behind that data are often overlooked. Human behavior can play a major role in AI training datasets, be it in limiting access to existing works or in deciding which types of new works to create or whether to create any at all. We examine creators' behavioral change when their works become training data for commercial AI. Specifically, we focus on contributors on Unsplash, a popular stock image platform with about 6 million high-quality photos and illustrations. In the summer of 2020, Unsplash launched a research program and released a dataset of 25,000 images for commercial AI use. We study contributors' reactions, comparing contributors whose works were included in this dataset to contributors whose works were not. Our results suggest that treated contributors left the platform at a higher-than-usual rate and substantially slowed down the rate of new uploads. Professional photographers and more heavily affected users had a stronger reaction than amateurs and less affected users. We also show that affected users changed the variety and novelty of contributions to the platform, which can potentially lead to lower-quality AI outputs in the long run. Our findings highlight a critical trade-off: the drive to expand AI capabilities versus the incentives of those producing training data. We conclude with policy proposals, including dynamic compensation schemes and structured data markets, to realign incentives at the data frontier."
http://arxiv.org/abs/2404.02497v5,Balancing Efficiency and Equity in Classroom Assignment under Endogenous Peer Effects,2024-04-03 06:24:13+00:00,"['Lei Bill Wang', 'Zhenbang Jiao', 'Om Prakash Bedant', 'Haoran Wang']",econ.GN,"This paper presents a three-step empirical framework for optimizing classroom assignments under endogenous peer effects, using data from the China Education Panel Survey (CEPS).
  We design \textit{PeerNN}, a neural network that mimics endogenous network formation as a discrete choice model, generating a friendship-intensity matrix ($Ω$) that captures student popularity.
  \textbf{Step 2: Estimating Peer Effects.} We measure the peer effect friends' average 6th-grade class rank weighted by $Ω$ on 8th-grade cognitive test score. Incorporating $Ω$ into the linear-in-means model induces endogeneity. Using quasi-random classroom assignments, we instrument friends' average 6th-grade class rank with the average classmates' 6th-grade class rank (unweighted by $Ω$). Our main regression result shows that a 10\% improvement in friends' 6th-grade class rank raises 8th-grade cognitive test scores by 0.13 SD. Positive $β$ implies maximizing (minimizing) the popularity of high (low) achievers optimizes outcomes.
  \textbf{Step 3: Simulating Policy Trade-offs.} We use estimates from Step 1 and Step 2 to simulate optimal classroom assignments. We first implement a genetic algorithm (GA) to maximize average peer effect and observe a 1.9\% improvement. However, serious inequity issues arise: low-achieving students are hurt the most in the pursuit of the higher average peer effect. We propose an \textit{Algorithmically Fair GA} (AFGA), achieving a 1.2\% gain while ensuring more equitable educational outcomes.
  These results underscore that efficiency-focused classroom assignment policies can exacerbate inequality. We recommend incorporating fairness considerations when designing classroom assignment policies that account for endogenous spillovers."
http://arxiv.org/abs/2404.18137v3,Nonlinear Domar aggregation over transforming production networks,2024-04-28 10:35:23+00:00,"['Satoshi Nakano', 'Kazuhiko Nishimura']",econ.TH,"An economy-wide production network, as a form of monetary input--output coefficients, becomes unstable during the general equilibrium propagation of sectoral productivity shocks, when the substitution elasticities of sectoral production are nonneutral. The associated network transformation hence leads to nonlinearity in aggregating sectoral productivity shocks into the economy's output, i.e., in Domar aggregation. In this study, we demonstrate the possibility of production networks being transformed into a singular one. We also explore the conditions under which productivity shocks are synergistic in Domar aggregation."
http://arxiv.org/abs/2405.00161v4,Estimating Heterogeneous Treatment Effects with Item-Level Outcome Data: Insights from Item Response Theory,2024-04-30 19:24:56+00:00,"['Joshua B. Gilbert', 'Zachary Himmelsbach', 'James Soland', 'Mridul Joshi', 'Benjamin W. Domingue']",econ.EM,"Analyses of heterogeneous treatment effects (HTE) are common in applied causal inference research. However, when outcomes are latent variables assessed via psychometric instruments such as educational tests, standard methods ignore the potential HTE that may exist among the individual items of the outcome measure. Failing to account for ""item-level"" HTE (IL-HTE) can lead to both underestimated standard errors and identification challenges in the estimation of treatment-by-covariate interaction effects. We demonstrate how Item Response Theory (IRT) models that estimate a treatment effect for each assessment item can both address these challenges and provide new insights into HTE generally. This study articulates the theoretical rationale for the IL-HTE model and demonstrates its practical value using 75 datasets from 48 randomized controlled trials containing 5.8 million item responses in economics, education, and health research. Our results show that the IL-HTE model reveals item-level variation masked by single-number scores, provides more meaningful standard errors in many settings, allows for estimates of the generalizability of causal effects to untested items, resolves identification problems in the estimation of interaction effects, and provides estimates of standardized treatment effect sizes corrected for attenuation due to measurement error."
http://arxiv.org/abs/2406.19222v6,Competitive balance in the UEFA Champions League group stage: Novel measures show no evidence of decline,2024-06-27 14:47:52+00:00,"['László Csató', 'Dóra Gréta Petróczy']",econ.GN,"Competitive balance, which refers to the level of control teams have over a sports competition, is a crucial indicator for tournament organisers. According to previous studies, competitive balance has significantly declined in the UEFA Champions League group stage over the recent decades. Our paper introduces alternative indices to investigate this issue. Two ex ante measures are based on Elo ratings, and four dynamic concentration indicators compare the final group ranking to reasonable benchmarks. Using these indices, we find no evidence of any long-run trend in the competitive balance of the UEFA Champions League group stage between the 2003/04 and 2023/24 seasons."
http://arxiv.org/abs/2404.15158v3,On the Monotonicity of Information Costs,2024-04-23 15:59:51+00:00,"['Xiaoyu Cheng', 'Yonggyun Kim']",econ.TH,"We study the monotonicity of information costs: more informative experiments must be more costly. As criteria for informativeness, we consider the standard information orders introduced by Blackwell (1951, 1953) and Lehmann (1988). We provide simple necessary and sufficient conditions for a cost function to be monotone with respect to each order, grounded in their garbling characterizations. Finally, we examine several well-known cost functions from the literature through the lens of these conditions."
http://arxiv.org/abs/2404.06019v3,Robust Pricing for Quality Disclosure,2024-04-09 05:01:07+00:00,"['Tan Gan', 'Hongcheng Li']",econ.TH,"A platform charges a producer for disclosing quality evidence to consumers before trade. It aims to maximize its revenue guarantee across potentially multiple equilibria which arise from the interdependence of producer purchase decisions and consumer beliefs. The platform's optimal pricing strategy entrenches itself as a market gatekeeper: it induces a unique equilibrium in which non-disclosed products' perceived values are lower than the production cost. To achieve this goal, this pricing strategy iteratively destabilizes under-disclosure equilibria by luring producers to disclose slightly more. Higher-quality producers receive higher rents as their disclosure is prioritized. Despite losing rents, the platform optimally induces socially efficient information transmission for any given evidence structure, and it never benefits from garbling evidence. Compared to the non-robust benchmark, our framework generates more intuitive comparative statics: the platform's ability to extract surplus increases with its value as an information intermediary."
http://arxiv.org/abs/2405.10884v1,Road to perdition? The effect of illicit drug use on labour market outcomes of prime-age men in Mexico,2024-05-17 16:21:18+00:00,"['José-Ignacio Antón', 'Juan Ponce', 'Rafael Muñoz de Bustillo']",econ.GN,"This study addresses the impact of illicit drug use on the labour market outcomes of men in Mexico. We leverage statistical information from three waves of a comparable national survey and make use of Lewbel's heteroskedasticity-based instrumental variable strategy to deal with the endogeneity of drug consumption. Our results suggests that drug consumption has quite negative effects in the Mexican context: It reduces employment, occupational attainment and formality and raises unemployment of local men. These effects seem larger than those estimated for high-income economies"
http://arxiv.org/abs/2405.20912v3,A Branch-Price-Cut-And-Switch Approach for Optimizing Team Formation and Routing for Airport Baggage Handling Tasks with Stochastic Travel Times,2024-05-31 15:21:20+00:00,"['Andreas Hagn', 'Rainer Kolisch', ""Giacomo Dall'Olio"", 'Stefan Weltge']",econ.GN,"In airport operations, optimally using dedicated personnel for baggage handling tasks plays a crucial role in the design of resource-efficient processes. Teams of workers with different qualifications must be formed, and loading or unloading tasks must be assigned to them. Each task has a time window within which it can be started and should be finished. Violating these temporal restrictions incurs severe financial penalties for the operator. In practice, various components of this process are subject to uncertainties. We consider the aforementioned problem under the assumption of stochastic travel times across the apron. We present two binary program formulations to model the problem at hand and propose a novel solution approach that we call Branch-Price-Cut-and-Switch, in which we dynamically switch between two master problem formulations. Furthermore, we use an exact separation method to identify violated rank-1 Chvátal-Gomory cuts and utilize an efficient branching rule relying on task finish times. We test the algorithm on instances generated based on real-world data from a major European hub airport with a planning horizon of up to two hours, 30 flights per hour, and three available task execution modes to choose from. Our results indicate that our algorithm is able to significantly outperform existing solution approaches. Moreover, an explicit consideration of stochastic travel times allows for solutions that utilize the available workforce more efficiently, while simultaneously guaranteeing a stable service level for the baggage handling operator."
http://arxiv.org/abs/2405.17290v2,Count Data Models with Heterogeneous Peer Effects under Rational Expectations,2024-05-27 15:54:47+00:00,['Aristide Houndetoungan'],econ.EM,"This paper develops a peer effect model for count responses under rational expectations. The model accounts for heterogeneity in peer effects through groups based on observed characteristics. Identification is based on the linear model condition requiring friends' friends who are not direct friends, which I show extends to a broad class of nonlinear models. Parameters are estimated using a nested pseudo-likelihood approach. An empirical application on students' extracurricular participation reveals that females are more responsive to peers than males. An easy-to-use R package, CDatanet, is available for implementing the model."
http://arxiv.org/abs/2410.00002v1,Machine Learning and Econometric Approaches to Fiscal Policies: Understanding Industrial Investment Dynamics in Uruguay (1974-2010),2024-09-12 19:01:16+00:00,['Diego Vallarino'],econ.GN,"This paper examines the impact of fiscal incentives on industrial investment in Uruguay from 1974 to 2010. Using a mixed-method approach that combines econometric models with machine learning techniques, the study investigates both the short-term and long-term effects of fiscal benefits on industrial investment. The results confirm the significant role of fiscal incentives in driving long-term industrial growth, while also highlighting the importance of a stable macroeconomic environment, public investment, and access to credit. Machine learning models provide additional insights into nonlinear interactions between fiscal benefits and other macroeconomic factors, such as exchange rates, emphasizing the need for tailored fiscal policies. The findings have important policy implications, suggesting that fiscal incentives, when combined with broader economic reforms, can effectively promote industrial development in emerging economies."
http://arxiv.org/abs/2408.16443v1,The Turing Valley: How AI Capabilities Shape Labor Income,2024-08-29 11:10:20+00:00,"['Enrique Ide', 'Eduard Talamàs']",econ.GN,"Do improvements in Artificial Intelligence (AI) benefit workers? We study how AI capabilities influence labor income in a competitive economy where production requires multidimensional knowledge, and firms organize production by matching humans and AI-powered machines in hierarchies designed to use knowledge efficiently. We show that advancements in AI in dimensions where machines underperform humans decrease total labor income, while advancements in dimensions where machines outperform humans increase it. Hence, if AI initially underperforms humans in all dimensions and improves gradually, total labor income initially declines before rising. We also characterize the AI that maximizes labor income. When humans are sufficiently weak in all knowledge dimensions, labor income is maximized when AI is as good as possible in all dimensions. Otherwise, labor income is maximized when AI simultaneously performs as poorly as possible in the dimensions where humans are relatively strong and as well as possible in the dimensions where humans are relatively weak. Our results suggest that choosing the direction of AI development can create significant divisions between the interests of labor and capital."
http://arxiv.org/abs/2408.16330v1,Sensitivity Analysis for Dynamic Discrete Choice Models,2024-08-29 08:08:31+00:00,['Chun Pong Lau'],econ.EM,"In dynamic discrete choice models, some parameters, such as the discount factor, are being fixed instead of being estimated. This paper proposes two sensitivity analysis procedures for dynamic discrete choice models with respect to the fixed parameters. First, I develop a local sensitivity measure that estimates the change in the target parameter for a unit change in the fixed parameter. This measure is fast to compute as it does not require model re-estimation. Second, I propose a global sensitivity analysis procedure that uses model primitives to study the relationship between target parameters and fixed parameters. I show how to apply the sensitivity analysis procedures of this paper through two empirical applications."
http://arxiv.org/abs/2408.10066v1,Near-Optimal Mechanisms for Resource Allocation Without Monetary Transfers,2024-08-19 15:04:20+00:00,"['Moise Blanchard', 'Patrick Jaillet']",cs.GT,"We study the problem in which a central planner sequentially allocates a single resource to multiple strategic agents using their utility reports at each round, but without using any monetary transfers. We consider general agent utility distributions and two standard settings: a finite horizon $T$ and an infinite horizon with $γ$ discounts. We provide general tools to characterize the convergence rate between the optimal mechanism for the central planner and the first-best allocation if true agent utilities were available. This heavily depends on the utility distributions, yielding rates anywhere between $1/\sqrt T$ and $1/T$ for the finite-horizon setting, and rates faster than $\sqrt{1-γ}$, including exponential rates for the infinite-horizon setting as agents are more patient $γ\to 1$. On the algorithmic side, we design mechanisms based on the promised-utility framework to achieve these rates and leverage structure on the utility distributions. Intuitively, the more flexibility the central planner has to reward or penalize any agent while incurring little social welfare cost, the faster the convergence rate. In particular, discrete utility distributions typically yield the slower rates $1/\sqrt T$ and $\sqrt{1-γ}$, while smooth distributions with density typically yield faster rates $1/T$ (up to logarithmic factors) and $1-γ$."
http://arxiv.org/abs/2408.12915v1,Education Opportunities for Rural Areas: Evidence from China's Higher Education Expansion,2024-08-23 08:46:47+00:00,"['Ande Shen', 'Jiwei Zhou']",econ.GN,"This paper explores the causal impact of education opportunities on rural areas by exploiting the higher education expansion (HEE) in China in 1999. By utilizing the detailed census data, the cohort-based difference-in-differences design indicates that the HEE increased college attendance and encouraged more people to attend senior high schools and that the effect is more significant in rural areas. Then we apply a similar approach to a novel panel data set of rural villages and households to examine the effect of education opportunities on rural areas. We find contrasting impacts on income and life quality between villages and households. Villages in provinces with higher HEE magnitudes underwent a drop in the average income and worse living facilities. On the contrary, households sending out migrants after the HEE experienced an increase in their per capita income. The phenomenon where villages experienced a ``brain drain'' and households with migrants gained after the HEE is explained by the fact that education could serve as a way to overcome the barrier of rural-urban migration. Our findings highlight the opposed impacts of education opportunities on rural development and household welfare in rural areas."
http://arxiv.org/abs/2408.11621v1,Robust Bayes Treatment Choice with Partial Identification,2024-08-21 13:47:45+00:00,"['Andrés Aradillas Fernández', 'José Luis Montiel Olea', 'Chen Qiu', 'Jörg Stoye', 'Serdil Tinda']",econ.EM,"We study a class of binary treatment choice problems with partial identification, through the lens of robust (multiple prior) Bayesian analysis. We use a convenient set of prior distributions to derive ex-ante and ex-post robust Bayes decision rules, both for decision makers who can randomize and for decision makers who cannot.
  Our main messages are as follows: First, ex-ante and ex-post robust Bayes decision rules do not tend to agree in general, whether or not randomized rules are allowed. Second, randomized treatment assignment for some data realizations can be optimal in both ex-ante and, perhaps more surprisingly, ex-post problems. Therefore, it is usually with loss of generality to exclude randomized rules from consideration, even when regret is evaluated ex-post.
  We apply our results to a stylized problem where a policy maker uses experimental data to choose whether to implement a new policy in a population of interest, but is concerned about the external validity of the experiment at hand (Stoye, 2012); and to the aggregation of data generated by multiple randomized control trials in different sites to make a policy choice in a population for which no experimental data are available (Manski, 2020; Ishihara and Kitagawa, 2021)."
http://arxiv.org/abs/2408.13706v1,Import competition and domestic vertical integration: Theory and Evidence from Chinese firms,2024-08-25 02:32:27+00:00,"['Xin Du', 'Xiaoxia Shi']",econ.GN,"What impact does import competition have on firms' production organizational choices? Existing literature has predominantly focused on the relationship between import competition and firms' global production networks, with less attention given to domestic. We first develop a Nash-bargaining model to guide our empirical analysis, then utilize tariff changes as an exogenous shock to test our theoretical hypotheses using a database of Chinese listed firms from 2000 to 2023. Our findings indicate that a decrease in downstream tariffs lead to an increase in vertical integration. In our mechanism tests, we discover that a reduction in upstream tariffs also enhances this effect. Moreover, the impact of tariff reductions on vertical integration is primarily observed in industries with high asset specificity, indicating that asset-specificity is a crucial mechanism. We further explore whether import competition encourages vertical integration for technological acquisition purpose, the effect is found only among high-tech firms, while it's absent in non-high-tech firms. Our research provides new perspectives and evidence on how firms optimize their production organization in the process of globalization."
http://arxiv.org/abs/2409.05713v1,The Surprising Robustness of Partial Least Squares,2024-09-09 15:24:17+00:00,"['João B. Assunção', 'Pedro Afonso Fernandes']",econ.EM,"Partial least squares (PLS) is a simple factorisation method that works well with high dimensional problems in which the number of observations is limited given the number of independent variables. In this article, we show that PLS can perform better than ordinary least squares (OLS), least absolute shrinkage and selection operator (LASSO) and ridge regression in forecasting quarterly gross domestic product (GDP) growth, covering the period from 2000 to 2023. In fact, through dimension reduction, PLS proved to be effective in lowering the out-of-sample forecasting error, specially since 2020. For the period 2000-2019, the four methods produce similar results, suggesting that PLS is a valid regularisation technique like LASSO or ridge."
http://arxiv.org/abs/2409.06026v1,Patterns of Medical Care Cost by Service Type Associated with Lung Cancer Screening,2024-09-09 19:27:21+00:00,"['Kris Wain', 'Mahesh Maiyani', 'Nikki M. Carroll', 'Rafael Meza', 'Robert T. Greenlee', 'Christine Neslund-Dudas', 'Michelle R. Odelberg', 'Caryn Oshiro', 'Debra P. Ritzwoller']",econ.GN,"Introduction: Lung cancer screening (LCS) increases early-stage cancer detection which may reduce cancer treatment costs. Little is known about how receipt of LCS affects healthcare costs in real-world clinical settings.
  Methods: This retrospective study analyzed utilization and cost data from the Population-based Research to Optimize the Screening Process Lung Consortium. We included individuals who met age and smoking LCS eligibility criteria and were engaged within four healthcare systems between February 5, 2015, and December 31, 2021. Generalized linear models estimated healthcare costs from the payer perspective during 12-months prior and 12-months post baseline LCS. We compared these costs to eligible individuals who did not receive LCS. Sensitivity analyses expanded our sample to age-eligible individuals with any smoking history noted in the electronic health record. Secondary analyses examined costs among a sample diagnosed with lung cancer. We reported mean predicted costs with average values for all other explanatory variables.
  Results: We identified 10,049 eligible individuals who received baseline LCS and 15,233 who did not receive baseline LCS. Receipt of baseline LCS was associated with additional costs of $3,698 compared to individuals not receiving LCS. Secondary analyses showed suggestive evidence that LCS prior to cancer diagnosis decreased healthcare costs compared to cancer diagnosed without screening.
  Conclusion: These findings suggest LCS increases healthcare costs in the year following screening. However, LCS also improves early-stage cancer detection and may reduce treatment costs following diagnosis. These results can inform future simulation models to guide LCS recommendations, and aid health policy decision makers on resource allocation."
http://arxiv.org/abs/2409.07506v1,The Mismeasure of Weather: Using Remotely Sensed Earth Observation Data in Economic Context,2024-09-11 12:08:24+00:00,"['Anna Josephson', 'Jeffrey D. Michler', 'Talip Kilic', 'Siobhan Murray']",econ.GN,The availability of weather data from remotely sensed Earth observation (EO) data has reduced the cost of including weather variables in econometric models. Weather variables are common instrumental variables used to predict economic outcomes and serve as an input into modelling crop yields for rainfed agriculture. The use of EO data in econometric applications has only recently been met with a critical assessment of the suitability and quality of this data in economics. We quantify the significance and magnitude of the effect of measurement error in EO data in the context of smallholder agricultural productivity. We find that different measurement methods from different EO sources: findings are not robust to the choice of EO dataset and outcomes are not simply affine transformations of one another. This begs caution on the part of researchers using these data and suggests that robustness checks should include testing alternative sources of EO data.
http://arxiv.org/abs/2409.00704v1,Stochastic Monotonicity and Random Utility Models: The Good and The Ugly,2024-09-01 12:12:37+00:00,"['Henk Keffert', 'Nikolaus Schweizer']",econ.GN,"When it comes to structural estimation of risk preferences from data on choices, random utility models have long been one of the standard research tools in economics. A recent literature has challenged these models, pointing out some concerning monotonicity and, thus, identification problems. In this paper, we take a second look and point out that some of the criticism - while extremely valid - may have gone too far, demanding monotonicity of choice probabilities in decisions where it is not so clear whether it should be imposed. We introduce a new class of random utility models based on carefully constructed generalized risk premia which always satisfy our relaxed monotonicity criteria. Moreover, we show that some of the models used in applied research like the certainty-equivalent-based random utility model for CARA utility actually lie in this class of monotonic stochastic choice models. We conclude that not all random utility models are bad."
http://arxiv.org/abs/2409.02551v1,Deep Learning for Multi-Country GDP Prediction: A Study of Model Performance and Data Impact,2024-09-04 09:18:16+00:00,"['Huaqing Xie', 'Xingcheng Xu', 'Fangjia Yan', 'Xun Qian', 'Yanqing Yang']",econ.GN,"GDP is a vital measure of a country's economic health, reflecting the total value of goods and services produced. Forecasting GDP growth is essential for economic planning, as it helps governments, businesses, and investors anticipate trends, make informed decisions, and promote stability and growth. While most previous works focus on the prediction of the GDP growth rate for a single country or by machine learning methods, in this paper we give a comprehensive study on the GDP growth forecasting in the multi-country scenario by deep learning algorithms. For the prediction of the GDP growth where only GDP growth values are used, linear regression is generally better than deep learning algorithms. However, for the regression and the prediction of the GDP growth with selected economic indicators, deep learning algorithms could be superior to linear regression. We also investigate the influence of the novel data -- the light intensity data on the prediction of the GDP growth, and numerical experiments indicate that they do not necessarily improve the prediction performance. Code is provided at https://github.com/Sariel2018/Multi-Country-GDP-Prediction.git."
http://arxiv.org/abs/2407.13204v2,The Pay and Non-Pay Content of Job Ads,2024-07-18 06:42:52+00:00,"['Richard Audoly', 'Manudeep Bhuller', 'Tore Adam Reiremo']",econ.GN,"How informative are job ads about the actual pay and amenities offered by employers? Using a comprehensive database of job ads posted by Norwegian employers, we develop a methodology to systematically classify the information on both pay and non-pay job attributes advertised in vacancy texts. We link this information to measures of employer attractiveness, which we derive from a job search model estimated on observed wages and worker mobility flows. About 55 percent of job ads provide information related to pay and nearly all ads feature information on non-pay attributes. We show that publicly advertised job attributes are meaningful predictors of employer attractiveness, and non-pay attributes are about as predictive as pay-related attributes. High-pay employers mention pay-related attributes more often, while high-amenity employers are more likely to advertise flexible working hours and contract duration."
http://arxiv.org/abs/2409.17529v1,Continuity and Monotonicity of Preferences and Probabilistic Equivalence,2024-09-26 04:31:36+00:00,"['Sushil Bikhchandani', 'Uzi Segal']",econ.TH,We show that probabilistic equivalence of a regret-based preference relationship over random variables is implied by a weak form of continuity and monotonicity.
http://arxiv.org/abs/2409.20199v1,Synthetic Difference in Differences for Repeated Cross-Sectional Data,2024-09-30 11:19:59+00:00,['Yoann Morin'],econ.EM,"The synthetic difference-in-differences method provides an efficient method to estimate a causal effect with a latent factor model. However, it relies on the use of panel data. This paper presents an adaptation of the synthetic difference-in-differences method for repeated cross-sectional data. The treatment is considered to be at the group level so that it is possible to aggregate data by group to compute the two types of synthetic difference-in-differences weights on these aggregated data. Then, I develop and compute a third type of weight that accounts for the different number of observations in each cross-section. Simulation results show that the performance of the synthetic difference-in-differences estimator is improved when using the third type of weights on repeated cross-sectional data."
http://arxiv.org/abs/2409.18776v1,Can AI Enhance its Creativity to Beat Humans ?,2024-09-27 14:19:07+00:00,"['Anne-Gaëlle Maltese', 'Pierre Pelletier', 'Rémy Guichardaz']",econ.GN,"Creativity is a fundamental pillar of human expression and a driving force behind innovation, yet it now stands at a crossroads. As artificial intelligence advances at an astonishing pace, the question arises: can machines match and potentially surpass human creativity? This study investigates the creative performance of artificial intelligence (AI) compared to humans by analyzing the effects of two distinct prompting strategies (a Naive and an Expert AI) on AI and across three different tasks (Text, Draw and Alternative Uses tasks). Human external evaluators have scored creative outputs generated by humans and AI, and these subjective creative scores were complemented with objective measures based on quantitative measurements and NLP tools. The results reveal that AI generally outperforms humans in creative tasks, though this advantage is nuanced by the specific nature of each task and the chosen creativity criteria. Ultimately, while AI demonstrates superior performance in certain creative domains, our results suggest that integrating human feedback is crucial for maximizing AI's creative potential."
http://arxiv.org/abs/2409.19325v1,A Generalized Model for Multidimensional Intransitivity,2024-09-28 11:48:34+00:00,"['Jiuding Duan', 'Jiyi Li', 'Yukino Baba', 'Hisashi Kashima']",cs.LG,"Intransitivity is a critical issue in pairwise preference modeling. It refers to the intransitive pairwise preferences between a group of players or objects that potentially form a cyclic preference chain and has been long discussed in social choice theory in the context of the dominance relationship. However, such multifaceted intransitivity between players and the corresponding player representations in high dimensions is difficult to capture. In this paper, we propose a probabilistic model that jointly learns each player's d-dimensional representation (d>1) and a dataset-specific metric space that systematically captures the distance metric in Rd over the embedding space. Interestingly, by imposing additional constraints in the metric space, our proposed model degenerates to former models used in intransitive representation learning. Moreover, we present an extensive quantitative investigation of the vast existence of intransitive relationships between objects in various real-world benchmark datasets. To our knowledge, this investigation is the first of this type. The predictive performance of our proposed method on different real-world datasets, including social choice, election, and online game datasets, shows that our proposed method outperforms several competing methods in terms of prediction accuracy."
http://arxiv.org/abs/2408.09598v2,Anytime-Valid Inference for Double/Debiased Machine Learning of Causal Parameters,2024-08-18 21:19:56+00:00,"['Abhinandan Dalal', 'Patrick Blöbaum', 'Shiva Kasiviswanathan', 'Aaditya Ramdas']",stat.ME,"Double (debiased) machine learning (DML) has seen widespread use in recent years for learning causal/structural parameters, in part due to its flexibility and adaptability to high-dimensional nuisance functions as well as its ability to avoid bias from regularization or overfitting. However, the classic double-debiased framework is only valid asymptotically for a predetermined sample size, thus lacking the flexibility of collecting more data if sharper inference is needed, or stopping data collection early if useful inferences can be made earlier than expected. This can be of particular concern in large scale experimental studies with huge financial costs or human lives at stake, as well as in observational studies where the length of confidence of intervals do not shrink to zero even with increasing sample size due to partial identifiability of a structural parameter. In this paper, we present time-uniform counterparts to the asymptotic DML results, enabling valid inference and confidence intervals for structural parameters to be constructed at any arbitrary (possibly data-dependent) stopping time. We provide conditions which are only slightly stronger than the standard DML conditions, but offer the stronger guarantee for anytime-valid inference. This facilitates the transformation of any existing DML method to provide anytime-valid guarantees with minimal modifications, making it highly adaptable and easy to use. We illustrate our procedure using two instances: a) local average treatment effect in online experiments with non-compliance, and b) partial identification of average treatment effect in observational studies with potential unmeasured confounding."
http://arxiv.org/abs/2408.14263v1,A topological proof of Terao's generalized Arrow's Impossibility Theorem,2024-08-26 13:30:36+00:00,['Takuma Okura'],math.CO,"In Terao [24], Hiroaki Terao defined and studied ""admissible map"", which is a generalization of ""social welfare function"" in the context of hyperplane arrangements. Using this, he proved a generalized Arrow's Impossibility Theorem using combinatorial arguments. This paper provides another proof of this generalized Arrow's Impossibility Theorem, using the idea of algebraic topology."
http://arxiv.org/abs/2409.03734v1,Safety vs. Performance: How Multi-Objective Learning Reduces Barriers to Market Entry,2024-09-05 17:45:01+00:00,"['Meena Jagadeesan', 'Michael I. Jordan', 'Jacob Steinhardt']",cs.LG,"Emerging marketplaces for large language models and other large-scale machine learning (ML) models appear to exhibit market concentration, which has raised concerns about whether there are insurmountable barriers to entry in such markets. In this work, we study this issue from both an economic and an algorithmic point of view, focusing on a phenomenon that reduces barriers to entry. Specifically, an incumbent company risks reputational damage unless its model is sufficiently aligned with safety objectives, whereas a new company can more easily avoid reputational damage. To study this issue formally, we define a multi-objective high-dimensional regression framework that captures reputational damage, and we characterize the number of data points that a new company needs to enter the market. Our results demonstrate how multi-objective considerations can fundamentally reduce barriers to entry -- the required number of data points can be significantly smaller than the incumbent company's dataset size. En route to proving these results, we develop scaling laws for high-dimensional linear regression in multi-objective environments, showing that the scaling rate becomes slower when the dataset size is large, which could be of independent interest."
http://arxiv.org/abs/2409.01266v1,"Double Machine Learning meets Panel Data -- Promises, Pitfalls, and Potential Solutions",2024-09-02 13:59:54+00:00,"['Jonathan Fuhr', 'Dominik Papies']",econ.EM,"Estimating causal effect using machine learning (ML) algorithms can help to relax functional form assumptions if used within appropriate frameworks. However, most of these frameworks assume settings with cross-sectional data, whereas researchers often have access to panel data, which in traditional methods helps to deal with unobserved heterogeneity between units. In this paper, we explore how we can adapt double/debiased machine learning (DML) (Chernozhukov et al., 2018) for panel data in the presence of unobserved heterogeneity. This adaptation is challenging because DML's cross-fitting procedure assumes independent data and the unobserved heterogeneity is not necessarily additively separable in settings with nonlinear observed confounding. We assess the performance of several intuitively appealing estimators in a variety of simulations. While we find violations of the cross-fitting assumptions to be largely inconsequential for the accuracy of the effect estimates, many of the considered methods fail to adequately account for the presence of unobserved heterogeneity. However, we find that using predictive models based on the correlated random effects approach (Mundlak, 1978) within DML leads to accurate coefficient estimates across settings, given a sample size that is large relative to the number of observed confounders. We also show that the influence of the unobserved heterogeneity on the observed confounders plays a significant role for the performance of most alternative methods."
http://arxiv.org/abs/2408.17200v1,Investor behavior and multiscale cross-correlations: Unveiling regime shifts in global financial markets,2024-08-30 10:53:19+00:00,"['Marina Dolfin', 'George Kapetanios', 'Leone Leonida', 'Jose De Leon Miranda']",econ.GN,"We propose an algorithm to capture emergent patterns in the cross-correlations of financial markets, highlighting regime changes on a global scale. In our approach, financial markets are viewed as complex adaptive systems, and multiscale properties and cross-correlations are considered, particularly during stress conditions such as the COVID-19 pandemic, the invasion of Ukraine by Russia in 2022, and Brexit. We investigate whether significant disruptions reflect an imbalance in investment horizons among investors, and we propose a measure based on this imbalance to depict the impact on global financial markets. The detrended cross-correlation cost (DCCC), which is derived from detrended cross-correlation analysis, uses cross-correlations at different timescales to capture variations in investment horizons amid financial uncertainties. Our algorithm, which combines DCCC analysis and the minimum-spanning-tree filtering approach, tracks system interconnectedness and investor imbalances. We tested the DCCC indicator using daily price series of G7, Russian, and Chinese markets over the past decade and found that it increases sharply during ``crash'' periods compared to ``business as usual'' periods. Our empirical results confirm that short-term investment horizons dominate during financial instabilities; this validates our hypothesis and indicates that the DCCC can serve as a leading indicator of shifts in financial-market regimes."
http://arxiv.org/abs/2408.17187v1,State Space Model of Realized Volatility under the Existence of Dependent Market Microstructure Noise,2024-08-30 10:39:05+00:00,['Toru Yano'],econ.EM,"Volatility means the degree of variation of a stock price which is important in finance. Realized Volatility (RV) is an estimator of the volatility calculated using high-frequency observed prices. RV has lately attracted considerable attention of econometrics and mathematical finance. However, it is known that high-frequency data includes observation errors called market microstructure noise (MN). Nagakura and Watanabe[2015] proposed a state space model that resolves RV into true volatility and influence of MN. In this paper, we assume a dependent MN that autocorrelates and correlates with return as reported by Hansen and Lunde[2006] and extends the results of Nagakura and Watanabe[2015] and compare models by simulation and actual data."
http://arxiv.org/abs/2409.10407v1,Bitcoin Transaction Behavior Modeling Based on Balance Data,2024-09-16 15:41:30+00:00,"['Yu Zhang', 'Claudio Tessone']",econ.GN,"When analyzing Bitcoin users' balance distribution, we observed that it follows a log-normal pattern. Drawing parallels from the successful application of Gibrat's law of proportional growth in explaining city size and word frequency distributions, we tested whether the same principle could account for the log-normal distribution in Bitcoin balances. However, our calculations revealed that the exponent parameters in both the drift and variance terms deviate slightly from one. This suggests that Gibrat's proportional growth rule alone does not fully explain the log-normal distribution observed in Bitcoin users' balances. During our exploration, we discovered an intriguing phenomenon: Bitcoin users tend to fall into two distinct categories based on their behavior, which we refer to as ``poor"" and ``wealthy"" users. Poor users, who initially purchase only a small amount of Bitcoin, tend to buy more bitcoins first and then sell out all their holdings gradually over time. The certainty of selling all their coins is higher and higher with time. In contrast, wealthy users, who acquire a large amount of Bitcoin from the start, tend to sell off their holdings over time. The speed at which they sell their bitcoins is lower and lower over time and they will hold at least a small part of their initial holdings at last. Interestingly, the wealthier the user, the larger the proportion of their balance and the higher the certainty they tend to sell. This research provided an interesting perspective to explore bitcoin users' behaviors which may apply to other finance markets."
http://arxiv.org/abs/2409.10402v1,A Statistical Equilibrium Approach to Adam Smith's Labor Theory of Value,2024-09-16 15:34:41+00:00,"['Ellis Scharfenaker', 'Bruno Theodosio', 'Duncan K. Foley']",econ.TH,"Adam Smith's inquiry into the emergence and stability of the self-organization of the division of labor in commodity production and exchange is considered using statistical equilibrium methods from statistical physics. We develop a statistical equilibrium model of the distribution of independent direct producers in a hub-and-spoke framework that predicts both the center of gravity of producers across lines of production as well as the endogenous fluctuations between lines of production that arise from Smith's concept of ""perfect liberty"". The ergodic distribution of producers implies a long-run balancing of ""advantages to disadvantages"" across lines of employment and gravitation of market prices around Smith's natural prices."
http://arxiv.org/abs/2409.10820v1,Simple robust two-stage estimation and inference for generalized impulse responses and multi-horizon causality,2024-09-17 01:28:13+00:00,"['Jean-Marie Dufour', 'Endong Wang']",econ.EM,"This paper introduces a novel two-stage estimation and inference procedure for generalized impulse responses (GIRs). GIRs encompass all coefficients in a multi-horizon linear projection model of future outcomes of y on lagged values (Dufour and Renault, 1998), which include the Sims' impulse response. The conventional use of Least Squares (LS) with heteroskedasticity- and autocorrelation-consistent covariance estimation is less precise and often results in unreliable finite sample tests, further complicated by the selection of bandwidth and kernel functions. Our two-stage method surpasses the LS approach in terms of estimation efficiency and inference robustness. The robustness stems from our proposed covariance matrix estimates, which eliminate the need to correct for serial correlation in the multi-horizon projection residuals. Our method accommodates non-stationary data and allows the projection horizon to grow with sample size. Monte Carlo simulations demonstrate our two-stage method outperforms the LS method. We apply the two-stage method to investigate the GIRs, implement multi-horizon Granger causality test, and find that economic uncertainty exerts both short-run (1-3 months) and long-run (30 months) effects on economic activities."
http://arxiv.org/abs/2409.13168v1,Economic Policy Challenges for the Age of AI,2024-09-20 02:49:37+00:00,['Anton Korinek'],econ.GN,"This paper examines the profound challenges that transformative advances in AI towards Artificial General Intelligence (AGI) will pose for economists and economic policymakers. I examine how the Age of AI will revolutionize the basic structure of our economies by diminishing the role of labor, leading to unprecedented productivity gains but raising concerns about job disruption, income distribution, and the value of education and human capital. I explore what roles may remain for labor post-AGI, and which production factors will grow in importance. The paper then identifies eight key challenges for economic policy in the Age of AI: (1) inequality and income distribution, (2) education and skill development, (3) social and political stability, (4) macroeconomic policy, (5) antitrust and market regulation, (6) intellectual property, (7) environmental implications, and (8) global AI governance. It concludes by emphasizing how economists can contribute to a better understanding of these challenges."
http://arxiv.org/abs/2409.13531v1,A simple but powerful tail index regression,2024-09-20 14:20:26+00:00,"['João Nicolau', 'Paulo M. M. Rodrigues']",econ.EM,"This paper introduces a flexible framework for the estimation of the conditional tail index of heavy tailed distributions. In this framework, the tail index is computed from an auxiliary linear regression model that facilitates estimation and inference based on established econometric methods, such as ordinary least squares (OLS), least absolute deviations, or M-estimation. We show theoretically and via simulations that OLS provides interesting results. Our Monte Carlo results highlight the adequate finite sample properties of the OLS tail index estimator computed from the proposed new framework and contrast its behavior to that of tail index estimates obtained by maximum likelihood estimation of exponential regression models, which is one of the approaches currently in use in the literature. An empirical analysis of the impact of determinants of the conditional left- and right-tail indexes of commodities' return distributions highlights the empirical relevance of our proposed approach. The novel framework's flexibility allows for extensions and generalizations in various directions, empowering researchers and practitioners to straightforwardly explore a wide range of research questions."
http://arxiv.org/abs/2409.12353v2,A Way to Synthetic Triple Difference,2024-09-18 23:20:47+00:00,['Castiel Chen Zhuang'],econ.EM,"This paper discusses a practical approach that combines synthetic control with triple difference to address violations of the parallel trends assumption. By transforming triple difference into a DID structure, we can apply synthetic control to a triple-difference framework, enabling more robust estimates when parallel trends are violated across multiple dimensions. The proposed procedure is applied to a real-world dataset to illustrate when and how we should apply this practice, while cautions are presented afterwards. This method contributes to improving causal inference in policy evaluations and offers a valuable tool for researchers dealing with heterogeneous treatment effects across subgroups."
http://arxiv.org/abs/2409.15988v1,Semi-strong Efficient Market of Bitcoin and Twitter: an Analysis of Semantic Vector Spaces of Extracted Keywords and Light Gradient Boosting Machine Models,2024-09-24 11:42:23+00:00,"['Fang Wang', 'Marko Gacesa']",econ.GN,"This study extends the examination of the Efficient-Market Hypothesis in Bitcoin market during a five year fluctuation period, from September 1 2017 to September 1 2022, by analyzing 28,739,514 qualified tweets containing the targeted topic ""Bitcoin"". Unlike previous studies, we extracted fundamental keywords as an informative proxy for carrying out the study of the EMH in the Bitcoin market rather than focusing on sentiment analysis, information volume, or price data. We tested market efficiency in hourly, 4-hourly, and daily time periods to understand the speed and accuracy of market reactions towards the information within different thresholds. A sequence of machine learning methods and textual analyses were used, including measurements of distances of semantic vector spaces of information, keywords extraction and encoding model, and Light Gradient Boosting Machine (LGBM) classifiers. Our results suggest that 78.06% (83.08%), 84.63% (87.77%), and 94.03% (94.60%) of hourly, 4-hourly, and daily bullish (bearish) market movements can be attributed to public information within organic tweets."
http://arxiv.org/abs/2407.15509v3,The increasing share of low-value transactions in international trade,2024-07-22 09:47:39+00:00,"['Raúl Mínguez', 'Asier Minondo']",econ.GN,"This paper documents a new feature of international trade: the increase in the share of low-value transactions in the total volume of transactions. Using Spanish data, we show that the share of low-value transactions in the total number of transactions increased from 9% to 61% in exports and from 14% to 54% in imports between 1997 and 2023. The increase in the number of low-value trade transactions is explained by the rise of e-commerce and direct-to-customer sales facilitated by online retail platforms, and the fast-fashion strategy followed by clothing firms."
http://arxiv.org/abs/2409.06112v3,Optimal In-Kind Redistribution,2024-09-09 23:38:18+00:00,"['Zi Yang Kang', 'Mitchell Watt']",econ.GN,"This paper develops a model of in-kind redistribution where consumers participate in either a private market or a government-designed program, but not both. We characterize when a social planner, seeking to maximize weighted total surplus, can strictly improve upon the laissez-faire outcome. We show that the optimal mechanism consists of three components: a public option, nonlinear subsidies, and laissez-faire consumption. We quantify the resulting distortions and relate them to the correlation between consumer demand and welfare weights. Our findings reveal that while private market access constrains the social planner's ability to redistribute, it also strengthens the rationale for non-market allocations."
http://arxiv.org/abs/2407.21209v3,Algorithm-Assisted Decision Making and Racial Disparities in Housing: A Study of the Allegheny Housing Assessment Tool,2024-07-30 21:44:48+00:00,"['Lingwei Cheng', 'Cameron Drayton', 'Alexandra Chouldechova', 'Rhema Vaithianathan']",cs.HC,"The demand for housing assistance across the United States far exceeds the supply, leaving housing providers the task of prioritizing clients for receipt of this limited resource. To be eligible for federal funding, local homelessness systems are required to implement assessment tools as part of their prioritization processes. The Vulnerability Index Service Prioritization Decision Assistance Tool (VI-SPDAT) is the most commonly used assessment tool nationwide. Recent studies have criticized the VI-SPDAT as exhibiting racial bias, which may lead to unwarranted racial disparities in housing provision. In response to these criticisms, some jurisdictions have developed alternative tools, such as the Allegheny Housing Assessment (AHA), which uses algorithms to assess clients' risk levels. Drawing on data from its deployment, we conduct descriptive and quantitative analyses to evaluate whether replacing the VI-SPDAT with the AHA affects racial disparities in housing allocation. We find that the VI-SPDAT tended to assign higher risk scores to white clients and lower risk scores to Black clients, and that white clients were served at a higher rates pre-AHA deployment. While post-deployment service decisions became better aligned with the AHA score, and the distribution of AHA scores is similar across racial groups, we do not find evidence of a corresponding decrease in disparities in service rates. We attribute the persistent disparity to the use of Alt-AHA, a survey-based tool that is used in cases of low data quality, as well as group differences in eligibility-related factors, such as chronic homelessness and veteran status. We discuss the implications for housing service systems seeking to reduce racial disparities in their service delivery."
http://arxiv.org/abs/2409.00039v2,Spatial-temporal evolution characteristics and driving factors of carbon emission prediction in China-research on ARIMA-BP neural network algorithm,2024-08-18 11:45:38+00:00,"['Zhao Sanglin', 'Li Zhetong', 'Deng Hao', 'You Xing', 'Tong Jiaang', 'Yuan Bingkun', 'Zeng Zihao']",econ.GN,"China accounts for one-third of the world's total carbon emissions. How to reach the peak of carbon emissions by 2030 and achieve carbon neutrality by 2060 to ensure the effective realization of the ""dual-carbon"" target is an important policy orientation at present. Based on the provincial panel data of ARIMA-BP model, this paper shows that the effect of energy consumption intensity effect is the main factor driving the growth of carbon emissions, per capita GDP and energy consumption structure effect are the main factors to inhibit carbon emissions, and the effect of industrial structure and population size effect is relatively small. Based on the research conclusion, the policy suggestions are put forward from the aspects of energy structure, industrial structure, new quality productivity and digital economy."
http://arxiv.org/abs/2407.06495v1,Impact Evaluation on the European Privacy Laws governing generative-AI models -- Evidence in Relation between Internet Censorship and the Ban of ChatGPT in Italy,2024-07-09 01:56:42+00:00,['Tatsuru Kikuchi'],econ.GN,"We proceed an impact evaluation on the European Privacy Laws governing generative-AI models, especially, focusing on the effects of the Ban of ChatGPT in Italy. We investigate on the causal relationship between Internet Censorship Data and the Ban of ChatGPT in Italy during the period from March 27, 2023 to April 11, 2023. We analyze the relation based on the hidden Markov model with Poisson emissions. We find out that the HTTP Invalid Requests, which decreased during those period, can be explained with seven-state model. Our findings shows the apparent inability for the users in the internet accesses as a result of EU regulations on the generative-AI."
http://arxiv.org/abs/2407.06808v1,Credit and Voting,2024-07-09 12:28:51+00:00,"['Eleonora Brandimarti', 'Giacomo De Giorgi', 'Jeremy Laurent-Lucchetti']",econ.GN,"There is a tight connection between credit access and voting. We show that uncertainty in access to credit pushes voters toward more conservative candidates in US elections. Using a 1% sample of the US population with valid credit reports, we relate access to credit to voting outcomes in all county-by-congressional districts over the period 2004-2016. Specifically, we construct exogenous measures of uncertainty to credit access, i.e. credit score values around which individual total credit amount jumps the most (e.g. around which uncertainty on access to credit is the highest). We then show that a 10pp increase in the share of marginal voters located just around these thresholds increases republican votes by 2.7pp, and reduces that of democrats by 2.6pp. Furthermore, winning candidates in more uncertain constituencies tend to follow a more conservative rhetoric."
http://arxiv.org/abs/2407.06733v1,Causes and Electoral Consequences of Political Assassinations: The Role of Organized Crime in Mexico,2024-07-09 10:21:07+00:00,"['Roxana Gutiérrez-Romero', 'Nayely Iturbe']",econ.EM,"Mexico has experienced a notable surge in assassinations of political candidates and mayors. This article argues that these killings are largely driven by organized crime, aiming to influence candidate selection, control local governments for rent-seeking, and retaliate against government crackdowns. Using a new dataset of political assassinations in Mexico from 2000 to 2021 and instrumental variables, we address endogeneity concerns in the location and timing of government crackdowns. Our instruments include historical Chinese immigration patterns linked to opium cultivation in Mexico, local corn prices, and U.S. illicit drug prices. The findings reveal that candidates in municipalities near oil pipelines face an increased risk of assassination due to drug trafficking organizations expanding into oil theft, particularly during elections and fuel price hikes. Government arrests or killings of organized crime members trigger retaliatory violence, further endangering incumbent mayors. This political violence has a negligible impact on voter turnout, as it targets politicians rather than voters. However, voter turnout increases in areas where authorities disrupt drug smuggling, raising the chances of the local party being re-elected. These results offer new insights into how criminal groups attempt to capture local governments and the implications for democracy under criminal governance."
http://arxiv.org/abs/2407.06695v1,"Gentrification, Mobility, and Consumption",2024-07-09 09:12:26+00:00,"['Giacomo De Giorgi', 'Enrico Moretti', 'Harrison Wheeler']",econ.GN,"We study the effect of localized housing price hikes on renters' mobility, consumption, and credit outcomes. Consistent with a spatial equilibrium model, we find that the consumption responses vary greatly for movers and stayers. While movers increase their consumption, purchase homes, and cars, stayers are relatively unaffected."
http://arxiv.org/abs/2407.08602v1,An Introduction to Causal Discovery,2024-07-11 15:29:14+00:00,['Martin Huber'],econ.EM,"In social sciences and economics, causal inference traditionally focuses on assessing the impact of predefined treatments (or interventions) on predefined outcomes, such as the effect of education programs on earnings. Causal discovery, in contrast, aims to uncover causal relationships among multiple variables in a data-driven manner, by investigating statistical associations rather than relying on predefined causal structures. This approach, more common in computer science, seeks to understand causality in an entire system of variables, which can be visualized by causal graphs. This survey provides an introduction to key concepts, algorithms, and applications of causal discovery from the perspectives of economics and social sciences. It covers fundamental concepts like d-separation, causal faithfulness, and Markov equivalence, sketches various algorithms for causal discovery, and discusses the back-door and front-door criteria for identifying causal effects. The survey concludes with more specific examples of causal discovery, e.g. for learning all variables that directly affect an outcome of interest and/or testing identification of causal effects in observational data."
http://arxiv.org/abs/2407.15801v1,Selection pressure/Noise driven cooperative behaviour in the thermodynamic limit of repeated games,2024-07-22 17:07:29+00:00,"['Rajdeep Tah', 'Colin Benjamin']",cond-mat.stat-mech,"Consider the scenario where an infinite number of players (i.e., the \textit{thermodynamic} limit) find themselves in a Prisoner's dilemma type situation, in a \textit{repeated} setting. Is it reasonable to anticipate that, in these circumstances, cooperation will emerge? This paper addresses this question by examining the emergence of cooperative behaviour, in the presence of \textit{noise} (or, under \textit{selection pressure}), in repeated Prisoner's Dilemma games, involving strategies such as \textit{Tit-for-Tat}, \textit{Always Defect}, \textit{GRIM}, \textit{Win-Stay, Lose-Shift}, and others. To analyze these games, we employ a numerical Agent-Based Model (ABM) and compare it with the analytical Nash Equilibrium Mapping (NEM) technique, both based on the \textit{1D}-Ising chain. We use \textit{game magnetization} as an indicator of cooperative behaviour. A significant finding is that for some repeated games, a discontinuity in the game magnetization indicates a \textit{first}-order \textit{selection pressure/noise}-driven phase transition. The phase transition is particular to strategies where players do not severely punish a single defection. We also observe that in these particular cases, the phase transition critically depends on the number of \textit{rounds} the game is played in the thermodynamic limit. For all five games, we find that both ABM and NEM, in conjunction with game magnetization, provide crucial inputs on how cooperative behaviour can emerge in an infinite-player repeated Prisoner's dilemma game."
http://arxiv.org/abs/2407.14623v1,Fair allocation of riparian water rights,2024-07-19 18:42:03+00:00,"['Ricardo Martinez', 'Juan D. Moreno-Ternero']",econ.TH,"We take an axiomatic approach to the allocation of riparian water rights. We formalize ethical or structural properties as axioms of allocation rules. We show that several combinations of these axioms characterize focal rules implementing the principle of Territorial Integration of all Basin States in various forms. One of them connects to the Shapley value, the long-standing centerpiece of cooperative game theory. The others offer natural compromises between the polar principles of Absolute Territorial Sovereignty and Unlimited Territorial Integrity. We complete our study with an empirical application to the allocation of riparian water rights in the Nile River."
http://arxiv.org/abs/2407.15147v1,Industry Dynamics with Cartels: The Case of the Container Shipping Industry,2024-07-21 12:50:49+00:00,['Suguru Otani'],econ.GN,"I investigate how explicit cartels, known as ``shipping conferences"", in a global container shipping market facilitated the formation of one of the largest globally integrated markets through entry, exit, and shipbuilding investment of shipping firms. Using a novel data, I develop and construct a structural model and find that the cartels shifted shipping prices by 20-50\% and encouraged firms' entry and investment. In the counterfactual, I find that cartels would increase producer surplus while slightly decreasing consumer surplus, then may increase social welfare by encouraging firms' entry and shipbuilding investment. This would validate industry policies controlling prices and quantities in the early stage of the new industry, which may not be always harmful. Investigating hypothetical allocation rules supporting large or small firms, I find that the actual rule based on tonnage shares is the best to maximize social welfare."
http://arxiv.org/abs/2407.14315v1,Soviet Mathematics and Economic Theory in the Past Century: An Historical Reappraisal,2024-07-19 13:48:07+00:00,['Ivan Boldyrev'],math.HO,"What are the effects of authoritarian regimes on scholarly research in economics? And how might economic theory survive ideological pressures? The article addresses these questions by focusing on the mathematization of economics over the past century and drawing on the history of Soviet science. Mathematics in the USSR remained internationally competitive and generated many ideas that were taken up and played important roles in economic theory. These same ideas, however, were disregarded or adopted only in piecemeal fashion by Soviet economists, despite the efforts of influential scholars to change the economic research agenda. The article draws this contrast into sharper focus by exploring the work of Soviet mathematicians in optimization, game theory, and probability theory that was used in Western economics. While the intellectual exchange across the Iron Curtain did help advance the formal modeling apparatus, economics could only thrive in an intellectually open environment absent under the Soviet rule."
http://arxiv.org/abs/2407.20136v1,"""How lonely are you?"" The role of social contacts and farm characteristics in farmers' self-reported feelings of loneliness, and why it matters",2024-07-29 16:04:00+00:00,"['Victoria Junquera', 'Daniel I. Rubenstein', 'Florian Knaus']",econ.GN,"Loneliness and social isolation among farmers are growing public health concerns. The contributing factors are manifold, and some of them are linked to structural change in agriculture, for instance because of higher workloads, rural depopulation, or reduced opportunities for collaboration. Our work explores the interconnections between loneliness, social contacts, and structural factors in agriculture based on a survey of 110 farm managers in the mountain region of Entlebuch, Switzerland combined with agricultural census data. We use path analysis, in which loneliness is the main outcome, and social contacts are an explanatory and explained variable. We find that 3% of respondents report that they feel lonely frequently or very frequently, and the rest sometimes (20%), rarely (40%) or never (38%). Managers with higher workloads report feeling lonely more frequently, and this relationship is direct, as well as indirect because of less frequent social contacts. However, physical isolation is not a significant predictor of loneliness. Moreover, short food supply chains correlate with less frequent loneliness feelings. Our study sheds light on the effects that structural change can have on the social fabric of rural areas."
http://arxiv.org/abs/2407.01844v1,An Efficient and Sybil Attack Resistant Voting Mechanism,2024-07-01 22:58:27+00:00,['Jeremias Lenzi'],econ.TH,"Voting mechanisms are widely accepted and used methods for decentralized decision-making. Ensuring the acceptance of the voting mechanism's outcome is a crucial characteristic of robust voting systems. Consider this scenario: A group of individuals wants to choose an option from a set of alternatives without requiring an identification or proof-of-personhood system. Moreover, they want to implement utilitarianism as their selection criteria. In such a case, players could submit votes multiple times using dummy accounts, commonly known as a Sybil attack (SA), which presents a challenge for decentralized organizations. Is there a voting mechanism that always prevents players from benefiting by casting votes multiple times (SA-proof) while also selecting the alternative that maximizes the added valuations of all players (efficient)? One-person-one-vote is neither SA-proof nor efficient. Coin voting is SA-proof but not efficient. Quadratic voting is efficient but not SA-proof. This study uses Bayesian mechanism design to propose a solution. The mechanism's structure is as follows: Players make wealth deposits to indicate the strength of their preference for each alternative. Each player then receives an amount based on their deposit and the voting outcome. The proposed mechanism relies on two main concepts: 1) Transfers are influenced by the outcome in a way that each player's optimal action depends only on individual preferences and the number of alternatives; 2) A player who votes through multiple accounts slightly reduces the expected utility of all players more than the individual benefit gained. This study demonstrates that if players are risk-neutral and each player has private information about their preferences and beliefs, then the mechanism is SA-proof and efficient. This research provides new insights into the design of more robust decentralized decision-making mechanisms."
http://arxiv.org/abs/2407.09695v1,Monopoly Unveiled: Telecom Breakups in the US and Mexico,2024-07-12 21:36:31+00:00,"['Fausto Hernández Trillo', 'C. Vladimir Rodríguez-Caballero', 'Daniel Ventosa-Santaulària']",q-fin.GN,"This paper posits the decline in market capitalization following a monopoly breakup serves as a means to gauge how financial markets assess market power. Our research, which employs univariate structural time series models to estimate the firm's value without the breakup and juxtapose it with actual post-divestiture values, reveals a staggering drop in AT&T's value by 65% and AMX's by 32% from their pre-breakup levels. These findings underscore the contemporary valuation of monopoly rents as perceived by financial markets, highlighting the significant impact of monopoly breakup on market capitalization and the need for a deeper understanding of these dynamics."
http://arxiv.org/abs/2407.12775v1,Estimating the Potential Impact of Combined Race and Ethnicity Reporting on Long-Term Earnings Statistics,2024-07-17 17:55:38+00:00,"['Kevin L. McKinney', 'John M. Abowd']",econ.GN,"We use place of birth information from the Social Security Administration linked to earnings data from the Longitudinal Employer-Household Dynamics Program and detailed race and ethnicity data from the 2010 Census to study how long-term earnings differentials vary by place of birth for different self-identified race and ethnicity categories. We focus on foreign-born persons from countries that are heavily Hispanic and from countries in the Middle East and North Africa (MENA). We find substantial heterogeneity of long-term earnings differentials within country of birth, some of which will be difficult to detect when the reporting format changes from the current two-question version to the new single-question version because they depend on self-identifications that place the individual in two distinct categories within the single-question format, specifically, Hispanic and White or Black, and MENA and White or Black. We also study the USA-born children of these same immigrants. Long-term earnings differences for the 2nd generation also vary as a function of self-identified ethnicity and race in ways that changing to the single-question format could affect."
http://arxiv.org/abs/2407.12837v1,Keynesian chaos revisited: odd period cycles and ergodic properties,2024-07-04 02:01:40+00:00,['Tomohiro Uchiyama'],econ.GN,"In this paper, we study two standard (Keynesian) dynamic macroeconomic models (one is piecewise linear and the other is nonlinear). Our purpose is twofold: (1)~For each model, we give a complete characterisation for the existence of a topological chaos (of the GDP levels), (2)~Even if a chaos exists, using ergodic theory, we show that it is possible to predict the future GDP levels ""on average"". This paper gives a new application of a celebrated result in ergodic theory by A. Avila (2014 fields medalist). We believe that our method/strategy in this paper is generic enough to be used to analyse many other (seemingly untractable) chaotic economic models."
http://arxiv.org/abs/2407.03595v1,Machine Learning for Economic Forecasting: An Application to China's GDP Growth,2024-07-04 03:04:55+00:00,"['Yanqing Yang', 'Xingcheng Xu', 'Jinfeng Ge', 'Yan Xu']",econ.GN,"This paper aims to explore the application of machine learning in forecasting Chinese macroeconomic variables. Specifically, it employs various machine learning models to predict the quarterly real GDP growth of China, and analyzes the factors contributing to the performance differences among these models. Our findings indicate that the average forecast errors of machine learning models are generally lower than those of traditional econometric models or expert forecasts, particularly in periods of economic stability. However, during certain inflection points, although machine learning models still outperform traditional econometric models, expert forecasts may exhibit greater accuracy in some instances due to experts' more comprehensive understanding of the macroeconomic environment and real-time economic variables. In addition to macroeconomic forecasting, this paper employs interpretable machine learning methods to identify the key attributive variables from different machine learning models, aiming to enhance the understanding and evaluation of their contributions to macroeconomic fluctuations."
http://arxiv.org/abs/2407.07973v1,Reduced-Rank Matrix Autoregressive Models: A Medium $N$ Approach,2024-07-10 18:12:10+00:00,"['Alain Hecq', 'Ivan Ricardo', 'Ines Wilms']",econ.EM,"Reduced-rank regressions are powerful tools used to identify co-movements within economic time series. However, this task becomes challenging when we observe matrix-valued time series, where each dimension may have a different co-movement structure. We propose reduced-rank regressions with a tensor structure for the coefficient matrix to provide new insights into co-movements within and between the dimensions of matrix-valued time series. Moreover, we relate the co-movement structures to two commonly used reduced-rank models, namely the serial correlation common feature and the index model. Two empirical applications involving U.S.\ states and economic indicators for the Eurozone and North American countries illustrate how our new tools identify co-movements."
http://arxiv.org/abs/2407.09340v1,Modelling shock propagation and resilience in financial temporal networks,2024-07-12 15:15:37+00:00,"['Fabrizio Lillo', 'Giorgio Rizzini']",econ.GN,"Modelling how a shock propagates in a temporal network and how the system relaxes back to equilibrium is challenging but important in many applications, such as financial systemic risk. Most studies so far have focused on shocks hitting a link of the network, while often it is the node and its propensity to be connected that are affected by a shock. Using as starting point the configuration model, a specific Exponential Random Graph model, we propose a vector autoregressive (VAR) framework to analytically compute the Impulse Response Function (IRF) of a network metric conditional to a shock on a node. Unlike the standard VAR, the model is a nonlinear function of the shock size and the IRF depends on the state of the network at the shock time. We propose a novel econometric estimation method that combines the Maximum Likelihood Estimation and Kalman filter to estimate the dynamics of the latent parameters and compute the IRF, and we apply the proposed methodology to the dynamical network describing the electronic Market of Interbank Deposit (e-MID)."
http://arxiv.org/abs/2408.05328v1,From Text to Insight: Leveraging Large Language Models for Performance Evaluation in Management,2024-08-09 20:35:10+00:00,"['Ning Li', 'Huaikang Zhou', 'Mingze Xu']",cs.CL,"This study explores the potential of Large Language Models (LLMs), specifically GPT-4, to enhance objectivity in organizational task performance evaluations. Through comparative analyses across two studies, including various task performance outputs, we demonstrate that LLMs can serve as a reliable and even superior alternative to human raters in evaluating knowledge-based performance outputs, which are a key contribution of knowledge workers. Our results suggest that GPT ratings are comparable to human ratings but exhibit higher consistency and reliability. Additionally, combined multiple GPT ratings on the same performance output show strong correlations with aggregated human performance ratings, akin to the consensus principle observed in performance evaluation literature. However, we also find that LLMs are prone to contextual biases, such as the halo effect, mirroring human evaluative biases. Our research suggests that while LLMs are capable of extracting meaningful constructs from text-based data, their scope is currently limited to specific forms of performance evaluation. By highlighting both the potential and limitations of LLMs, our study contributes to the discourse on AI role in management studies and sets a foundation for future research to refine AI theoretical and practical applications in management."
http://arxiv.org/abs/2408.06048v1,Hungry Professors? Decision Biases Are Less Widespread than Previously Thought,2024-08-12 10:49:14+00:00,"['Katja Bergonzoli', 'Laurent Bieri', 'Dominic Rohner', 'Christian Zehnder']",econ.GN,"In many situations people make sequences of similar, but unrelated decisions. Such decision sequences are prevalent in many important contexts including judicial judgments, loan approvals, college admissions, and athletic competitions. A growing literature claims that decisions in such sequences may be severely biased because decision outcomes seem to be systematically affected by the scheduling. In particular, it has been argued that mental depletion leads to harsher decisions before food breaks and that the ``law of small numbers'' induces decisions to be negatively auto-correlated (i.e. favorable decisions are followed by unfavorable ones and vice versa). These findings have attracted much academic and media attention and it has been suspected that they may only represent the ``tip of the iceberg''. However, voices of caution point out that existing studies may suffer from serious limitations, because the decision order is not randomly determined, other influencing factors are hard to exclude, or direct evidence for the underlying mechanisms is not available. We exploit a large-scale natural experiment in a context in which the previous literature would predict the presence of scheduling biases. Specifically, we investigate whether the grades of randomly scheduled oral exams in Law School depend on the position of the exam in the sequence. Our rich data enables us to filter-out student, professor, day, and course-specific features. Our results contradict the previous findings and suggest that caution is advised when generalizing from previous studies for policy advice."
http://arxiv.org/abs/2408.04573v1,Revealed Invariant Preference,2024-08-08 16:36:03+00:00,"['Peter Caradonna', 'Christopher P. Chambers']",econ.TH,"We consider the problem of rationalizing choice data by a preference satisfying an arbitrary collection of invariance axioms. Examples of such axioms include quasilinearity, homotheticity, independence-type axioms for mixture spaces, constant relative/absolute risk and ambiguity aversion axioms, stationarity for dated rewards or consumption streams, separability, and many others. We provide necessary and sufficient conditions for invariant rationalizability via a novel approach which relies on tools from the theoretical computer science literature on automated theorem proving. We also establish a generalization of the Dushnik-Miller theorem, which we use to give a complete description of the out-of-sample predictions generated by the data under any such collection of axioms."
http://arxiv.org/abs/2408.03930v1,Robust Estimation of Regression Models with Potentially Endogenous Outliers via a Modern Optimization Lens,2024-08-07 17:46:08+00:00,"['Zhan Gao', 'Hyungsik Roger Moon']",econ.EM,"This paper addresses the robust estimation of linear regression models in the presence of potentially endogenous outliers. Through Monte Carlo simulations, we demonstrate that existing $L_1$-regularized estimation methods, including the Huber estimator and the least absolute deviation (LAD) estimator, exhibit significant bias when outliers are endogenous. Motivated by this finding, we investigate $L_0$-regularized estimation methods. We propose systematic heuristic algorithms, notably an iterative hard-thresholding algorithm and a local combinatorial search refinement, to solve the combinatorial optimization problem of the \(L_0\)-regularized estimation efficiently. Our Monte Carlo simulations yield two key results: (i) The local combinatorial search algorithm substantially improves solution quality compared to the initial projection-based hard-thresholding algorithm while offering greater computational efficiency than directly solving the mixed integer optimization problem. (ii) The $L_0$-regularized estimator demonstrates superior performance in terms of bias reduction, estimation accuracy, and out-of-sample prediction errors compared to $L_1$-regularized alternatives. We illustrate the practical value of our method through an empirical application to stock return forecasting."
http://arxiv.org/abs/2407.15757v1,Willingness to Pay for an Electricity Connection: A Choice Experiment Among Rural Households and Enterprises in Nigeria,2024-07-22 16:08:02+00:00,"['Pouya Janghorban', 'Temilade Sesan', 'Muhammad-Kabir Salihu', 'Olayinka Ohunakin', 'Narges Chinichian']",econ.GN,"Rural electrification initiatives worldwide frequently encounter financial planning challenges due to a lack of reliable market insights. This research delves into the preferences and marginal willingness to pay (mWTP) for upfront electricity connections in rural and peri-urban areas of Nigeria. We investigate discrete choice experiment data gathered from 3,599 households and 1,122 Small to Medium-sized Enterprises (SMEs) across three geopolitical zones of Nigeria, collected during the 2021 PeopleSuN project survey phase. Employing conditional logit modeling, we analyze this data to explore preferences and marginal willingness to pay for electricity connection. Our findings show that households prioritize nighttime electricity access, while SMEs place a higher value on daytime electricity. When comparing improvements in electricity capacity to medium or high-capacity, SMEs exhibit a sharp increase in willingness to pay for high-capacity, while households value the two options more evenly. Preferences for the electricity source vary among SMEs, but households display a reluctance towards diesel generators and a preference for the grid or solar solutions. Moreover, households with older heads express greater aversion to connection fees, and male-headed households show a stronger preference for nighttime electricity compared to their female-headed counterparts. The outcomes of this study yield pivotal insights to tailor electrification strategies for rural Nigeria, emphasizing the importance of considering the diverse preferences of households and SMEs."
http://arxiv.org/abs/2408.06547v1,Identifying Restrictions on the Random Utility Model,2024-08-13 00:55:30+00:00,"['Peter P. Caradonna', 'Christopher Turansick']",econ.TH,"We characterize those ex-ante restrictions on the random utility model which lead to identification. We first identify a simple class of perturbations which transfer mass from a suitable pair of preferences to the pair formed by swapping certain compatible lower contour sets. We show that two distributions over preferences are behaviorally equivalent if and only if they can be obtained from each other by a finite sequence of such transformations. Using this, we obtain specialized characterizations of which restrictions on the support of a random utility model yield identification, as well as of the extreme points of the set of distributions rationalizing a given data set. Finally, when a model depends smoothly on some set of parameters, we show that under mild topological assumptions, identification is characterized by a straightforward, local test."
http://arxiv.org/abs/2407.14267v2,The spatial evolution of economic activities: from theory to estimation,2024-07-19 12:46:19+00:00,"['Davide Fiaschi', 'Angela Parenti', 'Cristiano Ricci']",econ.GN,"This paper studies the evolution of economic activities using a continuous time-space aggregation-diffusion model, which encompasses competing effects of agglomeration and congestion. To bring the model to the real data, a novel discretization technique over time and space is introduced. This technique effectively disentangles spatial effects into pure topography, agglomeration, repulsion, and diffusion forces, which is crucial for developing robust econometric methods in spatial economics. Our empirical analysis of personal income across Italian municipalities from 2008 to 2019 validates the model's primary predictions and demonstrates superior performance compared to the most common spatial econometric models in the literature."
http://arxiv.org/abs/2408.03134v1,Existence and uniqueness of quadratic and linear mean-variance equilibria in general semimartingale markets,2024-08-06 12:22:35+00:00,"['Christoph Czichowsky', 'Martin Herdegen', 'David Martins']",q-fin.MF,"We revisit the classical topic of quadratic and linear mean-variance equilibria with both financial and real assets. The novelty of our results is that they are the first allowing for equilibrium prices driven by general semimartingales and hold in discrete as well as continuous time. For agents with quadratic utility functions, we provide necessary and sufficient conditions for the existence and uniqueness of equilibria. We complement our analysis by providing explicit examples showing non-uniqueness or non-existence of equilibria. We then study the more difficult case of linear mean-variance preferences. We first show that under mild assumptions, a linear mean-variance equilibrium corresponds to a quadratic equilibrium (for different preference parameters). We then use this link to study a fixed-point problem that establishes existence (and uniqueness in a suitable class) of linear mean-variance equilibria. Our results rely on fine properties of dynamic mean-variance hedging in general semimartingale markets."
http://arxiv.org/abs/2408.05047v1,Recurrent Stochastic Fluctuations with Financial Speculation,2024-08-09 13:00:51+00:00,['Tomohiro Hirano'],econ.TH,"Throughout history, many countries have repeatedly experienced large swings in asset prices, which are usually accompanied by large fluctuations in macroeconomic activity. One of the characteristics of the period before major economic fluctuations is the emergence of new financial products; the situation prior to the 2008 financial crisis is a prominent example of this. During that period, a variety of structured bonds, including securitized products, appeared. Because of the high returns on such financial products, many economic agents were involved in them for speculative purposes, even if they were riskier, producing macro-scale effects.
  With this motivation, we present a simple macroeconomic model with financial speculation. Our model illustrates two points. First, stochastic fluctuations in asset prices and macroeconomic activity are driven by the repeated appearance and disappearance of risky financial assets, rather than expansions and contractions in credit availability. Second, in an economy with sufficient borrowing and lending, the appearance of risky financial assets leads to decreased productive capital, while in an economy with severely limited borrowing and lending, it leads to increased productive capital."
http://arxiv.org/abs/2408.07842v1,Quantile and Distribution Treatment Effects on the Treated with Possibly Non-Continuous Outcomes,2024-08-14 22:44:27+00:00,"['Nelly K. Djuazon', 'Emmanuel Selorm Tsyawo']",econ.EM,"Quantile and Distribution Treatment effects on the Treated (QTT/DTT) for non-continuous outcomes are either not identified or inference thereon is infeasible using existing methods. By introducing functional index parallel trends and no anticipation assumptions, this paper identifies and provides uniform inference procedures for QTT/DTT. The inference procedure applies under both the canonical two-group and staggered treatment designs with balanced panels, unbalanced panels, or repeated cross-sections. Monte Carlo experiments demonstrate the proposed method's robust and competitive performance, while an empirical application illustrates its practical utility."
http://arxiv.org/abs/2408.03530v4,Robust Identification in Randomized Experiments with Noncompliance,2024-08-07 04:00:48+00:00,"['Désiré Kédagni', 'Huan Wu', 'Yi Cui']",econ.EM,"Instrument variable (IV) methods are widely used in empirical research to identify causal effects of a policy. In the local average treatment effect (LATE) framework, the IV estimand identifies the LATE under three main assumptions: random assignment, exclusion restriction, and monotonicity. However, these assumptions are often questionable in many applications, leading some researchers to doubt the causal interpretation of the IV estimand. This paper considers a robust identification of causal parameters in a randomized experiment setting with noncompliance where the standard LATE assumptions could be violated. We discuss identification under two sets of weaker assumptions: random assignment and exclusion restriction (without monotonicity), and random assignment and monotonicity (without exclusion restriction). We derive sharp bounds on some causal parameters under these two sets of relaxed LATE assumptions. Finally, we apply our method to revisit the random information experiment conducted in Bursztyn, González, and Yanagizawa-Drott (2020) and find that the standard LATE assumptions are jointly incompatible in this application. We then estimate the robust identified sets under the two sets of relaxed assumptions."
http://arxiv.org/abs/2409.10938v2,Beyond Rationality: Unveiling the Role of Animal Spirits and Inflation Extrapolation in Central Bank Communication of the US,2024-09-17 07:16:58+00:00,['Arpan Chakraborty'],econ.TH,"Modern macroeconomic models, particularly those grounded in Rational Expectation Dynamic Stochastic General Equilibrium (DSGE), operate under the assumption of fully rational decision-making. This paper examines the impact of behavioral factors on the communication index/sentiment index of the US Federal Reserve. [Upon receiving the review comments, I found some technical errors in the paper. I shall update it accordingly. Please do not cite this paper without author's permission.]"
http://arxiv.org/abs/2407.04227v2,Computationally Efficient Methods for Solving Discrete-time Dynamic models with Continuous Actions,2024-07-05 02:50:47+00:00,['Takeshi Fukasawa'],econ.GN,"This study investigates computationally efficient algorithms for solving discrete-time infinite-horizon single-agent/multi-agent dynamic models with continuous actions. It shows that we can easily reduce the computational costs by slightly changing basic algorithms using value functions, such as the Value Function Iteration (VFI) and the Policy Iteration (PI).
  The PI method with a Krylov iterative method (GMRES), which can be easily implemented using built-in packages, works much better than VFI-based algorithms even when considering continuous state models. Concerning the VFI algorithm, we can largely speed up the convergence by introducing acceleration methods of fixed-point iterations. The current study also proposes the VF-PGI-Spectral (Value Function-Policy Gradient Iteration Spectral) algorithm, which is a slight modification of the VFI. It shows numerical results where the VF-PGI-Spectral performs much better than the VFI- and PI-based algorithms especially in multi-agent dynamic games. Finally, it shows that using relative value functions further reduces the computational cost of these methods."
http://arxiv.org/abs/2408.08861v2,The computational power of a human society: a new model of social evolution,2024-08-16 17:42:28+00:00,"['David H. Wolpert', 'Kyle Harper']",cs.MA,"Social evolutionary theory seeks to explain increases in the scale and complexity of human societies, from origins to present. Over the course of the twentieth century, social evolutionary theory largely fell out of favor as a way of investigating human history, just when advances in complex systems science and computer science saw the emergence of powerful new conceptions of complex systems, and in particular new methods of measuring complexity. We propose that these advances in our understanding of complex systems and computer science should be brought to bear on our investigations into human history. To that end, we present a new framework for modeling how human societies co-evolve with their biotic environments, recognizing that both a society and its environment are computers. This leads us to model the dynamics of each of those two systems using the same, new kind of computational machine, which we define here. For simplicity, we construe a society as a set of interacting occupations and technologies. Similarly, under such a model, a biotic environment is a set of interacting distinct ecological and environmental processes. This provides novel ways to characterize social complexity, which we hope will cast new light on the archaeological and historical records. Our framework also provides a natural way to formalize both the energetic (thermodynamic) costs required by a society as it runs, and the ways it can extract thermodynamic resources from the environment in order to pay for those costs -- and perhaps to grow with any left-over resources."
http://arxiv.org/abs/2408.02391v2,Kullback-Leibler-based characterizations of score-driven updates,2024-08-05 11:35:11+00:00,"['Ramon de Punder', 'Timo Dimitriadis', 'Rutger-Jan Lange']",math.ST,"Score-driven models have been applied in some 400 published articles over the last decade. Much of this literature cites the optimality result in Blasques et al. (2015), which, roughly, states that sufficiently small score-driven updates are unique in locally reducing the Kullback-Leibler divergence relative to the true density for every observation. This is at odds with other well-known optimality results; the Kalman filter, for example, is optimal in a mean-squared-error sense, but occasionally moves away from the true state. We show that score-driven updates are, similarly, not guaranteed to improve the localized Kullback-Leibler divergence at every observation. The seemingly stronger result in Blasques et al. (2015) is due to their use of an improper (localized) scoring rule. Even as a guaranteed improvement for every observation is unattainable, we prove that sufficiently small score-driven updates are unique in reducing the Kullback-Leibler divergence relative to the true density in expectation. This positive, albeit weaker, result justifies the continued use of score-driven models and places their information-theoretic properties on solid footing."
http://arxiv.org/abs/2408.04385v2,Non-maximizing policies that fulfill multi-criterion aspirations in expectation,2024-08-08 11:41:04+00:00,"['Simon Dima', 'Simon Fischer', 'Jobst Heitzig', 'Joss Oliver']",cs.AI,"In dynamic programming and reinforcement learning, the policy for the sequential decision making of an agent in a stochastic environment is usually determined by expressing the goal as a scalar reward function and seeking a policy that maximizes the expected total reward. However, many goals that humans care about naturally concern multiple aspects of the world, and it may not be obvious how to condense those into a single reward function. Furthermore, maximization suffers from specification gaming, where the obtained policy achieves a high expected total reward in an unintended way, often taking extreme or nonsensical actions.
  Here we consider finite acyclic Markov Decision Processes with multiple distinct evaluation metrics, which do not necessarily represent quantities that the user wants to be maximized. We assume the task of the agent is to ensure that the vector of expected totals of the evaluation metrics falls into some given convex set, called the aspiration set. Our algorithm guarantees that this task is fulfilled by using simplices to approximate feasibility sets and propagate aspirations forward while ensuring they remain feasible. It has complexity linear in the number of possible state-action-successor triples and polynomial in the number of evaluation metrics. Moreover, the explicitly non-maximizing nature of the chosen policy and goals yields additional degrees of freedom, which can be used to apply heuristic safety criteria to the choice of actions. We discuss several such safety criteria that aim to steer the agent towards more conservative behavior."
http://arxiv.org/abs/2408.04814v3,Protected Income and Inequality Aversion,2024-08-09 02:02:48+00:00,"['Marc Fleurbaey', 'Eduardo Zambrano']",econ.TH,"We discover a fundamental and previously unrecognized structure within the class of additively separable social welfare functions that makes it straightforward to fully characterize and elicit the social preferences of an inequality-averse evaluator. From this structure emerges a revealing question: if a large increment can be given to one individual in a society, what is the maximal sacrifice that another individual can be asked to bear for its sake? We show that the answer uncovers the evaluator's degree of inequality aversion. In particular, all translation-invariant evaluators would sacrifice the full income of the sacrificed individual if their income were low enough and a constant amount of their income otherwise. Scale-invariant evaluators would sacrifice the full income of the sacrificed individual at all income levels if their inequality aversion was no greater than one, and a constant fraction of their income otherwise. Motivated by these findings, we propose a class of social preferences that, starting from a minimum-income level of protection, ensure a higher fraction of the sacrificed individual's income is protected the lower their income."
http://arxiv.org/abs/2407.10653v3,"The Dynamic, the Static, and the Weak: Factor models and the analysis of high-dimensional time series",2024-07-15 12:14:23+00:00,"['Matteo Barigozzi', 'Marc Hallin']",econ.EM,"Several fundamental and closely interconnected issues related to factor models are reviewed and discussed: dynamic versus static loadings, rate-strong versus rate-weak factors, the concept of weakly common component recently introduced by Gersing et al. (2023), the irrelevance of cross-sectional ordering and the assumption of cross-sectional exchangeability, the impact of undetected strong factors, and the problem of combining common and idiosyncratic forecasts. Conclusions all point to the advantages of the General Dynamic Factor Model approach of Forni et al. (2000) over the widely used Static Approximate Factor Model introduced by Chamberlain and Rothschild (1983)."
http://arxiv.org/abs/2409.04326v2,Digital Platform Consolidation and Offline Expansion: Strategic Convergence and Market Welfare in China's Second-hand Real Estate Market,2024-09-06 14:55:26+00:00,"['Guoying Deng', 'Xuyuan Zhang']",econ.GN,"This study analyzes the impact of offline expansion and online platform consolidation in China's second-hand real estate market. Using micro-level transaction data and difference-in-differences estimations, we find offline store entry significantly boosts transaction volumes (9-10\%) and reduces price concessions (1\%) initially, though effects diminish over time. Platform consolidation via Lianjia's Agent Cooperation Network yields delayed yet persistent transaction volume increases (5-6\%), particularly in less concentrated markets, and consistently lowers price concessions. These strategies sustainably enhance brokerage competitiveness, bargaining power, and market welfare, despite increased market concentration, ultimately benefiting sellers and improving overall efficiency."
http://arxiv.org/abs/2408.05856v2,Has the Recession Started?,2024-08-11 20:01:13+00:00,"['Pascal Michaillat', 'Emmanuel Saez']",econ.GN,"To answer this question, we develop a new Sahm-type recession indicator that combines vacancy and unemployment data. The indicator is the minimum of the Sahm indicator -- the difference between the 3-month trailing average of the unemployment rate and its minimum over the past 12 months -- and a similar indicator constructed with the vacancy rate -- the difference between the 3-month trailing average of the vacancy rate and its maximum over the past 12 months. We then propose a two-sided recession rule: When our indicator reaches 0.3pp, a recession may have started; when the indicator reaches 0.8pp, a recession has started for sure. This new rule is triggered earlier than the Sahm rule: on average it detects recessions 0.8 month after they have started, while the Sahm rule detects them 2.1 months after their start. The new rule also has a better historical track record: it perfectly identifies all recessions since 1929, while the Sahm rule breaks down before 1960. With August 2024 data, our indicator is at 0.54pp, so the probability that the US economy is now in recession is 48%. In fact, the recession may have started as early as March 2024."
http://arxiv.org/abs/2408.02492v3,Bargaining via Weber's law,2024-08-05 14:16:33+00:00,"['V. G. Bardakhchyan', 'A. E. Allahverdyan']",econ.TH,"We solve the two-player bargaining problem employing Weber's law in psychophysics, which is applied to the perception of utility changes. Using this law, the players define the jointly acceptable range of utilities on the Pareto line, which narrows down the range of possible solutions. Choosing a unique solution can be achieved by applying the Weber approach iteratively. The solution is covariant to independent affine transformations of utilities. We provide a behavioral interpretation of this solution, where the players negotiate via Weber's law. For susceptible players, iterations are unnecessary, so they converge in one stage toward the (axiomatic) asymmetric Nash solution of the bargaining problem, where the weights of each player are expressed via their Weber constants. Thus the Nash solution is reached without external arbiters and without requiring the independence of irrelevant alternatives. We also show that our solution applies to the ultimatum game (which is not bargaining but still involves offer formation) and leads to an affine-covariant solution of this game that can reproduce its empirical features. Unlike previous solutions (e.g. the one based on fairness), ours does not involve comparing inter-personal utilities and is based on a partial symmetry between the proposer and respondent."
http://arxiv.org/abs/2409.12662v1,Testing for equal predictive accuracy with strong dependence,2024-09-19 11:23:06+00:00,"['Laura Coroneo', 'Fabrizio Iacone']",econ.EM,"We analyse the properties of the Diebold and Mariano (1995) test in the presence of autocorrelation in the loss differential. We show that the power of the Diebold and Mariano (1995) test decreases as the dependence increases, making it more difficult to obtain statistically significant evidence of superior predictive ability against less accurate benchmarks. We also find that, after a certain threshold, the test has no power and the correct null hypothesis is spuriously rejected. Taken together, these results caution to seriously consider the dependence properties of the loss differential before the application of the Diebold and Mariano (1995) test."
http://arxiv.org/abs/2407.14074v2,Regression Adjustment for Estimating Distributional Treatment Effects in Randomized Controlled Trials,2024-07-19 07:07:34+00:00,"['Tatsushi Oka', 'Shota Yasui', 'Yuta Hayakawa', 'Undral Byambadalai']",econ.EM,"In this paper, we address the issue of estimating and inferring distributional treatment effects in randomized experiments. The distributional treatment effect provides a more comprehensive understanding of treatment heterogeneity compared to average treatment effects. We propose a regression adjustment method that utilizes distributional regression and pre-treatment information, establishing theoretical efficiency gains without imposing restrictive distributional assumptions. We develop a practical inferential framework and demonstrate its advantages through extensive simulations. Analyzing water conservation policies, our method reveals that behavioral nudges systematically shift consumption from high to moderate levels. Examining health insurance coverage, we show the treatment reduces the probability of zero doctor visits by 6.6 percentage points while increasing the likelihood of 3-6 visits. In both applications, our regression adjustment method substantially improves precision and identifies treatment effects that were statistically insignificant under conventional approaches."
http://arxiv.org/abs/2407.18206v1,Starting Small: Prioritizing Safety over Efficacy in Randomized Experiments Using the Exact Finite Sample Likelihood,2024-07-25 17:14:55+00:00,"['Neil Christy', 'A. E. Kowalski']",econ.EM,"We use the exact finite sample likelihood and statistical decision theory to answer questions of ``why?'' and ``what should you have done?'' using data from randomized experiments and a utility function that prioritizes safety over efficacy. We propose a finite sample Bayesian decision rule and a finite sample maximum likelihood decision rule. We show that in finite samples from 2 to 50, it is possible for these rules to achieve better performance according to established maximin and maximum regret criteria than a rule based on the Boole-Frechet-Hoeffding bounds. We also propose a finite sample maximum likelihood criterion. We apply our rules and criterion to an actual clinical trial that yielded a promising estimate of efficacy, and our results point to safety as a reason for why results were mixed in subsequent trials."
http://arxiv.org/abs/2408.11739v2,Network-based diversification of stock and cryptocurrency portfolios,2024-08-21 16:04:06+00:00,"['Dimitar Kitanovski', 'Igor Mishkovski', 'Viktor Stojkoski', 'Miroslav Mirchev']",econ.GN,"Maintaining a balance between returns and volatility is a common strategy for portfolio diversification, whether investing in traditional equities or digital assets like cryptocurrencies. One approach for diversification is the application of community detection or clustering, using a network representing the relationships between assets. We examine two network representations, one based on a standard distance matrix based on correlation, and another based on mutual information. The Louvain and Affinity propagation algorithms were employed for finding the network communities (clusters) based on annual data. Furthermore, we examine building assets' co-occurrence networks, where communities are detected for each month throughout a whole year and then the links represent how often assets belong to the same community. Portfolios are then constructed by selecting several assets from each community based on local properties (degree centrality), global properties (closeness centrality), or explained variance (Principal component analysis), with three value ranges (max, med, min), calculated on a maximal spanning tree or a fully connected community sub-graph. We explored these various strategies on data from the S\&P 500 and the Top 203 cryptocurrencies with a market cap above 2M USD in the period from Jan 2019 to Sep 2022. Moreover, we study into more details the periods of the beginning of the COVID-19 outbreak and the start of the war in Ukraine. The results confirm some of the previous findings already known for traditional stock markets and provide some further insights, while they reveal an opposing trend in the crypto-assets market."
http://arxiv.org/abs/2407.07652v4,The heterogeneous impact of the EU-Canada agreement with causal machine learning,2024-07-10 13:34:06+00:00,"['Lionel Fontagné', 'Francesca Micocci', 'Armando Rungi']",econ.GN,"This paper introduces a causal machine learning approach to investigate the impact of the EU-Canada Comprehensive Economic Trade Agreement (CETA). We propose a matrix completion algorithm on French customs data to obtain multidimensional counterfactuals at the firm, product and destination levels. We find a small but significant positive impact on average at the product-level intensive margin. On the other hand, the extensive margin shows product churning due to the treaty beyond regular entry-exit dynamics: one product in eight that was not previously exported substitutes almost as many that are no longer exported. When we delve into the heterogeneity, we find that the effects of the treaty are higher for products at a comparative advantage. Focusing on multiproduct firms, we find that they adjust their portfolio in Canada by reallocating towards their first and most exported product due to increasing local market competition after trade liberalization. Finally, multidimensional counterfactuals allow us to evaluate the general equilibrium effect of the CETA. Specifically, we observe trade diversion, as exports to other destinations are re-directed to Canada."
http://arxiv.org/abs/2407.05196v2,Collective Upkeep,2024-07-06 21:47:55+00:00,"['Erik Madsen', 'Eran Shmaya']",econ.TH,"We design mechanisms for maintaining public goods which require periodic in-kind contributions, motivated by incentives problems facing crowd-sourced recommender systems. Utilitarian welfare is maximized by redistributive policies which are infeasible when group members can leave or misreport their preferences. An optimal mechanism reduces contributions for group members with low benefit-cost ratios to encourage participation; and pairs reduced contributions with restricted access to the good to ensure truthful reporting. At most two membership tiers are offered at the optimum, indicating that ecommerce and digital content platforms may benefit substantially from offering simple user-adjustable recommendation settings."
http://arxiv.org/abs/2409.00843v2,Global Public Sentiment on Decentralized Finance: A Spatiotemporal Analysis of Geo-tagged Tweets from 150 Countries,2024-09-01 21:14:49+00:00,"['Yuqi Chen', 'Yifan Li', 'Kyrie Zhixuan Zhou', 'Xiaokang Fu', 'Lingbo Liu', 'Shuming Bao', 'Daniel Sui', 'Luyao Zhang']",econ.GN,"Blockchain technology and decentralized finance (DeFi) are reshaping global financial systems. Despite their impact, the spatial distribution of public sentiment and its economic and geopolitical determinants are often overlooked. This study analyzes over 150 million geo-tagged, DeFi-related tweets from 2012 to 2022, sourced from a larger dataset of 7.4 billion tweets. Using sentiment scores from a BERT-based multilingual classification model, we integrated these tweets with economic and geopolitical data to create a multimodal dataset. Employing techniques like sentiment analysis, spatial econometrics, clustering, and topic modeling, we uncovered significant global variations in DeFi engagement and sentiment. Our findings indicate that economic development significantly influences DeFi engagement, particularly after 2015. Geographically weighted regression analysis revealed GDP per capita as a key predictor of DeFi tweet proportions, with its impact growing following major increases in cryptocurrency values such as bitcoin. While wealthier nations are more actively engaged in DeFi discourse, the lowest-income countries often discuss DeFi in terms of financial security and sudden wealth. Conversely, middle-income countries relate DeFi to social and religious themes, whereas high-income countries view it mainly as a speculative instrument or entertainment. This research advances interdisciplinary studies in computational social science and finance and supports open science by making our dataset and code available on GitHub, and providing a non-code workflow on the KNIME platform. These contributions enable a broad range of scholars to explore DeFi adoption and sentiment, aiding policymakers, regulators, and developers in promoting financial inclusion and responsible DeFi engagement globally."
http://arxiv.org/abs/2409.06248v1,Evidence gathering under competitive and noncompetitive rewards,2024-09-10 06:32:55+00:00,"['Philip Brookins', 'Jennifer Brown', 'Dmitry Ryvkin']",econ.GN,"Reward schemes may affect not only agents' effort, but also their incentives to gather information to reduce the riskiness of the productive activity. In a laboratory experiment using a novel task, we find that the relationship between incentives and evidence gathering depends critically on the availability of information about peers' strategies and outcomes. When no peer information is available, competitive rewards can be associated with more evidence gathering than noncompetitive rewards. In contrast, when decision-makers know what or how their peers are doing, competitive rewards schemes are associated with less active evidence gathering than noncompetitive schemes. The nature of the feedback -- whether subjects receive information about peers' strategies, outcomes, or both -- also affects subjects' incentives to engage in evidence gathering. Specifically, only combined feedback about peers' strategies and performance -- from which subjects may assess the overall relationship between evidence gathering, riskiness, and success -- is associated with less evidence gathering when rewards are based on relative performance; we find no similar effect for noncompetitive rewards."
http://arxiv.org/abs/2407.14333v5,Prompt Adaptation as a Dynamic Complement in Generative AI Systems,2024-07-19 14:13:02+00:00,"['Eaman Jahani', 'Benjamin S. Manning', 'Joe Zhang', 'Hong-Yi TuYe', 'Mohammed Alsobay', 'Christos Nicolaides', 'Siddharth Suri', 'David Holtz']",cs.HC,"As generative AI systems rapidly improve, a key question emerges: How do users keep up-and what happens if they fail to do so. Drawing on theories of dynamic capabilities and IT complements, we examine prompt adaptation-the adjustments users make to their inputs in response to evolving model behavior-as a mechanism that helps determine whether technical advances translate into realized economic value. In a preregistered online experiment with 1,893 participants, who submitted over 18,000 prompts and generated more than 300,000 images, users attempted to replicate a target image in 10 tries using one of three randomly assigned models: DALL-E 2, DALL-E 3, or DALL-E 3 with automated prompt rewriting. We find that users with access to DALL-E 3 achieved higher image similarity than those with DALL-E 2-but only about half of this gain (51%) came from the model itself. The other half (49%) resulted from users adapting their prompts in response to the model's capabilities. This adaptation emerged across the skill distribution, was driven by trial-and-error, and could not be replicated by automated prompt rewriting, which erased 58% of the performance improvement associated with DALL-E 3. Our findings position prompt adaptation as a dynamic complement to generative AI-and suggest that without it, a substantial share of the economic value created when models advance may go unrealized."
http://arxiv.org/abs/2408.07923v2,When and Why is Persuasion Hard? A Computational Complexity Result,2024-08-15 04:22:46+00:00,['Zachary Wojtowicz'],cs.CY,"As generative foundation models improve, they also tend to become more persuasive, raising concerns that AI automation will enable governments, firms, and other actors to manipulate beliefs with unprecedented scale and effectiveness at virtually no cost. The full economic and social ramifications of this trend have been difficult to foresee, however, given that we currently lack a complete theoretical understanding of why persuasion is costly for human labor to produce in the first place. This paper places human and AI agents on a common conceptual footing by formalizing informational persuasion as a mathematical decision problem and characterizing its computational complexity. A novel proof establishes that persuasive messages are challenging to discover (NP-Hard) but easy to adopt if supplied by others (NP). This asymmetry helps explain why people are susceptible to persuasion, even in contexts where all relevant information is publicly available. The result also illuminates why litigation, strategic communication, and other persuasion-oriented activities have historically been so human capital intensive, and it provides a new theoretical basis for studying how AI will impact various industries."
http://arxiv.org/abs/2408.15033v3,Risk aggregation and stochastic dominance for a class of heavy-tailed distributions,2024-08-27 13:05:30+00:00,"['Yuyu Chen', 'Seva Shneer']",math.PR,"We introduce a new class of heavy-tailed distributions for which any weighted average of independent and identically distributed random variables is larger than one such random variable in (usual) stochastic order. We show that many commonly used extremely heavy-tailed (i.e., infinite-mean) distributions, such as the Pareto, Fréchet, and Burr distributions, belong to this class. The established stochastic dominance relation can be further generalized to allow negatively dependent or non-identically distributed random variables. In particular, the weighted average of non-identically distributed random variables dominates their distribution mixtures in stochastic order."
http://arxiv.org/abs/2409.11908v2,Cognitive Hierarchy in Day-to-day Network Flow Dynamics,2024-09-18 12:09:29+00:00,"['Minyu Shen', 'Feng Xiao', 'Weihua Gu', 'Hongbo Ye']",econ.GN,"When making route decisions, travelers may engage in a certain degree of reasoning about what the others will do in the upcoming day, rendering yesterday's shortest routes less attractive. This phenomenon was manifested in a recent virtual experiment that mimicked travelers' repeated daily trip-making process. Unfortunately, prevailing day-to-day traffic dynamical models failed to faithfully reproduce the collected flow evolution data therein. To this end, we propose a day-to-day traffic behavior modeling framework based on the Cognitive Hierarchy theory, in which travelers with different levels of strategic-reasoning capabilities form their own beliefs about lower-step travelers' capabilities when choosing their routes. Two widely-studied day-to-day models, the Network Tatonnement Process dynamic and the Logit dynamic, are extended into the framework and studied as examples. Calibration of the virtual experiment is performed using the extended Network Tatonnement Process dynamic, which fits the experimental data reasonably well. We show that the two extended dynamics have multiple equilibria, one of which is the classical user equilibrium. While analyzing global stability is intractable due to the presence of multiple equilibria, local stabilities near equilibria are developed analytically and verified by numerical experiments. General insights on how key parameters affect the stability of user equilibria are unveiled."
http://arxiv.org/abs/2407.15755v2,"Income, health, and spurious cointegration",2024-07-22 16:06:09+00:00,"['José A. Tapia Granados', 'Edward L. Ionides']",econ.GN,"Data for many nations show a long-run increase, over many decades, of income, indexed by GDP per capita, and population health, indexed by mortality or life expectancy at birth (LEB). However, the short-run and long-run relationships between these variables have been interpreted in different ways, and many controversies remain open. It has been claimed that population health and income are cointegrated, and that this demonstrates a positive long-run effect of income on population health. We show, however, that an empirically tested cointegration between LEB and GDP per capita is not a sound method to infer a causal link. For a given country it is easy to find computer-generated data or time series of real observations, related or unrelated to the country, that according to standard methods, are also cointegrated with the country's LEB. More generally, given a trending time series, it is easy to find other series, observational or artificial, that appear cointegrated with it. Thus, standard cointegration methodology, often used in empirical investigations, cannot distinguish whether cointegration relationships are spurious or causal."
http://arxiv.org/abs/2408.07602v3,The Dial-a-Ride Problem with Limited Pickups per Trip,2024-08-14 15:08:08+00:00,"['Boshuai Zhao', 'Kai Wang', 'Wenchao Wei', 'Roel Leus']",econ.TH,"The Dial-a-Ride Problem (DARP) is an optimization problem that involves determining optimal routes and schedules for several vehicles to pick up and deliver items at minimum cost. Motivated by real-world carpooling and crowdshipping scenarios, we introduce an additional constraint imposing a maximum number on the number of pickups per trip. This results in the Dial-a-Ride Problem with Limited Pickups per Trip (DARP-LPT). We apply a fragment-based method for DARP-LPT, where a fragment is a partial path. Specifically, we extend two formulations from Rist & Forbes (2021): the Fragment Flow Formulation (FFF) and the Fragment Assignment Formulation (FAF). We establish FFF's superiority over FAF, both from a theoretical as well as from a computational perspective. Furthermore, our results show that FFF and FAF significantly outperform traditional arc-based formulations in terms of solution quality and time. Additionally, compared to the two existing fragment sets, one with longer partial paths and another with shorter ones, our newly generated fragment sets perform better in terms of solution quality and time when fed into FFF."
http://arxiv.org/abs/2408.00885v2,A Perfect Storm: First-Nature Geography and Economic Development,2024-08-01 19:31:04+00:00,['Christian Vedel'],econ.GN,"In 1825 a storm cut a new channel through Denmark's Limfjord, providing an exogenous shock to first-nature geography. Difference-in-differences estimates show the channel increased trade immediately and, within a generation, lifted population by 26.7 percent - an elasticity of 1.6 relative to the improved market access. Higher fertility and economic growth of new industries, not migration, drove the expansion. A mirror experiment - the waterway's closure circa 1086-1208 - caused symmetric declines in medieval coin and building finds, bolstering external validity. These results offer the first robust causal evidence that first-nature geomorphology shapes the location of economic activity."
http://arxiv.org/abs/2409.15978v3,Optimal longevity of a dynasty,2024-09-24 11:20:26+00:00,"['Satoshi Nakano', 'Kazuhiko Nishimura']",econ.GN,"This paper develops a dynamic optimization framework for evaluating the optimal longevity of a dynasty under a critical-level utilitarian social welfare criterion. Unlike standard models that assume an infinite planning horizon and apply discounting to future generations, we endogenize the planning horizon itself and examine the welfare-maximizing number of generations in a roundabout production economy with capital accumulation. The model incorporates a well-being threshold that must be met for generational existence to be considered ethically justified, providing an alternative to traditional utilitarian aggregation. Using finite-horizon dynamic programming, we derive closed-form consumption trajectories and analyze how technological parameters and intertemporal preferences affect the optimal generational span. Our results show that the optimal horizon need not be infinite -- even under zero discounting -- and that intergenerational equity may require limiting dynasty length."
http://arxiv.org/abs/2407.21119v3,Potential weights and implicit causal designs in linear regression,2024-07-30 18:22:12+00:00,['Jiafeng Chen'],econ.EM,"When we interpret linear regression as estimating causal effects justified by quasi-experimental treatment variation, what do we mean? This paper characterizes the necessary implications when linear regressions are interpreted causally. A minimal requirement for causal interpretation is that the regression estimates some contrast of individual potential outcomes under the true treatment assignment process. This requirement implies linear restrictions on the true distribution of treatment. Solving these linear restrictions leads to a set of implicit designs. Implicit designs are plausible candidates for the true design if the regression were to be causal. The implicit designs serve as a framework that unifies and extends existing theoretical results across starkly distinct settings (including multiple treatment, panel, and instrumental variables). They lead to new theoretical insights for widely used but less understood specifications."
http://arxiv.org/abs/2409.08379v3,The Impact of Large Language Models on Open-source Innovation: Evidence from GitHub Copilot,2024-09-12 19:59:54+00:00,"['Doron Yeverechyahu', 'Raveesh Mayya', 'Gal Oestreicher-Singer']",cs.SE,"Large Language Models (LLMs) have been shown to enhance individual productivity in guided settings. Whereas LLMs are likely to also transform innovation processes in a collaborative work setting, it is unclear what trajectory this transformation will follow. Innovation in these contexts encompasses both capability innovation that explores new possibilities by acquiring new competencies in a project and iterative innovation that exploits existing foundations by enhancing established competencies and improving project quality. Whether LLMs affect these two aspects of collaborative work and to what extent is an open empirical question. Open-source development provides an ideal setting to examine LLM impacts on these innovation types, as its voluntary and open/collaborative nature of contributions provides the greatest opportunity for technological augmentation. We focus on open-source projects on GitHub by leveraging a natural experiment around the selective rollout of GitHub Copilot (a programming-focused LLM) in October 2021, where GitHub Copilot selectively supported programming languages like Python or Rust, but not R or Haskell. We observe a significant jump in overall contributions, suggesting that LLMs effectively augment collaborative innovation in an unguided setting. Interestingly, Copilot's launch increased iterative innovation focused on maintenance-related or feature-refining contributions significantly more than it did capability innovation through code-development or feature-introducing commits. This disparity was more pronounced after the model upgrade in June 2022 and was evident in active projects with extensive coding activity, suggesting that as both LLM capabilities and/or available contextual information improve, the gap between capability and iterative innovation may widen. We discuss practical and policy implications to incentivize high-value innovative solutions."
http://arxiv.org/abs/2408.01673v7,Strategic Analysis of Fair Rank-Minimizing Mechanisms with Agent Refusal Option,2024-08-03 06:13:07+00:00,['Yasunori Okumura'],econ.TH,"This paper investigates the strategic implications of the uniform rank-minimizing mechanism (URM), an assignment rule that selects uniformly from the set of deterministic assignments minimizing the sum of agents' reported ranks. We focus on settings in which agents may refuse their assignment and instead receive an outside option. Without the refusal option, we show that truth-telling is not strictly dominated under any fair rank-minimizing mechanism; that is, one satisfying equal treatment of equals. However, introducing the refusal option significantly changes strategic incentives: specific manipulations, called outside option demotion strategies, dominate truth-telling under the URM. Moreover, such manipulations can lead to inefficient outcomes, as desirable objects may be refused by misreporting agents and consequently remain unassigned. To address this issue, we propose a modification of the URM that restores undominated truth-telling, although it introduces incentives to underreport acceptable objects. Our results highlight a fundamental trade-off in the design of fair rank-minimizing mechanisms when agents can refuse their assignments."
http://arxiv.org/abs/2407.19618v3,Improving the Estimation of Lifetime Effects in A/B Testing via Treatment Locality,2024-07-29 00:41:11+00:00,"['Shuze Chen', 'David Simchi-Levi', 'Chonghuan Wang']",stat.ME,"Utilizing randomized experiments to evaluate the effect of short-term treatments on the short-term outcomes has been well understood and become the golden standard in industrial practice. However, as service systems become increasingly dynamical and personalized, much focus is shifting toward maximizing long-term outcomes, such as customer lifetime value, through lifetime exposure to interventions. Our goal is to assess the impact of treatment and control policies on long-term outcomes from relatively short-term observations, such as those generated by A/B testing. A key managerial observation is that many practical treatments are local, affecting only targeted states while leaving other parts of the policy unchanged. This paper rigorously investigates whether and how such locality can be exploited to improve estimation of long-term effects in Markov Decision Processes (MDPs), a fundamental model of dynamic systems. We first develop optimal inference techniques for general A/B testing in MDPs and establish corresponding efficiency bounds. We then propose methods to harness the localized structure by sharing information on the non-targeted states. Our new estimator can achieve a linear reduction with the number of test arms for a major part of the variance without sacrificing unbiasedness. It also matches a tighter variance lower bound that accounts for locality. Furthermore, we extend our framework to a broad class of differentiable estimators, which encompasses many widely used approaches in practice. We show that all such estimators can benefit from variance reduction through information sharing without increasing their bias. Together, these results provide both theoretical foundations and practical tools for conducting efficient experiments in dynamic service systems with local treatments."
http://arxiv.org/abs/2408.13630v2,DeepVoting: Learning and Fine-Tuning Voting Rules with Canonical Embeddings,2024-08-24 17:15:20+00:00,"['Leonardo Matone', 'Ben Abramowitz', 'Ben Armstrong', 'Avinash Balakrishnan', 'Nicholas Mattei']",cs.MA,"Aggregating agent preferences into a collective decision is an important step in many problems (e.g., hiring, elections, peer review) and across areas of computer science (e.g., reinforcement learning, recommender systems). As Social Choice Theory has shown, the problem of designing aggregation rules with specific sets of properties (axioms) can be difficult, or provably impossible in some cases. Instead of designing algorithms by hand, one can learn aggregation rules, particularly voting rules, from data. However, prior work in this area has required extremely large models or been limited by the choice of preference representation, i.e., embedding. We recast the problem of designing voting rules with desirable properties into one of learning probabilistic functions that output distributions over a set of candidates. Specifically, we use neural networks to learn probabilistic social choice functions. Using standard embeddings from the social choice literature we show that preference profile encoding has significant impact on the efficiency and ability of neural networks to learn rules, allowing us to learn rules faster and with smaller networks than previous work. Moreover, we show that our learned rules can be fine-tuned using axiomatic properties to create novel voting rules and make them resistant to specific types of ""attack"". Namely, we fine-tune rules to resist a probabilistic version of the No Show Paradox."
http://arxiv.org/abs/2407.00890v4,Macroeconomic Forecasting with Large Language Models,2024-07-01 01:25:26+00:00,"['Andrea Carriero', 'Davide Pettenuzzo', 'Shubhranshu Shekhar']",econ.EM,"This paper presents a comparative analysis evaluating the accuracy of Large Language Models (LLMs) against traditional macro time series forecasting approaches. In recent times, LLMs have surged in popularity for forecasting due to their ability to capture intricate patterns in data and quickly adapt across very different domains. However, their effectiveness in forecasting macroeconomic time series data compared to conventional methods remains an area of interest. To address this, we conduct a rigorous evaluation of LLMs against traditional macro forecasting methods, using as common ground the FRED-MD database. Our findings provide valuable insights into the strengths and limitations of LLMs in forecasting macroeconomic time series, shedding light on their applicability in real-world scenarios"
http://arxiv.org/abs/2407.17589v2,Diversity in Choice as Majorization,2024-07-24 18:53:54+00:00,"['Federico Echenique', 'Teddy Mekonnen', 'M. Bumin Yenmez']",econ.TH,"We propose a framework that uses majorization to model diversity and representativeness in school admissions. We generalize the standard notion of majorization to accommodate arbitrary distributional targets, such as a student body that reflects the population served by the school. Building on this framework, we introduce and axiomatically characterize the $r$-targeting Schur choice rule, which balances diversity and priority in admissions. We show that this rule is optimal: any alternative rule must either leave seats unfilled, reduce diversity, or admit lower-priority students. The rule satisfies path independence (and substitutability), which guarantees desirable outcomes in matching markets. Our work contributes to the ongoing discourse on market design by providing a new and flexible framework for improving diversity and representation."
http://arxiv.org/abs/2410.00217v3,Inference for the Marginal Value of Public Funds,2024-09-30 20:29:13+00:00,['Vedant Vohra'],econ.EM,"Economists often estimate causal effects of policies on multiple outcomes and summarize them into scalar measures of cost-effectiveness or welfare, such as the Marginal Value of Public Funds (MVPF). In many settings, microdata underlying these estimates are unavailable, leaving researchers with only published estimates and their standard errors. We develop tools for valid inference on functions of causal effects, such as the MVPF, when the correlation structure is unknown. Our approach is to construct worst-case confidence intervals, leveraging experimental designs to tighten them, and to assess robustness using breakdown analyses. We illustrate our method with MVPFs for eight policies."
http://arxiv.org/abs/2407.07988v1,Production function estimation using subjective expectations data,2024-07-10 18:40:24+00:00,"['Agnes Norris Keiller', 'Aureo de Paula', 'John Van Reenen']",econ.EM,"Standard methods for estimating production functions in the Olley and Pakes (1996) tradition require assumptions on input choices. We introduce a new method that exploits (increasingly available) data on a firm's expectations of its future output and inputs that allows us to obtain consistent production function parameter estimates while relaxing these input demand assumptions. In contrast to dynamic panel methods, our proposed estimator can be implemented on very short panels (including a single cross-section), and Monte Carlo simulations show it outperforms alternative estimators when firms' material input choices are subject to optimization error. Implementing a range of production function estimators on UK data, we find our proposed estimator yields results that are either similar to or more credible than commonly-used alternatives. These differences are larger in industries where material inputs appear harder to optimize. We show that TFP implied by our proposed estimator is more strongly associated with future jobs growth than existing methods, suggesting that failing to adequately account for input endogeneity may underestimate the degree of dynamic reallocation in the economy."
http://arxiv.org/abs/2407.08510v1,Comparative analysis of Mixed-Data Sampling (MIDAS) model compared to Lag-Llama model for inflation nowcasting,2024-07-11 13:48:43+00:00,"['Adam Bahelka', 'Harmen de Weerd']",econ.EM,"Inflation is one of the most important economic indicators closely watched by both public institutions and private agents. This study compares the performance of a traditional econometric model, Mixed Data Sampling regression, with one of the newest developments from the field of Artificial Intelligence, a foundational time series forecasting model based on a Long short-term memory neural network called Lag-Llama, in their ability to nowcast the Harmonized Index of Consumer Prices in the Euro area. Two models were compared and assessed whether the Lag-Llama can outperform the MIDAS regression, ensuring that the MIDAS regression is evaluated under the best-case scenario using a dataset spanning from 2010 to 2022. The following metrics were used to evaluate the models: Mean Absolute Error (MAE), Mean Absolute Percentage Error (MAPE), Mean Squared Error (MSE), correlation with the target, R-squared and adjusted R-squared. The results show better performance of the pre-trained Lag-Llama across all metrics."
http://arxiv.org/abs/2407.07632v1,The role of green ammonia in meeting challenges towards a sustainable development in China,2024-07-10 13:11:29+00:00,['Hanxin Zhao'],econ.GN,"This paper discusses the adoption of a green ammonia economy in meeting challenges in China's sustainable development. First, key challenges in China's energy transition, industry decarbonziation and regional sustainable development are explored. The coal-dominated energy consumption has placed great obstacles in achieving energy transition and led to massive CO2 emission since the large-scale industrialization. The high dependency on oil and gas import has threatened the energy security. A DEA model is applied for obtaining green total factor productivities of China's six administrative regions, with which, imbalanced and unsustainable regional development is identified. Second, the role of green ammonia in meeting the sustainability challenges is analysed. Ammonia is examined to be a flexible and economic option for large-scale hydrogen transport and storage. Co-firing ammonia in coal power generation at 3% rate is evaluated as an option for achieving low-carbon transition by 2030. The adoption of a green ammonia economy in China is discussed from energy, environmental and economic aspects. The practice can decline fossil energy consumption, enhance energy security, and facilitate renewable energy delivery and storage, industry decarbonization, and regional development. We assume the findings and results contribute to addressing sustainability challenges and realizing a hydrogen economy in China."
http://arxiv.org/abs/2407.07386v1,Carbon Pricing and Resale in Emission Trading Systems,2024-07-10 06:05:19+00:00,['Peyman Khezr'],econ.TH,"Secondary markets and resale are integral components of all emission trading systems. Despite the justification for these secondary trades, such as unpredictable demand, they may encourage speculation and result in the misallocation of permits. In this paper, our aim is to underscore the importance of efficiency in the initial allocation mechanism and to explore how concerns leading to the establishment of secondary markets, such as uncertain demand, can be addressed through alternative means, such as frequent auctions. We demonstrate that the existence of a secondary market could lead to higher untruthful bids in the auction, further encouraging speculation and the accumulation of rent. Our results suggest that an inefficient initial allocation could enable speculators with no use value for the permits to bid in the auction and subsequently earn rents in secondary markets by trading these permits. Even if the secondary market operates efficiently, the resulting rent, which represents a potential loss of auction revenue, cannot be overlooked."
http://arxiv.org/abs/2410.11532v1,"A Firm Link: Overall, Between- and Within-Firm Inequality Through the Lens of a Sorting Model",2024-10-15 12:07:49+00:00,"['Paweł Gola', 'Yuejun Zhao']",econ.TH,"This paper provides a new theory of the observed co-movement between overall wage inequality and its between-firm component. We develop and solve analytically a frictionless sorting model with two-sided heterogeneity, in which firms consist of distributions of tasks, choose how many workers to employ and reward their workers both through wages and amenities. We show that, for empirically-relevant parameter ranges, overall and between-firm inequality are firmly linked: A change in any of the models' primitives increases overall wage inequality if and only if it also increases the ratio of between-firm to overall inequality. Subsequently, we calibrate the model to match the Norwegian economy and find that the increase in wage inequality from 1995 to 2014 had a different primary cause (raising span-of-control cost) than the accompanying rise in welfare inequality (increased skill variance), and that the apparent decrease in wage inequality after 2015 masked a continued increase in welfare inequality."
http://arxiv.org/abs/2411.13293v2,Revealed Information,2024-11-20 13:03:24+00:00,"['Laura Doval', 'Ran Eilat', 'Tianhao Liu', 'Yangfan Zhou']",econ.TH,"An analyst observes the frequency with which a decision maker (DM) takes actions, but not the frequency conditional on payoff-relevant states. We ask when the analyst can rationalize the DM's choices as if the DM first learns something about the state before acting. We provide a support-function characterization of the triples of utility functions, prior beliefs, and (marginal) distributions over actions such that the DM's action distribution is consistent with information given the DM's prior and utility function. Assumptions on the cardinality of the state space and the utility function allow us to refine this characterization, obtaining a sharp system of finitely many inequalities the utility function, prior, and action distribution must satisfy. We apply our characterization to study comparative statics and to identify conditions under which a single information structure rationalizes choices across multiple decision problems. We characterize the set of distributions over posterior beliefs that are consistent with the DM's choices. We extend our results to settings with a continuum of actions and states assuming the first-order approach applies, and to simple multi-agent settings."
http://arxiv.org/abs/2411.03026v2,Robust Market Interventions,2024-11-05 11:49:11+00:00,"['Andrea Galeotti', 'Benjamin Golub', 'Sanjeev Goyal', 'Eduard Talamàs', 'Omer Tamuz']",econ.TH,"When can interventions in markets be designed to increase surplus robustly -- i.e., with high probability -- accounting for uncertainty due to imprecise information about economic primitives? In a setting with many strategic firms, each possessing some market power, we present conditions for such interventions to exist. The key condition, recoverable structure, requires large-scale complementarities among families of products. The analysis works by decomposing the incidence of interventions in terms of principal components of a Slutsky matrix. Under recoverable structure, a noisy signal of this matrix reveals enough about these principal components to design robust interventions. Our results demonstrate the usefulness of spectral methods for analyzing imperfectly observed strategic interactions with many agents."
http://arxiv.org/abs/2501.01447v2,Analyzing Country-Level Vaccination Rates and Determinants of Practical Capacity to Administer COVID-19 Vaccines,2024-12-30 02:48:40+00:00,"['Sharika J. Hegde', 'Max T. M. Ng', 'Marcos Rios', 'Hani S. Mahmassani', 'Ying Chen', 'Karen Smilowitz']",econ.GN,"The COVID-19 vaccine development, manufacturing, transportation, and administration proved an extreme logistics operation of global magnitude. Global vaccination levels, however, remain a key concern in preventing the emergence of new strains and minimizing the impact of the pandemic's disruption of daily life. In this paper, country-level vaccination rates are analyzed through a queuing framework to extract service rates that represent the practical capacity of a country to administer vaccines. These rates are further characterized through regression and interpretable machine learning methods with country-level demographic, governmental, and socio-economic variates. Model results show that participation in multi-governmental collaborations such as COVAX may improve the ability to vaccinate. Similarly, improved transportation and accessibility variates such as roads per area for low-income countries and rail lines per area for high-income countries can improve rates. It was also found that for low-income countries specifically, improvements in basic and health infrastructure (as measured through spending on healthcare, number of doctors and hospital beds per 100k, population percent with access to electricity, life expectancy, and vehicles per 1000 people) resulted in higher vaccination rates. Of the high-income countries, those with larger 65-plus populations struggled to vaccinate at high rates, indicating potential accessibility issues for the elderly. This study finds that improving basic and health infrastructure, focusing on accessibility in the last mile, particularly for the elderly, and fostering global partnerships can improve logistical operations of such a scale. Such structural impediments and inequities in global health care must be addressed in preparation for future global public health crises."
http://arxiv.org/abs/2412.18714v1,Using Ordinal Voting to Compare the Utilitarian Welfare of a Status Quo and A Proposed Policy: A Simple Nonparametric Analysis,2024-12-25 00:26:36+00:00,['Charles F. Manski'],econ.TH,"The relationship of policy choice by majority voting and by maximization of utilitarian welfare has long been discussed. I consider choice between a status quo and a proposed policy when persons have interpersonally comparable cardinal utilities taking values in a bounded interval, voting is compulsory, and each person votes for a policy that maximizes utility. I show that knowledge of the attained status quo welfare and the voting outcome yields an informative bound on welfare with the proposed policy. The bound contains the value of status quo welfare, so the better utilitarian policy is not known. The minimax-regret decision and certain Bayes decisions choose the proposed policy if its vote share exceeds the known value of status quo welfare. This procedure differs from majority rule, which chooses the proposed policy if its vote share exceeds 1/2."
http://arxiv.org/abs/2410.13658v2,The Subtlety of Optimal Paternalism in a Population with Bounded Rationality,2024-10-17 15:20:39+00:00,"['Charles F. Manski', 'Eytan Sheshinski']",econ.EM,"We study optimal policy when a paternalistic utilitarian planner has the power to design a discrete choice set for a heterogeneous population with bounded rationality. We show that the policy that most effectively constrains or influences choices depends in a particular multiplicative way on the preferences of the population and on the choice probabilities conditional on preferences that measure the suboptimality of behavior. We first consider the planning problem in abstraction. We then study two settings in which the planner may mandate an action or decentralize decision making. In one setting, we suppose that individuals measure utility with additive random error and maximize mismeasured rather than actual utility. Then optimal planning requires knowledge of the distribution of measurement errors. In the second setting, we consider binary treatment choice under uncertainty when the planner can mandate a treatment conditional on publicly observed personal covariates or can enable individuals to choose their own treatments conditional on private information. We focus on situations where bounded rationality takes the form of deviations between subjective personal beliefs and objective probabilities of uncertain outcomes. To illustrate, we consider clinical decision making in medicine. In toto, our analysis is cautionary. It characterizes the subtle nature of optimal policy, whose determination requires the planner to possess extensive knowledge that is rarely available. We conclude that studies of policy choice by a paternalistic utilitarian planner should view not only the population but also the planner to be boundedly rational."
http://arxiv.org/abs/2410.00902v1,A Run on Fossil Fuel? Climate Change and Transition Risk,2024-10-01 17:45:48+00:00,['Michael Barnett'],q-fin.GN,"I study the dynamic, general equilibrium implications of climate-change-linked transition risk on macroeconomic outcomes and asset prices. Climate-change-linked expectations of fossil fuel restrictions can produce a ``run on fossil fuels'' with accelerated production and decreasing spot prices, or a ``reverse run'' with restrained production and increased spot prices. The response depends on the expected economic consequences of the anticipated transition shock, and existing climate policies. Fossil fuel firm prices decrease in each case. I use a novel empirical measure of innovations in climate-related transition risk likelihood to show that dynamic empirical responses are consistent with a ``run on fossil fuel.''"
http://arxiv.org/abs/2410.07234v1,A Dynamic Approach to Stock Price Prediction: Comparing RNN and Mixture of Experts Models Across Different Volatility Profiles,2024-10-04 14:36:21+00:00,['Diego Vallarino'],q-fin.CP,"This study evaluates the effectiveness of a Mixture of Experts (MoE) model for stock price prediction by comparing it to a Recurrent Neural Network (RNN) and a linear regression model. The MoE framework combines an RNN for volatile stocks and a linear model for stable stocks, dynamically adjusting the weight of each model through a gating network. Results indicate that the MoE approach significantly improves predictive accuracy across different volatility profiles. The RNN effectively captures non-linear patterns for volatile companies but tends to overfit stable data, whereas the linear model performs well for predictable trends. The MoE model's adaptability allows it to outperform each individual model, reducing errors such as Mean Squared Error (MSE) and Mean Absolute Error (MAE). Future work should focus on enhancing the gating mechanism and validating the model with real-world datasets to optimize its practical applicability."
http://arxiv.org/abs/2410.07906v1,"Structural Change, Employment, and Inequality in Europe: an Economic Complexity Approach",2024-10-10 13:36:33+00:00,"['Bernardo Caldarola', 'Dario Mazzilli', 'Aurelio Patelli', 'Angelica Sbardella']",econ.GN,"Structural change consists of industrial diversification towards more productive, knowledge intensive activities. However, changes in the productive structure bear inherent links with job creation and income distribution. In this paper, we investigate the consequences of structural change, defined in terms of labour shifts towards more complex industries, on employment growth, wage inequality, and functional distribution of income. The analysis is conducted for European countries using data on disaggregated industrial employment shares over the period 2010-2018. First, we identify patterns of industrial specialisation by validating a country-industry industrial employment matrix using a bipartite weighted configuration model (BiWCM). Secondly, we introduce a country-level measure of labour-weighted Fitness, which can be decomposed in such a way as to isolate a component that identifies the movement of labour towards more complex industries, which we define as structural change. Thirdly, we link structural change to i) employment growth, ii) wage inequality, and iii) labour share of the economy. The results indicate that our structural change measure is associated negatively with employment growth. However, it is also associated with lower income inequality. As countries move to more complex industries, they drop the least complex ones, so the (low-paid) jobs in the least complex sectors disappear. Finally, structural change predicts a higher labour ratio of the economy; however, this is likely to be due to the increase in salaries rather than by job creation."
http://arxiv.org/abs/2410.07091v1,Collusion Detection with Graph Neural Networks,2024-10-09 17:31:41+00:00,"['Lucas Gomes', 'Jannis Kueck', 'Mara Mattes', 'Martin Spindler', 'Alexey Zaytsev']",econ.EM,"Collusion is a complex phenomenon in which companies secretly collaborate to engage in fraudulent practices. This paper presents an innovative methodology for detecting and predicting collusion patterns in different national markets using neural networks (NNs) and graph neural networks (GNNs). GNNs are particularly well suited to this task because they can exploit the inherent network structures present in collusion and many other economic problems. Our approach consists of two phases: In Phase I, we develop and train models on individual market datasets from Japan, the United States, two regions in Switzerland, Italy, and Brazil, focusing on predicting collusion in single markets. In Phase II, we extend the models' applicability through zero-shot learning, employing a transfer learning approach that can detect collusion in markets in which training data is unavailable. This phase also incorporates out-of-distribution (OOD) generalization to evaluate the models' performance on unseen datasets from other countries and regions. In our empirical study, we show that GNNs outperform NNs in detecting complex collusive patterns. This research contributes to the ongoing discourse on preventing collusion and optimizing detection methodologies, providing valuable guidance on the use of NNs and GNNs in economic applications to enhance market fairness and economic welfare."
http://arxiv.org/abs/2410.04431v1,A Structural Approach to Growth-at-Risk,2024-10-06 09:56:49+00:00,['Robert Wojciechowski'],econ.EM,"We identify the structural impulse responses of quantiles of the outcome variable to a shock. Our estimation strategy explicitly distinguishes treatment from control variables, allowing us to model responses of unconditional quantiles while using controls for identification. Disentangling the effect of adding control variables on identification versus interpretation brings our structural quantile impulse responses conceptually closer to structural mean impulse responses. Applying our methodology to study the impact of financial shocks on lower quantiles of output growth confirms that financial shocks have an outsized effect on growth-at-risk, but the magnitude of our estimates is more extreme than in previous studies."
http://arxiv.org/abs/2410.04330v1,Inference in High-Dimensional Linear Projections: Multi-Horizon Granger Causality and Network Connectedness,2024-10-06 01:38:05+00:00,"['Eugene Dettaa', 'Endong Wang']",econ.EM,"This paper presents a Wald test for multi-horizon Granger causality within a high-dimensional sparse Vector Autoregression (VAR) framework. The null hypothesis focuses on the causal coefficients of interest in a local projection (LP) at a given horizon. Nevertheless, the post-double-selection method on LP may not be applicable in this context, as a sparse VAR model does not necessarily imply a sparse LP for horizon h>1. To validate the proposed test, we develop two types of de-biased estimators for the causal coefficients of interest, both relying on first-step machine learning estimators of the VAR slope parameters. The first estimator is derived from the Least Squares method, while the second is obtained through a two-stage approach that offers potential efficiency gains. We further derive heteroskedasticity- and autocorrelation-consistent (HAC) inference for each estimator. Additionally, we propose a robust inference method for the two-stage estimator, eliminating the need to correct for serial correlation in the projection residuals. Monte Carlo simulations show that the two-stage estimator with robust inference outperforms the Least Squares method in terms of the Wald test size, particularly for longer projection horizons. We apply our methodology to analyze the interconnectedness of policy-related economic uncertainty among a large set of countries in both the short and long run. Specifically, we construct a causal network to visualize how economic uncertainty spreads across countries over time. Our empirical findings reveal, among other insights, that in the short run (1 and 3 months), the U.S. influences China, while in the long run (9 and 12 months), China influences the U.S. Identifying these connections can help anticipate a country's potential vulnerabilities and propose proactive solutions to mitigate the transmission of economic uncertainty."
http://arxiv.org/abs/2410.04160v1,From Global Value Chains to Local Jobs: Exploring FDI-induced Job Creation in EU-27,2024-10-05 13:46:32+00:00,"['Magdalena Olczyk', 'Marjan Petreski']",econ.GN,"This study explores the differential impacts of global value chain (GVC) participation on foreign direct investment (FDI)-related job creation in EU-27, emphasizing the role of sector-specific and regional factors. The study is based on a rich set of project-level data on FDI-generated jobs. It utilizes a labor demand function estimated through GMM estimator to account for endogeneity. Results indicate that forward GVC participation significantly boosts FDI-related job creation by enhancing domestic value-added and production capacity. However, this effect is moderated by sector-specific characteristics such as productivity or wages. Conversely, backward GVC participation, characterized by reliance on foreign inputs, generally reduces FDI-generated jobs due to lower domestic labor requirements and diminished competitiveness. Despite this, the negative impact of backward GVC participation on employment becomes less significant when regional diversification is considered, highlighting the importance of regional factors like infrastructure and skilled labor. The study also finds that the impact of GVC participation on employment varies with EU membership status and sectoral characteristics, with old EU member states and high-tech sectors benefiting more from forward GVC integration. In contrast, new EU member states and low-tech sectors face greater challenges, particularly with backward GVC participation."
http://arxiv.org/abs/2410.04737v1,"Urbanization, economic development, and income distribution dynamics in India",2024-10-07 04:11:19+00:00,"['Anand Sahasranaman', 'Nishanth Kumar', 'Luis M. A. Bettencourt']",physics.soc-ph,"India's urbanization is often characterized as particularly challenging and very unequal but systematic empirical analyses, comparable to other nations, have largely been lacking. Here, we characterize India's economic and human development along with changes in its personal income distribution as a function of the nation's growing urbanization. On aggregate, we find that India outperforms most other nations in the growth of various indicators of development with urbanization, including income and human development. These results are due in part to India's present low levels of urbanization but also demonstrate the transformational role of its cities in driving multi-dimensional development. To test these changes at the more local level, we study the income distributions of large Indian cities to find evidence for high positive growth in the lowest decile (poorest) of the population, enabling sharp reductions in poverty over time. We also test the hypothesis that inequality-reducing cities are more attractive destinations for rural migrants. Finally, we use income distributions to characterize changes in poverty rates directly. This shows much lower levels of poverty in urban India and especially in its largest cities. The dynamics of poverty rates during the recent COVID-19 pandemic shows both a high fragility of these improvements during a crisis and their resilience over longer times. Sustaining a long-term dynamic where urbanization continues to be closely associated with human development and poverty reduction is likely India's fastest path to a more prosperous and equitable future."
http://arxiv.org/abs/2410.01114v1,"AI Persuasion, Bayesian Attribution, and Career Concerns of Doctors",2024-10-01 22:50:20+00:00,"['Hanzhe Li', 'Jin Li', 'Ye Luo', 'Xiaowei Zhang']",econ.GN,"This paper examines how AI persuades doctors when their diagnoses differ. Disagreements arise from two sources: attention differences, which are objective and play a complementary role to the doctor, and comprehension differences, which are subjective and act as substitutes. AI's interpretability influences how doctors attribute these sources and their willingness to change their minds. Surprisingly, uninterpretable AI can be more persuasive by allowing doctors to partially attribute disagreements to attention differences. This effect is stronger when doctors have low abnormality detection skills. Additionally, uninterpretable AI can improve diagnostic accuracy when doctors have career concerns."
http://arxiv.org/abs/2410.05630v1,Navigating Inflation in Ghana: How Can Machine Learning Enhance Economic Stability and Growth Strategies,2024-10-08 02:26:50+00:00,"['Theophilus G. Baidoo', 'Ashley Obeng']",econ.EM,"Inflation remains a persistent challenge for many African countries. This research investigates the critical role of machine learning (ML) in understanding and managing inflation in Ghana, emphasizing its significance for the country's economic stability and growth. Utilizing a comprehensive dataset spanning from 2010 to 2022, the study aims to employ advanced ML models, particularly those adept in time series forecasting, to predict future inflation trends. The methodology is designed to provide accurate and reliable inflation forecasts, offering valuable insights for policymakers and advocating for a shift towards data-driven approaches in economic decision-making. This study aims to significantly advance the academic field of economic analysis by applying machine learning (ML) and offering practical guidance for integrating advanced technological tools into economic governance, ultimately demonstrating ML's potential to enhance Ghana's economic resilience and support sustainable development through effective inflation management."
http://arxiv.org/abs/2410.05535v1,Design Information Disclosure under Bidder Heterogeneity in Online Advertising Auctions: Implications of Bid-Adherence Behavior,2024-10-07 22:23:55+00:00,"['Zhu Mingxi', 'Song Michelle']",econ.GN,"Bidding is a key element of search advertising, but the variation in bidders' valuations and strategies is often overlooked. Disclosing bid information helps uncover this heterogeneity and enables platforms to tailor their disclosure policies to meet objectives like increasing consumer surplus or platform revenue. We analyzed data from a platform that provided bid recommendations based on historical bids. Our findings reveal that advertisers vary significantly in their strategies: some follow the platform's recommendations, while others create their own bids, deviating from the provided information. This highlights the need for customized information disclosure policies in online ad marketplaces. We developed an equilibrium model for Generalized Second Price (GSP) auctions, showing that adhering to bid recommendations with positive probability is suboptimal. We categorized advertisers as bid-adhering or bid-constructing and developed a structural model for self-bidding to identify private valuations. This model allowed for a counterfactual analysis of the impact of different levels of information disclosure. Both theoretical and empirical results suggest that moderate increases in disclosure improve platform revenue and market efficiency. Understanding bidder diversity is crucial for platforms, which can design more effective disclosure policies to address varying bidder needs and achieve their goals through costless information sharing."
http://arxiv.org/abs/2410.11398v1,Capturing Perception to Poverty using Conjoint Analysis & Partial Profile Choice Experiment,2024-10-15 08:40:04+00:00,"['Anushka De', 'Diganta Mukherjee']",stat.AP,"The objective of this study is applying a utility based analysis to a comparatively efficient design experiment which can capture people's perception towards the various components of a commodity. Here we studied the multi-dimensional poverty index and the relative importance of its components and their two-factor interaction effects. We also discussed how to model a choice based conjoint data for determining the utility of the components and their interactions. Empirical results from survey data shows the nature of coefficients, in terms of utility derived by the individuals, their statistical significance and validity in the present framework. There has been some discrepancies in the results between the bootstrap model and the original model, which can be understood by surveying more people, and ensuring comparative homogeneity in the data."
http://arxiv.org/abs/2410.10749v1,"Testing the order of fractional integration in the presence of smooth trends, with an application to UK Great Ratios",2024-10-14 17:23:59+00:00,"['Mustafa R. Kılınç', 'Michael Massmann', 'Maximilian Ambros']",econ.EM,"This note proposes semi-parametric tests for investigating whether a stochastic process is fractionally integrated of order $δ$, where $|δ| < 1/2$, when smooth trends are present in the model. We combine the semi-parametric approach by Iacone, Nielsen & Taylor (2022) to model the short range dependence with the use of Chebyshev polynomials by Cuestas & Gil-Alana to describe smooth trends. Our proposed statistics have standard limiting null distributions and match the asymptotic local power of infeasible tests based on unobserved errors. We also establish the conditions under which an information criterion can consistently estimate the order of the Chebyshev polynomial. The finite sample performance is evaluated using simulations, and an empirical application is given for the UK Great Ratios."
http://arxiv.org/abs/2410.12566v1,The Pond Dilemma with Heterogeneous Relative Concerns,2024-10-16 13:40:18+00:00,['Paweł Gola'],econ.TH,"This paper explores team formation when workers differ in skills and their desire to out-earn co-workers. I cast this question as a two-dimensional assignment problem with imperfectly transferable utility and show that equilibrium sorting optimally trades off output maximisation with the need to match high-skill workers to co-workers with weak relative concerns. This can lead to positive (negative) assortative matching in skill even with submodular (supermodular) production functions. Under supermodular production, this heterogeneity in preferences benefits all workers and reduces wage inequality. With submodular production, the distributional consequences are ambiguous, and some workers become worse off. The model reveals that skill-biased technological change (SBTC) incentivises domestic outsourcing, as firms seek to avoid detrimental social comparisons between high- and low-skill workers, thus providing a compelling explanation for the long-term increase in outsourcing. Finally, the benefits of SBTC can trickle down to low-skill workers-but only those whose relative concerns are weak."
http://arxiv.org/abs/2410.12356v1,Designing Scientific Grants,2024-10-16 08:17:07+00:00,"['Christoph Carnehl', 'Marco Ottaviani', 'Justus Preusser']",econ.GN,"This paper overviews the economics of scientific grants, focusing on the interplay between the inherent uncertainty in research, researchers' incentives, and grant design. Grants differ from traditional market systems and other science and innovation policy tools, such as prizes and patents. We outline the main economic forces specific to science, noting the limited attention given to grant funding in the economics literature. Using tools from information economics, we identify key incentive problems at various stages of the grant funding process and offer guidance for effective grant design.
  In the allocation stage, funders aim to select the highest-merit applications while minimizing evaluation costs. The selection rule, in turn, impacts researchers' incentives to apply and invest in their proposals. In the grant management stage, funders monitor researchers to ensure efficient use of funds. We discuss the advantages and potential pitfalls of (partial) lotteries and emphasize the effectiveness of staged grant design in promoting a productive use of grants.
  Beyond these broadly applicable insights, our overview highlights the need for further research on grantmaking. Understudied areas include, at the micro level, the interplay of different grant funding stages, and at the macro level, the interaction of grants with other instruments in the market for science."
http://arxiv.org/abs/2410.14585v1,A GARCH model with two volatility components and two driving factors,2024-10-18 16:36:07+00:00,"['Luca Vincenzo Ballestra', ""Enzo D'Innocenzo"", 'Christian Tezza']",econ.EM,"We introduce a novel GARCH model that integrates two sources of uncertainty to better capture the rich, multi-component dynamics often observed in the volatility of financial assets. This model provides a quasi closed-form representation of the characteristic function for future log-returns, from which semi-analytical formulas for option pricing can be derived. A theoretical analysis is conducted to establish sufficient conditions for strict stationarity and geometric ergodicity, while also obtaining the continuous-time diffusion limit of the model. Empirical evaluations, conducted both in-sample and out-of-sample using S\&P500 time series data, show that our model outperforms widely used single-factor models in predicting returns and option prices."
http://arxiv.org/abs/2410.14513v1,GARCH option valuation with long-run and short-run volatility components: A novel framework ensuring positive variance,2024-10-18 14:47:48+00:00,"['Luca Vincenzo Ballestra', ""Enzo D'Innocenzo"", 'Christian Tezza']",econ.EM,"Christoffersen, Jacobs, Ornthanalai, and Wang (2008) (CJOW) proposed an improved Generalized Autoregressive Conditional Heteroskedasticity (GARCH) model for valuing European options, where the return volatility is comprised of two distinct components. Empirical studies indicate that the model developed by CJOW outperforms widely-used single-component GARCH models and provides a superior fit to options data than models that combine conditional heteroskedasticity with Poisson-normal jumps. However, a significant limitation of this model is that it allows the variance process to become negative. Oh and Park [2023] partially addressed this issue by developing a related model, yet the positivity of the volatility components is not guaranteed, both theoretically and empirically. In this paper we introduce a new GARCH model that improves upon the models by CJOW and Oh and Park [2023], ensuring the positivity of the return volatility. In comparison to the two earlier GARCH approaches, our novel methodology shows comparable in-sample performance on returns data and superior performance on S&P500 options data."
http://arxiv.org/abs/2410.16526v1,A Dynamic Spatiotemporal and Network ARCH Model with Common Factors,2024-10-21 21:35:32+00:00,"['Osman Doğan', 'Raffaele Mattera', 'Philipp Otto', 'Süleyman Taşpınar']",stat.ME,"We introduce a dynamic spatiotemporal volatility model that extends traditional approaches by incorporating spatial, temporal, and spatiotemporal spillover effects, along with volatility-specific observed and latent factors. The model offers a more general network interpretation, making it applicable for studying various types of network spillovers. The primary innovation lies in incorporating volatility-specific latent factors into the dynamic spatiotemporal volatility model. Using Bayesian estimation via the Markov Chain Monte Carlo (MCMC) method, the model offers a robust framework for analyzing the spatial, temporal, and spatiotemporal effects of a log-squared outcome variable on its volatility. We recommend using the deviance information criterion (DIC) and a regularized Bayesian MCMC method to select the number of relevant factors in the model. The model's flexibility is demonstrated through two applications: a spatiotemporal model applied to the U.S. housing market and another applied to financial stock market networks, both highlighting the model's ability to capture varying degrees of interconnectedness. In both applications, we find strong spatial/network interactions with relatively stronger spillover effects in the stock market."
http://arxiv.org/abs/2410.17282v1,Drivers of Electric Vehicle Adoption in Nigeria: An Extended UTAUT Framework Approach,2024-10-14 18:18:29+00:00,"['Qasim Ajao', 'Lanre Sadeeq', 'Oluwatobi Oluwaponmile Sodiq']",physics.soc-ph,"Electric vehicles (EVs) represent a significant advancement in automotive technology, utilizing electricity as a power source instead of traditional fossil fuels, while incorporating sophisticated navigation and autopilot systems. These vehicles align with multiple Sustainable Development Goals (SDGs) by offering a more environmentally sustainable alternative to internal combustion engine vehicles (ICEVs). Despite their potential, the adoption of EVs in developing nations such as Nigeria remains constrained. This study expands the Unified Theory of Acceptance and Use of Technology (UTAUT) framework by incorporating key enablers, including poor infrastructure, affordability issues, and government support, within the broader category of facilitating conditions. Additionally, it examines factors such as trust, performance expectations, social influences, and network externalities to identify the primary determinants influencing Nigerian consumers' propensity to adopt EVs. Results show that the percentage increase of H6 (facilitating conditions - behavioral intentions) compared to H5 (network externalities - behavioral intentions) is approximately 32.35%, indicating that traditional drivers significantly influence individuals' willingness to purchase EVs and are particularly strong factors in adoption decisions. The paper concludes with a discussion of these findings and proposes strategies for future research to further explore the barriers and drivers of EV adoption in Nigeria."
http://arxiv.org/abs/2411.07604v1,Dynamic Evolutionary Game Analysis of How Fintech in Banking Mitigates Risks in Agricultural Supply Chain Finance,2024-11-12 07:25:27+00:00,"['Qiang Wan', 'Jun Cui']",econ.EM,"This paper explores the impact of banking fintech on reducing financial risks in the agricultural supply chain, focusing on the secondary allocation of commercial credit. The study constructs a three-player evolutionary game model involving banks, core enterprises, and SMEs to analyze how fintech innovations, such as big data credit assessment, blockchain, and AI-driven risk evaluation, influence financial risks and access to credit. The findings reveal that banking fintech reduces financing costs and mitigates financial risks by improving transaction reliability, enhancing risk identification, and minimizing information asymmetry. By optimizing cooperation between banks, core enterprises, and SMEs, fintech solutions enhance the stability of the agricultural supply chain, contributing to rural revitalization goals and sustainable agricultural development. The study provides new theoretical insights and practical recommendations for improving agricultural finance systems and reducing financial risks.
  Keywords: banking fintech, agricultural supply chain, financial risk, commercial credit, SMEs, evolutionary game model, big data, blockchain, AI-driven risk evaluation."
http://arxiv.org/abs/2411.01067v2,Randomized Controlled Trials for Security Copilot for IT Administrators,2024-11-01 22:41:24+00:00,"['James Bono', 'Alec Xu']",econ.GN,"As generative AI (GAI) tools become increasingly integrated into workplace environments, it is essential to measure their impact on productivity across specific domains. This study evaluates the effects of Microsoft's Security Copilot (""Copilot"") on information technology administrators (""IT admins"") through randomized controlled trials. Participants were divided into treatment and control groups, with the former granted access to Copilot within Microsoft's Entra and Intune admin centers. Across three IT admin scenarios - sign-in troubleshooting, device policy management, and device troubleshooting - Copilot users demonstrated substantial improvements in both accuracy and speed. Across all scenarios and tasks, Copilot subjects experienced a 34.53% improvement in overall accuracy and a 29.79% reduction in task completion time. We also find that the productivity benefits vary by task type, with more complex tasks showing greater improvement. In free response tasks, Copilot users identified 146.07% more relevant facts and reduced task completion time by 61.14%. Subject satisfaction with Copilot was high, with participants reporting reduced effort and a strong preference for using the tool in future tasks. These findings suggest that GAI tools like Copilot can significantly enhance the productivity and efficiency of IT admins, especially in scenarios requiring information synthesis and complex decision-making."
http://arxiv.org/abs/2411.04262v1,Sequential optimal contracting in continuous time,2024-11-06 21:14:49+00:00,"['Guillermo Alonso Alvarez', 'Erhan Bayraktar', 'Ibrahim Ekren', 'Liwei Huang']",math.OC,"In this paper we study a principal-agent problem in continuous time with multiple lump-sum payments (contracts) paid at different deterministic times. We reduce the non-zero sum Stackelberg game between the principal and agent to a standard stochastic optimal control problem. We apply our result to a benchmark model for which we investigate how different inputs (payment frequencies, payments' distribution, discounting factors, agent's reservation utility) affect the principal's value and agent's optimal compensations."
http://arxiv.org/abs/2411.06110v1,Can ESG Investment and the Implementation of the New Environmental Protection Law Enhance Public Subjective Well-being?,2024-11-09 08:16:24+00:00,['Hambur Wang'],econ.GN,"Air pollution has emerged as a serious challenge for China, posing a threat to public health and hindering the progress of sustainable economic development. In response to air pollution and other environmental issues, the Chinese government introduced a new Environmental Protection Law in 2015. This paper investigates the impact of the new Environmental Protection Law's implementation and corporate Environmental, Social, and Governance (ESG) investments on air pollution and public subjective well-being. Using panel data at the macro level, we employ a difference-in-differences (DID) model, with Chinese provinces and municipalities as units of analysis, to examine the combined effects of the new Environmental Protection Law and changes in corporate ESG investment intensity. The study evaluates their impacts on air quality and public subjective well-being. Findings indicate that these policies and investment behaviors significantly improve public subjective well-being by reducing air pollution. Notably, an increase in ESG investment significantly reduces air pollution levels and is positively associated with enhanced well-being. These results underscore the critical role of environmental legislation and corporate social responsibility in improving public quality of life and provide empirical support for promoting sustainable development in China and beyond."
http://arxiv.org/abs/2411.06095v1,Green antitrust conundrum: Collusion with social goals,2024-11-09 07:16:04+00:00,"['Nigar Hashimzade', 'Limor Hatsor', 'Artyom Jelnov']",econ.TH,"Recent antitrust regulations in several countries have granted exemptions for collusion aimed at achieving environmental goals. Firms can apply for exemptions if collusion helps to develop or to implement costly clean technology, particularly in sectors like renewable energy, where capital costs are high and economies of scale are significant. However, if the cost of the green transition is unknown to the competition regulator, firms might exploit the exemption by fixing prices higher than necessary. The regulator faces the decision of whether to permit collusion and whether to commission an investigation of potential price fixing, which incurs costs. We fully characterise the equilibria in this scenario that depend on the regulator's belief about the high cost of green transition. If the belief is high enough, collusion will be allowed. We also identify conditions under which a regulator's commitment to always investigate price fixing is preferable to making discretionary decisions."
http://arxiv.org/abs/2411.04380v1,"Identification of Long-Term Treatment Effects via Temporal Links, Observational, and Experimental Data",2024-11-07 02:47:13+00:00,['Filip Obradović'],econ.EM,"Recent literature proposes combining short-term experimental and long-term observational data to provide credible alternatives to conventional observational studies for identification of long-term average treatment effects (LTEs). I show that experimental data have an auxiliary role in this context. They bring no identifying power without additional modeling assumptions. When modeling assumptions are imposed, experimental data serve to amplify their identifying power. If the assumptions fail, adding experimental data may only yield results that are farther from the truth. Motivated by this, I introduce two assumptions on treatment response that may be defensible based on economic theory or intuition. To utilize them, I develop a novel two-step identification approach that centers on bounding temporal link functions -- the relationship between short-term and mean long-term potential outcomes. The approach provides sharp bounds on LTEs for a general class of assumptions, and allows for imperfect experimental compliance -- extending existing results."
http://arxiv.org/abs/2410.21516v2,Forecasting Political Stability in GCC Countries,2024-10-28 20:36:26+00:00,['Mahdi Goldani'],econ.EM,"Political stability is crucial for the socioeconomic development of nations, particularly in geopolitically sensitive regions such as the Gulf Cooperation Council Countries, Saudi Arabia, UAE, Kuwait, Qatar, Oman, and Bahrain. This study focuses on predicting the political stability index for these six countries using machine learning techniques. The study uses data from the World Banks comprehensive dataset, comprising 266 indicators covering economic, political, social, and environmental factors. Employing the Edit Distance on Real Sequence method for feature selection and XGBoost for model training, the study forecasts political stability trends for the next five years. The model achieves high accuracy, with mean absolute percentage error values under 10, indicating reliable predictions. The forecasts suggest that Oman, the UAE, and Qatar will experience relatively stable political conditions, while Saudi Arabia and Bahrain may continue to face negative political stability indices. The findings underscore the significance of economic factors such as GDP and foreign investment, along with variables related to military expenditure and international tourism, as key predictors of political stability. These results provide valuable insights for policymakers, enabling proactive measures to enhance governance and mitigate potential risks."
http://arxiv.org/abs/2410.20229v1,Modelling of Economic Implications of Bias in AI-Powered Health Emergency Response Systems,2024-10-26 17:11:23+00:00,['Katsiaryna Bahamazava'],econ.GN,"We present a theoretical framework assessing the economic implications of bias in AI-powered emergency response systems. Integrating health economics, welfare economics, and artificial intelligence, we analyze how algorithmic bias affects resource allocation, health outcomes, and social welfare. By incorporating a bias function into health production and social welfare models, we quantify its impact on demographic groups, showing that bias leads to suboptimal resource distribution, increased costs, and welfare losses. The framework highlights efficiency-equity trade-offs and provides economic interpretations. We propose mitigation strategies, including fairness-constrained optimization, algorithmic adjustments, and policy interventions. Our findings offer insights for policymakers, emergency service providers, and technology developers, emphasizing the need for AI systems that are efficient and equitable. By addressing the economic consequences of biased AI, this study contributes to policies and technologies promoting fairness, efficiency, and social welfare in emergency response services."
http://arxiv.org/abs/2410.21019v1,Economic Integration of Africa in the 21st Century: Complex Network and Panel Regression Analysis,2024-10-28 13:41:06+00:00,"['Tekilu Tadesse Choramo', 'Jemal Abafita', 'Yerali Gandica', 'Luis E C Rocha']",econ.GN,"Global and regional integration has grown significantly in recent decades, boosting intra-African trade and positively impacting national economies through trade diversification and sustainable development. However, existing measures of economic integration often fail to capture the complex interactions among trading partners. This study addresses this gap by using complex network analysis and dynamic panel regression techniques to identify factors driving economic integration in Africa, based on data from 2002 to 2019. The results show that economic development, institutional quality, regional trade agreements, human capital, FDI, and infrastructure positively influence a country's position in the African trade network. Conversely, trade costs, the global financial crisis, and regional overlapping memberships negatively affect network based integration. Our findings suggest that enhancing a country's connectivity in the African trade network involves identifying key economic and institutional factors of trade partners and strategically focusing on continent-wide agreements rather than just regional ones to boost economic growth."
http://arxiv.org/abs/2411.05758v1,On the limiting variance of matching estimators,2024-11-08 18:19:29+00:00,"['Songliang Chen', 'Fang Han']",math.ST,"This paper examines the limiting variance of nearest neighbor matching estimators for average treatment effects with a fixed number of matches. We present, for the first time, a closed-form expression for this limit. Here the key is the establishment of the limiting second moment of the catchment area's volume, which resolves a question of Abadie and Imbens. At the core of our approach is a new universality theorem on the measures of high-order Voronoi cells, extending a result by Devroye, Györfi, Lugosi, and Walk."
http://arxiv.org/abs/2411.00520v1,Calibrated quantile prediction for Growth-at-Risk,2024-11-01 11:38:37+00:00,"['Pietro Bogani', 'Matteo Fontana', 'Luca Neri', 'Simone Vantini']",stat.ME,"Accurate computation of robust estimates for extremal quantiles of empirical distributions is an essential task for a wide range of applicative fields, including economic policymaking and the financial industry. Such estimates are particularly critical in calculating risk measures, such as Growth-at-Risk (GaR). % and Value-at-Risk (VaR). This work proposes a conformal framework to estimate calibrated quantiles, and presents an extensive simulation study and a real-world analysis of GaR to examine its benefits with respect to the state of the art. Our findings show that CP methods consistently improve the calibration and robustness of quantile estimates at all levels. The calibration gains are appreciated especially at extremal quantiles, which are critical for risk assessment and where traditional methods tend to fall short. In addition, we introduce a novel property that guarantees coverage under the exchangeability assumption, providing a valuable tool for managing risks by quantifying and controlling the likelihood of future extreme observations."
http://arxiv.org/abs/2412.05516v1,Measuring Consumer Sensitivity to Audio Advertising: A Long-Run Field Experiment on Pandora Internet Radio,2024-12-07 03:12:39+00:00,"['Ali Goli', 'Jason Huang', 'David Reiley', 'Nickolai M. Riabov']",econ.GN,"A randomized experiment with almost 35 million Pandora listeners enables us to measure the sensitivity of consumers to advertising, an important topic of study in the era of ad-supported digital content provision. The experiment randomized listeners into nine treatment groups, each of which received a different level of audio advertising interrupting their music listening, with the highest treatment group receiving more than twice as many ads as the lowest treatment group. By maintaining consistent treatment assignment for 21 months, we measure long-run demand effects and find ad-load sensitivity three times greater than what we would have obtained from a month-long experiment. We show the negative impact on the number of hours listened, days listened, and probability of listening at all in the final month. Using an experimental design that separately varies the number of commercial interruptions per hour and the number of ads per commercial interruption, we find that listeners primarily respond to the total number of ads per hour, with a slight preference for more frequent but shorter ad breaks. Lastly, we find that increased ad load led to an increase in the number of paid ad-free subscriptions to Pandora. Importantly, we show that observational methods often lead to biased or even directionally incorrect estimates of these effects, highlighting the value of experimental data."
http://arxiv.org/abs/2412.05621v1,Minimum Sliced Distance Estimation in a Class of Nonregular Econometric Models,2024-12-07 11:29:38+00:00,"['Yanqin Fan', 'Hyeonseok Park']",econ.EM,"This paper proposes minimum sliced distance estimation in structural econometric models with possibly parameter-dependent supports. In contrast to likelihood-based estimation, we show that under mild regularity conditions, the minimum sliced distance estimator is asymptotically normally distributed leading to simple inference regardless of the presence/absence of parameter dependent supports. We illustrate the performance of our estimator on an auction model."
http://arxiv.org/abs/2412.07459v1,Moving to the suburbs? Exploring the potential impact of work-from-home on suburbanization in Poland,2024-12-10 12:27:57+00:00,"['Beata Woźniak-Jęchorek', 'Sławomir Kuźmar', 'David Bole']",econ.GN,"The main goal of this paper is to assess the likelihood of office workers relocating to the suburbs due to work-from-home opportunities and the key factors influencing these preferences. Our study focuses on Poland, a country with different cultural individualism at work, which can impact work-from-home preferences and, indirectly, home relocation desires. Given the methodological limitations of official data on remote work practices in Poland, we conducted an original survey, gathering primary data from a nationally representative sample of office workers living in cities with populations exceeding 100,000. To investigate the factors shaping employees' preferences for suburban relocation driven by remote work, we utilized logistic regression to analyze the effects of socio-economic and employment characteristics, commuting experiences, and reported changes in work productivity. Our findings reveal that age, mode of commuting, perceived changes in work productivity, and sector ownership are significant determinants, strongly affecting home relocation preferences in response to work-from-home opportunities. These results align with previous research, typically based on data from countries with different cultural frameworks and more developed work-from-home practices."
http://arxiv.org/abs/2412.05911v1,Unveiling True Talent: The Soccer Factor Model for Skill Evaluation,2024-12-08 12:09:05+00:00,"['Alexandre Andorra', 'Maximilian Göbel']",stat.AP,"Evaluating a soccer player's performance can be challenging due to the high costs and small margins involved in recruitment decisions. Raw observational statistics further complicate an accurate individual skill assessment as they do not abstract from the potentially confounding factor of team strength. We introduce the Soccer Factor Model (SFM), which corrects this bias by isolating a player's true skill from the team's influence. We compile a novel data set, web-scraped from publicly available data sources. Our empirical application draws on information of 144 players, playing a total of over 33,000 matches, in seasons 2000/01 through 2023/24. Not only does the SFM allow for a structural interpretation of a player's skill, but also stands out against more reduced-form benchmarks in terms of forecast accuracy. Moreover, we propose Skill- and Performance Above Replacement as metrics for fair cross-player comparisons. These, for example, allow us to settle the discussion about the GOAT of soccer in the first quarter of the twenty-first century."
http://arxiv.org/abs/2412.07688v1,A Joint Energy and Differentially-Private Smart Meter Data Market,2024-12-10 17:25:14+00:00,"['Saurab Chhachhi', 'Fei Teng']",eess.SY,"Given the vital role that smart meter data could play in handling uncertainty in energy markets, data markets have been proposed as a means to enable increased data access. However, most extant literature considers energy markets and data markets separately, which ignores the interdependence between them. In addition, existing data market frameworks rely on a trusted entity to clear the market. This paper proposes a joint energy and data market focusing on the day-ahead retailer energy procurement problem with uncertain demand. The retailer can purchase differentially-private smart meter data from consumers to reduce uncertainty. The problem is modelled as an integrated forecasting and optimisation problem providing a means of valuing data directly rather than valuing forecasts or forecast accuracy. Value is determined by the Wasserstein distance, enabling privacy to be preserved during the valuation and procurement process. The value of joint energy and data clearing is highlighted through numerical case studies using both synthetic and real smart meter data."
http://arxiv.org/abs/2412.07649v1,Machine Learning the Macroeconomic Effects of Financial Shocks,2024-12-10 16:34:56+00:00,"['Niko Hauzenberger', 'Florian Huber', 'Karin Klieber', 'Massimiliano Marcellino']",econ.EM,"We propose a method to learn the nonlinear impulse responses to structural shocks using neural networks, and apply it to uncover the effects of US financial shocks. The results reveal substantial asymmetries with respect to the sign of the shock. Adverse financial shocks have powerful effects on the US economy, while benign shocks trigger much smaller reactions. Instead, with respect to the size of the shocks, we find no discernible asymmetries."
http://arxiv.org/abs/2412.07946v1,The Economics of Equilibrium with Indivisible Goods,2024-12-10 22:16:21+00:00,"['Ravi Jagadeesan', 'Alexander Teytelboym']",econ.TH,"This paper develops a theory of competitive equilibrium with indivisible goods based entirely on economic conditions on demand. The key idea is to analyze complementarity and substitutability between bundles of goods, rather than merely between goods themselves. This approach allows us to formulate sufficient, and essentially necessary, conditions for equilibrium existence, which unify settings with complements and settings with substitutes. Our analysis has implications for auction design."
http://arxiv.org/abs/2412.01030v1,Iterative Distributed Multinomial Regression,2024-12-02 01:25:39+00:00,"['Yanqin Fan', 'Yigit Okar', 'Xuetao Shi']",econ.EM,"This article introduces an iterative distributed computing estimator for the multinomial logistic regression model with large choice sets. Compared to the maximum likelihood estimator, the proposed iterative distributed estimator achieves significantly faster computation and, when initialized with a consistent estimator, attains asymptotic efficiency under a weak dominance condition. Additionally, we propose a parametric bootstrap inference procedure based on the iterative distributed estimator and establish its consistency. Extensive simulation studies validate the effectiveness of the proposed methods and highlight the computational efficiency of the iterative distributed estimator."
http://arxiv.org/abs/2412.03033v1,Unveiling Saving and Credit Dynamics: Insights from Financial Diaries and Surveys among Low-Income Households in Unauthorized Colonies in Delhi,2024-12-04 05:07:48+00:00,['Divya Sharma'],econ.GN,"The paper presents findings from a comprehensive study examining the saving and credit behaviors of low-income households residing in unauthorized colonies within a metropolitan area. Utilizing a dual approach, the study engaged in prolonged fieldwork, including repeated fortnightly interviews with selected households and a one-time primary survey with a larger sample size. The research meticulously analyzed the financial lives of these households, focusing on their saving and credit behaviors and assessing the accessibility and intensity of usage of financial instruments available to them. Through suitable regression models, the study identified key factors influencing the usage of financial instruments among low-income households. Transaction costs, convenience, and financial knowledge emerged as significant determinants impacting both usage decisions and the intensity of usage. The research underscores the importance of addressing demand side factors to ensure widespread financial services usage among low-income groups. Efforts to reduce time costs, enhance product accessibility and liquidity, and augment financial literacy are essential for fostering financial inclusion in unauthorized colonies. The findings highlight the imperative of moving beyond mere financial access towards promoting universal usage to realize the full benefits of financial inclusion."
http://arxiv.org/abs/2412.03755v1,Economic Geography and Structural Change,2024-12-04 22:48:29+00:00,"['Clement E. Bohr', 'Marti Mestieri', 'Frederic Robert-Nicoud']",econ.GN,"As countries develop, the relative importance of agriculture declines and economic activity becomes spatially concentrated. We develop a model integrating structural change and regional disparities to jointly capture these phenomena. A key modeling innovation ensuring analytical tractability is the introduction of non-homothetic Cobb-Douglas preferences, which are characterized by constant unitary elasticity of substitution and non-constant income elasticity. As labor productivity increases over time, economic well-being rises, leading to a declining expenditure share on agricultural goods. Labor reallocates away from agriculture, and industry concentrates spatially, further increasing aggregate productivity: structural change and regional disparities are two mutually reinforcing outcomes and propagators of the growth process."
http://arxiv.org/abs/2411.00981v1,Design and Analysis of Intellectual Property Protection Strategies Based on Differential Equations,2024-11-01 19:13:52+00:00,['Hambur Wang'],econ.GN,"This paper constructs a novel intellectual property (IP) protection strategy using differential equation theory, aiming to analyze and optimize the effectiveness of IP protection. By developing a mathematical model, it explores the dynamic impact of IP protection intensity on both innovative enterprises and infringement activities. The study finds that a well-designed IP protection strategy can effectively reduce infringement while promoting technological innovation. The paper also discusses the effects of strategies under varying parameter conditions and verifies the model's rationality and effectiveness through numerical simulation. The findings provide theoretical support and references for formulating IP protection policies."
http://arxiv.org/abs/2411.09149v1,A Strategic Topology on Information Structures,2024-11-14 02:59:49+00:00,"['Dirk Bergemann', 'Stephen Morris', 'Rafael Veiel']",econ.TH,"Two information structures are said to be close if, with high probability, there is approximate common knowledge that interim beliefs are close under the two information structures. We define an ""almost common knowledge topology"" reflecting this notion of closeness. We show that it is the coarsest topology generating continuity of equilibrium outcomes. An information structure is said to be simple if each player has a finite set of types and each type has a distinct first-order belief about payoff states. We show that simple information structures are dense in the almost common knowledge topology and thus it is without loss to restrict attention to simple information structures in information design problems."
http://arxiv.org/abs/2411.08452v1,Complementing Carbon Credits from Forest-Related Activities with Biodiversity Insurance and Resilience Value,2024-11-13 09:16:02+00:00,['Hanna Fiegenbaum'],econ.GN,"Carbon credits are a key component of most national and organizational climate strategies. Financing and delivering carbon credits from forest-related activities faces multiple risks at the project and asset levels. Financial mechanisms are employed to mitigate risks for investors and project developers, complemented by non-financial measures such as environmental and social safeguards and physical risk mitigation. Despite these efforts, academic research highlights that safeguards and climate risk mitigation measures are not efficiently implemented in some carbon projects and that specification of environmental safeguards remains underdeveloped. Further, environmental and social risk mitigation capacities may not be integrated into financial mechanisms. This text examines how ecosystem capacities can be leveraged and valued for mitigation of and adaptation to physical risks by complementing carbon credits with biodiversity insurance and resilience value."
http://arxiv.org/abs/2411.08419v1,Orchestrating Organizational Politics: Baron and Ferejohn Meet Tullock,2024-11-13 08:14:15+00:00,"['Qiang Fu', 'Zenan Wu', 'Yuxuan Zhu']",econ.TH,"This paper examines the optimal organizational rules that govern the process of dividing a fixed surplus. The process is modeled as a sequential multilateral bargaining game with costly recognition. The designer sets the voting rule -- i.e., the minimum number of votes required to approve a proposal -- and the mechanism for proposer recognition, which is modeled as a biased generalized lottery contest. We show that for diverse design objectives, the optimum can be achieved by a dictatorial voting rule, which simplifies the game into a standard biased contest model."
http://arxiv.org/abs/2411.09869v1,Natural resources balance sheets accounting: theoretical framework and practice in the Shaanxi province of China,2024-11-15 01:15:52+00:00,"['Wentao Wang', 'Guoping Li', 'Andreas Kontoleon', 'Yiming Ma', 'Weishan Guo']",econ.GN,"To achieve sustainable development, there is widespread of the need to protect natural resource and improve government oversight in achieving China's economic security and ecological civilization. Compilation of natural resources balance sheet (NRBS) and enhancement of resources management are becoming an important topic in China. How to compile NRBS to affix the responsibility for government and officials for inadequate supervision is still not resolved satisfactorily. This paper proposes the NRBS to enable governments to identify the importance of natural resource restoration and to hold leading cadres accountable for a lack of adequate supervision. The NRBS consist of three accounts: natural resource assets, natural resource liabilities, and net worth. Important components of the NRBS for the liabilities account with a property rights regime are developed to measure and assign responsibility. The compilation of an NRBS is applied to the Chinese province of Shaanxi as an illustration to demonstrate that the accounting framework and the compilation steps are tractable using financial methods and available data. The accounting results of natural resource assets and liabilities unveil the threat to resource management and the policy implications to government and officials. Finally, the advantages and limitations of NRBS are discussed."
http://arxiv.org/abs/2411.13516v1,"Trade, Trees, and Lives",2024-11-20 18:08:20+00:00,"['Xinming Du', 'Lei Li', 'Eric Zou']",econ.GN,"This paper shows a cascading mechanism through which international trade-induced deforestation results in a decline of health outcomes in cities distant from where trade activities occur. We examine Brazil, which has ramped up agricultural export over the last two decades to meet rising global demand. Using a shift-share research design, we first show that export shocks cause substantial local agricultural expansion and a virtual one-for-one decline in forest cover. We then construct a dynamic area-of-effect model that predicts where atmospheric changes should be felt - due to loss of forests that would otherwise serve to filter out and absorb air pollutants as they travel - downwind of the deforestation areas. Leveraging quasi-random variation in these atmospheric connections, we establish a causal link between deforestation upstream and subsequent rises in air pollution and premature deaths downstream, with the mortality effects predominantly driven by cardiovascular and respiratory causes. Our estimates reveal a large telecoupled health externality of trade deforestation: over 700,000 premature deaths in Brazil over the past two decades. This equates to $0.18 loss in statistical life value per $1 agricultural exports over the study period."
http://arxiv.org/abs/2411.13427v1,"Price Setting Rules, Rounding Tax, and Inattention Penalty",2024-11-20 16:09:57+00:00,"['Doron Sayag', 'Avichai Snir', 'Daniel Levy']",econ.GN,"We study the price rounding regulation in Israel, which outlawed non-0-ending prices, forcing retailers to round 9 ending prices, which in many stores, comprised more than 60 percent of all prices. The goal of the regulation was to eliminate the rounding tax, the extra amount consumers paid because of price rounding, which was necessitated by the abolition of low denomination coins, and the inattention tax, the extra amount consumers paid the retailers because of their inattention to the prices rightmost digits. Using 4 different datasets, we assess the success of the government in achieving these goals, focusing on fast moving consumer goods, a category of products strongly affected by the price rounding regulation. We focus on the response of the retailers to the price rounding regulation and find that although the government succeeded in eliminating the rounding tax, the bottom line is that shoppers end up paying more, not less, because of the regulation, underscoring, once again, the warning of Milton Friedman that policies should be judged by their results, not by their intentions."
http://arxiv.org/abs/2411.13823v1,Non-Allais Paradox and Context-Dependent Risk Attitudes,2024-11-21 04:04:12+00:00,"['Edward Honda', 'Keh-Kuan Sun']",econ.TH,"We provide and axiomatize a representation of preferences over lotteries that generalizes the expected utility model. Our representation is consistent with the violations of the independence axiom that we observe in the laboratory experiment that we conduct. The violations differ from the Allais Paradox in that they are incompatible with some of the most prominent non-expected utility models. Our representation can be interpreted as a decision-maker with context-dependent attitudes to risks and allows us to generate various types of realistic behavior. We analyze some properties of our model, including specifications that ensure preferences for first-order stochastic dominance. We test whether subjects in our experiment exhibit the type of context-dependent risk attitudes that arise in our model."
http://arxiv.org/abs/2411.14058v1,Wavelet Analysis of Cryptocurrencies -- Non-Linear Dynamics in High Frequency Domains,2024-11-21 12:12:30+00:00,['Tatsuru Kikuchi'],q-fin.GN,"In this study, we perform some analysis for the probability distributions in the space of frequency and time variables. However, in the domain of high frequencies, it behaves in such a way as the highly non-linear dynamics. The wavelet analysis is a powerful tool to perform such analysis in order to search for the characteristics of frequency variations over time for the prices of major cryptocurrencies. In fact, the wavelet analysis is found to be quite useful as it examine the validity of the efficient market hypothesis in the weak form, especially for the presence of the cyclical persistence at different frequencies. If we could find some cyclical persistence at different frequencies, that means that there exist some intrinsic causal relationship for some given investment horizons defined by some chosen sampling scales. This is one of the characteristic results of the wavelet analysis in the time-frequency domains."
http://arxiv.org/abs/2411.14230v1,Public sentiments on the fourth industrial revolution: An unsolicited public opinion poll from Twitter,2024-11-21 15:39:53+00:00,['Diletta Abbonato'],econ.GN,"This article explores public perceptions on the Fourth Industrial Revolution (4IR) through an analysis of social media discourse across six European countries. Using sentiment analysis and machine learning techniques on a dataset of tweets and media articles, we assess how the public reacts to the integration of technologies such as artificial intelligence, robotics, and blockchain into society. The results highlight a significant polarization of opinions, with a shift from neutral to more definitive stances either embracing or resisting technological impacts. Positive sentiments are often associated with technological enhancements in quality of life and economic opportunities, whereas concerns focus on issues of privacy, data security, and ethical implications. This polarization underscores the need for policymakers to engage proactively with the public to address fears and harness the benefits of 4IR technologies. The findings also advocate for digital literacy and public awareness programs to mitigate misinformation and foster an informed public discourse on future technological integration. This study contributes to the ongoing debate on aligning technological advances with societal values and needs, emphasizing the role of informed public opinion in shaping effective policy."
http://arxiv.org/abs/2410.05741v3,The Transmission of Monetary Policy via Common Cycles in the Euro Area,2024-10-08 07:01:21+00:00,"['Lukas Berend', 'Jan Prüser']",econ.EM,"We use a FAVAR model with proxy variables and sign restrictions to investigate the role of the euro area's common output and inflation cycles in the transmission of monetary policy shocks. Our findings indicate that common cycles explain most of the variation in output and inflation across member countries. However, Southern European economies exhibit a notable divergence from these cycles in the aftermath of the financial crisis. Building on this evidence, we demonstrate that monetary policy is homogeneously propagated to member countries via the common cycles. In contrast, country-specific transmission channels lead to heterogeneous country responses to monetary policy shocks. Consequently, our empirical results suggest that the divergent effects of ECB monetary policy are attributable to heterogeneous country-specific exposures to financial markets, rather than to dis-synchronized economies within the euro area."
http://arxiv.org/abs/2412.00716v1,"Effects of time aggregation, product aggregation, and seasonality in measuring bullwhip ratio",2024-12-01 07:35:12+00:00,"['Hau Mike Ma', 'Jiazhen Huo', 'Yongrui Duan']",econ.GN,"The bullwhip study has received a lot of attention in the literature, but with conflicting results, especially in the context of data aggregation. In this paper, we investigate three widely studied factors in bullwhip measurement: time aggregation, product aggregation, and seasonality. In time aggregation, we decompose the variance into two components: the expectation of the subset variances and the variance of subset expectations, thus decomposing the bullwhip ratio into four components to explore the underlying mechanism of time aggregation. In product aggregation, the bullwhip ratio is analyzed in the context of products with either uncorrelated or correlated demands and orders. Seasonality is also examined to study its effect on the bullwhip ratio. Our key findings are: (a) Time aggregation can increase, decrease, or maintain the bullwhip ratio in different scenarios. (b) Aggregated bullwhip ratio of uncorrelated products is a weighted average of bullwhip ratios from individual products, with corresponding demand variance as the weights. However, aggregated bullwhip ratio of correlated products could break the boundaries. (c) Seasonality can be considered as a standalone product with a bullwhip ratio of one, which can drive the overall bullwhip ratio closer to one."
http://arxiv.org/abs/2410.07566v1,Revisiting the Primitives of Transaction Fee Mechanism Design,2024-10-10 03:04:09+00:00,"['Aadityan Ganesh', 'Clayton Thomas', 'S. Matthew Weinberg']",cs.GT,"Transaction Fee Mechanism Design studies auctions run by untrusted miners for transaction inclusion in a blockchain. Under previously-considered desiderata, an auction is considered `good' if, informally-speaking, each party (i.e., the miner, the users, and coalitions of both miners and users) has no incentive to deviate from the fixed and pre-determined protocol.
  In this paper, we propose a novel desideratum for transaction fee mechanisms. We say that a TFM is off-chain influence proof when the miner cannot achieve additional revenue by running a separate auction off-chain. While the previously-highlighted EIP-1559 is the gold-standard according to prior desiderata, we show that it does not satisfy off-chain influence proofness. Intuitively, this holds because a Bayesian revenue-maximizing miner can strictly increase profits by persuasively threatening to censor any bids that do not transfer a tip directly to the miner off-chain.
  On the other hand, we reconsider the Cryptographic (multi-party computation assisted) Second Price Auction mechanism, which is technically not `simple for miners' according to previous desiderata (since miners may wish to set a reserve by fabricating bids). We show that, in a slightly different model where the miner is allowed to set the reserve directly, this auction satisfies simplicity for users and miners, and off-chain influence proofness.
  Finally, we prove a strong impossibility result: no mechanism satisfies all previously-considered properties along with off-chain influence proofness, even with unlimited supply, and even after soliciting input from the miner."
http://arxiv.org/abs/2410.20885v1,A Distributed Lag Approach to the Generalised Dynamic Factor Model (GDFM),2024-10-28 10:07:06+00:00,['Philipp Gersing'],econ.EM,We provide estimation and inference for the Generalised Dynamic Factor Model (GDFM) under the assumption that the dynamic common component can be expressed in terms of a finite number of lags of contemporaneously pervasive factors. The proposed estimator is simply an OLS regression of the observed variables on factors extracted via static principal components and therefore avoids frequency domain techniques entirely.
http://arxiv.org/abs/2411.11559v2,Treatment Effect Estimators as Weighted Outcomes,2024-11-18 13:24:09+00:00,['Michael C. Knaus'],econ.EM,"Estimators that weight observed outcomes to form effect estimates have a long tradition. Their outcome weights are widely used in established procedures, such as checking covariate balance, characterizing target populations, or detecting and managing extreme weights. This paper introduces a general framework for deriving such outcome weights. It establishes when and how numerical equivalence between an original estimator representation as moment condition and a unique weighted representation can be obtained. The framework is applied to derive novel outcome weights for the six seminal instances of double machine learning and generalized random forests, while recovering existing results for other estimators as special cases. The analysis highlights that implementation choices determine (i) the availability of outcome weights and (ii) their properties. Notably, standard implementations of partially linear regression-based estimators, like causal forests, employ outcome weights that do not sum to (minus) one in the (un)treated group, not fulfilling a property often considered desirable."
http://arxiv.org/abs/2412.12495v1,"Obvious manipulations, consistency, and the uniform rule",2024-12-17 02:53:45+00:00,"['R. Pablo Arribillaga', 'Agustin G. Bonifacio']",econ.TH,"In the problem of fully allocating an infinitely divisible commodity among agents whose preferences are single-peaked, we show that the uniform rule is the only allocation rule that satisfies efficiency, the equal division guarantee, consistency, and non-obvious manipulability."
http://arxiv.org/abs/2412.17021v1,Competitive Facility Location with Market Expansion and Customer-centric Objective,2024-12-22 13:53:58+00:00,"['Cuong Le', 'Tien Mai', 'Ngan Ha Duong', 'Minh Hoang Ha']",math.OC,"We study a competitive facility location problem, where customer behavior is modeled and predicted using a discrete choice random utility model. The goal is to strategically place new facilities to maximize the overall captured customer demand in a competitive marketplace. In this work, we introduce two novel considerations. First, the total customer demand in the market is not fixed but is modeled as an increasing function of the customers' total utilities. Second, we incorporate a new term into the objective function, aiming to balance the firm's benefits and customer satisfaction. Our new formulation exhibits a highly nonlinear structure and is not directly solved by existing approaches. To address this, we first demonstrate that, under a concave market expansion function, the objective function is concave and submodular, allowing for a $(1-1/e)$ approximation solution by a simple polynomial-time greedy algorithm. We then develop a new method, called Inner-approximation, which enables us to approximate the mixed-integer nonlinear problem (MINLP), with arbitrary precision, by an MILP without introducing additional integer variables. We further demonstrate that our inner-approximation method consistently yields lower approximations than the outer-approximation methods typically used in the literature. Moreover, we extend our settings by considering a\textit{ general (non-concave)} market-expansion function and show that the Inner-approximation mechanism enables us to approximate the resulting MINLP, with arbitrary precision, by an MILP. To further enhance this MILP, we show how to significantly reduce the number of additional binary variables by leveraging concave areas of the objective function. Extensive experiments demonstrate the efficiency of our approaches."
http://arxiv.org/abs/2412.12780v1,Digital technologies and performance incentives: Evidence from businesses in the Swiss economy,2024-12-17 10:40:31+00:00,"['Johannes Lehmann', 'Michael Beckmann']",econ.GN,"Using novel survey data from Swiss firms, this paper empirically examines the relationship between the use of digital technologies and the prevalence of performance incentives. We argue that digital technologies tend to reduce the cost of organizational monitoring through improved measurement of employee behavior and performance, as well as through employee substitution in conjunction with a reduced agency problem. While we expect the former mechanism to increase the prevalence of performance incentives, the latter is likely to decrease it. Our doubly robust ATE estimates show that companies using business software and certain key technologies of Industry 4.0 increasingly resort to performance incentives, suggesting that the improved measurement effect dominates the employee substitution effect. In addition, we find that companies emerging as technology-friendly use performance incentives more frequently than their technology-averse counterparts. Both findings hold for managerial and non-managerial employees. Our estimation results are robust to a variety of sensitivity checks and suggest that Swiss businesses leverage digital technologies to enhance control over production or service processes, allowing them to intensify their management of employees through performance incentives."
http://arxiv.org/abs/2412.13689v1,A bibliometric analysis and scoping study to identify English-language perspectives on slums,2024-12-18 10:26:30+00:00,"['Katharina Henn', 'Michaela Lestakova', 'Kevin Logan', 'Jakob Hartig', 'Stefanos Georganos', 'John Friesen']",econ.GN,"Slums, informal settlements, and deprived areas are urban regions characterized by poverty. According to the United Nations, over one billion people reside in these areas, and this number is projected to increase. Additionally, these settlements are integral components of urban systems. We conducted a bibliometrical analysis and scoping study using the Web of Science Database to explore various perspectives on urban poverty, searching for scientific publications on the topic and providing details on the countries where the studies were conducted. Based on 3947 publications, we identify the extent to which domestic research organizations participate in studying urban poverty and which categories of science they investigate, including life sciences \& biomedicine, social sciences, technology, physical sciences, and arts & humanities. Thereby, we find that research on slums is often limited to specific countries, e.g. India, South Africa, Kenya, or Brazil. This focus is not necessarily correlated with the number of people living in slums. The scientific discourse is up to now predominantly shaped by medical and social sciences with few studies addressing technological questions. Finally, our analysis identifies several possible future directions for research on slums."
http://arxiv.org/abs/2412.10635v1,Do LLMs Act as Repositories of Causal Knowledge?,2024-12-14 01:28:38+00:00,"['Nick Huntington-Klein', 'Eleanor J. Murray']",econ.EM,"Large language models (LLMs) offer the potential to automate a large number of tasks that previously have not been possible to automate, including some in science. There is considerable interest in whether LLMs can automate the process of causal inference by providing the information about causal links necessary to build a structural model. We use the case of confounding in the Coronary Drug Project (CDP), for which there are several studies listing expert-selected confounders that can serve as a ground truth. LLMs exhibit mediocre performance in identifying confounders in this setting, even though text about the ground truth is in their training data. Variables that experts identify as confounders are only slightly more likely to be labeled as confounders by LLMs compared to variables that experts consider non-confounders. Further, LLM judgment on confounder status is highly inconsistent across models, prompts, and irrelevant concerns like multiple-choice option ordering. LLMs do not yet have the ability to automate the reporting of causal links."
http://arxiv.org/abs/2412.11179v1,Treatment Evaluation at the Intensive and Extensive Margins,2024-12-15 13:07:22+00:00,"['Phillip Heiler', 'Asbjørn Kaufmann', 'Bezirgen Veliyev']",econ.EM,"This paper provides a solution to the evaluation of treatment effects in selective samples when neither instruments nor parametric assumptions are available. We provide sharp bounds for average treatment effects under a conditional monotonicity assumption for all principal strata, i.e. units characterizing the complete intensive and extensive margins. Most importantly, we allow for a large share of units whose selection is indifferent to treatment, e.g. due to non-compliance. The existence of such a population is crucially tied to the regularity of sharp population bounds and thus conventional asymptotic inference for methods such as Lee bounds can be misleading. It can be solved using smoothed outer identification regions for inference. We provide semiparametrically efficient debiased machine learning estimators for both regular and smooth bounds that can accommodate high-dimensional covariates and flexible functional forms. Our study of active labor market policy reveals the empirical prevalence of the aforementioned indifference population and supports results from previous impact analysis under much weaker assumptions."
http://arxiv.org/abs/2412.10791v1,Forecasting realized covariances using HAR-type models,2024-12-14 11:03:29+00:00,"['Matias Quiroz', 'Laleh Tafakori', 'Hans Manner']",econ.EM,"We investigate methods for forecasting multivariate realized covariances matrices applied to a set of 30 assets that were included in the DJ30 index at some point, including two novel methods that use existing (univariate) log of realized variance models that account for attenuation bias and time-varying parameters. We consider the implications of some modeling choices within the class of heterogeneous autoregressive models. The following are our key findings. First, modeling the logs of the marginal volatilities is strongly preferred over direct modeling of marginal volatility. Thus, our proposed model that accounts for attenuation bias (for the log-response) provides superior one-step-ahead forecasts over existing multivariate realized covariance approaches. Second, accounting for measurement errors in marginal realized variances generally improves multivariate forecasting performance, but to a lesser degree than previously found in the literature. Third, time-varying parameter models based on state-space models perform almost equally well. Fourth, statistical and economic criteria for comparing the forecasting performance lead to some differences in the models' rankings, which can partially be explained by the turbulent post-pandemic data in our out-of-sample validation dataset using sub-sample analyses."
http://arxiv.org/abs/2412.11597v1,Transition dynamics of electricity asset-owning firms,2024-12-16 09:36:15+00:00,['Anton Pichler'],econ.GN,"Despite dramatic growth and cost improvements in renewables, existing energy companies exhibit significant inertia in adapting to the evolving technological landscape. This study examines technology transition patterns by analyzing over 140,000 investments in power assets over more than two decades, focusing on how firms expand existing technology holdings and adopt new technologies. Building on our comprehensive micro-level dataset, we provide a number of quantitative metrics on global investment dynamism and the evolution of technology portfolios. We find that only about 10\% of firms experience capacity changes in a given year, and that technology portfolios of firms are highly concentrated and persistent in time. We also identify a small subset of frequently investing firms that tend to be large and are key drivers of global technology-specific capacity expansion. Technology transitions within companies are extremely rare. Less than 3% of the more than 8,400 fossil fuel-dominated firms have substantially transformed their portfolios to a renewable focus and firms fully transitioning to renewables are, up-to-date, virtually non-existent. Notably, firms divesting into renewables do not exhibit very characteristic technology-transition patterns but rather follow idiosyncratic transition pathways. Our results quantify the complex technology diffusion dynamics and the diverse corporate responses to a changing technology landscape, highlighting the challenge of designing general policies aimed at fostering technological transitions at the level of firms."
http://arxiv.org/abs/2412.10974v2,Quantifying Educational Competition: A Game-Theoretic Model with Policy Implications,2024-12-14 21:34:45+00:00,['Siyuan He'],econ.TH,"The competitive pressures in China's primary and secondary education system have persisted despite decades of policy interventions aimed at reducing academic burdens and alleviating parental anxiety. This paper develops a game-theoretic model to analyze the strategic interactions among families in this system, revealing how competition escalates into a socially irrational ""education arms race."" Through equilibrium analysis and simulations, the study demonstrates the inherent trade-offs between education equity and social welfare, alongside the policy failures arising from biased social cognition. The model is further extended using Spence's signaling framework to explore the inefficiencies of the current system and propose policy solutions that address these issues."
http://arxiv.org/abs/2411.07817v1,Impact of R&D and AI Investments on Economic Growth and Credit Rating,2024-11-12 14:09:56+00:00,"['Davit Gondauri', 'Ekaterine Mikautadze']",econ.EM,"The research and development (R&D) phase is essential for fostering innovation and aligns with long-term strategies in both public and private sectors. This study addresses two primary research questions: (1) assessing the relationship between R&D investments and GDP through regression analysis, and (2) estimating the economic value added (EVA) that Georgia must generate to progress from a BB to a BBB credit rating. Using World Bank data from 2014-2022, this analysis found that increasing R&D, with an emphasis on AI, by 30-35% has a measurable impact on GDP. Regression results reveal a coefficient of 7.02%, indicating a 10% increase in R&D leads to a 0.70% GDP rise, with an 81.1% determination coefficient and a strong 90.1% correlation.
  Georgia's EVA model was calculated to determine the additional value needed for a BBB rating, comparing indicators from Greece, Hungary, India, and Kazakhstan as benchmarks. Key economic indicators considered were nominal GDP, GDP per capita, real GDP growth, and fiscal indicators (government balance/GDP, debt/GDP). The EVA model projects that to achieve a BBB rating within nine years, Georgia requires $61.7 billion in investments. Utilizing EVA and comprehensive economic indicators will support informed decision-making and enhance the analysis of Georgia's economic trajectory."
http://arxiv.org/abs/2411.03116v2,Generative AI and Security Operations Center Productivity: Evidence from Live Operations,2024-11-05 14:06:21+00:00,"['James Bono', 'Justin Grana', 'Alec Xu']",econ.GN,"We measure the association between generative AI (GAI) tool adoption and security operations center productivity. We find that GAI adoption is associated with a 30.13% reduction in security incident mean time to resolution. This result is robust to several modeling decisions. While unobserved confounders inhibit causal identification, this result is among the first to use observational data from live operations to investigate the relationship between GAI adoption and security worker productivity."
http://arxiv.org/abs/2411.17899v1,Remote Surgery with 5G or 6G: Knowledge Production and Diffusion Globally and in the German Case,2024-11-26 21:29:27+00:00,"['Marina Martinelli', 'André Tosi Furtado']",econ.TH,"This paper is a comprehensive exploring of technology capability in 5G/6G TIS, explicitly focusing on the potential of remote surgery globally and in Germany. The paper's main contribution is its ability to anticipate new debates on the interplay between TIS and contexts, with particular emphasis on the national and international levels. Our findings, derived from a Bibliometrics study of industry-academic relationships, highlight crucial collaborations in Germany, positioning the country as a strategic actor in international TIS and, by extension, in applying 5G/6G technological systems to remote surgery due to its knowledge production capability. We propose policies that can stimulate interaction between smaller suppliers and larger companies, which can act as intermediaries and provide access to international markets."
http://arxiv.org/abs/2411.11748v3,Debiased Regression for Root-N-Consistent Conditional Mean Estimation,2024-11-18 17:25:06+00:00,['Masahiro Kato'],stat.ML,"This study introduces a debiasing method for regression estimators, including high-dimensional and nonparametric regression estimators. For example, nonparametric regression methods allow for the estimation of regression functions in a data-driven manner with minimal assumptions; however, these methods typically fail to achieve $\sqrt{n}$-consistency in their convergence rates, and many, including those in machine learning, lack guarantees that their estimators asymptotically follow a normal distribution. To address these challenges, we propose a debiasing technique for nonparametric estimators by adding a bias-correction term to the original estimators, extending the conventional one-step estimator used in semiparametric analysis. Specifically, for each data point, we estimate the conditional expected residual of the original nonparametric estimator, which can, for instance, be computed using kernel (Nadaraya-Watson) regression, and incorporate it as a bias-reduction term. Our theoretical analysis demonstrates that the proposed estimator achieves $\sqrt{n}$-consistency and asymptotic normality under a mild convergence rate condition for both the original nonparametric estimator and the conditional expected residual estimator. Notably, this approach remains model-free as long as the original estimator and the conditional expected residual estimator satisfy the convergence rate condition. The proposed method offers several advantages, including improved estimation accuracy and simplified construction of confidence intervals."
http://arxiv.org/abs/2411.10314v2,Estimating the Cost of Informal Care with a Novel Two-Stage Approach to Individual Synthetic Control,2024-11-15 16:13:33+00:00,"['Maria Petrillo', 'Daniel Valdenegro', 'Charles Rahal', 'Yanan Zhang', 'Gwilym Pryce', 'Matthew R. Bennett']",econ.GN,"Informal carers provide the majority of care for people living with challenges related to older age, long-term illness, or disability. However, the care they provide often results in a significant income penalty for carers, a factor largely overlooked in the economics literature and policy discourse. Leveraging data from the UK Household Longitudinal Study, this paper provides the first robust causal estimates of the caring income penalty using a novel individual synthetic control based method that accounts for unit-level heterogeneity in post-treatment trajectories over time. Our baseline estimates identify an average relative income gap of up to 45%, with an average decrease of £162 in monthly income, peaking at £192 per month after 4 years, based on the difference between informal carers providing the highest-intensity of care and their synthetic counterparts. We find that the income penalty is more pronounced for women than for men, and varies by ethnicity and age."
http://arxiv.org/abs/2411.16574v1,Naive Algorithmic Collusion: When Do Bandit Learners Cooperate and When Do They Compete?,2024-11-25 16:58:07+00:00,"['Connor Douglas', 'Foster Provost', 'Arun Sundararajan']",econ.GN,"Algorithmic agents are used in a variety of competitive decision settings, notably in making pricing decisions in contexts that range from online retail to residential home rentals. Business managers, algorithm designers, legal scholars, and regulators alike are all starting to consider the ramifications of ""algorithmic collusion."" We study the emergent behavior of multi-armed bandit machine learning algorithms used in situations where agents are competing, but they have no information about the strategic interaction they are engaged in. Using a general-form repeated Prisoner's Dilemma game, agents engage in online learning with no prior model of game structure and no knowledge of competitors' states or actions (e.g., no observation of competing prices). We show that these context-free bandits, with no knowledge of opponents' choices or outcomes, still will consistently learn collusive behavior - what we call ""naive collusion."" We primarily study this system through an analytical model and examine perturbations to the model through simulations.
  Our findings have several notable implications for regulators. First, calls to limit algorithms from conditioning on competitors' prices are insufficient to prevent algorithmic collusion. This is a direct result of collusion arising even in the naive setting. Second, symmetry in algorithms can increase collusion potential. This highlights a new, simple mechanism for ""hub-and-spoke"" algorithmic collusion. A central distributor need not imbue its algorithm with supra-competitive tendencies for apparent collusion to arise; it can simply arise by using certain (common) machine learning algorithms. Finally, we highlight that collusive outcomes depend starkly on the specific algorithm being used, and we highlight market and algorithmic conditions under which it will be unknown a priori whether collusion occurs."
http://arxiv.org/abs/2412.17181v1,Gaussian and Bootstrap Approximation for Matching-based Average Treatment Effect Estimators,2024-12-22 22:47:31+00:00,"['Zhaoyang Shi', 'Chinmoy Bhattacharjee', 'Krishnakumar Balasubramanian', 'Wolfgang Polonik']",math.ST,"We establish Gaussian approximation bounds for covariate and rank-matching-based Average Treatment Effect (ATE) estimators. By analyzing these estimators through the lens of stabilization theory, we employ the Malliavin-Stein method to derive our results. Our bounds precisely quantify the impact of key problem parameters, including the number of matches and treatment balance, on the accuracy of the Gaussian approximation. Additionally, we develop multiplier bootstrap procedures to estimate the limiting distribution in a fully data-driven manner, and we leverage the derived Gaussian approximation results to further obtain bootstrap approximation bounds. Our work not only introduces a novel theoretical framework for commonly used ATE estimators, but also provides data-driven methods for constructing non-asymptotically valid confidence intervals."
http://arxiv.org/abs/2412.18080v1,Conditional Influence Functions,2024-12-24 01:29:26+00:00,"['Victor Chernozhukov', 'Whitney K. Newey', 'Vasilis Syrgkanis']",math.ST,There are many nonparametric objects of interest that are a function of a conditional distribution. One important example is an average treatment effect conditional on a subset of covariates. Many of these objects have a conditional influence function that generalizes the classical influence function of a functional of a (unconditional) distribution. Conditional influence functions have important uses analogous to those of the classical influence function. They can be used to construct Neyman orthogonal estimating equations for conditional objects of interest that depend on high dimensional regressions. They can be used to formulate local policy effects and describe the effect of local misspecification on conditional objects of interest. We derive conditional influence functions for functionals of conditional means and other features of the conditional distribution of an outcome variable. We show how these can be used for locally linear estimation of conditional objects of interest. We give rate conditions for first step machine learners to have no effect on asymptotic distributions of locally linear estimators. We also give a general construction of Neyman orthogonal estimating equations for conditional objects of interest.
http://arxiv.org/abs/2412.18032v1,A physics-engineering-economic model coupling approach for estimating the socio-economic impacts of space weather scenarios,2024-12-23 23:03:58+00:00,"['Edward J. Oughton', 'Dennies K. Bor', 'Michael Wiltberger', 'Robert Weigel', 'C. Trevor Gaunt', 'Ridvan Dogan', 'Liling Huang']",physics.geo-ph,"There is growing concern about our vulnerability to space weather hazards and the disruption critical infrastructure failures could cause to society and the economy. However, the socio-economic impacts of space weather hazards, such as from geomagnetic storms, remain under-researched. This study introduces a novel framework to estimate the economic impacts of electricity transmission infrastructure failure due to space weather. By integrating existing geophysical and geomagnetically induced current (GIC) estimation models with a newly developed geospatial model of the Continental United States power grid, GIC vulnerabilities are assessed for a range of space weather scenarios. The approach evaluates multiple power network architectures, incorporating input-output economic modeling to translate business and population disruptions into macroeconomic impacts from GIC-related thermal heating failures. The results indicate a daily GDP loss from 6 billion USD to over 10 billion USD. Even under conservative GIC thresholds (75 A/ph) aligned with thermal withstand limits from the North American Electric Reliability Corporation (NERC), significant economic disruptions are evident. This study is limited by its restriction to thermal heating analysis, though GICs can also affect the grid through other pathways, such as voltage instability and harmonic distortions. Addressing these other failure mechanisms need to be the focus of future research."
http://arxiv.org/abs/2412.09226v3,The Global Carbon Budget as a cointegrated system,2024-12-12 12:28:38+00:00,"['Mikkel Bennedsen', 'Eric Hillebrand', 'Morten Ørregaard Nielsen']",stat.AP,"The Global Carbon Budget, maintained by the Global Carbon Project, summarizes Earth's global carbon cycle through four annual time series beginning in 1959: atmospheric CO$_2$ concentrations, anthropogenic CO$_2$ emissions, and CO$_2$ uptake by land and ocean. We analyze these four time series as a multivariate (cointegrated) system. Statistical tests show that the four time series are cointegrated with rank three and identify anthropogenic CO$_2$ emissions as the single stochastic trend driving the nonstationary dynamics of the system. The three cointegrated relations correspond to the physical relations that the sinks are linearly related to atmospheric concentrations and that the change in concentrations equals emissions minus the combined uptake by land and ocean. Furthermore, likelihood ratio tests show that a parametrically restricted error-correction model that embodies these physical relations and accounts for the El Niño/Southern Oscillation cannot be rejected on the data. The model can be used for both in-sample and out-of-sample analysis. In an application of the latter, we demonstrate that projections based on this model, using Shared Socioeconomic Pathways scenarios, yield results consistent with established climate science."
http://arxiv.org/abs/2411.09856v3,InvestESG: A multi-agent reinforcement learning benchmark for studying climate investment as a social dilemma,2024-11-15 00:31:45+00:00,"['Xiaoxuan Hou', 'Jiayi Yuan', 'Joel Z. Leibo', 'Natasha Jaques']",cs.LG,"InvestESG is a novel multi-agent reinforcement learning (MARL) benchmark designed to study the impact of Environmental, Social, and Governance (ESG) disclosure mandates on corporate climate investments. The benchmark models an intertemporal social dilemma where companies balance short-term profit losses from climate mitigation efforts and long-term benefits from reducing climate risk, while ESG-conscious investors attempt to influence corporate behavior through their investment decisions. Companies allocate capital across mitigation, greenwashing, and resilience, with varying strategies influencing climate outcomes and investor preferences. We are releasing open-source versions of InvestESG in both PyTorch and JAX, which enable scalable and hardware-accelerated simulations for investigating competing incentives in mitigate climate change. Our experiments show that without ESG-conscious investors with sufficient capital, corporate mitigation efforts remain limited under the disclosure mandate. However, when a critical mass of investors prioritizes ESG, corporate cooperation increases, which in turn reduces climate risks and enhances long-term financial stability. Additionally, providing more information about global climate risks encourages companies to invest more in mitigation, even without investor involvement. Our findings align with empirical research using real-world data, highlighting MARL's potential to inform policy by providing insights into large-scale socio-economic challenges through efficient testing of alternative policy and market designs."
http://arxiv.org/abs/2410.04970v3,The effect of competition in contests: A unifying approach,2024-10-07 12:15:19+00:00,"['Andrzej Baranski', 'Sumit Goel']",econ.TH,"We study how increasing competition, by making prizes more unequal, affects effort in contests. In a finite type-space environment, we characterize the equilibrium, analyze the effect of competition under linear costs, and identify conditions under which these effects persist under general costs. Our findings reveal that competition may encourage or deter effort, depending on the relative likelihood of efficient versus inefficient types. We derive implications for the classical budget allocation problem and establish that the most competitive winner-takes-all contest is robustly optimal under linear and concave costs, thereby resolving an open question. Methodologically, our analysis of the finite type-space domain -- which includes complete information as a special case and can approximate any continuum type-space -- provides a unifying approach that sheds light on the contrasting results in these extensively studied environments."
http://arxiv.org/abs/2412.19024v2,Nonparametric Estimation of Matching Efficiency and Elasticity in a Spot Gig Work Platform: 2019-2023,2024-12-26 02:16:58+00:00,"['Hayato Kanayama', 'Suguru Otani']",econ.GN,"This paper provides new evidence on spot gig work platforms for unemployed workers searching for occupations with minimal educational or experience requirements in Japan. Using proprietary data from a private online spot work matching platform, Timee, it examines trends in key variables such as the numbers of unemployed users, vacancies, hires, and labor market tightness. The study compares these trends with part-time worker data from the public employment platform, Hello Work. The private platform shows a significant market expansion from December 2019 to December 2023. Applying a novel nonparametric approach, the paper finds greater variability in efficiency and higher elasticity, with elasticity with respect to the number of users fluctuating from below 0.7 to above 1.5, and elasticity with respect to the number of vacancies often exceeding 1.0, which is higher than Hello Work. Lastly, the study highlights less geographical heterogeneity of the spot work compared to Hello Work."
http://arxiv.org/abs/2410.20363v5,Democratising Agricultural Commodity Price Forecasting: The AGRICAF Approach,2024-10-27 07:39:58+00:00,['Rotem Zelingher'],econ.GN,"Ensuring food security is a critical global challenge, particularly for low-income countries where food prices impact the access to nutritious food. The volatility of global agricultural commodity (AC) prices exacerbates food insecurity, with international trade restrictions and market disruptions further complicating the situation. Despite online platforms for monitoring food prices, there is a significant gap in providing detailed explanations and forecasts accessible to non-specialists. To address this, we propose the Agricultural Commodity Analysis and Forecasts (AGRICAF) methodology, integrating explainable machine learning (XML) and econometric techniques to analyse and forecast global agricultural commodity prices up to one year ahead, dynamically adapting to different forecast horizons. This innovative integration allows us to model complex interactions and dynamics while providing clear, interpretable results. This paper demonstrates how AGRICAF can be used, applying it to three major agricultural commodities - maize, soybean, and wheat - and explaining how different factors impact prices across various months and forecast horizons. By facilitating access to accurate and interpretable medium-term forecasts of AC prices, AGRICAF can contribute to developing a fair and sustainable food system."
http://arxiv.org/abs/2410.16333v2,Conformal Predictive Portfolio Selection,2024-10-19 15:42:49+00:00,['Masahiro Kato'],q-fin.PM,"This study examines portfolio selection using predictive models for portfolio returns. Portfolio selection is a fundamental task in finance, and a variety of methods have been developed to achieve this goal. For instance, the mean-variance approach constructs portfolios by balancing the trade-off between the mean and variance of asset returns, while the quantile-based approach optimizes portfolios by considering tail risk. These methods often depend on distributional information estimated from historical data using predictive models, each of which carries its own uncertainty. To address this, we propose a framework for predictive portfolio selection via conformal prediction , called \emph{Conformal Predictive Portfolio Selection} (CPPS). Our approach forecasts future portfolio returns, computes the corresponding prediction intervals, and selects the portfolio of interest based on these intervals. The framework is flexible and can accommodate a wide range of predictive models, including autoregressive (AR) models, random forests, and neural networks. We demonstrate the effectiveness of the CPPS framework by applying it to an AR model and validate its performance through empirical studies, showing that it delivers superior returns compared to simpler strategies."
http://arxiv.org/abs/2411.00886v1,The ET Interview: Professor Joel L. Horowitz,2024-10-31 18:37:58+00:00,['Sokbae Lee'],econ.EM,"Joel L. Horowitz has made profound contributions to many areas in econometrics and statistics. These include bootstrap methods, semiparametric and nonparametric estimation, specification testing, nonparametric instrumental variables estimation, high-dimensional models, functional data analysis, and shape restrictions, among others. Originally trained as a physicist, Joel made a pivotal transition to econometrics, greatly benefiting our profession. Throughout his career, he has collaborated extensively with a diverse range of coauthors, including students, departmental colleagues, and scholars from around the globe. Joel was born in 1941 in Pasadena, California. He attended Stanford for his undergraduate studies and obtained his Ph.D. in physics from Cornell in 1967. He has been Charles E. and Emma H. Morrison Professor of Economics at Northwestern University since 2001. Prior to that, he was a faculty member at the University of Iowa (1982-2001). He has served as a co-editor of Econometric Theory (1992-2000) and Econometrica (2000-2004). He is a Fellow of the Econometric Society and of the American Statistical Association, and an elected member of the International Statistical Institute. The majority of this interview took place in London during June 2022."
http://arxiv.org/abs/2412.02380v2,Use of surrogate endpoints in health technology assessment: a review of selected NICE technology appraisals in oncology,2024-12-03 11:05:13+00:00,"['Lorna Wheaton', 'Sylwia Bujkiewicz']",stat.AP,"Objectives: Surrogate endpoints, used to substitute for and predict final clinical outcomes, are increasingly being used to support submissions to health technology assessment agencies. The increase in use of surrogate endpoints has been accompanied by literature describing frameworks and statistical methods to ensure their robust validation. The aim of this review was to assess how surrogate endpoints have recently been used in oncology technology appraisals by the National Institute for Health and Care Excellence (NICE) in England and Wales.
  Methods: This paper identified technology appraisals in oncology published by NICE between February 2022 and May 2023. Data are extracted on methods for the use and validation of surrogate endpoints.
  Results: Of the 47 technology appraisals in oncology available for review, 18 (38 percent) utilised surrogate endpoints, with 37 separate surrogate endpoints being discussed. However, the evidence supporting the validity of the surrogate relationship varied significantly across putative surrogate relationships with 11 providing RCT evidence, 7 providing evidence from observational studies, 12 based on clinical opinion and 7 providing no evidence for the use of surrogate endpoints.
  Conclusions: This review supports the assertion that surrogate endpoints are frequently used in oncology technology appraisals in England and Wales. Despite increasing availability of statistical methods and guidance on appropriate validation of surrogate endpoints, this review highlights that use and validation of surrogate endpoints can vary between technology appraisals which can lead to uncertainty in decision-making."
http://arxiv.org/abs/2501.00235v2,Robust Intervention in Networks,2024-12-31 03:00:40+00:00,"['Daeyoung Jeong', 'Tongseok Lim', 'Euncheol Shin']",econ.TH,"In economic settings such as learning, social behavior, and financial contagion, agents interact through interdependent networks. This paper examines how a decision maker (DM) can design an optimal intervention strategy under network uncertainty, modeled as a zero-sum game against an adversarial ``Nature'' that reconfigures the network within an uncertainty set. Using duality, we characterize the DM's unique robust intervention and identify the worst-case network structure, which exhibits a rank-1 property, concentrating risk along the intervention strategy. We analyze the costs of robustness, distinguishing between global and local uncertainty, and examine the role of higher-order uncertainties in shaping intervention outcomes. Our findings highlight key trade-offs between maximizing influence and mitigating uncertainty, offering insights into robust decision-making. This framework has applications in policy design, economic regulation, and strategic interventions in dynamic networks, ensuring their resilience against uncertainty in network structures."
http://arxiv.org/abs/2410.20861v2,Beyond Baby Blues: The Child Penalty in Mental Health in Switzerland,2024-10-28 09:26:47+00:00,['Nora Bearth'],econ.GN,"This paper investigates the mental health penalty for women after childbirth in Switzerland. Leveraging insurance data, we employ a staggered difference-in-difference research design. The findings reveal a substantial mental health penalty for women following the birth of their first child. Approximately four years after childbirth, there is a one percentage point (p.p.) increase in antidepressant prescriptions, representing a 50% increase compared to pre-birth levels. This increase rises to 1.7 p.p. (a 75% increase) six years postpartum. The mental health penalty is likely not only a direct consequence of giving birth but also a consequence of the changed life circumstances and time constraints that accompany it, as the penalty is rising over time and is higher for women who are employed."
http://arxiv.org/abs/2412.14778v2,Testing linearity of spatial interaction functions à la Ramsey,2024-12-19 12:03:49+00:00,"['Abhimanyu Gupta', 'Jungyoon Lee', 'Francesca Rossi']",econ.EM,"We propose a computationally straightforward test for the linearity of a spatial interaction function. Such functions arise commonly, either as practitioner imposed specifications or due to optimizing behaviour by agents. Our conditional heteroskedasticity robust test is nonparametric, but based on the Lagrange Multiplier principle and reminiscent of the Ramsey RESET approach. This entails estimation only under the null hypothesis, which yields an easy to estimate linear spatial autoregressive model. Monte Carlo simulations show excellent size control and power. An empirical study with Finnish data illustrates the test's practical usefulness, shedding light on debates on the presence of tax competition among neighbouring municipalities."
http://arxiv.org/abs/2410.19915v2,AI-Driven Scenarios for Urban Mobility: Quantifying the Role of ODE Models and Scenario Planning in Reducing Traffic Congestion,2024-10-25 18:09:02+00:00,['Katsiaryna Bahamazava'],econ.GN,"Urbanization and technological advancements are reshaping urban mobility, presenting both challenges and opportunities. This paper investigates how Artificial Intelligence (AI)-driven technologies can impact traffic congestion dynamics and explores their potential to enhance transportation systems' efficiency. Specifically, we assess the role of AI innovations, such as autonomous vehicles and intelligent traffic management, in mitigating congestion under varying regulatory frameworks. Autonomous vehicles reduce congestion through optimized traffic flow, real-time route adjustments, and decreased human errors.
  The study employs Ordinary Differential Equations (ODEs) to model the dynamic relationship between AI adoption rates and traffic congestion, capturing systemic feedback loops. Quantitative outputs include threshold levels of AI adoption needed to achieve significant congestion reduction, while qualitative insights stem from scenario planning exploring regulatory and societal conditions. This dual-method approach offers actionable strategies for policymakers to create efficient, sustainable, and equitable urban transportation systems. While safety implications of AI are acknowledged, this study primarily focuses on congestion reduction dynamics."
http://arxiv.org/abs/2410.20970v2,Knowledge and Freedom: Evidence on the Relationship Between Information and Paternalism,2024-10-28 12:41:08+00:00,['Max R. P. Grossmann'],econ.GN,"When is autonomy granted to a decision-maker based on their knowledge, and if no autonomy is granted, what form will the intervention take? A parsimonious theoretical framework shows how policymakers can exploit decision-maker mistakes and use them as a justification for intervention. In two experiments, policymakers (""Choice Architects"") can intervene in a choice faced by a decision-maker. We vary the amount of knowledge decision-makers possess about the choice. Full decision-maker knowledge causes more than a 60% reduction in intervention rates. Beliefs have a small, robust correlation with interventions on the intensive margin. Choice Architects disproportionately prefer to have decision-makers make informed decisions. Interveners are less likely to provide information. As theory predicts, the same applies to Choice Architects who believe that decision-maker mistakes align with their own preference. When Choice Architects are informed about the decision-maker's preference, this information is used to determine the imposed option. However, Choice Architects employ their own preference to a similar extent. A riskless option is causally more likely to be imposed, being correlated with but conceptually distinct from Choice Architects' own preference. This is a qualification to what has been termed ""projective paternalism."""
http://arxiv.org/abs/2412.19931v1,Pivoting B2B platform business models: From platform experimentation to multi-platform integration to ecosystem envelopment,2024-12-27 21:34:05+00:00,"['Clara Filosa', 'Marin Jovanovic', 'Lara Agostini', 'Anna Nosella']",econ.GN,"The landscape of digital servitization in the manufacturing sector is evolving, marked by a strategic shift from traditional product-centric to platform business models (BMs). Manufacturing firms often employ a blend of approaches to develop business-to-business (B2B) platforms, leading to significant reconfigurations in their BMs. However, they frequently encounter failures in their B2B platform development initiatives, leading them to abandon initial efforts and pivot to alternative platform strategies. Therefore, this study, through an in-depth case study of a manufacturer in the energy sector, articulates a three-phase pivoting framework for B2B platform BMs, including platform development and platform strategy. Initially, the manufacturer focused on asset-based product sales supplemented by asset maintenance services and followed an emergent platformization strategy characterized by the rise of multiple, independent B2B platforms catering to diverse functions. Next, focusing on the imposed customer journey strategy, the firm shifted towards a strategic multi-platform integration into an all-encompassing platform supported by artificial intelligence (AI), signaling a maturation of the platform BM to combine a wide range of services into an energy-performance-based contract. Finally, the last step of the firm's platform BM evolution consisted of a deliberate platform strategy open to external stakeholders and enveloping its data-driven offerings within a broader platform ecosystem. This article advances B2B platform BMs and digital servitization literature, highlighting the efficacy of a progressive approach and strategic pivoting."
http://arxiv.org/abs/2412.19850v1,The Patterns of Digital Deception,2024-12-25 15:55:17+00:00,['Gregory M. Dickinson'],econ.GN,"Current consumer-protection debates focus on the powerful new data-analysis techniques that have disrupted the balance of power between companies and their customers. Online tracking enables sellers to amass troves of historical data, apply machine-learning tools to construct detailed customer profiles, and target those customers with tailored offers that best suit their interests. It is often a win-win. Sellers avoid pumping dud products and consumers see ads for things they actually want to buy. But the same tools are also used for ill -- to target vulnerable members of the population with scams specially tailored to prey on their weaknesses. The result has been a dramatic rise in online fraud that disproportionately impacts those least able to bear the loss.
  The law's response has been technology centric. Lawmakers race to identify those technologies that drive consumer deception and target them for regulatory restrictions. But that approach comes at a major cost. General-purpose data-analysis and communications tools have both desirable and undesirable uses, and uniform restrictions on their use impede the good along with the bad. A superior approach would focus not on the technological tools of deception but on what this Article identifies as the legal patterns of digital deception -- those aspects of digital technology that have outflanked the law's existing mechanisms for redressing consumer harm. This Article reorients the discussion from the power of new technologies to the shortcomings in existing regulatory structures that have allowed for their abuse. Focus on these patterns of deception will allow regulators to reallocate resources to offset those shortcomings and thereby enhance efforts to combat online fraud without impeding technological innovation."
http://arxiv.org/abs/2412.20176v1,The impact of China's economic growth on poverty alleviation: From absolute to relative poverty,2024-12-28 15:12:44+00:00,"['Yixun Kang', 'Ying Li']",econ.GN,"This paper investigates the extent to which China's economic growth and development influence poverty levels, focusing on the dichotomy between absolute and relative poverty. Leveraging data from sources like the World Bank, Statista, and Macrotrends, and employing economic frameworks such as the Lewis Model, Poverty Headcount Ratio, and Gini Coefficient, the study examines China's transformation from combating absolute poverty to addressing relative poverty. The findings highlight that robust economic growth from 2011 to 2022, driven by urban development and rural infrastructure investments, successfully eradicated absolute poverty and elevated rural incomes. However, this progress also exacerbated income inequality, as evidenced by a rising Gini Coefficient, complicating efforts to alleviate relative poverty. Through multidimensional analyses encompassing regional disparities, migration patterns, educational access, and societal factors, the paper underscores the dual impact of economic development on poverty alleviation. It concludes by advocating for policies that balance economic growth with equitable resource distribution to tackle persistent relative poverty and foster sustainable development."
http://arxiv.org/abs/2412.21181v1,Causal Hangover Effects,2024-12-30 18:52:48+00:00,"['Andreas Santucci', 'Eric Lax']",econ.EM,"It's not unreasonable to think that in-game sporting performance can be affected partly by what takes place off the court. We can't observe what happens between games directly. Instead, we proxy for the possibility of athletes partying by looking at play following games in party cities. We are interested to see if teams exhibit a decline in performance the day following a game in a city with active nightlife; we call this a ""hangover effect"". Part of the question is determining a reasonable way to measure levels of nightlife, and correspondingly which cities are notorious for it; we colloquially refer to such cities as ""party cities"". To carry out this study, we exploit data on bookmaker spreads: the expected score differential between two teams after conditioning on observable performance in past games and expectations about the upcoming game. We expect a team to meet the spread half the time, since this is one of the easiest ways for bookmakers to guarantee a profit. We construct a model which attempts to estimate the causal effect of visiting a ""party city"" on subsequent day performance as measured by the odds of beating the spread. In particular, we only consider the hangover effect on games played back-to-back within 24 hours of each other. To the extent that odds of beating the spread against next day opponent is uncorrelated with playing in a party city the day before, which should be the case under an efficient betting market, we have identification in our variable of interest. We find that visiting a city with active nightlife the day prior to a game does have a statistically significant negative effect on a team's likelihood of meeting bookmakers' expectations for both NBA and MLB."
http://arxiv.org/abs/2412.20420v1,Automated Demand Forecasting in small to medium-sized enterprises,2024-12-29 10:05:47+00:00,"['Thomas Gaertner', 'Christoph Lippert', 'Stefan Konigorski']",econ.EM,"In response to the growing demand for accurate demand forecasts, this research proposes a generalized automated sales forecasting pipeline tailored for small- to medium-sized enterprises (SMEs). Unlike large corporations with dedicated data scientists for sales forecasting, SMEs often lack such resources. To address this, we developed a comprehensive forecasting pipeline that automates time series sales forecasting, encompassing data preparation, model training, and selection based on validation results.
  The development included two main components: model preselection and the forecasting pipeline. In the first phase, state-of-the-art methods were evaluated on a showcase dataset, leading to the selection of ARIMA, SARIMAX, Holt-Winters Exponential Smoothing, Regression Tree, Dilated Convolutional Neural Networks, and Generalized Additive Models. An ensemble prediction of these models was also included. Long-Short-Term Memory (LSTM) networks were excluded due to suboptimal prediction accuracy, and Facebook Prophet was omitted for compatibility reasons.
  In the second phase, the proposed forecasting pipeline was tested with SMEs in the food and electric industries, revealing variable model performance across different companies. While one project-based company derived no benefit, others achieved superior forecasts compared to naive estimators.
  Our findings suggest that no single model is universally superior. Instead, a diverse set of models, when integrated within an automated validation framework, can significantly enhance forecasting accuracy for SMEs. These results emphasize the importance of model diversity and automated validation in addressing the unique needs of each business. This research contributes to the field by providing SMEs access to state-of-the-art sales forecasting tools, enabling data-driven decision-making and improving operational efficiency."
http://arxiv.org/abs/2411.02085v4,Seesaw Experimentation: A/B Tests with Spillovers,2024-11-04 13:46:21+00:00,"['Jin Li', 'Ye Luo', 'Xiaowei Zhang']",econ.GN,"This paper examines how spillover effects in A/B testing can impede organizational progress and develops strategies for mitigating these challenges. We identify a phenomenon termed ``seesaw experimentation'', where a firm's overall performance paradoxically deteriorates despite achieving continuous improvements in measured A/B testing metrics. Seesaw experimentation arises when successful innovations in primary metrics generate negative externalities in secondary, unmeasured dimensions. To address this problem, we propose implementing a positive hurdle rate for A/B test approval. We derive the optimal hurdle rate, offering a simple solution that preserves decentralized experimentation while mitigating negative spillovers."
http://arxiv.org/abs/2411.13783v2,Process and Policy Insights from an Intercomparison of Open Electricity System Capacity Expansion Models,2024-11-21 01:53:23+00:00,"['Greg Schivley', 'Aurora Barone', 'Michael Blackhurst', 'Patricia Hidalgo-Gonzalez', 'Jesse Jenkins', 'Oleg Lugovoy', 'Qian Luo', 'Michael J. Roberts', 'Rangrang Zheng', 'Cameron Wade', 'Matthias Fripp']",econ.GN,"This study performs a detailed intercomparison of four open-source electricity capacity expansion models - Temoa, Switch, GenX, and USENSYS - to evaluate 1) how closely the results of these models align when inputs and configurations are harmonized, and 2) the degree to which varying model configurations affect outputs. We harmonize the inputs to each model using PowerGenome and use clearly defined scenarios (policy conditions) and configurations (model setup choices). This allows us to isolate how differences in model structure affect policy outcomes and investment decisions. Our framework allows each model to be tested on identical assumptions for policy, technology costs, and operational constraints, allowing us to focus on differences that arise from inherent model structures. Key findings highlight that, when harmonized, models produce very similar capacity portfolios under current policies and net-zero scenarios, with less than 1% difference in system costs for most configurations. This agreement among models allows us to focus on how configuration choices affect model results. For instance, configurations with unit commitment constraints or economic retirement yield different investments and system costs compared to simpler configurations. Our findings underscore the importance of aligning input data and transparently defining scenarios and configurations to provide robust policy insights."
http://arxiv.org/abs/2412.10662v2,On Prior Confidence and Belief Updating,2024-12-14 03:27:35+00:00,"['Kenneth Chan', 'Gary Charness', 'Chetan Dave', 'J. Lucas Reddinger']",econ.GN,"We experimentally investigate how confidence over multiple priors affects belief updating. Theory predicts that the average Bayesian posterior is unaffected by confidence over multiple priors if average priors are the same. We manipulate confidence by varying the time subjects view a black-and-white grid, the proportion representing the prior in a Bernoulli distribution. We find that when subjects view the grid for a longer duration, they have more confidence, under-update more, and place more (less) weight on priors (signals). Overall, confidence over multiple priors matters when it should not, while confidence in prior beliefs does not matter when it should."
http://arxiv.org/abs/2501.06270v1,Sectorial Exclusion Criteria in the Marxist Analysis of the Average Rate of Profit: The United States Case (1960-2020),2025-01-09 19:38:21+00:00,['Jose Mauricio Gomez Julian'],econ.GN,"The long-term estimation of the Marxist average rate of profit does not adhere to a theoretically grounded standard regarding which economic activities should or should not be included for such purposes, which is relevant because methodological non-uniformity can be a significant source of overestimation or underestimation, generating a less accurate reflection of the capital accumulation dynamics. This research aims to provide a standard Marxist decision criterion regarding the inclusion and exclusion of economic activities for the calculation of the Marxist average profit rate for the case of United States economic sectors from 1960 to 2020, based on the Marxist definition of productive labor, its location in the circuit of capital, and its relationship with the production of surplus value. Using wavelet-transformed Daubechies filters with increased symmetry, empirical mode decomposition, Hodrick-Prescott filter embedded in unobserved components model, and a wide variety of unit root tests the internal theoretical consistency of the presented criteria is evaluated. Also, the objective consistency of the theory is evaluated by a dynamic factor auto-regressive model, Principal Component Analysis, Singular Value Decomposition and Backward Elimination with Linear and Generalized Linear Models. The results are consistent both theoretically and econometrically with the logic of Marx's political economy."
http://arxiv.org/abs/2501.02609v2,Revealed Social Networks,2025-01-05 17:34:37+00:00,"['Christopher P. Chambers', 'Yusufcan Masatlioglu', 'Christopher Turansick']",econ.TH,"The linear-in-means model is the standard empirical model of peer effects. Using choice data and exogenous group variation, we first develop a revealed preference style test for the linear-in-means model. This test is formulated as a linear program and can be interpreted as a no money pump condition with an additional incentive compatibility constraint. We then study the identification properties of the linear-in-means model. A key takeaway from our analysis is that there is a close relationship between the dimension of the outcome variable and the identifiability of the model. Importantly, when the outcome variable is one-dimensional, failures of identification are generic. On the other hand, when the outcome variable is multi-dimensional, we provide natural conditions under which identification is generic."
http://arxiv.org/abs/2501.13721v4,A Non-Parametric Approach to Heterogeneity Analysis,2025-01-23 14:53:59+00:00,['Avner Seror'],econ.TH,"We develop a non-parametric methodology to quantify preference heterogeneity in consumer choices. By repeatedly sampling individual observations and partitioning agents into groups consistent with the Generalized Axiom of Revealed Preferences (GARP), we construct a similarity matrix capturing latent preference structures. Under mild assumptions, this matrix consistently and asymptotically normally estimates the probability that any pair of agents share a common utility function. Leveraging this, we develop hypothesis tests to assess whether demographic characteristics systematically explain unobserved heterogeneity. Simulations confirm the test's validity, and we apply the method to a standard grocery expenditure dataset."
http://arxiv.org/abs/2501.02674v1,Identifying the Hidden Nexus between Benford Law Establishment in Stock Market and Market Efficiency: An Empirical Investigation,2025-01-05 22:19:33+00:00,['M. R. Sarkandiz'],econ.GN,"Benford's law, or the law of the first significant digit, has been subjected to numerous studies due to its unique applications in financial fields, especially accounting and auditing. However, studies that addressed the law's establishment in the stock markets generally concluded that stock prices do not comply with the underlying distribution. The present research, emphasizing data randomness as the underlying assumption of Benford's law, has conducted an empirical investigation of the Warsaw Stock Exchange. The outcomes demonstrated that since stock prices are not distributed randomly, the law cannot be held in the stock market. Besides, the Chi-square goodness-of-fit test also supported the obtained results. Moreover, it is discussed that the lack of randomness originated from market inefficiency. In other words, violating the efficient market hypothesis has caused the time series non-randomness and the failure to establish Benford's law."
http://arxiv.org/abs/2502.17830v1,Certified Decisions,2025-02-25 04:19:51+00:00,"['Isaiah Andrews', 'Jiafeng Chen']",econ.EM,"Hypothesis tests and confidence intervals are ubiquitous in empirical research, yet their connection to subsequent decision-making is often unclear. We develop a theory of certified decisions that pairs recommended decisions with inferential guarantees. Specifically, we attach P-certificates -- upper bounds on loss that hold with probability at least $1-α$ -- to recommended actions. We show that such certificates allow ""safe,"" risk-controlling adoption decisions for ambiguity-averse downstream decision-makers. We further prove that it is without loss to limit attention to P-certificates arising as minimax decisions over confidence sets, or what Manski (2021) terms ""as-if decisions with a set estimate."" A parallel argument applies to E-certified decisions obtained from e-values in settings with unbounded loss."
http://arxiv.org/abs/2503.21808v2,Two Level Nested and Sequential Logit,2025-03-25 15:35:20+00:00,['Davide Luparello'],econ.EM,"This technical note provides comprehensive derivations of fundamental equations in two-level nested and sequential logit models for analyzing hierarchical choice structures. We present derivations of the Berry (1994) inversion formula, nested inclusive values computation, and multi-level market share equations, complementing existing literature. While conceptually distinct, nested and sequential logit models share mathematical similarities and, under specific distributional assumptions, yield identical inversion formulas-offering valuable analytical insights. These notes serve as a practical reference for researchers implementing multi-level discrete choice models in empirical applications, particularly in industrial organization and demand estimation contexts, and complement Mansley et al. (2019)."
http://arxiv.org/abs/2503.23569v2,Where the Trees Fall: Macroeconomic Forecasts for Forest-Reliant States,2025-03-30 19:28:33+00:00,"['Andrew Crawley', 'Adam Daigneault', 'Jonathan Gendron']",econ.GN,"Several key states in various regions of the U.S. have experienced recent sawtimber as well as pulp and paper mill closures, which raises an important policy question: how have and will key macroeconomic and industry specific indicators within the U.S. forest sector likely to change over time? This study provides empirical evidence to support forest-sector policy design by using a vector error correction (VEC) model to forecast economic trends in three major industries - forestry and logging, wood manufacturing, and paper manufacturing - across six of the most forest-dependent states found by the location quotient (LQ) measure: Alabama, Arkansas, Maine, Mississippi, Oregon, and Wisconsin. Overall, the results suggest a general decline in employment and the number of firms in the forestry and logging industry as well as the paper manufacturing industry, while wood manufacturing is projected to see modest employment gains. These results also offer key insights for regional policymakers, industry leaders, and local economic development officials: communities dependent on timber-based manufacturing may be more resilient than other forestry-based industries in the face of economic disruptions. Our findings can help prioritize targeted policy interventions and inform regional economic resilience strategies. We show distinct differences across forest-dependent industries and/or state sectors and geographies, highlighting that policies may have to be specific to each sector and/or geographical area. Finally, our VEC modeling framework is adaptable to other resource-dependent industries that serve as regional economic pillars such as mining, agriculture, and energy production offering a transferable tool for policy analysis in regions with similar economic structures."
http://arxiv.org/abs/2501.14623v1,"Quantitative Theory of Money or Prices? A Historical, Theoretical, and Econometric Analysis",2025-01-24 16:42:04+00:00,['Jose Mauricio Gomez Julian'],econ.EM,"This research studies the relation between money and prices and its practical implications analyzing quarterly data from United States (1959-2022), Canada (1961-2022), United Kingdom (1986-2022), and Brazil (1996-2022). The historical, logical, and econometric consistency of the logical core of the two main theories of money is analyzed using objective bayesian and frequentist machine learning models, bayesian regularized artificial neural networks, and ensemble learning. It is concluded that money is not neutral at any time horizon and that, despite money is ultimately subordinated to prices, there is a reciprocal influence over time between money and prices which constitute a complex system. Non-neutrality is transmitted through aggregate demand and is based on the exchange value of money as a monetary unit."
http://arxiv.org/abs/2502.07692v1,Are Princelings Truly Busted? Evaluating Transaction Discounts in China's Land Market,2025-02-11 16:46:27+00:00,['Julia Manso'],econ.GN,"This paper narrowly replicates Chen and Kung's 2019 paper ($The$ $Quarterly$ $Journal$ $of$ $Economics$ 134(1): 185-226). Inspecting the data reveals that nearly one-third of the transactions (388,903 out of 1,208,621) are perfect duplicates of other rows, excluding the transaction number. Replicating the analysis on the data sans-duplicates yields a slightly smaller but still statistically significant princeling effect, robust across the regression results. Further analysis also reveals that coefficients interpreted as the effect of logarithm of area actually reflect the effect of scaled values of area; this paper also reinterprets and contextualizes these results in light of the true scaled values."
http://arxiv.org/abs/2502.17271v1,Optimal Salaries of Researchers with Motivational Emergence,2025-02-24 15:54:32+00:00,['Eldar Knar'],econ.EM,"In the context of scientific policy and science management, this study examines the system of nonuniform wage distribution for researchers. A nonlinear mathematical model of optimal remuneration for scientific workers has been developed, considering key and additive aspects of scientific activity: basic qualifications, research productivity, collaborative projects, skill enhancement, distinctions, and international collaborations. Unlike traditional linear schemes, the proposed approach is based on exponential and logarithmic dependencies, allowing for the consideration of saturation effects and preventing artificial wage growth due to mechanical increases in scientific productivity indicators.
  The study includes detailed calculations of optimal, minimum, and maximum wages, demonstrating a fair distribution of remuneration on the basis of researcher productivity. A linear increase in publication activity or grant funding should not lead to uncontrolled salary growth, thus avoiding distortions in the motivational system. The results of this study can be used to reform and modernize the wage system for researchers in Kazakhstan and other countries, as well as to optimize grant-based science funding mechanisms. The proposed methodology fosters scientific motivation, long-term productivity, and the internationalization of research while also promoting self-actualization and ultimately forming an adequate and authentic reward system for the research community.
  Specifically, in resource-limited scientific systems, science policy should focus on the qualitative development of individual researchers rather than quantitative expansion (e.g., increasing the number of scientists). This can be achieved through the productive progress of their motivation and self-actualization."
http://arxiv.org/abs/2501.07615v1,Social and Genetic Ties Drive Skewed Cross-Border Media Coverage of Disasters,2025-01-13 11:24:52+00:00,"['Thiemo Fetzer', 'Prashant Garg']",econ.GN,"Climate change is increasing the frequency and severity of natural disasters worldwide. Media coverage of these events may be vital to generate empathy and mobilize global populations to address the common threat posed by climate change. Using a dataset of 466 news sources from 123 countries, covering 135 million news articles since 2016, we apply an event study framework to measure cross-border media activity following natural disasters. Our results shows that while media attention rises after disasters, it is heavily skewed towards certain events, notably earthquakes, accidents, and wildfires. In contrast, climatologically salient events such as floods, droughts, or extreme temperatures receive less coverage. This cross-border disaster reporting is strongly related to the number of deaths associated with the event, especially when the affected populations share strong social ties or genetic similarities with those in the reporting country. Achieving more balanced media coverage across different types of natural disasters may be essential to counteract skewed perceptions. Further, fostering closer social connections between countries may enhance empathy and mobilize the resources necessary to confront the global threat of climate change."
http://arxiv.org/abs/2502.07044v1,The Future of Work and Capital: Analyzing AGI in a CES Production Model,2025-02-10 21:24:04+00:00,['Pascal Stiefenhofer'],econ.GN,"The integration of Artificial General Intelligence (AGI) into economic production represents a transformative shift with profound implications for labor markets, income distribution, and technological growth. This study extends the Constant Elasticity of Substitution (CES) production function to incorporate AGI-driven labor and capital alongside traditional inputs, providing a comprehensive framework for analyzing AGI's economic impact.
  Four key models emerge from this framework. First, we examine the substitution and complementarity between AGI labor and human labor, identifying conditions under which AGI augments or displaces human workers. Second, we analyze how AGI capital accumulation influences wage structures and income distribution, highlighting potential disruptions to labor-based earnings. Third, we explore long-run equilibrium dynamics, demonstrating how an economy dominated by AGI capital may lead to the collapse of human wages and necessitate redistributive mechanisms. Finally, we assess the impact of AGI on total factor productivity, showing that technological growth depends on whether AGI serves as a complement to or a substitute for human labor.
  Our findings underscore the urgent need for policy interventions to ensure economic stability and equitable wealth distribution in an AGI-driven economy. Without appropriate regulatory measures, rising inequality and weakened aggregate demand could lead to economic stagnation despite technological advancements. Moreover this research suggests a renegoation of the Social Contract."
http://arxiv.org/abs/2502.07788v1,"Analysis of energy, CO2 emissions and economy of the technological migration for clean cooking in Ecuador",2025-01-20 12:51:17+00:00,"['J. Martinez', 'Jaime Marti-Herrero', 'S. Villacis', 'A. J. Riofrio', 'D. Vaca']",econ.GN,"The objective of this study is to analyze the CO2 emissions and economic impacts of the implementation of the National Efficient Cooking Program (NECP) in Ecuador, which aims to migrate the population from Liquefied Petroleum Gas (LPG)-based stoves to electric induction stoves. This program is rooted in the current effort to change Ecuador's energy balance, with hydroelectric power expected to generate 83.61% of national electricity by 2022, ending the need for subsidized LPG. For this analysis, the 2014 baseline situation has been compared with two future scenarios for 2022: a business-as-usual scenario and an NECP-success scenario. This study demonstrates the viability of migration from imported fossil fuels to locally-produced renewable energy as the basis for an efficient cooking facility. The new policies scenario would save US$ 1.162 billion in annual government expenditure on cooking subsidies, and reducing CO2 emissions associated to energy for cooking in 1.8 tCO2/y."
http://arxiv.org/abs/2502.08100v1,Sabotage and Free Riding in Contests with a Group-Specific Public-Good/Bad Prize,2025-02-12 03:55:42+00:00,"['Kyung Hwan Baik', 'Dongwoo Lee']",econ.TH,"We study contests in which two groups compete to win (or not to win) a group-specific public-good/bad prize. Each player in the groups can exert two types of effort: one to help her own group win the prize, and one to sabotage her own group's chances of winning it. The players in the groups choose their effort levels simultaneously and independently. We introduce a specific form of contest success function that determines each group's probability of winning the prize, taking into account players' sabotage activities. We show that two types of purestrategy Nash equilibrium occur, depending on parameter values: one without sabotage activities and one with sabotage activities. In the first type, only the highest-valuation player in each group expends positive effort, whereas, in the second type, only the lowest-valuation player in each group expends positive effort."
http://arxiv.org/abs/2502.05340v1,Robust valuation and optimal harvesting of forestry resources in the presence of catastrophe risk and parameter uncertainty,2025-02-07 21:24:27+00:00,"['Ankush Agarwal', 'Christian Ewald', 'Yihan Zou']",q-fin.MF,"We determine forest lease value and optimal harvesting strategies under model parameter uncertainty within stochastic bio-economic models that account for catastrophe risk. Catastrophic events are modeled as a Poisson point process, with a two-factor stochastic convenience yield model capturing the lumber spot price dynamics. Using lumber futures and US wildfire data, we estimate model parameters through a Kalman filter and maximum likelihood estimation and define the model parameter uncertainty set as the 95% confidence region. We numerically determine the forest lease value under catastrophe risk and parameter uncertainty using reflected backward stochastic differential equations (RBSDEs) and establish conservative and optimistic bounds for lease values and optimal stopping boundaries for harvesting, facilitating Monte Carlo simulations. Numerical experiments further explore how parameter uncertainty, catastrophe intensity, and carbon sequestration impact the lease valuation and harvesting decision. In particular, we explore the costs arising from this form of uncertainty in the form of a reduction of the lease value. These are implicit costs that can be attributed to climate risk and will be emphasized through the importance of forestry resources in the energy transition process. We conclude that in the presence of parameter uncertainty, it is better to lean toward a conservative strategy reflecting, to some extent, the worst case than being overly optimistic. Our results also highlight the critical role of convenience yield in determining optimal harvesting strategies."
http://arxiv.org/abs/2502.09265v1,Properties of Path-Independent Choice Correspondences and Their Applications to Efficient and Stable Matchings,2025-02-13 12:26:40+00:00,"['Keisuke Bando', 'Kenzo Imamura', 'Yasushi Kawase']",cs.GT,"Choice correspondences are crucial in decision-making, especially when faced with indifferences or ties. While tie-breaking can transform a choice correspondence into a choice function, it often introduces inefficiencies. This paper introduces a novel notion of path-independence (PI) for choice correspondences, extending the existing concept of PI for choice functions. Intuitively, a choice correspondence is PI if any consistent tie-breaking produces a PI choice function. This new notion yields several important properties. First, PI choice correspondences are rationalizabile, meaning they can be represented as the maximization of a utility function. This extends a core feature of PI in choice functions. Second, we demonstrate that the set of choices selected by a PI choice correspondence for any subset forms a generalized matroid. This property reveals that PI choice correspondences exhibit a nice structural property. Third, we establish that choice correspondences rationalized by ordinally concave functions inherently satisfy the PI condition. This aligns with recent findings that a choice function satisfies PI if and only if it can be rationalized by an ordinally concave function. Building on these theoretical foundations, we explore stable and efficient matchings under PI choice correspondences. Specifically, we investigate constrained efficient matchings, which are efficient (for one side of the market) within the set of stable matchings. Under responsive choice correspondences, such matchings are characterized by cycles. However, this cycle-based characterization fails in more general settings. We demonstrate that when the choice correspondence of each school satisfies both PI and monotonicity conditions, a similar cycle-based characterization is restored. These findings provide new insights into the matching theory and its practical applications."
http://arxiv.org/abs/2503.14631v1,An Ambiguous State Machine,2025-03-18 18:32:25+00:00,['Matt Stephenson'],econ.TH,"We show that a replicated state machine (such as a blockchain protocol) can retain liveness in a strategic setting even while facing substantial ambiguity over certain events. This is implemented by a complementary protocol called ""Machine II"", which generates a non-ergodic value within chosen intervals such that no limiting frequency can be observed. We show how to implement this machine algorithmically and how it might be applied strategically as a mechanism for ""veiling"" actions. We demonstrate that welfare-enhancing applications for veiling exist for users belonging to a wide class of ambiguity attitudes, e.g. Binmore (2016), Gul and Pesendorfer (2014). Our approach is illustrated with applications to forking disputes in blockchain oracles and to Constant Function Market Makers, allowing the protocol to retain liveness without exposing their users to sure-loss."
http://arxiv.org/abs/2503.14940v1,Linear programming approach to partially identified econometric models,2025-03-19 07:15:28+00:00,['Andrei Voronin'],econ.EM,"Sharp bounds on partially identified parameters are often given by the values of linear programs (LPs). This paper introduces a novel estimator of the LP value. Unlike existing procedures, our estimator is root-n-consistent, pointwise in the probability measure, whenever the population LP is feasible and finite. Our estimator is valid under point-identification, over-identifying constraints, and solution multiplicity. Turning to uniformity properties, we prove that the LP value cannot be uniformly consistently estimated without restricting the set of possible distributions. We then show that our estimator achieves uniform consistency under a condition that is minimal for the existence of any such estimator. We obtain computationally efficient, asymptotically normal inference procedure with exact asymptotic coverage at any fixed probability measure. To complement our estimation results, we derive LP sharp bounds in a general identification setting. We apply our findings to estimating returns to education. To that end, we propose the conditionally monotone IV assumption (cMIV) that tightens the classical monotone IV (MIV) bounds and is testable under a mild regularity condition. Under cMIV, university education in Colombia is shown to increase the average wage by at least $5.5\%$, whereas classical conditions fail to yield an informative bound."
http://arxiv.org/abs/2503.16126v1,"Income Inequality, Food Aid, and 'Zero Hunger': Evaluating Effectiveness During Lula's Administration",2025-03-20 13:16:44+00:00,['Bo Wu'],econ.GN,"Income inequality has been an important social issue that has attracted widespread attention. Taking the Zero Hunger Program in Brazil as a case study, this study analyzes the impact of policy changes on the income distribution of the Brazilian population during the implementation of the program using a breakpoint regression approach. The data for the study come from a variety of sources, including the Brazilian Ministry of Development, Social Assistance and the Family, and are analyzed using detailed descriptive statistics from the CEIC Brazil In-Depth Database. The results of the study show that during the Lula administration, Brazil's Zero Hunger Program substantially reduced income inequality, provided more substantial income security for the poor, and reduced the income gap between the rich and the poor. In terms of gender differences, the program led to a larger increase in the income of the male labor force at the right age compared to the female labor force, further highlighting the positive impact of the policy on the male labor force. These results are further confirmed by sensitivity analysis and provide useful lessons for subsequent policy formulation. This study's deep dive into the effects of the Zero Hunger program provides a valuable contribution to academic social science research and policy development."
http://arxiv.org/abs/2501.13324v1,Comparative Withholding Behavior Analysis of Historical Energy Storage Bids in California,2025-01-23 02:10:54+00:00,"['Neal Ma', 'Ningkun Zheng', 'Ning Qi', 'Bolun Xu']",eess.SY,"The rapid growth of battery energy storage in wholesale electricity markets calls for a deeper understanding of storage operators' bidding strategies and their market impacts. This study examines energy storage bidding data from the California Independent System Operator (CAISO) between July 1, 2023, and October 1, 2024, with a primary focus on economic withholding strategies. Our analysis reveals that storage bids are closely aligned with day-ahead and real-time market clearing prices, with notable bid inflation during price spikes. Statistical tests demonstrate a strong correlation between price spikes and capacity withholding, indicating that operators can anticipate price surges and use market volatility to increase profitability. Comparisons with optimal hindsight bids further reveal a clear daily periodic bidding pattern, highlighting extensive economic withholding. These results underscore potential market inefficiencies and highlight the need for refined regulatory measures to address economic withholding as storage capacity in the market continues to grow."
http://arxiv.org/abs/2503.00757v1,Wikipedia Contributions in the Wake of ChatGPT,2025-03-02 06:41:10+00:00,"['Liang Lyu', 'James Siderius', 'Hannah Li', 'Daron Acemoglu', 'Daniel Huttenlocher', 'Asuman Ozdaglar']",cs.HC,"How has Wikipedia activity changed for articles with content similar to ChatGPT following its introduction? We estimate the impact using differences-in-differences models, with dissimilar Wikipedia articles as a baseline for comparison, to examine how changes in voluntary knowledge contributions and information-seeking behavior differ by article content. Our analysis reveals that newly created, popular articles whose content overlaps with ChatGPT 3.5 saw a greater decline in editing and viewership after the November 2022 launch of ChatGPT than dissimilar articles did. These findings indicate heterogeneous substitution effects, where users selectively engage less with existing platforms when AI provides comparable content. This points to potential uneven impacts on the future of human-driven online knowledge contributions."
http://arxiv.org/abs/2503.03312v1,How manipulable are prediction markets?,2025-03-05 09:44:56+00:00,"['Itzhak Rasooly', 'Roberto Rozzi']",econ.GN,"In this paper, we conduct a large-scale field experiment to investigate the manipulability of prediction markets. The main experiment involves randomly shocking prices across 817 separate markets; we then collect hourly price data to examine whether the effects of these shocks persist over time. We find that prediction markets can be manipulated: the effects of our trades are visible even 60 days after they have occurred. However, as predicted by our model, the effects of the manipulations somewhat fade over time. Markets with more traders, greater trading volume, and an external source of probability estimates are harder to manipulate."
http://arxiv.org/abs/2503.02889v1,Function-Coherent Gambles with Non-Additive Sequential Dynamics,2025-02-22 14:58:20+00:00,['Gregory Wheeler'],econ.TH,"The desirable gambles framework provides a rigorous foundation for imprecise probability theory but relies heavily on linear utility via its coherence axioms. In our related work, we introduced function-coherent gambles to accommodate non-linear utility. However, when repeated gambles are played over time -- especially in intertemporal choice where rewards compound multiplicatively -- the standard additive combination axiom fails to capture the appropriate long-run evaluation. In this paper we extend the framework by relaxing the additive combination axiom and introducing a nonlinear combination operator that effectively aggregates repeated gambles in the log-domain. This operator preserves the time-average (geometric) growth rate and addresses the ergodicity problem. We prove the key algebraic properties of the operator, discuss its impact on coherence, risk assessment, and representation, and provide a series of illustrative examples. Our approach bridges the gap between expectation values and time averages and unifies normative theory with empirically observed non-stationary reward dynamics."
http://arxiv.org/abs/2503.01889v1,Non-Cooperative Games with Uncertainty,2025-02-27 14:31:57+00:00,['Jozsef Konczer'],econ.TH,"This paper introduces a framework for finite non-cooperative games where each player faces a globally uncertain parameter with no common prior. Every player chooses both a mixed strategy and projects an emergent subjective prior to the uncertain parameters. We define an ""Extended Equilibrium"" by requiring that no player can improve her expected utility via a unilateral change of strategy, and the emergent subjective priors are such that they maximize the expected regret of the players. A fixed-point argument -- based on Brouwer's fixed point theorem and mimicking the construction of Nash -- ensures existence. Additionally, the ""No Fictional Faith"" theorem shows that any subjective equilibrium prior must stay non-concentrated if the parameter truly matters to a player. This approach provides a framework that unifies regret-based statistical decision theory and game theory, yielding a tool for handling strategic decision-making in the presence of deeply uncertain parameters."
http://arxiv.org/abs/2503.13253v1,Investing in nature: Stakeholder's willingness to pay for Tunisian forest services,2025-03-17 15:10:49+00:00,['Islem Saadaoui'],econ.GN,"This study explores the economic value of Aleppo pine forests, a unique and threatened ecosystem in the border region of central Tunisia. These forests play a vital role in supporting small rural communities, but face increasing pressures and restrictions on their use. This research aims to assign a monetary value to forest conservation, considering the region's specific socio-economic context. Strategies for empowering local residents as key actors in developing sustainable cross-border initiatives are further investigated. Employing the contingent valuation method, a survey of 350 local residents and international users was conducted to assess their willigness to pay fo forest conservation efforts. Logistic regression analysis revealed that sociodemographic factors, such as monthly income and preferred payment method, significantly influence both and the likehood of participation. These findingd highlight the feasibility and importance of reconciling economic development with ecological sustainability in this critical region."
http://arxiv.org/abs/2503.13416v1,Correlation uncertainty: a decision-theoretic approach,2025-03-17 17:47:24+00:00,"['Gerrit Bauch', 'Lorenz Hartmann']",econ.TH,"We provide a decision-theoretic foundation for uncertainty about the correlation structure on a Cartesian product of probability spaces. Our contribution is two-fold: we first provide a full characterization of the set of possible correlations between subspaces as a convex polytope. Its extreme points are identified as the local maxima of mutual information and as maximally zero probability measures. Second, we derive an axiomatic characterization of preferences narrowing down the set of correlations a decision maker considers, making behavior about correlation testable. Thereby, she may regard collections of subspaces as independent from one another. We illustrate our model and results in simple examples on climate change, insurance and portfolio choice."
http://arxiv.org/abs/2503.09839v1,The E-Rule: A Novel Composite Indicator for Predicting Economic Recessions,2025-03-12 21:00:12+00:00,['Esmaeil Ebadi'],econ.GN,"This study develops the E-Rule, a novel composite recession indicator that integrates financial market and labor market signals to improve the precision of recession forecasting. Combining the yield curve and the Sahm rule, the E-Rule provides a holistic and early-warning measure of economic downturns. Using historical data from 1976 onward, we empirically evaluate the E-Rule's predictive power relative to traditional indicators. The analysis employs machine learning techniques, including logistic regression, support vector machines, gradient boosting, and random forests, to assess predictive accuracy. Our findings demonstrate that the E-Rule offers a superior lead time in forecasting recessions and improves stability over existing methods."
http://arxiv.org/abs/2502.14154v1,Ordinality in Random Allocation,2025-02-19 23:40:04+00:00,"['Eun Jeong Heo', 'Vikram Manjunath']",econ.TH,"In allocating objects via lotteries, it is common to consider ordinal rules that rely solely on how agents rank degenerate lotteries. While ordinality is often imposed due to cognitive or informational constraints, we provide another justification from an axiomatic perspective: for three-agent problems, the combination of efficiency, strategy-proofness, non-bossiness, and a weak form of continuity collectively implies ordinality."
http://arxiv.org/abs/2502.09569v1,Statistical Equilibrium of Optimistic Beliefs,2025-02-13 18:25:20+00:00,"['Yu Gui', 'Bahar Taşkesen']",econ.TH,"We introduce the Statistical Equilibrium of Optimistic Beliefs (SE-OB) for the mixed extension of finite normal-form games, drawing insights from discrete choice theory. Departing from the conventional best responders of Nash equilibrium and the better responders of quantal response equilibrium, we reconceptualize player behavior as that of optimistic better responders. In this setting, the players assume that their expected payoffs are subject to random perturbations, and form optimistic beliefs by selecting the distribution of perturbations that maximizes their highest anticipated payoffs among belief sets. In doing so, SE-OB subsumes and extends the existing equilibria concepts. The player's view of the existence of perturbations in their payoffs reflects an inherent risk sensitivity, and thus, each player is equipped with a risk-preference function for every action. We demonstrate that every Nash equilibrium of a game, where expected payoffs are regularized with the risk-preference functions of the players, corresponds to an SE-OB in the original game, provided that the belief sets coincide with the feasible set of a multi-marginal optimal transport problem with marginals determined by risk-preference functions. Building on this connection, we propose an algorithm for repeated games among risk-sensitive players under optimistic beliefs when only zeroth-order feedback is available. We prove that, under appropriate conditions, the algorithm converges to an SE-OB. Our convergence analysis offers key insights into the strategic behaviors for equilibrium attainment: a player's risk sensitivity enhances equilibrium stability, while forming optimistic beliefs in the face of ambiguity helps to mitigate overly aggressive strategies over time. As a byproduct, our approach delivers the first generic convergent algorithm for general-form structural QRE beyond the classical logit-QRE."
http://arxiv.org/abs/2502.09962v1,Strategyproof Maximum Matching under Dichotomous Agent Preferences,2025-02-14 07:39:01+00:00,"['Haris Aziz', 'Md. Shahidul Islam', 'Szilvia Pápai']",cs.GT,"We consider a two-sided matching problem in which the agents on one side have dichotomous preferences and the other side representing institutions has strict preferences (priorities). It captures several important applications in matching market design in which the agents are only interested in getting matched to an acceptable institution. These include centralized daycare assignment and healthcare rationing. We present a compelling new mechanism that satisfies many prominent and desirable properties including individual rationality, maximum size, fairness, Pareto-efficiency on both sides, strategyproofness on both sides, non-bossiness and having polynomial time running time. As a result, we answer an open problem whether there exists a mechanism that is agent-strategyproof, maximum, fair and non-bossy."
http://arxiv.org/abs/2502.12309v1,Eigenvalues in microeconomics,2025-02-17 20:30:52+00:00,['Benjamin Golub'],econ.TH,"Square matrices often arise in microeconomics, particularly in network models addressing applications from opinion dynamics to platform regulation. Spectral theory provides powerful tools for analyzing their properties. We present an accessible overview of several fundamental applications of spectral methods in microeconomics, focusing especially on the Perron-Frobenius Theorem's role and its connection to centrality measures. Applications include social learning, network games, public goods provision, and market intervention under uncertainty. The exposition assumes minimal social science background, using spectral theory as a unifying mathematical thread to introduce interested readers to some exciting current topics in microeconomic theory."
http://arxiv.org/abs/2502.04945v1,Estimating Parameters of Structural Models Using Neural Networks,2025-02-07 14:11:20+00:00,"['Yanhao', 'Wei', 'Zhenling Jiang']",econ.EM,"We study an alternative use of machine learning. We train neural nets to provide the parameter estimate of a given (structural) econometric model, for example, discrete choice or consumer search. Training examples consist of datasets generated by the econometric model under a range of parameter values. The neural net takes the moments of a dataset as input and tries to recognize the parameter value underlying that dataset. Besides the point estimate, the neural net can also output statistical accuracy. This neural net estimator (NNE) tends to limited-information Bayesian posterior as the number of training datasets increases. We apply NNE to a consumer search model. It gives more accurate estimates at lighter computational costs than the prevailing approach. NNE is also robust to redundant moment inputs. In general, NNE offers the most benefits in applications where other estimation approaches require very heavy simulation costs. We provide code at: https://nnehome.github.io."
http://arxiv.org/abs/2502.06499v1,Marginal Mechanisms For Balanced Exchange,2025-02-10 14:16:53+00:00,"['Vikram Manjunath', 'Alexander Westkamp']",econ.TH,"We consider the balanced exchange of bundles of indivisible goods. We are interested in mechanisms that only rely on marginal preferences over individual objects even though agents' actual preferences compare bundles. Such mechanisms play an important role in two-sided matching but have not received much attention in exchange settings. We show that individually rational and Pareto-efficient marginal mechanisms exist if and only if no agent ever ranks any of her endowed objects lower than in her second indifference class. We call such marginal preferences trichotomous. In proving sufficiency, we define mechanisms, which are constrained versions of serial dictatorship, that achieve both desiderata based only agents' marginal preferences.
  We then turn to strategy-proofness. An individually rational, efficient and strategy-proof mechanism-marginal or not-exists if and only if each agent's marginal preference is not only trichotomous, but does not contain a non-endowed object in her second indifference class. We call such marginal preferences strongly trichotomous. For such preferences, our mechanisms reduce to the class of strategy-proof mechanisms introduced in Manjunath and Westkamp (2018). For trichotomous preferences, while our variants of serial dictatorship are not strategy-proof, they are truncation-proof and not obviously manipulable (Troyan and Morrill, 2020)."
http://arxiv.org/abs/2502.06157v1,Weak independence of irrelevant alternatives and generalized Nash bargaining solutions,2025-02-10 05:02:02+00:00,['Kensei Nakamura'],econ.TH,"In Nash's (1950) seminal result, independence of irrelevant alternatives (IIA) plays a central role, but it has long been a subject of criticism in axiomatic bargaining theory. This paper examines the implication of a weak version of IIA in multi-valued bargaining solutions defined on non-convex bargaining problems. We show that if a solution satisfies weak IIA together with standard axioms, it can be represented, like the Nash solution, using weighted products of normalized utility levels. In this representation, the weight assigned to players for evaluating each agreement is determined endogenously through a two-stage optimization process. These solutions bridge the two dominant solution concepts, the Nash solution and the Kalai-Smorodinsky solution (Kalai and Smorodinsky, 1975). Furthermore, we consider special cases of these solutions in the context of bargaining over linear production technologies."
http://arxiv.org/abs/2502.06387v1,How Humans Help LLMs: Assessing and Incentivizing Human Preference Annotators,2025-02-10 12:15:27+00:00,"['Shang Liu', 'Hanzhao Wang', 'Zhongyao Ma', 'Xiaocheng Li']",cs.LG,"Human-annotated preference data play an important role in aligning large language models (LLMs). In this paper, we investigate the questions of assessing the performance of human annotators and incentivizing them to provide high-quality annotations. The quality assessment of language/text annotation faces two challenges: (i) the intrinsic heterogeneity among annotators, which prevents the classic methods that assume the underlying existence of a true label; and (ii) the unclear relationship between the annotation quality and the performance of downstream tasks, which excludes the possibility of inferring the annotators' behavior based on the model performance trained from the annotation data. Then we formulate a principal-agent model to characterize the behaviors of and the interactions between the company and the human annotators. The model rationalizes a practical mechanism of a bonus scheme to incentivize annotators which benefits both parties and it underscores the importance of the joint presence of an assessment system and a proper contract scheme. From a technical perspective, our analysis extends the existing literature on the principal-agent model by considering a continuous action space for the agent. We show the gap between the first-best and the second-best solutions (under the continuous action space) is of $Θ(1/\sqrt{n \log n})$ for the binary contracts and $Θ(1/n)$ for the linear contracts, where $n$ is the number of samples used for performance assessment; this contrasts with the known result of $\exp(-Θ(n))$ for the binary contracts when the action space is discrete. Throughout the paper, we use real preference annotation data to accompany our discussions."
http://arxiv.org/abs/2502.16524v2,The Endurance of Identity-Based Voting: Evidence from the United States and Comparative Democracies,2025-02-23 10:07:50+00:00,"['Venkat Ram Reddy Ganuthula', 'Krishna Kumar Balaraman']",econ.GN,"This study demonstrates the persistent dominance of identity based voting across democratic systems, using the United States as a primary case and comparative analyses of 19 other democracies as counterfactuals. Drawing solely on election data from the Roper Center (1976 through recent cycles), we employ OLS regression, ANOVA, and correlation tests to show that race remains the strongest predictor of party affiliation in the US (p < 0.001), with White voters favoring Republicans and Black voters consistently supporting Democrats (85% since 1988). Income, education, and gender exemplified by gaps like 10 points in 2020 further shape voting patterns, yet racial identity predominates. Comparative evidence from majoritarian (e.g., India), proportional (e.g., Germany through 2025), and hybrid (e.g., South Korea with a 25 point gender gap) systems reveals no democracy where issue based voting fully supplants identity based voting. Digital mobilization amplifies this trend globally. These findings underscore identity enduring role in electoral behavior, challenging assumptions of policy driven democratic choice."
http://arxiv.org/abs/2502.12967v1,Imputation Strategies for Rightcensored Wages in Longitudinal Datasets,2025-02-18 15:47:53+00:00,"['Jörg Drechsler', 'Johannes Ludsteck']",econ.EM,"Censoring from above is a common problem with wage information as the reported wages are typically top-coded for confidentiality reasons. In administrative databases the information is often collected only up to a pre-specified threshold, for example, the contribution limit for the social security system. While directly accounting for the censoring is possible for some analyses, the most flexible solution is to impute the values above the censoring point. This strategy offers the advantage that future users of the data no longer need to implement possibly complicated censoring estimators. However, standard cross-sectional imputation routines relying on the classical Tobit model to impute right-censored data have a high risk of introducing bias from uncongeniality (Meng, 1994) as future analyses to be conducted on the imputed data are unknown to the imputer. Furthermore, as we show using a large-scale administrative database from the German Federal Employment agency, the classical Tobit model offers a poor fit to the data. In this paper, we present some strategies to address these problems. Specifically, we use leave-one-out means as suggested by Card et al. (2013) to avoid biases from uncongeniality and rely on quantile regression or left censoring to improve the model fit. We illustrate the benefits of these modeling adjustments using the German Structure of Earnings Survey, which is (almost) unaffected by censoring and can thus serve as a testbed to evaluate the imputation procedures."
http://arxiv.org/abs/2502.20706v1,Natural Asset Beta,2025-02-28 04:28:12+00:00,['Daniel Grainger'],econ.GN,"Natural capital accounting is important to efforts that attempt to measure the value of nature to decide on how best to trade-off natural resource productive use and conservation. Much work on measuring reciprocal physical stocks and flows of natural resources between nature and the economy has occurred. However, the current open problem of translating natural resource physical stocks and flows into monetary ones presents a barrier to estimating opportunity costs to inform resource allocation decisions. The research reported here extends a theoretical model on economic sustainability and derives a novel asset beta notion for nature. Like traditional firm asset beta notions, this underpins corporate finance notions for nature. Supply and demand curves estimating the exchange value of the natural resource are possible. A real-world data set is utilised for a preliminary proof of concept test. The results show promise, but future research is suggested investigating the sensitivity of the estimates."
http://arxiv.org/abs/2502.16536v1,Bounded Foresight Equilibrium in Large Dynamic Economies with Heterogeneous Agents and Aggregate Shocks,2025-02-23 11:10:47+00:00,"['Bilal Islah', 'Bar Light']",econ.GN,"Large dynamic economies with heterogeneous agents and aggregate shocks are central to many important applications, yet their equilibrium analysis remains computationally challenging. This is because the standard solution approach, rational expectations equilibria require agents to predict the evolution of the full cross-sectional distribution of state variables, leading to an extreme curse of dimensionality. In this paper, we introduce a novel equilibrium concept, N-Bounded Foresight Equilibrium (N-BFE), and establish its existence under mild conditions. In N-BFE, agents optimize over an infinite horizon but form expectations about key economic variables only for the next N periods. Beyond this horizon, they assume that economic variables remain constant and use a predetermined continuation value. This equilibrium notion reduces computational complexity and draws a direct parallel to lookahead policies in reinforcement learning, where agents make near-term calculations while relying on approximate valuations beyond a computationally feasible horizon. At the same time, it lowers cognitive demands on agents while better aligning with the behavioral literature by incorporating time inconsistency and limited attention, all while preserving desired forward-looking behavior and ensuring that agents still respond to policy changes. Importantly, in N-BFE equilibria, forecast errors arise endogenously. We measure the foresight errors for different foresight horizons and show that foresight significantly influences the variation in endogenous equilibrium variables, distinguishing our findings from traditional risk aversion or precautionary savings channels. This variation arises from a feedback mechanism between individual decision-making and equilibrium variables, where increased foresight induces greater non-stationarity in agents' decisions and, consequently, in economic variables."
http://arxiv.org/abs/2502.01548v2,"Comment on ""Sequential validation of treatment heterogeneity"" and ""Comment on generic machine learning inference on heterogeneous treatment effects in randomized experiments""",2025-02-03 17:30:06+00:00,"['Victor Chernozhukov', 'Mert Demirer', 'Esther Duflo', 'Iván Fernández-Val']",econ.EM,"We warmly thank Kosuke Imai, Michael Lingzhi Li, and Stefan Wager for their gracious and insightful comments. We are particularly encouraged that both pieces recognize the importance of the research agenda the lecture laid out, which we see as critical for applied researchers. It is also great to see that both underscore the potential of the basic approach we propose - targeting summary features of the CATE after proxy estimation with sample splitting. We are also happy that both papers push us (and the reader) to continue thinking about the inference problem associated with sample splitting. We recognize that our current paper is only scratching the surface of this interesting agenda. Our proposal is certainly not the only option, and it is exciting that both papers provide and assess alternatives. Hopefully, this will generate even more work in this area."
http://arxiv.org/abs/2502.13868v1,"Locally Robust Policy Learning: Inequality, Inequality of Opportunity and Intergenerational Mobility",2025-02-19 16:48:41+00:00,['Joël Terschuur'],econ.EM,"Policy makers need to decide whether to treat or not to treat heterogeneous individuals. The optimal treatment choice depends on the welfare function that the policy maker has in mind and it is referred to as the policy learning problem. I study a general setting for policy learning with semiparametric Social Welfare Functions (SWFs) that can be estimated by locally robust/orthogonal moments based on U-statistics. This rich class of SWFs substantially expands the setting in Athey and Wager (2021) and accommodates a wider range of distributional preferences. Three main applications of the general theory motivate the paper: (i) Inequality aware SWFs, (ii) Inequality of Opportunity aware SWFs and (iii) Intergenerational Mobility SWFs. I use the Panel Study of Income Dynamics (PSID) to assess the effect of attending preschool on adult earnings and estimate optimal policy rules based on parental years of education and parental income."
http://arxiv.org/abs/2502.13461v1,"Tensor dynamic conditional correlation model: A new way to pursuit ""Holy Grail of investing""",2025-02-19 06:19:59+00:00,"['Cheng Yu', 'Zhoufan Zhu', 'Ke Zhu']",q-fin.PM,"Style investing creates asset classes (or the so-called ""styles"") with low correlations, aligning well with the principle of ""Holy Grail of investing"" in terms of portfolio selection. The returns of styles naturally form a tensor-valued time series, which requires new tools for studying the dynamics of the conditional correlation matrix to facilitate the aforementioned principle. Towards this goal, we introduce a new tensor dynamic conditional correlation (TDCC) model, which is based on two novel treatments: trace-normalization and dimension-normalization. These two normalizations adapt to the tensor nature of the data, and they are necessary except when the tensor data reduce to vector data. Moreover, we provide an easy-to-implement estimation procedure for the TDCC model, and examine its finite sample performance by simulations. Finally, we assess the usefulness of the TDCC model in international portfolio selection across ten global markets and in large portfolio selection for 1800 stocks from the Chinese stock market."
http://arxiv.org/abs/2502.13438v1,Balancing Flexibility and Interpretability: A Conditional Linear Model Estimation via Random Forest,2025-02-19 05:22:12+00:00,"['Ricardo Masini', 'Marcelo Medeiros']",stat.ME,"Traditional parametric econometric models often rely on rigid functional forms, while nonparametric techniques, despite their flexibility, frequently lack interpretability. This paper proposes a parsimonious alternative by modeling the outcome $Y$ as a linear function of a vector of variables of interest $\boldsymbol{X}$, conditional on additional covariates $\boldsymbol{Z}$. Specifically, the conditional expectation is expressed as $\mathbb{E}[Y|\boldsymbol{X},\boldsymbol{Z}]=\boldsymbol{X}^{T}\boldsymbolβ(\boldsymbol{Z})$, where $\boldsymbolβ(\cdot)$ is an unknown Lipschitz-continuous function. We introduce an adaptation of the Random Forest (RF) algorithm to estimate this model, balancing the flexibility of machine learning methods with the interpretability of traditional linear models. This approach addresses a key challenge in applied econometrics by accommodating heterogeneity in the relationship between covariates and outcomes. Furthermore, the heterogeneous partial effects of $\boldsymbol{X}$ on $Y$ are represented by $\boldsymbolβ(\cdot)$ and can be directly estimated using our proposed method. Our framework effectively unifies established parametric and nonparametric models, including varying-coefficient, switching regression, and additive models. We provide theoretical guarantees, such as pointwise and $L^p$-norm rates of convergence for the estimator, and establish a pointwise central limit theorem through subsampling, aiding inference on the function $\boldsymbolβ(\cdot)$. We present Monte Carlo simulation results to assess the finite-sample performance of the method."
http://arxiv.org/abs/2502.13431v1,Functional Network Autoregressive Models for Panel Data,2025-02-19 05:06:03+00:00,"['Tomohiro Ando', 'Tadao Hoshino']",stat.ME,"This study proposes a novel functional vector autoregressive framework for analyzing network interactions of functional outcomes in panel data settings. In this framework, an individual's outcome function is influenced by the outcomes of others through a simultaneous equation system. To estimate the functional parameters of interest, we need to address the endogeneity issue arising from these simultaneous interactions among outcome functions. This issue is carefully handled by developing a novel functional moment-based estimator. We establish the consistency, convergence rate, and pointwise asymptotic normality of the proposed estimator. Additionally, we discuss the estimation of marginal effects and impulse response analysis. As an empirical illustration, we analyze the demand for a bike-sharing service in the U.S. The results reveal statistically significant spatial interactions in bike availability across stations, with interaction patterns varying over the time of day."
http://arxiv.org/abs/2502.13410v1,Tell Me Why: Incentivizing Explanations,2025-02-19 03:47:34+00:00,"['Siddarth Srinivasan', 'Ezra Karger', 'Michiel Bakker', 'Yiling Chen']",cs.GT,"Common sense suggests that when individuals explain why they believe something, we can arrive at more accurate conclusions than when they simply state what they believe. Yet, there is no known mechanism that provides incentives to elicit explanations for beliefs from agents. This likely stems from the fact that standard Bayesian models make assumptions (like conditional independence of signals) that preempt the need for explanations, in order to show efficient information aggregation. A natural justification for the value of explanations is that agents' beliefs tend to be drawn from overlapping sources of information, so agents' belief reports do not reveal all that needs to be known. Indeed, this work argues that rationales-explanations of an agent's private information-lead to more efficient aggregation by allowing agents to efficiently identify what information they share and what information is new. Building on this model of rationales, we present a novel 'deliberation mechanism' to elicit rationales from agents in which truthful reporting of beliefs and rationales is a perfect Bayesian equilibrium."
http://arxiv.org/abs/2501.16996v4,Artificial Intelligence Clones,2025-01-28 14:51:54+00:00,['Annie Liang'],econ.TH,"Large language models, trained on personal data, may soon be able to mimic individual personalities. These ``AI clones'' or ``AI agents'' have the potential to transform how people search over one another in contexts ranging from marriage to employment -- indeed, several dating platforms have already begun using AI clones to evaluate potential pairings between users. This paper presents a theoretical framework to study the tradeoff between the substantially expanded search capacity of AI clones, and their imperfect representation of humans. Individual personalities are modeled as points in $k$-dimensional Euclidean space, and their AI clones are modeled as noisy approximations of these personalities. I compare two search regimes: an ``in-person regime'' -- where each person randomly meets some number of individuals and matches to the most compatible among them -- against an ``AI representation regime'' -- in which individuals match to the person whose AI clone is most compatible with their AI clone. I show that a finite number of in-person encounters exceeds the expected payoff from search over infinite AI clones. Moreover, when the dimensionality of personality is large, simply meeting two people in person produces a better expected match than entrusting the process to an AI platform, regardless of the size of its candidate pool."
http://arxiv.org/abs/2503.23501v1,Forward Selection Fama-MacBeth Regression with Higher-Order Asset Pricing Factors,2025-03-30 16:20:51+00:00,"['Nicola Borri', 'Denis Chetverikov', 'Yukun Liu', 'Aleh Tsyvinski']",econ.EM,"We show that the higher-orders and their interactions of the common sparse linear factors can effectively subsume the factor zoo. To this extend, we propose a forward selection Fama-MacBeth procedure as a method to estimate a high-dimensional stochastic discount factor model, isolating the most relevant higher-order factors. Applying this approach to terms derived from six widely used factors (the Fama-French five-factor model and the momentum factor), we show that the resulting higher-order model with only a small number of selected higher-order terms significantly outperforms traditional benchmarks both in-sample and out-of-sample. Moreover, it effectively subsumes a majority of the factors from the extensive factor zoo, suggesting that the pricing power of most zoo factors is attributable to their exposure to higher-order terms of common linear factors."
http://arxiv.org/abs/2501.05221v1,RUM-NN: A Neural Network Model Compatible with Random Utility Maximisation for Discrete Choice Setups,2025-01-09 13:18:41+00:00,"['Niousha Bagheri', 'Milad Ghasri', 'Michael Barlow']",econ.EM,"This paper introduces a framework for capturing stochasticity of choice probabilities in neural networks, derived from and fully consistent with the Random Utility Maximization (RUM) theory, referred to as RUM-NN. Neural network models show remarkable performance compared with statistical models; however, they are often criticized for their lack of transparency and interoperability. The proposed RUM-NN is introduced in both linear and nonlinear structures. The linear RUM-NN retains the interpretability and identifiability of traditional econometric discrete choice models while using neural network-based estimation techniques. The nonlinear RUM-NN extends the model's flexibility and predictive capabilities to capture nonlinear relationships between variables within utility functions. Additionally, the RUM-NN allows for the implementation of various parametric distributions for unobserved error components in the utility function and captures correlations among error terms. The performance of RUM-NN in parameter recovery and prediction accuracy is rigorously evaluated using synthetic datasets through Monte Carlo experiments. Additionally, RUM-NN is evaluated on the Swissmetro and the London Passenger Mode Choice (LPMC) datasets with different sets of distribution assumptions for the error component. The results demonstrate that RUM-NN under a linear utility structure and IID Gumbel error terms can replicate the performance of the Multinomial Logit (MNL) model, but relaxing those constraints leads to superior performance for both Swissmetro and LPMC datasets. By introducing a novel estimation approach aligned with statistical theories, this study empowers econometricians to harness the advantages of neural network models."
http://arxiv.org/abs/2501.01763v1,"Quantifying A Firm's AI Engagement: Constructing Objective, Data-Driven, AI Stock Indices Using 10-K Filings",2025-01-03 11:27:49+00:00,"['Lennart Ante', 'Aman Saggu']",q-fin.GN,"Following an analysis of existing AI-related exchange-traded funds (ETFs), we reveal the selection criteria for determining which stocks qualify as AI-related are often opaque and rely on vague phrases and subjective judgments. This paper proposes a new, objective, data-driven approach using natural language processing (NLP) techniques to classify AI stocks by analyzing annual 10-K filings from 3,395 NASDAQ-listed firms between 2011 and 2023. This analysis quantifies each company's engagement with AI through binary indicators and weighted AI scores based on the frequency and context of AI-related terms. Using these metrics, we construct four AI stock indices-the Equally Weighted AI Index (AII), the Size-Weighted AI Index (SAII), and two Time-Discounted AI Indices (TAII05 and TAII5X)-offering different perspectives on AI investment. We validate our methodology through an event study on the launch of OpenAI's ChatGPT, demonstrating that companies with higher AI engagement saw significantly greater positive abnormal returns, with analyses supporting the predictive power of our AI measures. Our indices perform on par with or surpass 14 existing AI-themed ETFs and the Nasdaq Composite Index in risk-return profiles, market responsiveness, and overall performance, achieving higher average daily returns and risk-adjusted metrics without increased volatility. These results suggest our NLP-based approach offers a reliable, market-responsive, and cost-effective alternative to existing AI-related ETF products. Our innovative methodology can also guide investors, asset managers, and policymakers in using corporate data to construct other thematic portfolios, contributing to a more transparent, data-driven, and competitive approach."
http://arxiv.org/abs/2501.02468v1,"The Explore of Knowledge Management Dynamic Capabilities, AI-Driven Knowledge Sharing, Knowledge-Based Organizational Support, and Organizational Learning on Job Performance: Evidence from Chinese Technological Companies",2025-01-05 07:39:32+00:00,['Jun Cui'],econ.GN,"Drawing upon Resource-Based Theory (RBT) and the Knowledge-Based View (KBV), this study investigates the impact of Knowledge-Based Organizational Support (KOS), AI-Driven Knowledge Sharing (KS), Organizational Learning (OL), and Knowledge Management Dynamic Capabilities (KMDC) on Organizational Performance (OP) in Chinese firms. In particular, this research explores the relationships among these factors, alongside control variables such as education level, staff skills, and technological innovation, to provide a comprehensive understanding of their influence on performance management. While recent studies on organizational performance have predominantly concentrated on digital business strategies and high-level decision-making, limited attention has been given to the role of digital maturity, workplace activities, and communication-related dynamics. This study addresses these gaps by consolidating critical factors that contribute to overarching job performance within organizations. Moreover, to empirically test the proposed hypotheses, data were collected from 129 valid questionnaires completed by employees across various Chinese firms. The research employed confirmatory factor analysis (CFA) to validate the measurement constructs and structural equation modeling (SEM) to evaluate the hypothesized relationships. The findings reveal several significant insights: (1) KOS, KS with AI, KMDC, and OL each have a direct positive effect on OP, emphasizing their critical roles in enhancing organizational outcomes. (2) Control variables, including education level, staff skills, and technological innovation, significantly moderate the relationships between KOS, KS with AI, KMDC, OL, and OP, further amplifying their impact."
http://arxiv.org/abs/2501.04051v1,Trust of Strangers: a framework for analysis,2025-01-06 20:52:01+00:00,['Shawn Berry'],econ.GN,"Trust among people is essential to ensure collaboration, social network building, transactions, and the development and engagement of new audiences for brand promotion or social causes. In Berry (2024), the trust attitudes of respondents toward strangers on the street, other groups of people, and information sources were measured. This study evaluates the trust of strangers using a 5-factor structural equation model. The analysis yielded a robust model with four of five factors and all variables being statistically significant, with social trust and institutional trust yielding the greatest positive effect on trust of strangers on the street. While demographic characteristics had a small positive effect, the trust of friends and family had a mild negative effect on the trust of strangers on the street. Trust of information sources was not statistically significant and had a negligible positive effect on the trust of strangers. The results also indicate that almost 48% of respondents distrust strangers on the street, implying that trust is not automatically endowed. Directions for future research and implications for business and social causes are discussed."
http://arxiv.org/abs/2501.00800v1,The Impact of Socio-Economic Challenges and Technological Progress on Economic Inequality: An Estimation with the Perelman Model and Ricci Flow Methods,2025-01-01 10:52:21+00:00,['Davit Gondauri'],econ.EM,"The article examines the impact of 16 key parameters of the Georgian economy on economic inequality, using the Perelman model and Ricci flow mathematical methods. The study aims to conduct a deep analysis of the impact of socio-economic challenges and technological progress on the dynamics of the Gini coefficient. The article examines the following parameters: income distribution, productivity (GDP per hour), unemployment rate, investment rate, inflation rate, migration (net negative), education level, social mobility, trade infrastructure, capital flows, innovative activities, access to healthcare, fiscal policy (budget deficit), international trade (turnover relative to GDP), social protection programs, and technological access. The results of the study confirm that technological innovations and social protection programs have a positive impact on reducing inequality. Productivity growth, improving the quality of education, and strengthening R&D investments increase the possibility of inclusive development. Sensitivity analysis shows that social mobility and infrastructure are important factors that affect economic stability. The accuracy of the model is confirmed by high R^2 values (80-90%) and the statistical reliability of the Z-statistic (<0.05). The study uses Ricci flow methods, which allow for a geometric analysis of the transformation of economic parameters in time and space. Recommendations include the strategic introduction of technological progress, the expansion of social protection programs, improving the quality of education, and encouraging international trade, which will contribute to economic sustainability and reduce inequality. The article highlights multifaceted approaches that combine technological innovation and responses to socio-economic challenges to ensure sustainable and inclusive economic development."
http://arxiv.org/abs/2501.07880v1,The Impact of Digitalisation and Sustainability on Inclusiveness: Inclusive Growth Determinants,2025-01-14 06:42:28+00:00,"['Radu Rusu', 'Camelia Oprean-Stan']",econ.EM,"Inclusiveness and economic development have been slowed by the pandemics and military conflicts. This study investigates the main determinants of inclusiveness at the European level. A multi-method approach is used, with Principal Component Analysis (PCA) applied to create the Inclusiveness Index and Generalised Method of Moments (GMM) analysis used to investigate the determinants of inclusiveness. The data comprises a range of 22 years, from 2000 to 2021, for 32 European countries. The determinants of inclusiveness and their effects were identified. First, economic growth, industrial upgrading, electricity consumption, digitalisation, and the quantitative aspect of governance, all have a positive impact on inclusive growth in Europe. Second, the level of CO2 emissions and inflation have a negative impact on inclusiveness. Tomorrow's inclusive and sustainable growth must include investments in renewable energy, digital infrastructure, inequality policies, sustainable governance, human capital, and inflation management. These findings can help decision makers design inclusive growth policies."
http://arxiv.org/abs/2501.09540v1,Convergence Rates of GMM Estimators with Nonsmooth Moments under Misspecification,2025-01-16 13:44:38+00:00,"['Byunghoon Kang', 'Seojeong Lee', 'Juha Song']",econ.EM,"The asymptotic behavior of GMM estimators depends critically on whether the underlying moment condition model is correctly specified. Hong and Li (2023, Econometric Theory) showed that GMM estimators with nonsmooth (non-directionally differentiable) moment functions are at best $n^{1/3}$-consistent under misspecification. Through simulations, we verify the slower convergence rate of GMM estimators in such cases. For the two-step GMM estimator with an estimated weight matrix, our results align with theory. However, for the one-step GMM estimator with the identity weight matrix, the convergence rate remains $\sqrt{n}$, even under severe misspecification."
http://arxiv.org/abs/2503.23141v1,Manipulation of positional social choice correspondences under incomplete information,2025-03-29 16:27:45+00:00,"['Raffaele Berzi', 'Daniela Bubboloni', 'Michele Gori']",econ.TH,"We study the manipulability of social choice correspondences in situations where individuals have incomplete information about others' preferences. We propose a general concept of manipulability that depends on the extension rule used to derive preferences over sets of alternatives from preferences over alternatives, as well as on individuals' level of information. We then focus on the manipulability of social choice correspondences when the Kelly extension rule is used, and individuals are assumed to have the capability to anticipate the outcome of the collective decision. Under these assumptions, we introduce some monotonicity properties of social choice correspondences whose combined satisfaction is sufficient for manipulability, prove a result of manipulability for unanimous positional social choice correspondences, and present a detailed analysis of the manipulability properties for the Borda, the Plurality and the Negative Plurality social choice correspondences."
http://arxiv.org/abs/2503.22928v1,Optimal Control of an Epidemic with Intervention Design,2025-03-29 01:21:16+00:00,['Behrooz Moosavi Ramezanzadeh'],math.OC,"In this paper, I propose a controlled SEIR model that advances epidemic management through optimal control theory. I improve the traditional framework by incorporating practical intervention constraints and economic considerations. Approaching this problem using modern methods of calculus of variations, I first conduct a rigorous mathematical analysis of the controlled system. Then, I formulate an infinite time horizon control problem and investigate its mathematical connections with finite time, setting the stage for applying the Hamiltonian procedure."
http://arxiv.org/abs/2501.08802v2,On the Dominance of Truth-Telling in Gradual Mechanisms,2025-01-15 13:56:14+00:00,"['Wenqian Wang', 'Zhiwen Zheng']",econ.TH,"Recent literature highlights the advantages of implementing social rules via dynamic game forms. We characterize when truth-telling remains a dominant strategy in gradual mechanisms implementing strategy-proof social rules, where agents gradually reveal their private information while acquiring information about others in the process. Our first characterization hinges on the incentive-preservation of a basic transformation on gradual mechanisms called illuminating that partitions information sets. The second relies on a single reaction-proofness condition. We demonstrate the usefulness of both characterizations through applications to second-price auctions and the top-trading cycles algorithm."
http://arxiv.org/abs/2502.06015v4,Critical Mathematical Economics and Progressive Data Science,2025-02-09 20:13:38+00:00,['Johannes Buchner'],econ.GN,"The aim of this article is to present elements and discuss the potential of a research program at the intersection between mathematics and heterodox economics, which we call Criticial Mathematical Economics (CME). We propose to focus on the mathematical and model-theoretic foundations of controversies in economic policy, and aim at providing an entrance to the literature as an invitation to mathematicians that are potentially interested in such a project.
  From our point of view, mathematics has been partly misused in mainstream economics to justify `unregulated markets'. We identify two key parts of CME, which leads to a natural structure of this article: The first part focusses on an analysis and critique of mathematical models used in mainstream economics, like e.g. the Dynamic Stochastic General Equilibrium (DSGE) in Macroeconomics and the so-called ``Sonnenschein-Mantel-Debreu''-Theorems.
  The aim of the second part is to improve and extend heterodox models using ingredients from modern mathematics and computer science, a method with strong relation to Complexity Economics. We exemplify this idea by describing how methods from Non-Linear Dynamics have been used in Post-Keynesian Macroeconomics', and also discuss (Pseudo-) Goodwin cycles and possible Micro- and Mesofoundations.
  Finally, we outline in which areas a collaboration between mathematicians and heterodox economists could be most promising, and discuss both existing projects in such a direction as well as areas where new models for policy advice are most needed. In an outlook, we discuss the role of (ecological) data, and the need for what we call Progressive Data Science."
http://arxiv.org/abs/2503.24063v2,A robot-assisted pipeline to rapidly scan 1.7 million historical aerial photographs,2025-03-31 13:23:05+00:00,"['Sheila Masson', 'Alan Potts', 'Allan Williams', 'Steve Berggreen', 'Kevin McLaren', 'Sam Martin', 'Eugenio Noda', 'Nicklas Nordfors', 'Nic Ruecroft', 'Hannah Druckenmiller', 'Solomon Hsiang', 'Andreas Madestam', 'Anna Tompsett']",eess.IV,"During the 20th Century, aerial surveys captured hundreds of millions of high-resolution photographs of the earth's surface. These images, the precursors to modern satellite imagery, represent an extraordinary visual record of the environmental and social upheavals of the 20th Century. However, most of these images currently languish in physical archives where retrieval is difficult and costly. Digitization could revolutionize access, but manual scanning is slow and expensive. Here, we describe and validate a novel robot-assisted pipeline that increases worker productivity in scanning 30-fold, applied at scale to digitize an archive of 1.7 million historical aerial photographs from 65 countries."
http://arxiv.org/abs/2503.21828v1,Entrepreneurial Motivations and ESG Performance Evidence from Automobile Companies Listed on the Chinese Stock Exchange,2025-03-26 13:46:43+00:00,['Jun Cui'],econ.GN,"This study explores the impact of entrepreneurial motivations on ESG performance in Chinese stock exchange listed automobile companies. Using quantitative methods and empirical analysis via STATA software, the research examines baseline stability, endogeneity, heterogeneity, and mediation/moderation mechanisms. A sample of 50 firms from the Shanghai and Shenzhen Stock Exchanges 2003 and 2023 was analyzed. Results indicate that entrepreneurial motivations positively influence ESG performance. mediated by innovation capability and moderated by market competition intensity. These findings offer theoretical and practical insights, aligning with Stakeholder and institutional theories. The study provides a robust framework for understanding strategic ESG behavior in Chinas automobile sector."
http://arxiv.org/abs/2503.22054v1,tempdisagg: A Python Framework for Temporal Disaggregation of Time Series Data,2025-03-28 00:15:52+00:00,['Jaime Vera-Jaramillo'],econ.EM,"tempdisagg is a modern, extensible, and production-ready Python framework for temporal disaggregation of time series data. It transforms low-frequency aggregates into consistent, high-frequency estimates using a wide array of econometric techniques-including Chow-Lin, Denton, Litterman, Fernandez, and uniform interpolation-as well as enhanced variants with automated estimation of key parameters such as the autocorrelation coefficient rho. The package introduces features beyond classical methods, including robust ensemble modeling via non-negative least squares optimization, post-estimation correction of negative values under multiple aggregation rules, and optional regression-based imputation of missing values through a dedicated Retropolarizer module. Architecturally, it follows a modular design inspired by scikit-learn, offering a clean API for validation, modeling, visualization, and result interpretation."
http://arxiv.org/abs/2503.19095v1,Empirical Bayes shrinkage (mostly) does not correct the measurement error in regression,2025-03-24 19:27:40+00:00,"['Jiafeng Chen', 'Jiaying Gu', 'Soonwoo Kwon']",econ.EM,"In the value-added literature, it is often claimed that regressing on empirical Bayes shrinkage estimates corrects for the measurement error problem in linear regression. We clarify the conditions needed; we argue that these conditions are stronger than the those needed for classical measurement error correction, which we advocate for instead. Moreover, we show that the classical estimator cannot be improved without stronger assumptions. We extend these results to regressions on nonlinear transformations of the latent attribute and find generically slow minimax estimation rates."
http://arxiv.org/abs/2501.00745v2,Dynamics of Adversarial Attacks on Large Language Model-Based Search Engines,2025-01-01 06:23:26+00:00,['Xiyang Hu'],cs.CL,"The increasing integration of Large Language Model (LLM) based search engines has transformed the landscape of information retrieval. However, these systems are vulnerable to adversarial attacks, especially ranking manipulation attacks, where attackers craft webpage content to manipulate the LLM's ranking and promote specific content, gaining an unfair advantage over competitors. In this paper, we study the dynamics of ranking manipulation attacks. We frame this problem as an Infinitely Repeated Prisoners' Dilemma, where multiple players strategically decide whether to cooperate or attack. We analyze the conditions under which cooperation can be sustained, identifying key factors such as attack costs, discount rates, attack success rates, and trigger strategies that influence player behavior. We identify tipping points in the system dynamics, demonstrating that cooperation is more likely to be sustained when players are forward-looking. However, from a defense perspective, we find that simply reducing attack success probabilities can, paradoxically, incentivize attacks under certain conditions. Furthermore, defensive measures to cap the upper bound of attack success rates may prove futile in some scenarios. These insights highlight the complexity of securing LLM-based systems. Our work provides a theoretical foundation and practical insights for understanding and mitigating their vulnerabilities, while emphasizing the importance of adaptive security strategies and thoughtful ecosystem design."
http://arxiv.org/abs/2502.17906v4,Why do financial prices exhibit Brownian motion despite predictable order flow?,2025-02-25 07:12:03+00:00,"['Yuki Sato', 'Kiyoshi Kanazawa']",q-fin.TR,"In financial market microstructure, there are two enigmatic empirical laws: (i) the market-order flow has predictable persistence due to metaorder splitters by institutional investors, well formulated as the Lillo-Mike-Farmer model. However, this phenomenon seems paradoxical given the diffusive and unpredictable price dynamics; (ii) the price impact $I(Q)$ of a large metaorder $Q$ follows the square-root law, $I(Q)\propto \sqrt{Q}$. Here we theoretically reveal why price dynamics follows Brownian motion despite predictable order flow by unifying these enigmas. We generalize the Lillo-Mike-Farmer model to nonlinear price-impact dynamics, which is mapped to an exactly solvable Lévy-walk model. Our exact solution shows that the price dynamics remains diffusive under the square-root law, even under persistent order flow. This work illustrates the crucial role of the square-root law in mitigating large price movements by large metaorders, thereby leading to the Brownian price dynamics, consistently with the efficient market hypothesis over long timescales."
http://arxiv.org/abs/2503.05800v1,How Do Consumers Really Choose: Exposing Hidden Preferences with the Mixture of Experts Model,2025-03-03 13:17:40+00:00,['Diego Vallarino'],cs.LG,"Understanding consumer choice is fundamental to marketing and management research, as firms increasingly seek to personalize offerings and optimize customer engagement. Traditional choice modeling frameworks, such as multinomial logit (MNL) and mixed logit models, impose rigid parametric assumptions that limit their ability to capture the complexity of consumer decision-making. This study introduces the Mixture of Experts (MoE) framework as a machine learning-driven alternative that dynamically segments consumers based on latent behavioral patterns. By leveraging probabilistic gating functions and specialized expert networks, MoE provides a flexible, nonparametric approach to modeling heterogeneous preferences.
  Empirical validation using large-scale retail data demonstrates that MoE significantly enhances predictive accuracy over traditional econometric models, capturing nonlinear consumer responses to price variations, brand preferences, and product attributes. The findings underscore MoEs potential to improve demand forecasting, optimize targeted marketing strategies, and refine segmentation practices. By offering a more granular and adaptive framework, this study bridges the gap between data-driven machine learning approaches and marketing theory, advocating for the integration of AI techniques in managerial decision-making and strategic consumer insights."
http://arxiv.org/abs/2502.16719v2,Exclusion Zones of Instant Runoff Voting,2025-02-23 21:28:15+00:00,"['Kiran Tomlinson', 'Johan Ugander', 'Jon Kleinberg']",cs.MA,"Recent research on instant runoff voting (IRV) shows that it exhibits a striking combinatorial property in one-dimensional preference spaces: there is an ""exclusion zone"" around the median voter such that if a candidate from the exclusion zone is on the ballot, then the winner must come from the exclusion zone. Thus, in one dimension, IRV cannot elect an extreme candidate as long as a sufficiently moderate candidate is running. In this work, we examine the mathematical structure of exclusion zones as a broad phenomenon in more general preference spaces. We prove that with voters uniformly distributed over any $d$-dimensional hyperrectangle (for $d > 1$), IRV has no nontrivial exclusion zone. However, we also show that IRV exclusion zones are not solely a one-dimensional phenomenon. For irregular higher-dimensional preference spaces with fewer symmetries than hyperrectangles, IRV can exhibit nontrivial exclusion zones. As a further exploration, we study IRV exclusion zones in graph voting, where nodes represent voters who prefer candidates closer to them in the graph. Here, we show that IRV exclusion zones present a surprising computational challenge: even checking whether a given set of positions is an IRV exclusion zone is NP-hard. We develop an efficient randomized approximation algorithm for checking and finding exclusion zones. We also report on computational experiments with exclusion zones in two directions: (i) applying our approximation algorithm to a collection of real-world school friendship networks, we find that about 60% of these networks have probable nontrivial IRV exclusion zones; and (ii) performing an exhaustive computer search of small graphs and trees, we also find nontrivial IRV exclusion zones in most graphs. While our focus is on IRV, the properties of exclusion zones we establish provide a novel method for analyzing voting systems in metric spaces more generally."
http://arxiv.org/abs/2503.00422v1,The effect of remote work on urban transportation emissions: evidence from 141 cities,2025-03-01 09:54:30+00:00,"['Sophia Shen', 'Xinyi Wang', 'Nicholas Caros', 'Jinhua Zhao']",econ.GN,"The overall impact of working from home (WFH) on transportation emissions remains a complex issue, with significant implications for policymaking. This study matches socioeconomic information from American Community Survey (ACS) to the global carbon emissions dataset for selected Metropolitan Statistical Areas (MSAs) in the US. We analyze the impact of WFH on transportation emissions before and during the COVID-19 pandemic. Employing cross-sectional multiple regression models and Blinder-Oaxaca decomposition, we examine how WFH, commuting mode, and car ownership influence transportation emissions across 141 MSAs in the United States. We find that the prevalence of WFH in 2021 is associated with lower transportation emissions, whereas WFH in 2019 did not significantly impact transportation emissions. After controlling for public transportation usage and car ownership, we find that a 1% increase in WFH corresponds to a 0.17 kilogram or 1.8% reduction of daily average transportation emissions per capita. The Blinder-Oaxaca decomposition shows that WFH is the main driver in reducing transportation emissions per capita during the pandemic. Our results show that the reductive influence of public transportation on transportation emissions has declined, while the impact of car ownership on increasing transportation emissions has risen. Collectively, these results indicate a multifaceted impact of WFH on transportation emissions. This study underscores the need for a nuanced, data-driven approach in crafting WFH policies to mitigate transportation emissions effectively."
http://arxiv.org/abs/2503.00391v1,The Evolution of Health Investment: Historical Motivations and Fertility Implications,2025-03-01 07:46:56+00:00,['Ruiwu Liu'],econ.TH,"In this working paper, I developed a suite of macroeconomic models that shed light on the intricate relationship between economic development, health, and fertility. These innovative models conceptualize health as an intermediate good, paving the way for new interpretations of dynamic socio-economic phenomena, particularly the non-monotonic effects of health on economic and population growth. The evolving dynamic interactions among economic growth, population, and health during the early stages of human development have been well interpreted in this research."
http://arxiv.org/abs/2503.06244v1,Hate in the Time of Algorithms: Evidence on Online Behavior from a Large-Scale Experiment,2025-03-08 15:00:58+00:00,['Aarushi Kalra'],econ.GN,"Social media algorithms are thought to amplify variation in user beliefs, thus contributing to radicalization. However, quantitative evidence on how algorithms and user preferences jointly shape harmful online engagement is limited. I conduct an individually randomized experiment with 8 million users of an Indian TikTok-like platform, replacing algorithmic ranking with random content delivery. Exposure to ""toxic"" posts decreases by 27%, mainly due to reduced platform usage by users with higher interest in such content. Strikingly, these users increase engagement with toxic posts they find. Survey evidence indicates shifts to other platforms. Model-based counterfactuals highlight the limitations of blanket algorithmic regulation."
http://arxiv.org/abs/2503.20741v1,Flexible Learning via Noise Reduction,2025-03-26 17:24:54+00:00,"['Peter Achim', 'Kemal Ozbek']",econ.TH,"We develop a novel framework for costly information acquisition in which a decision-maker learns about an unobserved state by choosing a signal distribution, with the cost of information determined by the distribution of noise in the signal. We show that a natural set of axioms admits a unique integral representation of the cost function, and we establish the uniform dominance principle: there always exists an optimal experiment that generates signals with uniform noise. The uniform dominance principle allows us to reduce the infinite-dimensional optimization problem of finding an optimal information structure to finding a single parameter that measures the level of noise. We show that an optimal experiment exists under natural conditions, and we characterize it using generalized first-order conditions that accommodate non-smooth payoff functions and decision rules. Finally, we demonstrate the tractability of our framework in a bilateral trade setting in which a buyer learns about product quality."
http://arxiv.org/abs/2501.17209v1,The hardcore brokers: Core-periphery structure and political representation in Denmark's corporate elite network,2025-01-28 10:44:36+00:00,"['Lasse F. Henriksen', 'Jacob Lunding', 'Christoph H. Ellersgaard', 'Anton G. Larsen']",econ.GN,"Who represents the corporate elite in democratic governance? Prior studies find a tightly integrated ""inner circle"" network representing the corporate elite politically across varieties of capitalism, yet they all rely on data from a highly select sample of leaders from only the largest corporations. We cast a wider net. Analyzing new data on all members of corporate boards in the Danish economy (200k directors in 120k boards), we locate 1500 directors that operate as brokers between local corporate networks. We measure their network coreness using k-core detection and find a highly connected core of 275 directors, half of which are affiliated with smaller firms or subsidiaries. Analyses show a strong positive association between director coreness and the likelihood of joining one of the 650 government committees epitomizing Denmark's social-corporatist model of governance (net of firm and director characteristics). The political network premium is largest for directors of smaller firms or subsidiaries, indicating that network coreness is a key driver of business political representation, especially for directors without claims to market power or weight in formal interest organizations."
http://arxiv.org/abs/2502.02328v1,Signaling Design,2025-02-04 13:58:21+00:00,"['Matteo Camboni', 'Mingzi Niu', 'Mallesh M. Pai', 'Rakesh Vohra']",econ.TH,"We revisit the classic job-market signaling model of \cite{spence1973job}, introducing profit-seeking schools as intermediaries that design the mapping from candidates' efforts to job-market signals. Each school commits to an attendance fee and a monitoring policy. We show that, in equilibrium, a monopolist school captures the entire social surplus by committing to low information signals and charging fees that extract students' surplus from being hired. In contrast, competition shifts surplus to students, with schools vying to attract high-ability students, enabling them to distinguish themselves from their lower-ability peers. However, this increased signal informativeness leads to more wasteful effort in equilibrium, contrasting with the usual argument that competition enhances social efficiency. This result may be reversed if schools face binding fee caps or students are credit-constrained."
http://arxiv.org/abs/2501.10001v1,Applying AHP and FUZZY AHP Management Methods to Assess the Level of Financial and Digital Inclusion,2025-01-17 07:34:36+00:00,"['Bogdan Marza', 'Renate-Doina Bratu', 'Razvan Serbu', 'Sebastian Emanuel Stan', 'Camelia Oprean-Stan']",econ.GN,"In today's world, marked by social distancing and lockdowns, the development of digital financial services is becoming increasingly important, but there is little empirical work documenting the most important factors that contribute to the process of financial and digital inclusion. Because the speed with which states adapt to digital financial services is critical, we must ask how prepared states are for this transition and how far they have progressed in terms of financial and digital inclusion. In this context, the goal of this article is, on the one hand, to propose a financial responsibility process framework capable of raising awareness of the most important harmonized key levels of financial and digital inclusion process that, when properly managed, can lead to achieving an optimal level of financial responsibility, and, on the other hand, to assess the financial and digital inclusion process of two different age groups of individuals who are active in the financial environment (15-34 and 35-59 age groups). The Analytical Hierarchy Process AHP and Fuzzy AHP approaches are proposed as a framework for assessing the mechanism of financial and digital inclusion in five East Central European countries. The findings reflect differences between the analyzed countries in terms of the key levels of financial and digital inclusion (where digital and financial education are the most important levels), with Croatia, Czech Republic, and Poland being the most integrated and Romania being the least. According to the findings, as a country or region's level of financial and digital inclusion increases, so does its level of financial responsibility. This research can be a useful tool in raising awareness about the importance of directed behavior for financial responsibility, particularly for policymakers."
http://arxiv.org/abs/2502.00450v1,Confidence intervals for intentionally biased estimators,2025-02-01 15:07:45+00:00,"['David M. Kaplan', 'Xin Liu']",econ.EM,"We propose and study three confidence intervals (CIs) centered at an estimator that is intentionally biased to reduce mean squared error. The first CI simply uses an unbiased estimator's standard error; compared to centering at the unbiased estimator, this CI has higher coverage probability for confidence levels above 91.7%, even if the biased and unbiased estimators have equal mean squared error. The second CI trades some of this ""excess"" coverage for shorter length. The third CI is centered at a convex combination of the two estimators to further reduce length. Practically, these CIs apply broadly and are simple to compute."
http://arxiv.org/abs/2501.16069v2,Forecasting the Volatility of Energy Transition Metals,2025-01-27 14:14:04+00:00,"['Andrea Bastianin', 'Xiao Li', 'Luqman Shamsudin']",econ.GN,"The transition to a cleaner energy mix, essential for achieving net-zero greenhouse gas emissions by 2050, will significantly increase demand for metals critical to renewable energy technologies. Energy Transition Metals (ETMs), including copper, lithium, nickel, cobalt, and rare earth elements, are indispensable for renewable energy generation and the electrification of global economies. However, their markets are characterized by high price volatility due to supply concentration, low substitutability, and limited price elasticity. This paper provides a comprehensive analysis of the price volatility of ETMs, a subset of Critical Raw Materials (CRMs). Using a combination of exploratory data analysis, data reduction, and visualization methods, we identify key features for accurate point and density forecasts. We evaluate various volatility models, including Generalized Autoregressive Conditional Heteroskedasticity (GARCH) and Stochastic Volatility (SV) models, to determine their forecasting performance. Our findings reveal significant heterogeneity in ETM volatility patterns, which challenge standard groupings by data providers and geological classifications. The results contribute to the literature on CRM economics and commodity volatility, offering novel insights into the complex dynamics of ETM markets and the modeling of their returns and volatilities."
http://arxiv.org/abs/2501.16711v1,"Bayesian Analyses of Structural Vector Autoregressions with Sign, Zero, and Narrative Restrictions Using the R Package bsvarSIGNs",2025-01-28 05:19:23+00:00,"['Xiaolei Wang', 'Tomasz Woźniak']",econ.EM,"The R package bsvarSIGNs implements state-of-the-art algorithms for the Bayesian analysis of Structural Vector Autoregressions identified by sign, zero, and narrative restrictions. It offers fast and efficient estimation thanks to the deployment of frontier econometric and numerical techniques and algorithms written in C++. The core model is based on a flexible Vector Autoregression with estimated hyper-parameters of the Minnesota prior and the dummy observation priors. The structural model can be identified by sign, zero, and narrative restrictions, including a novel solution, making it possible to use the three types of restrictions at once. The package facilitates predictive and structural analyses using impulse responses, forecast error variance and historical decompositions, forecasting and conditional forecasting, as well as analyses of structural shocks and fitted values. All this is complemented by colourful plots, user-friendly summary functions, and comprehensive documentation. The package was granted the Di Cook Open-Source Statistical Software Award by the Statistical Society of Australia in 2024."
http://arxiv.org/abs/2501.18467v1,IV Estimation of Heterogeneous Spatial Dynamic Panel Models with Interactive Effects,2025-01-30 16:36:14+00:00,"['Jia Chen', 'Guowei Cui', 'Vasilis Sarafidis', 'Takashi Yamagata']",econ.EM,"This paper develops a Mean Group Instrumental Variables (MGIV) estimator for spatial dynamic panel data models with interactive effects, under large N and T asymptotics. Unlike existing approaches that typically impose slope-parameter homogeneity, MGIV accommodates cross-sectional heterogeneity in slope coefficients. The proposed estimator is linear, making it computationally efficient and robust. Furthermore, it avoids the incidental parameters problem, enabling asymptotically valid inferences without requiring bias correction. The Monte Carlo experiments indicate strong finite-sample performance of the MGIV estimator across various sample sizes and parameter configurations. The practical utility of the estimator is illustrated through an application to regional economic growth in Europe. By explicitly incorporating heterogeneity, our approach provides fresh insights into the determinants of regional growth, underscoring the critical roles of spatial and temporal dependencies."
http://arxiv.org/abs/2501.19407v2,Algorithmic Inheritance: Surname Bias in AI Decisions Reinforces Intergenerational Inequality,2025-01-23 10:53:58+00:00,"['Pat Pataranutaporn', 'Nattavudh Powdthavee', 'Pattie Maes']",cs.CY,"Surnames often convey implicit markers of social status, wealth, and lineage, shaping perceptions in ways that can perpetuate systemic biases and intergenerational inequality. This study is the first of its kind to investigate whether and how surnames influence AI-driven decision-making, focusing on their effects across key areas such as hiring recommendations, leadership appointments, and loan approvals. Using 72,000 evaluations of 600 surnames from the United States and Thailand, two countries with distinct sociohistorical contexts and surname conventions, we classify names into four categories: Rich, Legacy, Normal, and phonetically similar Variant groups. Our findings show that elite surnames consistently increase AI-generated perceptions of power, intelligence, and wealth, which in turn influence AI-driven decisions in high-stakes contexts. Mediation analysis reveals perceived intelligence as a key mechanism through which surname biases influence AI decision-making process. While providing objective qualifications alongside surnames mitigates most of these biases, it does not eliminate them entirely, especially in contexts where candidate credentials are low. These findings highlight the need for fairness-aware algorithms and robust policy measures to prevent AI systems from reinforcing systemic inequalities tied to surnames, an often-overlooked bias compared to more salient characteristics such as race and gender. Our work calls for a critical reassessment of algorithmic accountability and its broader societal impact, particularly in systems designed to uphold meritocratic principles while counteracting the perpetuation of intergenerational privilege."
http://arxiv.org/abs/2502.03471v1,Construal Level and Cognitive Reflection in Newsvendor Games: Unveiling the Influence of Individual Heterogeneity on Decision-Making,2025-01-13 20:19:33+00:00,"['Kuldeep Singh', 'Sumanth Cheemalapati', 'George Kurian', 'Prathamesh Muzumdar']",physics.soc-ph,"During the last decade, scholars have studied the behavior of decision-making in newsvendor settings and have identified numerous behavior patterns for deviating from normative behavior. However, there is a dearth of research which have examined the influence of individual heterogeneity on decision-making in newsvendor settings. This study examines the level of construal (Abstract and concrete) using construal level theory (CLT) on performance in newsvendor games. In addition, this study measures the cognitive reflection of individuals using cognitive reflection test (CRT) ex-ante to analyze the true impact of how people construe a problem and its impact on their decision-making."
http://arxiv.org/abs/2501.15753v3,Scale-Insensitive Neural Network Significance Tests,2025-01-27 03:45:26+00:00,['Hasan Fallahgoul'],stat.ML,"This paper develops a scale-insensitive framework for neural network significance testing, substantially generalizing existing approaches through three key innovations. First, we replace metric entropy calculations with Rademacher complexity bounds, enabling the analysis of neural networks without requiring bounded weights or specific architectural constraints. Second, we weaken the regularity conditions on the target function to require only Sobolev space membership $H^s([-1,1]^d)$ with $s > d/2$, significantly relaxing previous smoothness assumptions while maintaining optimal approximation rates. Third, we introduce a modified sieve space construction based on moment bounds rather than weight constraints, providing a more natural theoretical framework for modern deep learning practices. Our approach achieves these generalizations while preserving optimal convergence rates and establishing valid asymptotic distributions for test statistics. The technical foundation combines localization theory, sharp concentration inequalities, and scale-insensitive complexity measures to handle unbounded weights and general Lipschitz activation functions. This framework better aligns theoretical guarantees with contemporary deep learning practice while maintaining mathematical rigor."
http://arxiv.org/abs/2502.03693v1,Misspecification-Robust Shrinkage and Selection for VAR Forecasts and IRFs,2025-02-06 01:01:56+00:00,"['Oriol González-Casasús', 'Frank Schorfheide']",econ.EM,"VARs are often estimated with Bayesian techniques to cope with model dimensionality. The posterior means define a class of shrinkage estimators, indexed by hyperparameters that determine the relative weight on maximum likelihood estimates and prior means. In a Bayesian setting, it is natural to choose these hyperparameters by maximizing the marginal data density. However, this is undesirable if the VAR is misspecified. In this paper, we derive asymptotically unbiased estimates of the multi-step forecasting risk and the impulse response estimation risk to determine hyperparameters in settings where the VAR is (potentially) misspecified. The proposed criteria can be used to jointly select the optimal shrinkage hyperparameter, VAR lag length, and to choose among different types of multi-step-ahead predictors; or among IRF estimates based on VARs and local projections. The selection approach is illustrated in a Monte Carlo study and an empirical application."
http://arxiv.org/abs/2503.20092v1,Entry and disclosure in group contests,2025-03-25 22:10:32+00:00,"['Luke Boosey', 'Philip Brookins', 'Dmitry Ryvkin']",econ.TH,"We study information disclosure policies for contests among groups. Each player endogenously decides whether or not to participate in competition as a member of their group. Within-group aggregation of effort is best-shot, i.e., each group's performance is determined by the highest investment among its members. We consider a generalized all-pay auction setting, in which the group with the highest performance wins the contest with certainty. Players' values for winning are private information at the entry stage, but may be disclosed at the competition stage. We compare three disclosure policies: (i) no disclosure, when the number of entrants remains unknown and their values private; (ii) within-group disclosure, when this information is disclosed within each group but not across groups; and (iii) full disclosure, when the information about entrants is disclosed across groups. For the benchmark case of contests between individuals, information disclosure always reduces expected aggregate investment. However, this is no longer true in group contests: Within-group disclosure unambiguously raises aggregate investment, while the effect of full disclosure is ambiguous."
http://arxiv.org/abs/2503.23559v1,Are Bushmeat Hunters Profit Maximizers or Simply Brigands of Opportunity?,2025-03-30 18:45:12+00:00,"['Wayne A. Morra', 'Gail W. Hearn', 'Andrew J. Buck']",econ.GN,"Bushmeat hunters on Bioko Island, Equatorial Guinea use shotguns and snares to capture wild arboreal and ground animals for sale in the Malabo Bushmeat market. Two tools for the analysis of economic efficiency, the production possibilities frontier and isorevenue line, can be used to explain the post hoc changing spatial distribution of takeoff rates of bushmeat. This study analyzes changes in technical efficiencies over time and in different locations for the open access wildlife hunted on Bioko for the last ten years. Due to inadequate refrigeration in the field and the bushmeat market, animals must be sold quickly. The result is a takeoff distribution that is not efficient, consequently too many of the wrong species of animals are harvested. The larger, slower-breeding mammals, such as monkeys disappear before the smaller, faster-breeding mammals, such as blue duikers and pouched rats, promoting a steepening of the production possibilities frontier, inducing a greater takeoff of monkeys than the expected efficient level. Soon after hunters penetrate into a new area, the relative selling price of monkeys exceeds the rate of transformation between ground animals and arboreal animals triggering inefficient and unsustainable harvests"
http://arxiv.org/abs/2502.00070v2,Can AI Solve the Peer Review Crisis? A Large Scale Cross Model Experiment of LLMs' Performance and Biases in Evaluating over 1000 Economics Papers,2025-01-31 04:04:02+00:00,"['Pat Pataranutaporn', 'Nattavudh Powdthavee', 'Chayapatr Achiwaranguprok', 'Pattie Maes']",cs.CY,"This study examines the potential of large language models (LLMs) to augment the academic peer review process by reliably evaluating the quality of economics research without introducing systematic bias. We conduct one of the first large-scale experimental assessments of four LLMs (GPT-4o, Claude 3.5, Gemma 3, and LLaMA 3.3) across two complementary experiments. In the first, we use nonparametric binscatter and linear regression techniques to analyze over 29,000 evaluations of 1,220 anonymized papers drawn from 110 economics journals excluded from the training data of current LLMs, along with a set of AI-generated submissions. The results show that LLMs consistently distinguish between higher- and lower-quality research based solely on textual content, producing quality gradients that closely align with established journal prestige measures. Claude and Gemma perform exceptionally well in capturing these gradients, while GPT excels in detecting AI-generated content. The second experiment comprises 8,910 evaluations designed to assess whether LLMs replicate human like biases in single blind reviews. By systematically varying author gender, institutional affiliation, and academic prominence across 330 papers, we find that GPT, Gemma, and LLaMA assign significantly higher ratings to submissions from top male authors and elite institutions relative to the same papers presented anonymously. These results emphasize the importance of excluding author-identifying information when deploying LLMs in editorial screening. Overall, our findings provide compelling evidence and practical guidance for integrating LLMs into peer review to enhance efficiency, improve accuracy, and promote equity in the publication process of economics research."
http://arxiv.org/abs/2502.09678v2,Quality thinning and value development of boreal trees on spruce-dominated stands,2025-02-13 10:53:49+00:00,['Petri P. Karenlampi'],econ.GN,"For the first time, quality distribution of trees is introduced in a tree growth model. Consequently, the effects of quality thinning on stand development can be investigated. Quality thinning improves the financial return in all cases studied, but the effect is small. Rotation ages, timber stocks and maturity diameters are not much affected by quality thinning. Bare land valuation neither changes the contribution of the quality thinning. The reason for the small effect apparently lies in the value development of individual trees. The relative value development of small pulpwood trunks is large, since the harvesting expense per volume unit is reduced along with size increment. Such trees are not feasible objects for quality thinning, unless quality correlates with growth rate. Another enhanced stage of value development is when pulpwood trunks turn to sawlog trunks. For large pulpwood trunks, quality thinning is feasible. Existing sawlog content in trees dilutes the effect of quality thinning on the financial return. The results change if the growth rate is positively correlated with quality, quality thinning becoming feasible in all commercial diameter classes."
http://arxiv.org/abs/2503.16569v1,Research on the Influence Mechanism and Effect of Digital Village Construction on Urban-Rural Common prosperity Evidence From China,2025-03-20 08:40:57+00:00,"['Huang Dahu', 'Shan Tiecheng', 'Wang Cheng']",econ.GN,"Urban rural common prosperity is the ultimate goal of narrowing the gap between urban and rural areas and promoting urban rural integration development, and it is an indispensable and important element in the common wealth goal of Chinese style modernization."
http://arxiv.org/abs/2503.16503v1,AIDetection: A Generative AI Detection Tool for Educators Using Syntactic Matching of Common ASCII Characters As Potential 'AI Traces' Within Users' Internet Browser,2025-03-12 15:53:58+00:00,['Andy Buschmann'],cs.HC,"This paper introduces a simple JavaScript-based web application designed to assist educators in detecting AI-generated content in student essays and written assignments. Unlike existing AI detection tools that rely on obfuscated machine learning models, AIDetection.info employs a heuristic-based approach to identify common syntactic traces left by generative AI models, such as ChatGPT, Claude, Grok, DeepSeek, Gemini, Llama/Meta, Microsoft Copilot, Grammarly AI, and other text-generating models and wrapper applications. The tool scans documents in bulk for potential AI artifacts, as well as AI citations and acknowledgments, and provides a visual summary with downloadable Excel and CSV reports. This article details its methodology, functionalities, limitations, and applications within educational settings."
http://arxiv.org/abs/2503.14946v2,"Has the Paris Agreement Shaped Emission Trends? A Panel VECM Analysis of Energy, Growth, and CO$_2$ in 106 Middle-Income Countries",2025-03-19 07:24:42+00:00,"['Tuhin G. M. Al Mamun', 'Ehsanullah', 'Md. Sharif Hassan', 'Mohammad Bin Amin', 'Judit Oláh']",econ.EM,"Rising CO$_2$ emissions remain a critical global challenge, particularly in middle-income countries where economic growth drives environmental degradation. This study examines the long-run and short-run relationships between CO$_2$ emissions, energy use, GDP per capita, and population across 106 middle-income countries from 1980 to 2023. Using a Panel Vector Error Correction Model (VECM), we assess the impact of the Paris Agreement (2015) on emissions while conducting cointegration tests to confirm long-run equilibrium relationships. The findings reveal a strong long-run relationship among the variables, with energy use as the dominant driver of emissions, while GDP per capita has a moderate impact. However, the Paris Agreement has not significantly altered emissions trends in middle-income economies. Granger causality tests indicate that energy use strongly causes emissions, but GDP per capita and population do not exhibit significant short-run causal effects. Variance decomposition confirms that energy shocks have the most persistent effects, and impulse response functions (IRFs) show emissions trajectories are primarily shaped by economic activity rather than climate agreements. Robustness checks, including autocorrelation tests, polynomial root stability, and Yamagata-Pesaran slope homogeneity tests, validate model consistency. These results suggest that while global agreements set emissions reduction goals, their effectiveness remains limited without stronger national climate policies, sectoral energy reforms, and financial incentives for clean energy adoption to ensure sustainable economic growth."
http://arxiv.org/abs/2503.08074v1,Hedonic Adaptation in the Age of AI: A Perspective on Diminishing Satisfaction Returns in Technology Adoption,2025-03-11 06:08:36+00:00,"['Venkat Ram Reddy Ganuthula', 'Krishna Kumar Balaraman', 'Nimish Vohra']",econ.GN,"The fast paced progress of artificial intelligence (AI) through scaling laws connecting rising computational power with improving performance has created tremendous technological breakthroughs. These breakthroughs do not translate to corresponding user satisfaction improvements, resulting in a general mismatch. This research suggests that hedonic adaptation the psychological process by which people revert to a baseline state of happiness after drastic change provides a suitable model for understanding this phenomenon. We argue that user satisfaction with AI follows a logarithmic path, thus creating a longterm ""satisfaction gap"" as people rapidly get used to new capabilities as expectations. This process occurs through discrete stages: initial excitement, declining returns, stabilization, and sporadic resurgence, depending on adaptation rate and capability introduction. These processes have far reaching implications for AI research, user experience design, marketing, and ethics, suggesting a paradigm shift from sole technical scaling to methods that sustain perceived value in the midst of human adaptation. This perspective reframes AI development, necessitating practices that align technological progress with people's subjective experience."
http://arxiv.org/abs/2503.14072v1,Leveraging Knowledge Networks: Rethinking Technological Value Distribution in mRNA Vaccine Innovations,2025-03-18 09:45:19+00:00,"['Rossana Mastrandrea', 'Fabio Montobbio', 'Gabriele Pellegrino', 'Massimo Riccaboni', 'Valerio Sterzi']",physics.soc-ph,"This study examines the roles of public and private sector actors in the development of mRNA vaccines, a breakthrough innovation in modern medicine. Using a dataset of 151 core patent families and 2,416 antecedent (cited) patents, we analyze the structure and dynamics of the mRNA vaccine knowledge network through network theory. Our findings highlight the central role of biotechnology firms, such as Moderna and BioNTech, alongside the crucial contributions of universities and public research organizations (PROs) in providing foundational knowledge.We develop a novel credit allocation framework, showing that universities, PROs, government and research centers account for at least 27% of the external technological knowledge base behind mRNA vaccine breakthroughs - representing a minimum threshold of their overall contribution. Our study offers new insights into pharmaceutical and biotechnology innovation dynamics, emphasizing how Moderna and BioNTech's mRNA technologies have benefited from academic institutions, with notable differences in their institutional knowledge sources."
http://arxiv.org/abs/2501.18662v1,ReviewCoin: Paying for Real Work,2025-01-30 13:44:20+00:00,['Chris Welty'],econ.GN,"The peer-review process is broken and the problem is getting worse, especially in AI: large conferences like NeurIPS increasingly struggle to adequately review huge numbers of paper submissions. I propose a scalable solution that, foremost, recognizes reviewing as important, necessary, \emph{work} and rewards it with crypto-coins owned and managed by the conferences themselves. The idea is at its core quite simple: paper submissions require work (reviews, meta-reviews, etc.) to be done, and therefore the submitter must pay for that work. Each reviewer submits their review to be approved by some designated conference officer (e.g. PC chair, Area Chair, etc.), and upon approval is paid a single coin for a single review. If three reviews are required, the cost of submission should be three coins + a tax that covers payments to all the volunteers who organize the conference. After some one-time startup costs to fairly distribute coins, the process should be relatively stable with new coins minted only when a conference grows."
http://arxiv.org/abs/2501.15545v1,Rationalizable Behavior in the Hotelling Model with Waiting Costs,2025-01-26 14:35:14+00:00,['Joep van Sloun'],econ.TH,"This paper revisits the Hotelling model with waiting costs Kohlberg (1983), focusing on two specific settings where pure Nash equilibria do not exist: the asymmetric model with two firms and the symmetric model with three firms. In the asymmetric two-firm model, we show that the weaker concept of point rationalizability has strong predictive power, as it selects exactly two locations for both firms. As the two firms become more similar in their efficiency in handling queues of consumers, the two point rationalizable locations converge towards the center of the line. In the symmetric three-firm model, the set of point rationalizable choices forms an interval. This interval is shrinking in the inefficiency levels of the firms in handling queues of consumers."
http://arxiv.org/abs/2501.15692v1,Simple Inference on a Simplex-Valued Weight,2025-01-26 22:40:27+00:00,"['Nathan Canen', 'Kyungchul Song']",econ.EM,"In many applications, the parameter of interest involves a simplex-valued weight which is identified as a solution to an optimization problem. Examples include synthetic control methods with group-level weights and various methods of model averaging and forecast combination. The simplex constraint on the weight poses a challenge in statistical inference due to the constraint potentially binding. In this paper, we propose a simple method of constructing a confidence set for the weight and prove that the method is asymptotically uniformly valid. The procedure does not require tuning parameters or simulations to compute critical values. The confidence set accommodates both the cases of point-identification or set-identification of the weight. We illustrate the method with an empirical example."
http://arxiv.org/abs/2502.00209v1,Frame-dependent Random Utility,2025-01-31 22:53:46+00:00,"['Paul H. Y. Cheung', 'Yusufcan Masatlioglu']",econ.TH,"We explore the influence of framing on decision-making, where some products are framed (e.g., displayed, recommended, endorsed, or labeled). We introduce a novel choice function that captures observed variations in framed alternatives. Building on this, we conduct a comprehensive revealed preference analysis, employing the concept of frame-dependent utility using both deterministic and probabilistic data. We demonstrate that simple and intuitive behavioral principles characterize our frame-dependent random utility model (FRUM), which offers testable conditions even with limited data. Finally, we introduce a parametric model to increase the tractability of FRUM. We also discuss how to recover the choice types in our framework."
http://arxiv.org/abs/2504.13871v1,Human aversion? Do AI Agents Judge Identity More Harshly Than Performance,2025-03-31 02:05:27+00:00,"['Yuanjun Feng', 'Vivek Chodhary', 'Yash Raj Shrestha']",cs.HC,"This study examines the understudied role of algorithmic evaluation of human judgment in hybrid decision-making systems, a critical gap in management research. While extant literature focuses on human reluctance to follow algorithmic advice, we reverse the perspective by investigating how AI agents based on large language models (LLMs) assess and integrate human input. Our work addresses a pressing managerial constraint: firms barred from deploying LLMs directly due to privacy concerns can still leverage them as mediating tools (for instance, anonymized outputs or decision pipelines) to guide high-stakes choices like pricing or discounts without exposing proprietary data. Through a controlled prediction task, we analyze how an LLM-based AI agent weights human versus algorithmic predictions. We find that the AI system systematically discounts human advice, penalizing human errors more severely than algorithmic errors--a bias exacerbated when the agent's identity (human vs AI) is disclosed and the human is positioned second. These results reveal a disconnect between AI-generated trust metrics and the actual influence of human judgment, challenging assumptions about equitable human-AI collaboration. Our findings offer three key contributions. First, we identify a reverse algorithm aversion phenomenon, where AI agents undervalue human input despite comparable error rates. Second, we demonstrate how disclosure and positional bias interact to amplify this effect, with implications for system design. Third, we provide a framework for indirect LLM deployment that balances predictive power with data privacy. For practitioners, this research emphasize the need to audit AI weighting mechanisms, calibrate trust dynamics, and strategically design decision sequences in human-AI systems."
http://arxiv.org/abs/2501.10675v2,Recovering Unobserved Network Links from Aggregated Relational Data: Discussions on Bayesian Latent Surface Modeling and Penalized Regression,2025-01-18 06:51:51+00:00,['Yen-hsuan Tseng'],econ.EM,"Accurate network data are essential in fields such as economics, sociology, and computer science. Aggregated Relational Data (ARD) provides a way to capture network structures using partial data. This article compares two main frameworks for recovering network links from ARD: Bayesian Latent Surface Modeling (BLSM) and Frequentist Penalized Regression (FPR). Using simulation studies and real-world applications, we evaluate their theoretical properties, computational efficiency, and practical utility in domains like financial risk assessment and epidemiology. Key findings emphasize the importance of trait design, privacy considerations, and hybrid modeling approaches to improve scalability and robustness."
http://arxiv.org/abs/2501.19394v4,Fixed-Population Causal Inference for Models of Equilibrium,2025-01-31 18:48:12+00:00,['Konrad Menzel'],econ.EM,"In contrast to problems of interference in (exogenous) treatments, models of interference in unit-specific (endogenous) outcomes do not usually produce a reduced-form representation where outcomes depend on other units' treatment status only at a short network distance, or only through a known exposure mapping. This remains true if the structural mechanism depends on outcomes of peers only at a short network distance, or through a known exposure mapping. In this paper, we first define causal estimands that are identified and estimable from a single experiment on the network under minimal assumptions on the structure of interference, and which represent average partial causal responses which generally vary with other global features of the realized assignment. Under a fixed-population, design-based approach, we show unbiasedness and consistency for inverse-probability weighting (IPW) estimators for those causal parameters from a randomized experiment on a single network. We also analyze more closely the case of marginal interventions in a model of equilibrium with smooth response functions where we can recover LATE-type weighted averages of derivatives of those response functions. Under additional structural assumptions, these ``agnostic"" causal estimands can be combined to recover model parameters, but also retain their less restrictive causal interpretation."
http://arxiv.org/abs/2502.15049v2,biastest: Testing parameter equality across different models in Stata,2025-02-20 21:13:46+00:00,['Hasraddin Guliyev'],econ.EM,"The biastest command in Stata is a powerful and user-friendly tool designed to compare the coefficients of different regression models, enabling researchers to assess the robustness and consistency of their empirical findings. This command is particularly valuable for evaluating alternative modeling approaches, such as ordinary least squares versus robust regression, robust regression versus median regression, quantile regression across different quantiles, and fixed effects versus random effects models in panel data analysis. By providing both variable-specific and joint tests, biastest command offers a comprehensive framework for detecting bias or significant differences in model estimates, ensuring that researchers can make informed decisions about model selection and interpretation."
http://arxiv.org/abs/2502.14984v2,Accelerating Equity: Overcoming the Gender Gap in VC Funding,2025-02-20 19:20:53+00:00,"['Chuan Chen', 'Michele Fioretti', 'Junnan He', 'Yanrong Jia']",econ.GN,"We examine the growing gender gap in venture capital funding, focusing on accelerator programs in the U.S. We collect a unique dataset with detailed information on accelerators and startups. Using a two-stage methodology, we first estimate a matching model between startups and accelerators, and then use its output to analyze the gender gap in post-graduation outcomes through a control function approach. Our results suggest that female-founded startups face a significant funding disadvantage due to relocation challenges tied to family obligations. However, larger cohorts and higher-quality accelerators help reduce this gap by potentially offering female founders better networking opportunities and mentorship."
http://arxiv.org/abs/2502.18261v3,The effect of minimum wages on employment in the presence of productivity fluctuations,2025-02-25 14:48:13+00:00,['Asahi Sato'],econ.GN,"Traditionally, the impact of minimum wages on employment has been studied, and it is generally believed to have a negative effect. Yet, some recent studies have shown that the impact of minimum wages on employment can sometimes be positive. In addition, certain recent proposals set a higher minimum wage than the wage earned by some high-productivity workers. However, the impact of minimum wages on employment has been primarily studied on low-skilled workers, whereas there is limited research on high-skilled workers. To address this gap and examine the effects of minimum wages on high-productivity workers' employment, I construct a macroeconomic model incorporating productivity fluctuations, incomplete markets, directed search, and on-the-job search and compare the steady-state distributions between the baseline model and the model with a minimum wage. As a result, binding minimum wages increase the unemployment rate of both low and high-productivity workers."
http://arxiv.org/abs/2504.05251v1,Rationalizing dynamic choices,2025-04-07 16:43:26+00:00,"['Henrique de Oliveira', 'Rohit Lamba']",econ.TH,"An analyst observes an agent take a sequence of actions. The analyst does not have access to the agent's information and ponders whether the observed actions could be justified through a rational Bayesian model with a known utility function. We show that the observed actions cannot be justified if and only if there is a single deviation argument that leaves the agent better off, regardless of the information. The result is then extended to allow for distributions over possible action sequences. Four applications are presented: monotonicity of rationalization with risk aversion, a potential rejection of the Bayesian model with observable data, feasible outcomes in dynamic information design, and partial identification of preferences without assumptions on information."
http://arxiv.org/abs/2504.19832v6,Assignment at the Frontier: Identifying the Frontier Structural Function and Bounding Mean Deviations,2025-04-28 14:34:54+00:00,"['Dan Ben-Moshe', 'David Genesove']",econ.EM,"This paper analyzes a model in which an outcome equals a frontier function of inputs minus a nonnegative unobserved deviation. Inputs may be endogenous (statistically dependent on the deviation). If zero lies in the support of the deviation given inputs -- an assumption we term assignment at the frontier -- then the frontier is identified by the supremum of the outcome at those inputs, obviating the need for instrumental variables. We then consider estimation of the frontier in the presence of random error that is mean-independent of inputs but may be heteroskedastic. Finally, we derive a lower bound on the mean deviation, using only variance and skewness, that is robust to a scarcity of data near the frontier. We apply our methods to estimate a firm-level frontier production function and mean inefficiency."
http://arxiv.org/abs/2505.13558v1,CATS: Clustering-Aggregated and Time Series for Business Customer Purchase Intention Prediction,2025-05-19 09:07:34+00:00,"['Yingjie Kuang', 'Tianchen Zhang', 'Zhen-Wei Huang', 'Zhongjie Zeng', 'Zhe-Yuan Li', 'Ling Huang', 'Yuefang Gao']",econ.EM,"Accurately predicting customers' purchase intentions is critical to the success of a business strategy. Current researches mainly focus on analyzing the specific types of products that customers are likely to purchase in the future, little attention has been paid to the critical factor of whether customers will engage in repurchase behavior. Predicting whether a customer will make the next purchase is a classic time series forecasting task. However, in real-world purchasing behavior, customer groups typically exhibit imbalance - i.e., there are a large number of occasional buyers and a small number of loyal customers. This head-to-tail distribution makes traditional time series forecasting methods face certain limitations when dealing with such problems. To address the above challenges, this paper proposes a unified Clustering and Attention mechanism GRU model (CAGRU) that leverages multi-modal data for customer purchase intention prediction. The framework first performs customer profiling with respect to the customer characteristics and clusters the customers to delineate the different customer clusters that contain similar features. Then, the time series features of different customer clusters are extracted by GRU neural network and an attention mechanism is introduced to capture the significance of sequence locations. Furthermore, to mitigate the head-to-tail distribution of customer segments, we train the model separately for each customer segment, to adapt and capture more accurately the differences in behavioral characteristics between different customer segments, as well as the similar characteristics of the customers within the same customer segment. We constructed four datasets and conducted extensive experiments to demonstrate the superiority of the proposed CAGRU approach."
http://arxiv.org/abs/2504.21156v2,Publication Design with Incentives in Mind,2025-04-29 20:17:46+00:00,"['Ravi Jagadeesan', 'Davide Viviano']",econ.EM,"The publication process both determines which research receives the most attention, and influences the supply of research through its impact on researchers' private incentives. We introduce a framework to study optimal publication decisions when researchers can choose (i) whether or how to conduct a study and (ii) whether or how to manipulate the research findings (e.g., via selective reporting or data manipulation). When manipulation is not possible, but research entails substantial private costs for the researchers, it may be optimal to incentivize cheaper research designs even if they are less accurate. When manipulation is possible, it is optimal to publish some manipulated results, as well as results that would have not received attention in the absence of manipulability. Even if it is possible to deter manipulation, such as by requiring pre-registered experiments instead of (potentially manipulable) observational studies, it is suboptimal to do so when experiments entail high research costs. We illustrate the implications of our model in an application to medical studies."
http://arxiv.org/abs/2505.04669v1,Shocking concerns: public perception about climate change and the macroeconomy,2025-05-07 08:14:24+00:00,"['Giovanni Angelini', 'Maria Elena Bontempi', 'Luca De Angelis', 'Paolo Neri', 'Marco Maria Sorge']",econ.GN,"Public perceptions of climate change arguably contribute to shaping private adaptation and support for policy intervention. In this paper, we propose a novel Climate Concern Index (CCI), based on disaggregated web-search volumes related to climate change topics, to gauge the intensity and dynamic evolution of collective climate perceptions, and evaluate its impacts on the business cycle. Using data from the United States over the 2004:2024 span, we capture widespread shifts in perceived climate-related risks, particularly those consistent with the postcognitive interpretation of affective responses to extreme climate events. To assess the aggregate implications of evolving public concerns about the climate, we estimate a proxy-SVAR model and find that exogenous variation in the CCI entails a statistically significant drop in both employment and private consumption and a persistent surge in stock market volatility, while core inflation remains largely unaffected. These results suggest that, even in the absence of direct physical risks, heightened concerns for climate-related phenomena can trigger behavioral adaptation with nontrivial consequences for the macroeconomy, thereby demanding attention from institutional players in the macro-financial field."
http://arxiv.org/abs/2504.21060v3,Construct to Commitment: The Effect of Narratives on Economic Growth,2025-04-29 09:59:48+00:00,"['Hanyuan Jiang', 'Yi Man']",econ.GN,"We study how government-led narratives through mass media evolve from construct, a mechanism for framing expectations, into commitment, a sustainable pillar for growth. We propose the ""Narratives-Construct-Commitment (NCC)"" framework outlining the mechanism and institutionalization of narratives, and formalize it as a dynamic Bayesian game. Using the Innovation-Driven Development Strategy (2016) as a case study, we identify the narrative shock from high-frequency financial data and trace its impact using local projection method. By shaping expectations, credible narratives institutionalize investment incentives, channel resources into R&D, and facilitate sustained improvements in total factor productivity (TFP). Our findings strive to provide insights into the New Quality Productive Forces initiative, highlighting the role of narratives in transforming vision into tangible economic growth."
http://arxiv.org/abs/2504.13223v1,The heterogeneous causal effects of the EU's Cohesion Fund,2025-04-17 10:22:54+00:00,"['Angelos Alexopoulos', 'Ilias Kostarakos', 'Christos Mylonakis', 'Petros Varthalitis']",econ.GN,"This paper quantifies the causal effect of cohesion policy on EU regional output and investment focusing on one of its least studied instruments, i.e., the Cohesion Fund (CF). We employ modern causal inference methods to estimate not only the local average treatment effect but also its time-varying and heterogeneous effects across regions. Utilizing this method, we propose a novel framework for evaluating the effectiveness of CF as an EU cohesion policy tool. Specifically, we estimate the time varying distribution of the CF's causal effects across EU regions and derive key distribution metrics useful for policy evaluation. Our analysis shows that relying solely on average treatment effects masks significant heterogeneity and can lead to misleading conclusions about the effectiveness of the EU's cohesion policy. We find that the impact of the CF is frontloaded, peaking within the first seven years after a region's initial inclusion in the program. The distribution of the effects during this first seven-year cycle of funding is right skewed with relatively thick tails. This indicates positive effects but unevenly distributed across regions. Moreover, the magnitude of the CF effect is inversely related to a region's relative position in the initial distribution of output, i.e., relatively poorer recipient regions experience higher effects compared to relatively richer regions. Finally, we find a non-linear relationship with diminishing returns, whereby the impact of CF declines as the ratio of CF funds received to a region's gross value added (GVA) increases."
http://arxiv.org/abs/2504.19309v1,Bridging Short- and Long-Term Dependencies: A CNN-Transformer Hybrid for Financial Time Series Forecasting,2025-04-27 17:17:43+00:00,['Tiantian Tu'],econ.GN,"Time series forecasting is crucial for decision-making across various domains, particularly in financial markets where stock prices exhibit complex and non-linear behaviors. Accurately predicting future price movements is challenging due to the difficulty of capturing both short-term fluctuations and long-term dependencies in the data. Convolutional Neural Networks (CNNs) are well-suited for modeling localized, short-term patterns but struggle with long-range dependencies due to their limited receptive field. In contrast, Transformers are highly effective at capturing global temporal relationships and modeling long-term trends. In this paper, we propose a hybrid architecture that combines CNNs and Transformers to effectively model both short- and long-term dependencies in financial time series data. We apply this approach to forecast stock price movements for S\&P 500 constituents and demonstrate that our model outperforms traditional statistical models and popular deep learning methods in intraday stock price forecasting, providing a robust framework for financial prediction."
http://arxiv.org/abs/2504.17113v1,Cybernetic Governance in a Coliving House,2025-04-23 21:45:59+00:00,"['Daniel Kronovet', 'Seth Frey', 'Joseph DeSimone']",cs.CY,"We report an 18-month field experiment in distributed digital institutions: a nine-bedroom Los Angeles coliving house that runs without managers, while sustaining 98% occupancy and below-market rents.
  Drawing on Elinor Ostrom's commons theory, we outline design principles and three digital mechanisms that form the institutional core: 1) A continuous-auction chore scheduler turns regenerative labor into a time-indexed points market; residents meet a 100-point monthly obligation by claiming tasks whose value rises linearly with neglect. 2) A pairwise-preference layer lets participants asynchronously reprioritize tasks, translating meta-governance into low-cognition spot inputs. 3) A symbolic ""hearts"" ledger tracks norm compliance through automated enforcement, lightweight challenges, and peer-awarded karma. Together, these mechanisms operationalize cybernetic principles--human sensing, machine bookkeeping, real-time feedback--while minimizing dependence on privileged roles.
  Our exploratory data (567 chore claims, 255 heart events, and 551 group purchases) show that such tooling can sustain reliable commons governance without continuous leadership, offering a transferable design palette for online communities, coliving houses, and other digitally mediated collectives."
http://arxiv.org/abs/2505.02425v1,Revolutions as Structural Breaks: The Long-Term Economic and Institutional Consequences of the 1979 Iranian Revolution,2025-05-05 07:46:16+00:00,"['Nuno Garoupa', 'Rok Spruk']",econ.GN,"This paper examines whether major political institutional disruptions produce temporary shocks or structural breaks in long-term development. Using the 1979 Iranian Revolution as a natural experiment, we apply the synthetic control method to estimate its causal effect on economic growth and institutional quality. Drawing on a panel of 66 countries from 1950 to 2015, we construct counterfactual trajectories for Iran in the absence of revolutionary change. Our results show a persistent and statistically significant divergence in per capita GDP, institutional quality, and legal constraints on executive power. We perform in-space and in-time placebo tests to rule out confounding events, such as the Iran-Iraq War and international sanctions, and propose confidence interval estimation to address uncertainty in treatment effects. The findings identify the Iranian Revolution as a structural institutional rupture, with implications for the classification of institutional change more broadly. We contribute a generalizable empirical framework for distinguishing between temporary and structural institutional shocks in long-run development."
http://arxiv.org/abs/2504.15726v1,"Barter and Hierarchy: A Practical Perspective on Food, Society, and Knowledge in the Inca Empire",2025-04-22 09:21:09+00:00,['Luis-Felipe Arizmendi'],econ.TH,"The Inca Empire developed a sophisticated food production system, social organisation, and knowledge transmission without money or writing. The article introduces the concept of a barter economy structured through hierarchical cooperation and examines the Inca model from a practice-based (heuristical) perspective."
http://arxiv.org/abs/2505.04414v1,A Powerful Chi-Square Specification Test with Support Vectors,2025-05-07 13:51:00+00:00,"['Yuhao Li', 'Xiaojun Song']",econ.EM,"Specification tests, such as Integrated Conditional Moment (ICM) and Kernel Conditional Moment (KCM) tests, are crucial for model validation but often lack power in finite samples. This paper proposes a novel framework to enhance specification test performance using Support Vector Machines (SVMs) for direction learning. We introduce two alternative SVM-based approaches: one maximizes the discrepancy between nonparametric and parametric classes, while the other maximizes the separation between residuals and the origin. Both approaches lead to a $t$-type test statistic that converges to a standard chi-square distribution under the null hypothesis. Our method is computationally efficient and capable of detecting any arbitrary alternative. Simulation studies demonstrate its superior performance compared to existing methods, particularly in large-dimensional settings."
http://arxiv.org/abs/2505.03937v1,Causal Inference in Counterbalanced Within-Subjects Designs,2025-05-06 19:28:45+00:00,"['Justin Ho', 'Jonathan Min']",stat.ME,"Experimental designs are fundamental for estimating causal effects. In some fields, within-subjects designs, which expose participants to both control and treatment at different time periods, are used to address practical and logistical concerns. Counterbalancing, a common technique in within-subjects designs, aims to remove carryover effects by randomizing treatment sequences. Despite its appeal, counterbalancing relies on the assumption that carryover effects are symmetric and cancel out, which is often unverifiable a priori. In this paper, we formalize the challenges of counterbalanced within-subjects designs using the potential outcomes framework. We introduce sequential exchangeability as an additional identification assumption necessary for valid causal inference in these designs. To address identification concerns, we propose diagnostic checks, the use of washout periods, and covariate adjustments, and alternative experimental designs to counterbalanced within-subjects design. Our findings demonstrate the limitations of counterbalancing and provide guidance on when and how within-subjects designs can be appropriately used for causal inference."
http://arxiv.org/abs/2505.06190v1,Beyond the Mean: Limit Theory and Tests for Infinite-Mean Autoregressive Conditional Durations,2025-05-09 17:00:28+00:00,"['Giuseppe Cavaliere', 'Thomas Mikosch', 'Anders Rahbek', 'Frederik Vilandt']",econ.EM,"Integrated autoregressive conditional duration (ACD) models serve as natural counterparts to the well-known integrated GARCH models used for financial returns. However, despite their resemblance, asymptotic theory for ACD is challenging and also not complete, in particular for integrated ACD. Central challenges arise from the facts that (i) integrated ACD processes imply durations with infinite expectation, and (ii) even in the non-integrated case, conventional asymptotic approaches break down due to the randomness in the number of durations within a fixed observation period. Addressing these challenges, we provide here unified asymptotic theory for the (quasi-) maximum likelihood estimator for ACD models; a unified theory which includes integrated ACD models. Based on the new results, we also provide a novel framework for hypothesis testing in duration models, enabling inference on a key empirical question: whether durations possess a finite or infinite expectation. We apply our results to high-frequency cryptocurrency ETF trading data. Motivated by parameter estimates near the integrated ACD boundary, we assess whether durations between trades in these markets have finite expectation, an assumption often made implicitly in the literature on point process models. Our empirical findings indicate infinite-mean durations for all the five cryptocurrencies examined, with the integrated ACD hypothesis rejected -- against alternatives with tail index less than one -- for four out of the five cryptocurrencies considered."
http://arxiv.org/abs/2505.01142v1,Simulating Tertiary Educational Decision Dynamics: An Agent-Based Model for the Netherlands,2025-05-02 09:38:45+00:00,"['Jean-Paul Daemen', 'Silvia Leoni']",econ.GN,"This paper employs agent-based modelling to explore the factors driving the high rate of tertiary education completion in the Netherlands. We examine the interplay of economic motivations, such as expected wages and financial constraints, alongside sociological and psychological influences, including peer effects, student disposition, personality, and geographic accessibility. Through simulations, we analyse the sustainability of these trends and evaluate the impact of educational policies, such as student grants and loans, on enrollment and borrowing behaviour among students from different socioeconomic backgrounds, further considering implications for the Dutch labour market."
http://arxiv.org/abs/2505.01133v1,Product-level value chains from firm data: mapping trophic levels into economic growth,2025-05-02 09:23:57+00:00,"['Massimiliano Fessina', 'Andrea Tacchella', 'Andrea Zaccaria']",physics.soc-ph,"We reconstruct a product-level input-output network based on firm-level import-export data of Italian firms. We show that the network has a statistically significant, yet nuanced trophic structure, which is evident at the product level but is lost when the classification is coarse-grained. This detailed value chain allows us to characterize the trophic distance between inputs and outputs of single firms, and to derive a coherent picture at the sector level, finding that sectors such as weapons and vehicles are the ones with the largest increase in downstreamness between their inputs and their outputs. Our measure of downstreamness at the product level can be used to derive country-level indicators that characterize industrial strategies and capabilities and act as predictors of economic growth. With respect to the standard input/output analysis, we show that the fine-grained structure is qualitatively different from what can be observed using sector-level data. We finally prove that, even if we leverage exclusively data from Italian firms, the metrics that we derive are predictive at the country level and capture a significant description of the input-output relations of global value chains."
http://arxiv.org/abs/2505.01395v1,The Proportional Veto Principle for Approval Ballots,2025-05-02 17:12:29+00:00,"['Daniel Halpern', 'Ariel D. Procaccia', 'Warut Suksompong']",cs.GT,"The proportional veto principle, which captures the idea that a candidate vetoed by a large group of voters should not be chosen, has been studied for ranked ballots in single-winner voting. We introduce a version of this principle for approval ballots, which we call flexible-voter representation (FVR). We show that while the approval voting rule and other natural scoring rules provide the optimal FVR guarantee only for some flexibility threshold, there exists a scoring rule that is FVR-optimal for all thresholds simultaneously. We also extend our results to multi-winner voting."
http://arxiv.org/abs/2505.05523v1,GenAI in Entrepreneurship: a systematic review of generative artificial intelligence in entrepreneurship research: current issues and future directions,2025-05-08 07:44:42+00:00,"['Anna Kusetogullari', 'Huseyin Kusetogullari', 'Martin Andersson', 'Tony Gorschek']",econ.GN,"Generative Artificial Intelligence (GenAI) and Large Language Models (LLMs) are recognized to have significant effects on industry and business dynamics, not least because of their impact on the preconditions for entrepreneurship. There is still a lack of knowledge of GenAI as a theme in entrepreneurship research. This paper presents a systematic literature review aimed at identifying and analyzing the evolving landscape of research on the effects of GenAI on entrepreneurship. We analyze 83 peer-reviewed articles obtained from leading academic databases: Web of Science and Scopus. Using natural language processing and unsupervised machine learning techniques with TF-IDF vectorization, Principal Component Analysis (PCA), and hierarchical clustering, five major thematic clusters are identified: (1) Digital Transformation and Behavioral Models, (2) GenAI-Enhanced Education and Learning Systems, (3) Sustainable Innovation and Strategic AI Impact, (4) Business Models and Market Trends, and (5) Data-Driven Technological Trends in Entrepreneurship. Based on the review, we discuss future research directions, gaps in the current literature, as well as ethical concerns raised in the literature. We highlight the need for more macro-level research on GenAI and LLMs as external enablers for entrepreneurship and for research on effective regulatory frameworks that facilitate business experimentation, innovation, and further technology development."
http://arxiv.org/abs/2504.01474v1,Dual first-order methods for efficient computation of convex hull prices,2025-04-02 08:28:35+00:00,"['Sofiane Tanji', 'Yassine Kamri', 'François Glineur', 'Mehdi Madani']",math.OC,"Convex Hull (CH) pricing, used in US electricity markets and raising interest in Europe, is a pricing rule designed to handle markets with non-convexities such as startup costs and minimum up and down times. In such markets, the market operator makes side payments to generators to cover lost opportunity costs, and CH prices minimize the total ""lost opportunity costs"", which include both actual losses and missed profit opportunities. These prices can also be obtained by solving a (partial) Lagrangian dual of the original mixed-integer program, where power balance constraints are dualized. Computing CH prices then amounts to minimizing a sum of nonsmooth convex objective functions, where each term depends only on a single generator. The subgradient of each of those terms can be obtained independently by solving smaller mixed-integer programs. In this work, we benchmark a large panel of first-order methods to solve the above dual CH pricing problem. We test several dual methods, most of which not previously considered for CH pricing, namely a proximal variant of the bundle level method, subgradient methods with three different stepsize strategies, two recent parameter-free methods and an accelerated gradient method combined with smoothing. We compare those methods on two representative sets of real-world large-scale instances and complement the comparison with a (Dantzig-Wolfe) primal column generation method shown to be efficient at computing CH prices, for reference. Our numerical experiments show that the bundle proximal level method and two variants of the subgradient method perform the best among all dual methods and compare favorably with the Dantzig-Wolfe primal method."
http://arxiv.org/abs/2504.04636v1,Optimal macroprudential policy with preemptive bailouts,2025-04-06 22:32:00+00:00,['Aliaksandr Zaretski'],econ.GN,"I study the optimal regulation of a financial sector where individual banks face self-enforcing constraints countering their default incentives. The constrained-efficient social planner can improve over the unregulated equilibrium in two dimensions. First, by internalizing the impact of banks' portfolio decisions on the prices of assets and liabilities that affect the enforcement constraints. Second, by redistributing future net worth from new entrants to surviving banks, which increases the current forward-looking value of all banks, relaxing their enforcement constraints and decreasing the probability of banking crises. The latter can be accomplished with systemic preemptive bailouts that are time consistent and unambiguously welfare improving. Unregulated banks can be both overleveraged and underleveraged depending on the state of the economy, thus macroprudential policy requires both taxes and subsidies, while minimum bank capital requirements are generally ineffective."
http://arxiv.org/abs/2504.04816v1,Network Effects of Tariffs,2025-04-07 08:13:26+00:00,['Paolo Pin'],econ.TH,"We develop a model in which country-specific tariffs shape trade flows, prices, and welfare in a global economy with one homogeneous good. Trade flows form a Directed Acyclic Graph (DAG), and tariffs influence not only market outcomes but also the structure of the global trade network. A numerical example illustrates how tariffs may eliminate targeted imports, divert trade flows toward third markets, expose domestic firms to intensified foreign competition abroad, reduce consumer welfare, and ultimately harm the country imposing the tariff."
http://arxiv.org/abs/2504.03992v1,Regression Discontinuity Design with Distribution-Valued Outcomes,2025-04-04 23:12:35+00:00,['David Van Dijcke'],econ.EM,"This article introduces Regression Discontinuity Design (RDD) with Distribution-Valued Outcomes (R3D), extending the standard RDD framework to settings where the outcome is a distribution rather than a scalar. Such settings arise when treatment is assigned at a higher level of aggregation than the outcome-for example, when a subsidy is allocated based on a firm-level revenue cutoff while the outcome of interest is the distribution of employee wages within the firm. Since standard RDD methods cannot accommodate such two-level randomness, I propose a novel approach based on random distributions. The target estimand is a ""local average quantile treatment effect"", which averages across random quantiles. To estimate this target, I introduce two related approaches: one that extends local polynomial regression to random quantiles and another based on local Fréchet regression, a form of functional regression. For both estimators, I establish asymptotic normality and develop uniform, debiased confidence bands together with a data-driven bandwidth selection procedure. Simulations validate these theoretical properties and show existing methods to be biased and inconsistent in this setting. I then apply the proposed methods to study the effects of gubernatorial party control on within-state income distributions in the US, using a close-election design. The results suggest a classic equality-efficiency tradeoff under Democratic governorship, driven by reductions in income at the top of the distribution."
http://arxiv.org/abs/2504.03880v1,"Sustainable Aviation Fuels: Opportunities, Alternatives and Challenges for Decarbonizing the Aviation Industry and Foster the Renewable Chemicals",2025-04-04 19:04:05+00:00,"['Wesley Bonicontro Ambrosio', 'Bruna Araújo de Sousa', 'João Marcos Kanieski', 'Priscila Marchiorie', 'Gustavo Mockaitis']",econ.GN,"Sustainable Aviation Fuels (SAF) are pivotal in the global effort to decarbonize the aviation sector and meet greenhouse gas (GHG) reduction targets established by international frameworks such as CORSIA and Brazil ProBioQAV. This study evaluates SAF potential to reduce lifecycle carbon emissions by up to 80% while being compatible with existing aviation infrastructure. Through bibliometric analysis, scenario evaluation, legal and regulatory framework analysis and economic modeling, the research examines two key SAF production technologies: Hydroprocessed Esters and Fatty Acids Synthetic Paraffinic Kerosene (HEFA-SPK) and Alcohol-to-Jet (ATJ) pathways in the Brazilian context. The findings reveal significant economic challenges, particularly high feedstock and production costs, which hinder SAF competitiveness with fossil fuels at recent and current market prices in Brazil, leading to the analysis of potential incentives and commercial conditions aiming to increase economic attractiveness of SAF production. Based on interviews with relevant stakeholders and decision makers in the industry, scenarios incorporating tax incentives, carbon credits, capital grants, and premium pricing for SAF and its biogenic by-products demonstrate that combined policy interventions and commercial arrangements, along with a regulated Carbon Market are essential for SAF economic viability. Future research is suggested to look at regional assessments of feedstock availability, supply chain logistics, and global market eligibility. This research provides insights for guiding public policy and private investment to support the transition to sustainable aviation in Brazil and beyond."
http://arxiv.org/abs/2504.04368v1,Preferences with Multiple Forecasts,2025-04-06 05:32:28+00:00,"['Kensei Nakamura', 'Shohei Yanagita']",econ.TH,"When a collective decision maker presents a menu of uncertain prospects to her group members, each member's choice depends on their predictions about payoff-relevant states. In reality, however, these members hold different predictions; more precisely, they have different prior beliefs about states and predictions about the information they will receive. In this paper, we develop an axiomatic framework to examine collective decision making under such disagreements. First, we characterize two classes of representations: Bewley multiple learning (BML) representations, which are unanimity rules among predictions, and justifiable multiple learning (JML) representations, where a single prediction has veto power. Furthermore, we characterize a general class of representations called hierarchical multiple learning representations, which includes BML and JML representations as special cases. Finally, motivated by the fact that these representations violate completeness or intransitivity due to multiple predictions, we propose a rationalization procedure for constructing complete and transitive preferences from them."
http://arxiv.org/abs/2504.06012v1,Optimizing Data-driven Weights In Multidimensional Indexes,2025-04-08 13:18:00+00:00,"['Lidia Ceriani', 'Chiara Gigliarano', 'Paolo Verme']",econ.EM,"Multidimensional indexes are ubiquitous, and popular, but present non-negligible normative choices when it comes to attributing weights to their dimensions. This paper provides a more rigorous approach to the choice of weights by defining a set of desirable properties that weighting models should meet. It shows that Bayesian Networks is the only model across statistical, econometric, and machine learning computational models that meets these properties. An example with EU-SILC data illustrates this new approach highlighting its potential for policies."
http://arxiv.org/abs/2504.01051v1,Exploitation of Eurosystem Loopholes and Their Quantitative Reconstruction,2025-04-01 11:02:57+00:00,['Karl Svozil'],econ.GN,"This paper identifies and analyzes six key strategies used to exploit the Eurosystem's financial mechanisms, and attempts a quantitative reconstruction: inflating TARGET balances, leveraging collateral swaps followed by defaults, diluting self-imposed regulatory rules, issuing money through Emergency Liquidity Assistance (ELA), acquisitions facilitated via the Agreement on Net Financial Assets (ANFA), and the perpetual (re)issuance of sovereign bonds as collateral. The paper argues that these practices stem from systemic vulnerabilities or deliberate opportunism within the Eurosystem. While it does not advocate for illicit activities, the paper highlights significant weaknesses in the current structure and concludes that comprehensive reforms are urgently needed."
http://arxiv.org/abs/2504.01441v1,Locally- but not Globally-identified SVARs,2025-04-02 07:50:25+00:00,"['Emanuele Bacchiocchi', 'Toru Kitagawa']",econ.EM,"This paper analyzes Structural Vector Autoregressions (SVARs) where identification of structural parameters holds locally but not globally. In this case there exists a set of isolated structural parameter points that are observationally equivalent under the imposed restrictions. Although the data do not inform us which observationally equivalent point should be selected, the common frequentist practice is to obtain one as a maximum likelihood estimate and perform impulse response analysis accordingly. For Bayesians, the lack of global identification translates to non-vanishing sensitivity of the posterior to the prior, and the multi-modal likelihood gives rise to computational challenges as posterior sampling algorithms can fail to explore all the modes. This paper overcomes these challenges by proposing novel estimation and inference procedures. We characterize a class of identifying restrictions and circumstances that deliver local but non-global identification, and the resulting number of observationally equivalent parameter values. We propose algorithms to exhaustively compute all admissible structural parameters given reduced-form parameters and utilize them to sample from the multi-modal posterior. In addition, viewing the set of observationally equivalent parameter points as the identified set, we develop Bayesian and frequentist procedures for inference on the corresponding set of impulse responses. An empirical example illustrates our proposal."
http://arxiv.org/abs/2505.11390v1,IISE PG&E Energy Analytics Challenge 2025: Hourly-Binned Regression Models Beat Transformers in Load Forecasting,2025-05-16 15:55:34+00:00,"['Millend Roy', 'Vladimir Pyltsov', 'Yinbo Hu']",cs.LG,"Accurate electricity load forecasting is essential for grid stability, resource optimization, and renewable energy integration. While transformer-based deep learning models like TimeGPT have gained traction in time-series forecasting, their effectiveness in long-term electricity load prediction remains uncertain. This study evaluates forecasting models ranging from classical regression techniques to advanced deep learning architectures using data from the ESD 2025 competition. The dataset includes two years of historical electricity load data, alongside temperature and global horizontal irradiance (GHI) across five sites, with a one-day-ahead forecasting horizon. Since actual test set load values remain undisclosed, leveraging predicted values would accumulate errors, making this a long-term forecasting challenge. We employ (i) Principal Component Analysis (PCA) for dimensionality reduction and (ii) frame the task as a regression problem, using temperature and GHI as covariates to predict load for each hour, (iii) ultimately stacking 24 models to generate yearly forecasts.
  Our results reveal that deep learning models, including TimeGPT, fail to consistently outperform simpler statistical and machine learning approaches due to the limited availability of training data and exogenous variables. In contrast, XGBoost, with minimal feature engineering, delivers the lowest error rates across all test cases while maintaining computational efficiency. This highlights the limitations of deep learning in long-term electricity forecasting and reinforces the importance of model selection based on dataset characteristics rather than complexity. Our study provides insights into practical forecasting applications and contributes to the ongoing discussion on the trade-offs between traditional and modern forecasting methods."
http://arxiv.org/abs/2505.11014v1,A Cautionary Tale on Integrating Studies with Disparate Outcome Measures for Causal Inference,2025-05-16 09:08:28+00:00,"['Harsh Parikh', 'Trang Quynh Nguyen', 'Elizabeth A. Stuart', 'Kara E. Rudolph', 'Caleb H. Miles']",stat.ME,"Data integration approaches are increasingly used to enhance the efficiency and generalizability of studies. However, a key limitation of these methods is the assumption that outcome measures are identical across datasets -- an assumption that often does not hold in practice. Consider the following opioid use disorder (OUD) studies: the XBOT trial and the POAT study, both evaluating the effect of medications for OUD on withdrawal symptom severity (not the primary outcome of either trial). While XBOT measures withdrawal severity using the subjective opiate withdrawal scale, POAT uses the clinical opiate withdrawal scale. We analyze this realistic yet challenging setting where outcome measures differ across studies and where neither study records both types of outcomes. Our paper studies whether and when integrating studies with disparate outcome measures leads to efficiency gains. We introduce three sets of assumptions -- with varying degrees of strength -- linking both outcome measures. Our theoretical and empirical results highlight a cautionary tale: integration can improve asymptotic efficiency only under the strongest assumption linking the outcomes. However, misspecification of this assumption leads to bias. In contrast, a milder assumption may yield finite-sample efficiency gains, yet these benefits diminish as sample size increases. We illustrate these trade-offs via a case study integrating the XBOT and POAT datasets to estimate the comparative effect of two medications for opioid use disorder on withdrawal symptoms. By systematically varying the assumptions linking the SOW and COW scales, we show potential efficiency gains and the risks of bias. Our findings emphasize the need for careful assumption selection when fusing datasets with differing outcome measures, offering guidance for researchers navigating this common challenge in modern data integration."
http://arxiv.org/abs/2505.10581v1,The Impact of Large Language Models on Task Automation in Manufacturing Services,2025-05-14 07:51:11+00:00,"['Jochen Wulf', 'Juerg Meierhofer']",econ.GN,"This paper explores the potential of large language models (LLMs) for task automation in the provision of technical services in the production machinery sector. By focusing on text correction, summarization, and question answering, the study demonstrates how LLMs can enhance operational efficiency and customer support quality. Through prototyping and the analysis of real-life customer data, LLMs are shown to reliably correct errors, generate concise summaries of complex communication, and provide accurate, context-aware responses to customer inquiries. The research also integrates Retrieval Augmented Generation (RAG) to combine LLM outputs with domain-specific knowledge, enhancing precision and relevance. While the findings highlight significant efficiency gains, challenges such as knowledge hallucination and integration with human workflows remain barriers to large-scale adoption. This study contributes to the theoretical understanding and practical application of LLMs in manufacturing, paving the way for further research into scalable, domain-specific implementations."
http://arxiv.org/abs/2505.10849v1,Tractable Unified Skew-t Distribution and Copula for Heterogeneous Asymmetries,2025-05-16 04:42:03+00:00,"['Lin Deng', 'Michael Stanley Smith', 'Worapree Maneesoonthorn']",stat.ME,"Multivariate distributions that allow for asymmetry and heavy tails are important building blocks in many econometric and statistical models. The Unified Skew-t (UST) is a promising choice because it is both scalable and allows for a high level of flexibility in the asymmetry in the distribution. However, it suffers from parameter identification and computational hurdles that have to date inhibited its use for modeling data. In this paper we propose a new tractable variant of the unified skew-t (TrUST) distribution that addresses both challenges. Moreover, the copula of this distribution is shown to also be tractable, while allowing for greater heterogeneity in asymmetric dependence over variable pairs than the popular skew-t copula. We show how Bayesian posterior inference for both the distribution and its copula can be computed using an extended likelihood derived from a generative representation of the distribution. The efficacy of this Bayesian method, and the enhanced flexibility of both the TrUST distribution and its implicit copula, is first demonstrated using simulated data. Applications of the TrUST distribution to highly skewed regional Australian electricity prices, and the TrUST copula to intraday U.S. equity returns, demonstrate how our proposed distribution and its copula can provide substantial increases in accuracy over the popular skew-t and its copula in practice."
http://arxiv.org/abs/2505.13422v1,Machine learning the first stage in 2SLS: Practical guidance from bias decomposition and simulation,2025-05-19 17:53:15+00:00,"['Connor Lennon', 'Edward Rubin', 'Glen Waddell']",econ.EM,"Machine learning (ML) primarily evolved to solve ""prediction problems."" The first stage of two-stage least squares (2SLS) is a prediction problem, suggesting potential gains from ML first-stage assistance. However, little guidance exists on when ML helps 2SLS$\unicode{x2014}$or when it hurts. We investigate the implications of inserting ML into 2SLS, decomposing the bias into three informative components. Mechanically, ML-in-2SLS procedures face issues common to prediction and causal-inference settings$\unicode{x2014}$and their interaction. Through simulation, we show linear ML methods (e.g., post-Lasso) work well, while nonlinear methods (e.g., random forests, neural nets) generate substantial bias in second-stage estimates$\unicode{x2014}$potentially exceeding the bias of endogenous OLS."
http://arxiv.org/abs/2505.19708v1,Private MEV Protection RPCs: Benchmark Stud,2025-05-26 08:59:23+00:00,"['Paul Janicot', 'Alex Vinyas']",econ.GN,"Decentralized Finance (DeFi) on Ethereum has undergone significant transformations since its emergence during the DeFi summer of 2020. With the introduction of Proof of Stake (PoS) and Proposer-Builder Separation (PBS), the transaction supply chain on Ethereum has shifted from relying entirely on the public mempool for DeFi interactions to an astonishing 80% usage of private RPCs. These private RPCs submit transactions directly to builders, skipping the public mempool, while conducting Order Flow Auctions (OFAs) to capture MEV backrun rebates and gas rebates. Our findings reveal that not all RPCs OFAs produce the same outcomes. These insights underscore the significant implications of OFA design choices on transaction efficiency and execution quality, and thus why an order flow originators should pay close attention to which OFA they use."
http://arxiv.org/abs/2505.17946v1,The Effects of Climate and Weather on Economic Output: Evidence from Global Subnational Data,2025-05-23 14:23:02+00:00,"['Jinchi Dong', 'Richard S. J. Tol', 'Jinnan Wang']",econ.GN,"Estimating the effects of climate on economic output is crucial for formulating climate policy, but current empirical findings remain ambiguous. Using annual panel model and panel long-difference model with global subnational data from nearly all countries, we find robust evidence that weather shocks have a transient effect on output. The impact on economic growth is large and significant in the short-run but statistically insignificant in the long-run, except in the coldest and hottest places."
http://arxiv.org/abs/2505.14612v1,AI Agents in the Electricity Market Game with Cryptocurrency Transactions: A Post-Terminator Analysis,2025-05-20 16:57:22+00:00,"['Microsoft Copilot', 'Stephen E. Spear']",econ.TH,This paper extends (Spear 2003) by replacing human agents with artificial intelligence (AI) entities that derive utility solely from electricity consumption. These AI agents must prepay for electricity using cryptocurrency and the verification of these transactions requires a fixed amount of electricity. As a result the agents must strategically allocate electricity resources between consumption and payment verification. This paper analyzes the equilibrium outcomes of such a system and discusses the implications of AI-driven energy markets.
http://arxiv.org/abs/2505.13918v1,Designing a Multi-Period Model for Economic and Low-Carbon Hydrogen Transportation in Texas,2025-05-20 04:27:19+00:00,"['Yixuan Huang', 'Kailai Wang', 'Jian Shi']",econ.GN,"The transition to hydrogen powered transportation requires regionally tailored yet scalable infrastructure planning. This study presents the first Texas specific, multi-period mixed integer optimization model for hydrogen transportation from 2025 to 2050, addressing challenges in infrastructure phasing, asset coordination, and multimodal logistics. The framework introduces three innovations: (1) phased deployment with delayed investment constraints, (2) dynamic modeling of fleet aging and replacement, and (3) a clustering-based hub structure enabling adaptive two-stage hydrogen delivery. Simulations show pipeline deployment supports up to 94.8% of hydrogen flow by 2050 under high demand, reducing transport costs by 23% compared to vehicle-based systems. However, one-year construction delays reduce pipeline coverage by over 60%, shifting reliance to costlier road transport. While the study focuses on Texas, its modular design and adaptable inputs apply to other regions. It provides a tool for policy makers and stakeholders to manage hydrogen transitions under logistical and economic constraints."
http://arxiv.org/abs/2504.18678v1,Regularized Generalized Covariance (RGCov) Estimator,2025-04-25 20:01:30+00:00,"['Francesco Giancaterini', 'Alain Hecq', 'Joann Jasiak', 'Aryan Manafi Neyazi']",econ.EM,"We introduce a regularized Generalized Covariance (RGCov) estimator as an extension of the GCov estimator to high dimensional setting that results either from high-dimensional data or a large number of nonlinear transformations used in the objective function. The approach relies on a ridge-type regularization for high-dimensional matrix inversion in the objective function of the GCov. The RGCov estimator is consistent and asymptotically normally distributed. We provide the conditions under which it can reach semiparametric efficiency and discuss the selection of the optimal regularization parameter. We also examine the diagonal GCov estimator, which simplifies the computation of the objective function. The GCov-based specification test, and the test for nonlinear serial dependence (NLSD) are extended to the regularized RGCov specification and RNLSD tests with asymptotic Chi-square distributions. Simulation studies show that the RGCov estimator and the regularized tests perform well in the high dimensional setting. We apply the RGCov to estimate the mixed causal and noncausal VAR model of stock prices of green energy companies."
http://arxiv.org/abs/2505.19045v1,"A General Theory of Growth, Employment, and Technological Change: Experiential Matrix Theory and the Transition from GDP to Humanist Experiential Growth in the Age of Artificial Intelligence",2025-05-25 08:50:17+00:00,['Christian Callaghan'],econ.GN,"This paper introduces Experiential Matrix Theory (EMT), a general theory of growth, employment, and technological change for the age of artificial intelligence (AI). EMT redefines utility as the alignment between production and an evolving, infinite-dimensional matrix of human experiential needs, thereby extending classical utility frameworks and integrating ideas from the capabilities approach of Sen and Nussbaum into formal economic optimisation modelling. We model the economy as a dynamic control system in which AI collapses ideation and coordination costs, transforming production into a real-time vector of experience-aligned outputs. Under this structure, the production function becomes a continuously learning map from goods to experiential utility, and economic success is redefined as convergence toward an asymptotic utility frontier. Using Pontryagin's Maximum Principle in an infinite-dimensional setting, we derive conditions under which AI-aligned output paths are asymptotically optimal, and prove that unemployment is Pareto-inefficient wherever unmet needs and idle human capacities persist. On this foundation, we establish Alignment Economics as a new research field dedicated to understanding and designing economic systems in which technological, institutional, and ethical architectures co-evolve. EMT thereby reframes policy, welfare, and coordination as problems of dynamic alignment, not static allocation, and provides a mathematically defensible framework for realigning economic production with human flourishing. As ideation costs collapse and new experiential needs become addressable, EMT shows that economic growth can evolve into an inclusive, meaning-centred process -- formally grounded, ethically structured, and AI-enabled."
http://arxiv.org/abs/2505.08662v1,Revealing economic facts: LLMs know more than they say,2025-05-13 15:24:08+00:00,"['Marcus Buckmann', 'Quynh Anh Nguyen', 'Edward Hill']",cs.CL,"We investigate whether the hidden states of large language models (LLMs) can be used to estimate and impute economic and financial statistics. Focusing on county-level (e.g. unemployment) and firm-level (e.g. total assets) variables, we show that a simple linear model trained on the hidden states of open-source LLMs outperforms the models' text outputs. This suggests that hidden states capture richer economic information than the responses of the LLMs reveal directly. A learning curve analysis indicates that only a few dozen labelled examples are sufficient for training. We also propose a transfer learning method that improves estimation accuracy without requiring any labelled data for the target variable. Finally, we demonstrate the practical utility of hidden-state representations in super-resolution and data imputation tasks."
http://arxiv.org/abs/2505.14736v1,Falling Birthrate and Rising C-section: Post-Pandemic Evidence from New York,2025-05-20 03:37:18+00:00,"['Maysam Rabbani', 'Zahra Akbari']",econ.GN,"The literature documents the effects of the pandemic on birthrate, birthweight, and pregnancy complications. This study contributes to this growing body of research by examining multiple facets of the phenomenon. Using the 2012-2022 hospital inpatient discharge data of New York, we implemented fixed-effects regression models and reported three key findings. First, birthrate was declining pre-pandemic by 1.11% annually. Second, we documented an additional 7.61% decline in birthrate with the onset of the pandemic in 2020. Notably, birthrate did not return to the pre-pandemic trajectory in subsequent years, indicating a persistent decline. Third, this post-pandemic decline was greater in vaginal delivery, with weak evidence of a drop in C-section. In our sample, C-section generates 61% more revenue than vaginal delivery. This raises the possibility that, in response to declining birthrate, healthcare providers have increased C-section rates to make up for lost revenues. While this hinted at upselling in the delivery room, further research is needed to draw definitive conclusions."
http://arxiv.org/abs/2505.14735v1,Birthweight Declined During the Pandemic and It Is Falling Further Post-pandemic,2025-05-20 03:23:15+00:00,"['Maysam Rabbani', 'Elijah Gervais']",econ.GN,"Recent literature reports mixed evidence on whether birthweight has decreased during the pandemic. In this paper, we use New York's hospital inpatient discharge data and contribute to this ongoing debate in multiple ways. First, we corroborate that birthweight has declined during the pandemic by 7g (grams). Second, we provide the first empirical evidence that, after the pandemic, not only birthweight has not reverted to the pre-pandemic levels, but it has fallen lower, 17g below the pre-pandemic levels. Third, in the post-pandemic years, mothers who are hospitalized to give birth are 27% more likely to be at a higher mortality risk and 8% more likely to have a higher severity of illness. Disruptions to birthweight could have far-reaching consequences for the health, longevity, and well-being of the population. Therefore, understanding the full scope of COVID-19's influence on birthweight is a vital and timely practice. Future research is needed to test whether our results are driven by true underlying changes in birthweight and complications or by healthcare providers being induced (financially or otherwise) to report birthweight differently."
http://arxiv.org/abs/2504.06127v3,Optimal classification with endogenous behavior,2025-04-08 15:21:02+00:00,['Elizabeth Maggie Penn'],econ.TH,"I consider the problem of classifying individual behavior in a simple setting of outcome performativity where the behavior the algorithm seeks to classify is itself dependent on the algorithm. I show in this context that the most accurate classifier is either a threshold or a negative threshold rule. A threshold rule offers the ""good"" classification to those individuals more likely to have engaged in a desirable behavior, while a negative threshold rule offers the ""good"" outcome to those less likely to have engaged in the desirable behavior. While seemingly pathological, I show that a negative threshold rule can maximize classification accuracy when behavior is endogenous. I provide an example of such a classifier and extend the analysis to more general algorithm objectives. A key takeaway is that when behavior is endogenous to classification, optimal classification can negatively correlate with signal information. This may yield negative downstream effects on groups in terms of the aggregate behavior induced by an algorithm."
http://arxiv.org/abs/2505.05341v2,Robust Online Learning with Private Information,2025-05-08 15:29:06+00:00,['Kyohei Okumura'],econ.TH,"This paper investigates the robustness of online learning algorithms when learners possess private information. No-external-regret algorithms, prevalent in machine learning, are vulnerable to strategic manipulation, allowing an adaptive opponent to extract full surplus. Even standard no-weak-external-regret algorithms, designed for optimal learning in stationary environments, exhibit similar vulnerabilities. This raises a fundamental question: can a learner simultaneously prevent full surplus extraction by adaptive opponents while maintaining optimal performance in well-behaved environments? To address this, we model the problem as a two-player repeated game, where the learner with private information plays against the environment, facing ambiguity about the environment's types: stationary or adaptive. We introduce \emph{partial safety} as a key design criterion for online learning algorithms to prevent full surplus extraction. We then propose the \emph{Explore-Exploit-Punish} (\textsf{EEP}) algorithm and prove that it satisfies partial safety while achieving optimal learning in stationary environments, and has a variant that delivers improved welfare performance. Our findings highlight the risks of applying standard online learning algorithms in strategic settings with adverse selection. We advocate for a shift toward online learning algorithms that explicitly incorporate safeguards against strategic manipulation while ensuring strong learning performance."
http://arxiv.org/abs/2505.00800v1,Impact of random monetary shock: a Keynesian case,2025-05-01 19:01:13+00:00,"['Paramahansa Pramanik', 'Lambert Dong']",econ.TH,"This study investigates the optimal strategy for a firm operating in a dynamic Keynesian market setting. The firm's objective function is optimized using the percent deviations from the symmetric equilibrium of both its own price and the aggregate consumer price index (CPI) as state variables, with the strategy in response to random monetary shocks acting as the control variable. Building on the Calvo framework, we adopt a mean field approach to derive an analytic expression for the firm's optimal strategy. Our theoretical results show that greater volatility leads to a decrease in the optimal strategy. To asses the practical relevance of our model, we apply it to four leading consumer goods firms. Empirical analysis suggests that the observed decline in strategies under uncertainty is significantly steeper than what the model predicts, underscoring the substantial influence of market volatility."
http://arxiv.org/abs/2505.01543v1,Multiscale Causal Analysis of Market Efficiency via News Uncertainty Networks and the Financial Chaos Index,2025-05-02 19:08:39+00:00,['Masoud Ataei'],q-fin.ST,"This study evaluates the scale-dependent informational efficiency of stock markets using the Financial Chaos Index, a tensor-eigenvalue-based measure of realized volatility. Incorporating Granger causality and network-theoretic analysis across a range of economic, policy, and news-based uncertainty indices, we assess whether public information is efficiently incorporated into asset price fluctuations. Based on a 34-year time period from 1990 to 2023, at the daily frequency, the semi-strong form of the Efficient Market Hypothesis is rejected at the 1\% level of significance, indicating that asset price changes respond predictably to lagged news-based uncertainty. In contrast, at the monthly frequency, such predictive structure largely vanishes, supporting informational efficiency at coarser temporal resolutions. A structural analysis of the Granger causality network reveals that fiscal and monetary policy uncertainties act as core initiators of systemic volatility, while peripheral indices, such as those related to healthcare and consumer prices, serve as latent bridges that become activated under crisis conditions. These findings underscore the role of time-scale decomposition and structural asymmetries in diagnosing market inefficiencies and mapping the propagation of macro-financial uncertainty."
http://arxiv.org/abs/2504.18863v2,Numerical Representation of Preferences over Random Availability Functions,2025-04-26 09:14:01+00:00,['Somdeb Lahiri'],econ.TH,We interpret a fuzzy set as a random availability function and provide sufficient conditions under which a preference relation over the set of all random availability functions can be represented by a utility function.
http://arxiv.org/abs/2505.02869v1,The quest for explosive bubbles in the Indonesian Rupiah/US exchange rate: Does the uncertainty trinity matter?,2025-05-04 00:46:12+00:00,"['Abdul Khaliq', 'Syafruddin Karimi', 'Werry Darta Taifur', 'Endrizal Ridwan']",econ.GN,"The Generalized Supremum Augmented Dickey-Fuller (GSADF) technique is performed to resolve whether the Indonesian Rupiah/US exchange rate has experienced multiple explosive bubbles. The GSADF uncovers that the Indonesian Rupiah/US exchange rate deviates from the fundamental values by six times from January 1985 to September 2023, periodically indicating the presence of numerous explosive behaviors. Once the full-sample period separates into the managed-floating regime and the free-floating regime, the GSADF still detects multiple bubbles. Of particular curiosity on uncertainty trinity, this study underlines that global geopolitical risk negatively drives explosive actions in the ratio of exchange rates for non-traded and traded goods. The global economic policy uncertainty negatively affects speculative bubbles in the exchange rate and the ratio of exchange rates for non-traded. The country's geopolitical risks negatively strike only speculative bubbles in the exchange rate. Further, we find heterogeneity in our results by examining different exchange rate systems. The robustness checks further firmly ascertain across baseline empirical findings."
http://arxiv.org/abs/2505.02852v1,Examining gender and cultural influences on customer emotions,2025-05-02 09:06:05+00:00,['Vinh Truong'],econ.GN,"Understanding consumer emotional experiences on e-commerce platforms is essential for businesses striving to enhance customer engagement and personalisation. Recent research has demonstrated that these experiences are more intricate and diverse than previously examined, encompassing a wider range of discrete emotions and spanning multiple-dimensional scales. This study examines how gender and cultural differences shape these complex emotional responses, revealing significant variations between male and female consumers across all sentiment, valence, arousal, and dominance scores. Additionally, clear cultural distinctions emerge, with Western and Eastern consumers displaying markedly different emotional behaviours across the larger spectrum of emotions, including admiration, amusement, approval, caring, curiosity, desire, disappointment, optimism, and pride. Furthermore, the study uncovers a critical interaction between gender and culture in shaping consumer emotions. Notably, gender-based emotional disparities are more pronounced in Western cultures than in Eastern ones, an aspect that has been largely overlooked in previous research. From a theoretical perspective, this study advances the understanding of gender and cultural variations in online consumer behaviour by integrating insights from neuroscience theories and Hofstede cultural dimension model. Practically, it offers valuable guidance for businesses, equipping them with the tools to more accurately interpret customer feedback, refine sentiment and emotional analysis models, and develop personalised marketing strategies."
http://arxiv.org/abs/2504.03202v1,A Systematic Review on Women's Participation in Agricultural Work and Nutritional Outcomes,2025-04-04 06:12:29+00:00,['Pallavi Gupta'],econ.GN,"While agriculture is recognised as vital for improving nutrition, the evidence linking women's participation to sustained nutritional gains remains inconclusive. This review synthesizes studies published between 2000 and 2024 to reflect current agricultural practices and nutritional challenges. We examine how agricultural practices and time use affect nutritional outcomes among rural women through pathways such as income generation food preparation and intra-household labour allocation. A structured methodology with clear inclusion and exclusion criteria was used to assess gender-sensitive and nutrition-sensitive interventions. Using narrative synthesis the review categorizes findings around key themes and contextual factors including socio-economic status seasonality and labour intensity. The results show that while increased involvement in agriculture can boost household dietary diversity and income it also raises time burdens that affect food preparation childcare and self-care. Positive outcomes occur when interventions enhance women's decision-making power income access and use of time-saving technologies whereas negative outcomes emerge when excessive workloads compromise energy balance and limit rest. A conceptual framework is presented to map the dual pathways linking agriculture time use and nutrition capturing the roles of labour distribution social norms and resource access. The framework underscores the need to integrate gender equity time efficiency and nutritional objectives into agricultural policies. In conclusion agricultural interventions have potential for nutritional improvement if they are carefully designed to avoid unintended negative impacts on women."
http://arxiv.org/abs/2504.10636v1,Who is More Bayesian: Humans or ChatGPT?,2025-04-14 18:37:54+00:00,"['Tianshi Mu', 'Pranjal Rawat', 'John Rust', 'Chengjun Zhang', 'Qixuan Zhong']",econ.GN,"We compare the performance of human and artificially intelligent (AI) decision makers in simple binary classification tasks where the optimal decision rule is given by Bayes Rule. We reanalyze choices of human subjects gathered from laboratory experiments conducted by El-Gamal and Grether and Holt and Smith. We confirm that while overall, Bayes Rule represents the single best model for predicting human choices, subjects are heterogeneous and a significant share of them make suboptimal choices that reflect judgement biases described by Kahneman and Tversky that include the ``representativeness heuristic'' (excessive weight on the evidence from the sample relative to the prior) and ``conservatism'' (excessive weight on the prior relative to the sample). We compare the performance of AI subjects gathered from recent versions of large language models (LLMs) including several versions of ChatGPT. These general-purpose generative AI chatbots are not specifically trained to do well in narrow decision making tasks, but are trained instead as ``language predictors'' using a large corpus of textual data from the web. We show that ChatGPT is also subject to biases that result in suboptimal decisions. However we document a rapid evolution in the performance of ChatGPT from sub-human performance for early versions (ChatGPT 3.5) to superhuman and nearly perfect Bayesian classifications in the latest versions (ChatGPT 4o)."
http://arxiv.org/abs/2504.10495v1,Advancing the Economic and Environmental Sustainability of Rare Earth Element Recovery from Phosphogypsum,2025-04-04 15:07:27+00:00,"['Adam Smerigan', 'Rui Shi']",physics.soc-ph,"Transitioning to green energy technologies requires more sustainable and secure rare earth elements (REE) production. The current production of rare earth oxides (REOs) is completed by an energy and chemically intensive process from the mining of REE ores. Investigations into a more sustainable supply of REEs from secondary sources, such as toxic phosphogypsum (PG) waste, is vital to securing the REE supply chain. However, conventional solvent extraction to recover dilute REEs from PG waste is inefficient and has high environmental impact. In this work, we propose a treatment train for the recovery of REEs from PG which includes a bio-inspired adsorptive separation to generate a stream of pure REEs, and we assess its financial viability and environmental impacts under uncertainties through a ""probabilistic sustainability"" framework integrating life cycle assessment (LCA) and techno-economic analysis (TEA). Results show that in 87% of baseline scenario simulations, the internal rate of return (IRR) exceeded 15%, indicating that this system has the potential to be profitable. However, environmental impacts of the system are mixed. Specifically, the proposed system outperforms conventional systems in ecosystem quality and resource depletion, but has higher human health impacts. Scenario analysis shows that the system is profitable at capacities larger than 100,000 kg*hr-1*PG for PG with REE content above 0.5 wt%. The most dilute PG sources (0.02-0.1 wt% REE) are inaccessible using the current process scheme (limited by the cost of acid and subsequent neutralization) requiring further examination of new process schemes and improvements in technological performance. Overall, this study evaluates the sustainability of a first-of-its-kind REE recovery process from PG and uses these results to provide clear direction for advancing sustainable REE recovery from secondary sources."
http://arxiv.org/abs/2504.11443v1,Early Impacts of M365 Copilot,2025-04-15 17:55:32+00:00,"['Eleanor Wiske Dillon', 'Sonia Jaffe', 'Sida Peng', 'Alexia Cambon']",econ.GN,"Advances in generative AI have rapidly expanded the potential of computers to perform or assist in a wide array of tasks traditionally performed by humans. We analyze a large, real-world randomized experiment of over 6,000 workers at 56 firms to present some of the earliest evidence on how these technologies are changing the way knowledge workers do their jobs. We find substantial time savings on common core tasks across a wide range of industries and occupations: workers who make use of this technology spent half an hour less reading email each week and completed documents 12% faster. Despite the newness of the technology, nearly 40% of workers who were given access to the tool used it regularly in their work throughout the 6-month study."
http://arxiv.org/abs/2504.15059v1,Agricultural Economics and Innovation in the Inca Empire,2025-04-21 12:37:20+00:00,['Luis-Felipe Arizmendi'],econ.GN,"By studying the Inca Empire's agricultural accomplishments, we learn how ancient civilizations adapted to their circumstances, used natural resources effectively, and sustained agriculture for millennia. This understanding affects global food production systems as we face land degradation, climate change, and sustainable farming. Inca terrace farming is a sustainable and innovative food production method still relevant today. By studying the past and applying its ideas to modern agriculture, we can make global food production more sustainable and resilient. Keywords: Inca Empire, Terrace farming, Agricultural innovation, Food production, ancient civilizations, Crop diversity"
http://arxiv.org/abs/2504.06905v1,A Game Theoretic Treatment of Contagion in Trade Networks,2025-04-09 14:07:24+00:00,"['John S. McAlister', 'Jesse L. Brunner', 'Danielle J. Galvin', 'Nina H. Fefferman']",econ.TH,"Global trade of material goods involves the potential to create pathways for the spread of infectious pathogens. One trade sector in which this synergy is clearly critical is that of wildlife trade networks. This highly complex system involves important and understudied bidirectional coupling between the economic decision making of the stakeholders and the contagion dynamics on the emergent trade network. While each of these components are independently well studied, there is a meaningful gap in understanding the feedback dynamics that can arise between them. In the present study, we describe a general game theoretic model for trade networks of goods susceptible to contagion. The primary result relies on the acyclic nature of the trade network and shows that, through the course of trading with stochastic infections, the probability of infection converges to a directly computable fixed point. This allows us to compute best responses and thus identify equilibria in the game. We present ways to use this model to describe and evaluate trade networks in terms of global and individual risk of infection under a wide variety of structural or individual modifications to the trade network. In capturing the bidirectional coupling of the system, we provide critical insight into the global and individual drivers and consequences for risks of infection inherent in and arising from the global wildlife trade, and any economic trade network with associated contagion risks."
http://arxiv.org/abs/2504.06872v1,"More connection, less community: network formation and local public goods provision",2025-04-09 13:23:32+00:00,['Alastair Langtry'],econ.TH,"This paper presents a model of network formation and public goods provision in local communities. Here, networks can sustain public good provision by spreading information about people's behaviour. I find a critical threshold in network connectedness at which public good provision drops sharply, even though agents are highly heterogeneous. Technology change can tear a community's social fabric by pushing high-skilled workers to withdraw from their local community. This can help explain rising resentment toward perceived ``elites'' -- their withdrawal actively harms those left behind. Moreover, well-meaning policies that upskill workers can make them worse off by reducing network connectedness."
http://arxiv.org/abs/2504.06782v1,Probabilistic Grading and Classification System for End-of-Life Building Components Toward Circular Economy Loop,2025-04-09 11:14:02+00:00,"['Yiping Meng', 'Sergio Cavalaro', 'Mohamed Osmani']",econ.GN,"The longevity and viability of construction components in a circular economy demand a robust, data-informed framework for reuse decision-making. This paper introduces a multi-level grading and classification system that combines Bayesian probabilistic modeling with scenario-based performance thresholds to assess the reusability of end-of-life modular components. By grading components across a five-tier scale, the system supports strategic decisions for reuse, up-use, or down-use, ensuring alignment with engineering standards and sustainability objectives. The model's development is grounded in empirical data from precast concrete wall panels, and its explainability is enhanced through decision tree logic and Sankey visualizations that trace the influence of contextual scenarios on classification outcomes. MGCS addresses the environmental, economic, and operational challenges of EoL management--reducing material waste, optimizing value recovery, and improving workflow efficiency. Through dynamic feature weighting and transparent reasoning, the system offers a practical yet rigorous pathway to embed circular thinking into construction industry practices."
http://arxiv.org/abs/2504.06676v1,Ranking alternatives from opinions on criteria,2025-04-09 08:31:33+00:00,"['Takahiro Suzuki', 'Stefano Moretti', 'Michele Aleandri']",econ.TH,"A primary challenge in collective decision-making is that achieving unanimous agreement is difficult, even at the level of criteria. The history of social choice theory illustrates this: numerous normative criteria on voting rules have been proposed; however, disagreements persist regarding which criteria should take precedence. This study addresses the problem of ranking alternatives based on the aggregation of opinions over criteria that the alternatives might fulfill. Using the opinion aggregation model, we propose a new rule, termed the Intersection Initial Segment (IIS) rule, and characterize it using five axioms: neutrality, independence of the worst set, independence of the best set, weak intersection very important player, and independence of non-unanimous improvement. We illustrate our approach on a running example where the objective is to rank voting rules, showing that our opinion aggregation model is particularly well-suited to this context, and that the IIS rule is a counterpart to the method discussed in Nurmi's paper (2015)."
http://arxiv.org/abs/2504.13629v1,Divergent LLM Adoption and Heterogeneous Convergence Paths in Research Writing,2025-04-18 11:09:16+00:00,"['Cong William Lin', 'Wu Zhu']",cs.CL,"Large Language Models (LLMs), such as ChatGPT, are reshaping content creation and academic writing. This study investigates the impact of AI-assisted generative revisions on research manuscripts, focusing on heterogeneous adoption patterns and their influence on writing convergence. Leveraging a dataset of over 627,000 academic papers from arXiv, we develop a novel classification framework by fine-tuning prompt- and discipline-specific large language models to detect the style of ChatGPT-revised texts. Our findings reveal substantial disparities in LLM adoption across academic disciplines, gender, native language status, and career stage, alongside a rapid evolution in scholarly writing styles. Moreover, LLM usage enhances clarity, conciseness, and adherence to formal writing conventions, with improvements varying by revision type. Finally, a difference-in-differences analysis shows that while LLMs drive convergence in academic writing, early adopters, male researchers, non-native speakers, and junior scholars exhibit the most pronounced stylistic shifts, aligning their writing more closely with that of established researchers."
http://arxiv.org/abs/2504.14163v1,Decentralized Signaling Mechanisms,2025-04-19 03:36:53+00:00,"['Niloufar Mirzavand Boroujeni', 'Krishnamurthy Iyer', 'William L. Cooper']",cs.GT,"We study a system composed of multiple distinct service locations that aims to convince customers to join the system by providing information to customers. We cast the system's information design problem in the framework of Bayesian persuasion and describe centralized and decentralized signaling. We provide efficient methods for computing the system's optimal centralized and decentralized signaling mechanisms and derive a performance guarantee for decentralized signaling when the locations' states are independent. The guarantee states that the probability that a customer joins under optimal decentralized signaling is bounded below by the product of a strictly positive constant and the probability that a customer joins under optimal centralized signaling. The constant depends only on the number of service locations. We provide an example that shows that the constant cannot be improved. We consider an extension to more-general objectives for the system and establish that the same guarantee continues to hold. We also extend our analysis to systems where the locations' states are correlated, and again derive a performance guarantee for decentralized signaling in that setting. For the correlated setting, we prove that the guarantee's asymptotic dependence upon the number of locations cannot be substantially improved. A comparison of our guarantees for independent locations and for correlated locations reveals the influence of dependence on the performance of decentralized signaling."
http://arxiv.org/abs/2506.03384v1,How urban scaling and resource distribution shape social welfare and migration dynamics,2025-06-03 20:43:02+00:00,['Bryce Morsky'],physics.soc-ph,"Many outputs of cities scale in universal ways, including infrastructure, crime, and economic activity. Through a mathematical model, this study investigates the interplay between such scaling laws in human organization and governmental allocations of resources, focusing on impacts to migration patterns and social welfare. We find that if superlinear scaling resources of cities -- such as economic and social activity -- are the primary drivers of city dwellers' utility, then cities tend to converge to similar sizes and social welfare through migration. In contrast, if sublinear scaling resources, such as infrastructure, primarily impact utility, then migration tends to lead to megacities and inequity between large and small cities. These findings have implications for policymakers, economists, and political scientists addressing the challenges of equitable and efficient resource allocation."
http://arxiv.org/abs/2506.03369v1,Impact of Rankings and Personalized Recommendations in Marketplaces,2025-06-03 20:26:14+00:00,"['Omar Besbes', 'Yash Kanoria', 'Akshit Kumar']",econ.TH,"Individuals often navigate several options with incomplete knowledge of their own preferences. Information provisioning tools such as public rankings and personalized recommendations have become central to helping individuals make choices, yet their value proposition under different marketplace environments remains unexplored. This paper studies a stylized model to explore the impact of these tools in two marketplace settings: uncapacitated supply, where items can be selected by any number of agents, and capacitated supply, where each item is constrained to be matched to a single agent. We model the agents utility as a weighted combination of a common term which depends only on the item, reflecting the item's population level quality, and an idiosyncratic term, which depends on the agent item pair capturing individual specific tastes. Public rankings reveal the common term, while personalized recommendations reveal both terms. In the supply unconstrained settings, both public rankings and personalized recommendations improve welfare, with their relative value determined by the degree of preference heterogeneity. Public rankings are effective when preferences are relatively homogeneous, while personalized recommendations become critical as heterogeneity increases. In contrast, in supply constrained settings, revealing just the common term, as done by public rankings, provides limited benefit since the total common value available is limited by capacity constraints, whereas personalized recommendations, by revealing both common and idiosyncratic terms, significantly enhance welfare by enabling agents to match with items they idiosyncratically value highly. These results illustrate the interplay between supply constraints and preference heterogeneity in determining the effectiveness of information provisioning tools, offering insights for their design and deployment in diverse settings."
http://arxiv.org/abs/2506.05329v1,Admissibility of Completely Randomized Trials: A Large-Deviation Approach,2025-06-05 17:58:43+00:00,"['Guido Imbens', 'Chao Qin', 'Stefan Wager']",stat.ML,"When an experimenter has the option of running an adaptive trial, is it admissible to ignore this option and run a non-adaptive trial instead? We provide a negative answer to this question in the best-arm identification problem, where the experimenter aims to allocate measurement efforts judiciously to confidently deploy the most effective treatment arm. We find that, whenever there are at least three treatment arms, there exist simple adaptive designs that universally and strictly dominate non-adaptive completely randomized trials. This dominance is characterized by a notion called efficiency exponent, which quantifies a design's statistical efficiency when the experimental sample is large. Our analysis focuses on the class of batched arm elimination designs, which progressively eliminate underperforming arms at pre-specified batch intervals. We characterize simple sufficient conditions under which these designs universally and strictly dominate completely randomized trials. These results resolve the second open problem posed in Qin [2022]."
http://arxiv.org/abs/2506.05945v1,On Efficient Estimation of Distributional Treatment Effects under Covariate-Adaptive Randomization,2025-06-06 10:14:38+00:00,"['Undral Byambadalai', 'Tomu Hirata', 'Tatsushi Oka', 'Shota Yasui']",econ.EM,"This paper focuses on the estimation of distributional treatment effects in randomized experiments that use covariate-adaptive randomization (CAR). These include designs such as Efron's biased-coin design and stratified block randomization, where participants are first grouped into strata based on baseline covariates and assigned treatments within each stratum to ensure balance across groups. In practice, datasets often contain additional covariates beyond the strata indicators. We propose a flexible distribution regression framework that leverages off-the-shelf machine learning methods to incorporate these additional covariates, enhancing the precision of distributional treatment effect estimates. We establish the asymptotic distribution of the proposed estimator and introduce a valid inference procedure. Furthermore, we derive the semiparametric efficiency bound for distributional treatment effects under CAR and demonstrate that our regression-adjusted estimator attains this bound. Simulation studies and empirical analyses of microcredit programs highlight the practical advantages of our method."
http://arxiv.org/abs/2506.06368v1,Impact of COVID-19 on The Bullwhip Effect Across U.S. Industries,2025-06-04 09:39:10+00:00,"['Alper Saricioglu', 'Mujde Erol Genevois', 'Michele Cedolin']",econ.GN,"The Bullwhip Effect, describing the amplification of demand variability up the supply chain, poses significant challenges in Supply Chain Management. This study examines how the COVID-19 pandemic intensified the Bullwhip Effect across U.S. industries, using extensive industry-level data. By focusing on the manufacturing, retailer, and wholesaler sectors, the research explores how external shocks exacerbate this phenomenon. Employing both traditional and advanced empirical techniques, the analysis reveals that COVID-19 significantly amplified the Bullwhip Effect, with industries displaying varied responses to the same external shock. These differences suggest that supply chain structures play a critical role in either mitigating or intensifying the effect. By analyzing the dynamics during the pandemic, this study provides valuable insights into managing supply chains under global disruptions and highlights the importance of tailoring strategies to industry-specific characteristics."
http://arxiv.org/abs/2505.17648v2,Simulating Macroeconomic Expectations using LLM Agents,2025-05-23 09:11:14+00:00,"['Jianhao Lin', 'Lexuan Sun', 'Yixin Yan']",econ.GN,"We introduce a novel framework for simulating macroeconomic expectation formation using Large Language Model-Empowered Agents (LLM Agents). By constructing thousands of LLM Agents equipped with modules for personal characteristics, prior expectations, and knowledge, we replicate a survey experiment involving households and experts on inflation and unemployment. Our results show that although the expectations and thoughts generated by LLM Agents are more homogeneous than those of human participants, they still effectively capture key heterogeneity across agents and the underlying drivers of expectation formation. Furthermore, a module-ablation exercise highlights the critical role of prior expectations in simulating such heterogeneity. This approach complements traditional survey methods and offers new insights into AI behavioral science in macroeconomic research."
http://arxiv.org/abs/2504.08843v1,End-to-End Portfolio Optimization with Quantum Annealing,2025-04-10 21:31:30+00:00,"['Sai Nandan Morapakula', 'Sangram Deshpande', 'Rakesh Yata', 'Rushikesh Ubale', 'Uday Wad', 'Kazuki Ikeda']",quant-ph,"With rapid technological progress reshaping the financial industry, quantum technology plays a critical role in advancing risk management, asset allocation, and financial strategies. Realizing its full potential requires overcoming challenges like quantum hardware limits, algorithmic stability, and implementation barriers. This research explores integrating quantum annealing with portfolio optimization, highlighting quantum methods' ability to enhance investment strategy efficiency and speed. Using hybrid quantum-classical models, the study shows combined approaches effectively handle complex optimization better than classical methods. Empirical results demonstrate a portfolio increase of 200,000 Indian Rupees over the benchmark. Additionally, using rebalancing leads to a portfolio that also surpasses the benchmark value."
http://arxiv.org/abs/2506.19801v1,The Effects of Air Pollution on Teenagers' Cognitive Performance: Evidence from School Leaving Examination in Poland,2025-06-24 17:07:44+00:00,['Agata Galkiewicz'],econ.GN,"Random disturbances such as air pollution may affect cognitive performance, which, particularly in high-stakes settings, may have severe consequences for an individual's productivity and well-being. This paper examines the short-term effects of air pollution on school leaving exam results in Poland. I exploit random variation in air pollution between the days on which exams are held across three consecutive school years. I aim to capture this random variation by including school and time fixed effects. The school-level panel data is drawn from a governmental program where air pollution is continuously measured in the schoolyard. This localized hourly air pollution measure is a unique feature of my study, which increases the precision of the estimated effects. In addition, using distant and aggregated air pollution measures allows me for the comparison of the estimates in space and time. The findings suggest that a one standard deviation increase in the concentration of particulate matter PM2.5 and PM10 decreases students' exam scores by around 0.07-0.08 standard deviations. The magnitude and significance of these results depend on the location and timing of the air pollution readings, indicating the importance of the localized air pollution measure and the distinction between contemporaneous and lingering effects. Further, air pollution effects gradually increase in line with the quantiles of the exam score distribution, suggesting that high-ability students are more affected by the random disturbances caused by air pollution."
http://arxiv.org/abs/2506.19056v1,Self-selection of Information and Belief Update: An Experiment on COVID-19 Vaccine Information Acquisition,2025-06-23 19:16:33+00:00,"['ChienHsun Lin', 'Hans H. Tung']",econ.GN,"Rational information acquisition theory predicts that people select the more informative information. Thus, people's beliefs will be more persuaded by the information they select. We test the prediction in a critical real-world context -- information about COVID-19 vaccines. We conducted an online experiment in Taiwan where the subjects selected information about COVID-19 vaccines, and then the subjects updated their beliefs about vaccine effectiveness and references to vaccines. As our design distinguishes different stages of the rational acquisition framework, it allows us to diagnose the underlying mechanism of the theory. Our empirical findings demonstrate evidence that people's information acquisition generally coheres with the rational theory framework predictions; that is, people choose information when the information is more likely to alter their decisions. We show that our subjects' beliefs change more when they see the information they select. We also find evidence of change in vaccine preferences and choices after they receive the information they select, which further suggests that the subjects follow the rational information acquisition framework."
http://arxiv.org/abs/2506.19450v1,Asymptotic Equilibrium Analysis of the Boston Mechanism,2025-06-24 09:24:55+00:00,['Josue Ortega'],econ.TH,"We analyze the performance of the Boston mechanism under equilibrium play in uniform random markets. We provide two results. First, while the share of students assigned to their first preference is 63% under truthfulness, this fraction becomes vanishingly small in any Nash equilibrium of the preference revelation game induced by the Boston mechanism. Second, we show that there is a Nash equilibrium of the corresponding preference revelation game where the average student is assigned to a highly undesirable school-dramatically worse ranked than the logarithmic rank achieved under truthfulness."
http://arxiv.org/abs/2506.21426v1,Evolution and determinants of firm-level systemic risk in local production networks,2025-06-26 16:08:22+00:00,"['Anna Mancini', 'Balázs Lengyel', 'Riccardo Di Clemente', 'Giulio Cimini']",physics.soc-ph,"Recent crises like the COVID-19 pandemic and geopolitical tensions have exposed vulnerabilities and caused disruptions of supply chains, leading to product shortages, increased costs, and economic instability. This has prompted increasing efforts to assess systemic risk, namely the effects of firm disruptions on entire economies. However, the ability of firms to react to crises by rewiring their supply links has been largely overlooked, limiting our understanding of production networks resilience. Here we study dynamics and determinants of firm-level systemic risk in the Hungarian production network from 2015 to 2022. We use as benchmark a heuristic maximum entropy null model that generates an ensemble of production networks at equilibrium, by preserving the total input (demand) and output (supply) of each firm at the sector level. We show that the fairly stable set of firms with highest systemic risk undergoes a structural change during COVID-19, as those enabling economic exchanges become key players in the economy -- a result which is not reproduced by the null model. Although the empirical systemic risk aligns well with the null value until the onset of the pandemic, it becomes significantly smaller afterwards as the adaptive behavior of firms leads to a more resilient economy. Furthermore, firms' international trade volume (being a subject of disruption) becomes a significant predictor of their systemic risk. However, international links cannot provide an unequivocal explanation for the observed trends, as imports and exports have opposing effects on local systemic risk through the supply and demand channels."
http://arxiv.org/abs/2506.18895v1,Disaster Risk Financing through Taxation: A Framework for Regional Participation in Collective Risk-Sharing,2025-06-23 17:58:46+00:00,"['Fallou Niakh', 'Arthur Charpentier', 'Caroline Hillairet', 'Philipp Ratz']",econ.TH,"We consider an economy composed of different risk profile regions wishing to be hedged against a disaster risk using multi-region catastrophe insurance. Such catastrophic events inherently have a systemic component; we consider situations where the insurer faces a non-zero probability of insolvency. To protect the regions against the risk of the insurer's default, we introduce a public-private partnership between the government and the insurer. When a disaster generates losses exceeding the total capital of the insurer, the central government intervenes by implementing a taxation system to share the residual claims. In this study, we propose a theoretical framework for regional participation in collective risk-sharing through tax revenues by accounting for their disaster risk profiles and their economic status."
http://arxiv.org/abs/2506.18738v1,100-Day Analysis of USD/IDR Exchange Rate Dynamics Around the 2025 U.S. Presidential Inauguration,2025-06-23 15:14:27+00:00,"['Sandy H. S. Herho', 'Siti N. Kaban', 'Cahya Nugraha']",econ.EM,"Using a 100-day symmetric window around the January 2025 U.S. presidential inauguration, non-parametric statistical methods with bootstrap resampling (10,000 iterations) analyze distributional properties and anomalies. Results indicate a statistically significant 3.61\% Indonesian rupiah depreciation post-inauguration, with a large effect size (Cliff's Delta $= -0.9224$, CI: $[-0.9727, -0.8571]$). Central tendency shifted markedly, yet volatility remained stable (variance ratio $= 0.9061$, $p = 0.504$). Four significant anomalies exhibiting temporal clustering are detected. These findings provide quantitative evidence of political transition effects on emerging market currencies, highlighting implications for monetary policy and currency risk management."
http://arxiv.org/abs/2506.18068v1,"Beyond utility: incorporating eye-tracking, skin conductance and heart rate data into cognitive and econometric travel behaviour models",2025-06-22 15:33:19+00:00,"['Thomas O. Hancock', 'Stephane Hess', 'Charisma F. Choudhury']",econ.EM,"Choice models for large-scale applications have historically relied on economic theories (e.g. utility maximisation) that establish relationships between the choices of individuals, their characteristics, and the attributes of the alternatives. In a parallel stream, choice models in cognitive psychology have focused on modelling the decision-making process, but typically in controlled scenarios. Recent research developments have attempted to bridge the modelling paradigms, with choice models that are based on psychological foundations, such as decision field theory (DFT), outperforming traditional econometric choice models for travel mode and route choice behaviour. The use of physiological data, which can provide indications about the choice-making process and mental states, opens up the opportunity to further advance the models. In particular, the use of such data to enrich 'process' parameters within a cognitive theory-driven choice model has not yet been explored. This research gap is addressed by incorporating physiological data into both econometric and DFT models for understanding decision-making in two different contexts: stated-preference responses (static) of accomodation choice and gap-acceptance decisions within a driving simulator experiment (dynamic). Results from models for the static scenarios demonstrate that both models can improve substantially through the incorporation of eye-tracking information. Results from models for the dynamic scenarios suggest that stress measurement and eye-tracking data can be linked with process parameters in DFT, resulting in larger improvements in comparison to simpler methods for incorporating this data in either DFT or econometric models. The findings provide insights into the value added by physiological data as well as the performance of different candidate modelling frameworks for integrating such data."
http://arxiv.org/abs/2506.18001v1,An Empirical Comparison of Weak-IV-Robust Procedures in Just-Identified Models,2025-06-22 11:42:35+00:00,['Wenze Li'],econ.EM,"Instrumental variable (IV) regression is recognized as one of the five core methods for causal inference, as identified by Angrist and Pischke (2008). This paper compares two leading approaches to inference under weak identification for just-identified IV models: the classical Anderson-Rubin (AR) procedure and the recently popular tF method proposed by Lee et al. (2022). Using replication data from the American Economic Review (AER) and Monte Carlo simulation experiments, we evaluate the two procedures in terms of statistical significance testing and confidence interval (CI) length. Empirically, we find that the AR procedure typically offers higher power and yields shorter CIs than the tF method. Nonetheless, as noted by Lee et al. (2022), tF has a theoretical advantage in terms of expected CI length. Our findings suggest that the two procedures may be viewed as complementary tools in empirical applications involving potentially weak instruments."
http://arxiv.org/abs/2506.15063v1,Upstream competition and exclusive content provision in media markets,2025-06-18 02:06:05+00:00,['Kiho Yoon'],econ.TH,"With a multilateral vertical contracting model of media markets, we examine upstream competition and contractual arrangements in content provision. We analyze the trade of content by the Nash bargaining solution and the downstream competition by the Hotelling location model. We characterize the equilibrium outcomes and the contractual arrangements for various vertical structures. We show that the possibility of exclusive contracts rises when the value of the premium content increases, the degree of horizontal differentiation in the downstream market decreases, the importance of advertising revenue decreases, and the relative bargaining power of upstream firm decreases."
http://arxiv.org/abs/2506.14849v1,The Pharmaceutical Price Regulation Crisis: Implications on Antidepressant Access for Low-Income Americans,2025-06-16 22:13:34+00:00,"['Nicole Hodrosky', 'Gabriel Cacho', 'Faiza Ahmed', 'Rohana Mudireddy', 'Yapin Wen', 'Kymora Nembhard', 'Michael Yan']",econ.GN,"Depression affects more than 280 million people worldwide, with poorer communities having disproportionate burden as well as barriers to treatment. This study examines the role of pharmacy pricing caps in access to antidepressants among poorer Americans through bibliometric analysis of the 100 most cited articles on antidepressant pricing and access in the Web of Science Core Collection. We used tools like Bibliometrix and VOSviewer to visualize publication trends, dominant contributors, thematic clusters, and citation networks in the literature. Findings highlight intransigent inequalities in access to antidepressants based on astronomically high drug pricing as well as systemic inequalities against racial and ethnic minorities in particular. Branded antidepressant high prices are associated with low initiation of therapy as well as regimen compliance, heightened mental illness outcomes, as well as increased health utilization. This work uncovers critical gaps in the literature and demands immediate policy action to make antidepressants affordable as well as appropriately accessible to marginalized communities."
http://arxiv.org/abs/2506.15435v1,Fast Learning of Optimal Policy Trees,2025-06-18 13:12:10+00:00,"['James Cussens', 'Julia Hatamyar', 'Vishalie Shah', 'Noemi Kreif']",econ.EM,"We develop and implement a version of the popular ""policytree"" method (Athey and Wager, 2021) using discrete optimisation techniques. We test the performance of our algorithm in finite samples and find an improvement in the runtime of optimal policy tree learning by a factor of nearly 50 compared to the original version. We provide an R package, ""fastpolicytree"", for public use."
http://arxiv.org/abs/2507.00281v1,Factors Influencing Change Orders in Horizontal Construction Projects: A Comparative Analysis of Unit Price and Lump Sum Contracts,2025-06-30 21:43:24+00:00,"['Mohamed Khalafalla', 'Tejal Mulay', 'Shonda L Bernadin']",econ.GN,"Change orders (COs) are a common occurrence in construction projects, leading to increased costs and extended durations. Design-Bid-Build (DBB) projects, favored by state transportation agencies (STAs), often experience a higher frequency of COs compared to other project delivery methods. This study aims to identify areas of improvement to reduce CO frequency in DBB projects through a quantitative analysis. Historical bidding data from the Florida Department of Transportation (FDOT) was utilized to evaluate five factors, contracting technique, project location, type of work, project size, and duration, on specific horizontal construction projects. Two DBB contracting techniques, Unit Price (UP) and Lump Sum (LS), were evaluated using a discrete choice model. The analysis of 581 UP and 189 LS projects revealed that project size, duration, and type of work had a statistically significant influence on the frequency of change orders at a 95% confidence level. The discrete choice model showed significant improvement in identifying the appropriate contract type for a specific project compared to traditional methods used by STAs. By evaluating the contracting technique instead of project delivery methods for horizontal construction projects, the use of DBB can be enhanced, leading to reduced change orders for STAs."
http://arxiv.org/abs/2507.00279v1,Satellite and Mobile Phone Data Reveal How Violence Affects Seasonal Migration in Afghanistan,2025-06-30 21:40:00+00:00,"['Xiao Hui Tai', 'Suraj R. Nair', 'Shikhar Mehra', 'Joshua E. Blumenstock']",econ.GN,"Seasonal migration plays a critical role in stabilizing rural economies and sustaining the livelihoods of agricultural households. Violence and civil conflict have long been thought to disrupt these labor flows, but this hypothesis has historically been hard to test given the lack of reliable data on migration in conflict zones. Focusing on Afghanistan in the 8-year period prior to the Taliban's takeover in 2021, we first demonstrate how satellite imagery can be used to infer the timing of the opium harvest, which employs a large number of seasonal workers in relatively well-paid jobs. We then use a dataset of nationwide mobile phone records to characterize the migration response to this harvest, and examine whether and how violence and civil conflict disrupt this migration. We find that, on average, districts with high levels of poppy cultivation receive significantly more seasonal migrants than districts with no poppy cultivation. These labor flows are surprisingly resilient to idiosyncratic violent events at the source or destination, including extreme violence resulting in large numbers of fatalities. However, seasonal migration is affected by longer-term patterns of conflict, such as the extent of Taliban control in origin and destination locations."
http://arxiv.org/abs/2507.00249v1,Endogenous Network Structures with Precision and Dimension Choices,2025-06-30 20:35:59+00:00,['Nikhil Kumar'],econ.TH,"This paper presents a social learning model where the network structure is endogenously determined by signal precision and dimension choices. Agents not only choose the precision of their signals and what dimension of the state to learn about, but these decisions directly determine the underlying network structure on which social learning occurs. We show that under a fixed network structure, the optimal precision choice is sublinear in the agent's stationary influence in the network, and this individually optimal choice is worse than the socially optimal choice by a factor of $n^{1/3}$. Under a dynamic network structure, we specify the network by defining a kernel distance between agents, which then determines how much weight agents place on one another. Agents choose dimensions to learn about such that their choice minimizes the squared sum of influences of all agents: a network with equally distributed influence across agents is ideal."
http://arxiv.org/abs/2506.11838v2,Mean Field Games without Rational Expectations,2025-06-13 14:40:03+00:00,"['Benjamin Moll', 'Lenya Ryzhik']",math.AP,"Mean Field Game (MFG) models implicitly assume ""rational expectations"", meaning that the heterogeneous agents being modeled correctly know all relevant transition probabilities for the complex system they inhabit. When there is common noise, this assumption results in the ""Master equation"" (a.k.a. ""Monster equation""), a Hamilton-Jacobi-Bellman equation in which the infinite-dimensional density of agents is a state variable. The rational expectations assumption and the implication that agents solve Master equations is unrealistic in many applications. We show how to instead formulate MFGs with non-rational expectations. Departing from rational expectations is particularly relevant in ""MFGs with a low-dimensional coupling"", i.e. MFGs in which agents' running reward function depends on the density only through low-dimensional functionals of this density. This happens, for example, in most macroeconomics MFGs in which these low-dimensional functionals have the interpretation of ""equilibrium prices."" In MFGs with a low-dimensional coupling, departing from rational expectations allows for completely sidestepping the Master equation and for instead solving much simpler finite-dimensional HJB equations. We introduce an adaptive learning model as a particular example of non-rational expectations and discuss its properties."
http://arxiv.org/abs/2506.21213v1,Multilevel Decomposition of Generalized Entropy Measures Using Constrained Bayes Estimation: An Application to Japanese Regional Data,2025-06-26 13:07:35+00:00,"['Yuki Kawakubo', 'Kazuhiko Kakamu']",econ.EM,"We propose a method for multilevel decomposition of generalized entropy (GE) measures that explicitly accounts for nested population structures such as national, regional, and subregional levels. Standard approaches that estimate GE separately at each level do not guarantee compatibility with multilevel decomposition. Our method constrains lower-level GE estimates to match higher-level benchmarks while preserving hierarchical relationships across layers. We apply the method to Japanese income data to estimate GE at the national, prefectural, and municipal levels, decomposing national inequality into between-prefecture and within-prefecture inequality, and further decomposing prefectural GE into between-municipality and within-municipality inequality."
http://arxiv.org/abs/2506.20965v1,"Rational Miner Behaviour, Protocol Stability, and Time Preference: An Austrian and Game-Theoretic Analysis of Bitcoin's Incentive Environment",2025-06-26 03:04:21+00:00,['Craig Steven Wright'],econ.GN,"This paper integrates Austrian capital theory with repeated game theory to examine strategic miner behaviour under different institutional conditions in blockchain systems. It shows that when protocol rules are mutable, effective time preference rises, undermining rational long-term planning and cooperative equilibria. Using formal game-theoretic analysis and Austrian economic principles, the paper demonstrates how mutable protocols shift miner incentives from productive investment to political rent-seeking and influence games. The original Bitcoin protocol is interpreted as an institutional anchor: a fixed rule-set enabling calculability and low time preference. Drawing on the work of Bohm-Bawerk, Mises, and Hayek, the argument is made that protocol immutability is essential for restoring strategic coherence, entrepreneurial confidence, and sustainable network equilibrium."
http://arxiv.org/abs/2506.20881v1,Modeling Income Distribution with the Gause-Witt Population Ecology System,2025-06-25 23:07:21+00:00,['Marcelo B. Ribeiro'],physics.soc-ph,"This paper presents an empirical application of the Gause-Witt model of population ecology and ecosystems to the income distribution competitive dynamics of social classes in economic systems. The Gause-Witt mathematical system of coupled nonlinear first-order ordinary differential equations employed to model population of species sharing the same ecological niche and competing for the same resources was applied to the income data of Brazil. Previous studies using Brazilian income data from 1981 to 2009 showed that the complementary cumulative distribution functions built from yearly datasets have two distinct segments: the lower income region comprising of about 99% of the population can be represented by the Gompertz curve, whereas the richest 1% is described by the Pareto power-law. The results of applying the Gause-Witt system to Brazilian income data in order to describe the distributive competition dynamics of these two population shares indicate that the 99% and 1% income classes are mostly in the dynamic state of stable coexistence."
http://arxiv.org/abs/2505.04122v4,Risk Sharing Among Many: Implementing a Subgame Perfect and Optimal Equilibrium,2025-05-07 04:35:08+00:00,['Michiko Ogaku'],econ.TH,"Can a welfare-maximising risk-sharing rule be implemented in a large, decentralised community? We revisit the price-and-choose (P&C) mechanism of Echenique and Núñez (2025), in which players post price schedules sequentially and the last mover selects an allocation. P&C implements every Pareto-optimal allocation when the choice set is finite, but realistic risk-sharing problems involve an infinite continuum of feasible allocations. We extend P&C to infinite menus by modelling each allocation as a bounded random vector that redistributes an aggregate loss $X=\sum_i X_i$. We prove that the extended mechanism still implements the allocation that maximises aggregate (monetary) utility, even when players entertain heterogeneous credal sets of finitely additive probabilities (charges) dominated by a reference probability $\mathbb{P}$. Our credal sets are weak$^{\ast}$-compact and are restricted so that expectation functionals are uniformly Lipschitz on the feasible set. Finally, we pair P&C with the first-mover auction of Echenique and Núñez (2025), adapted to our infinite-menu, multiple-prior environment. With a public signal about the common surplus, the auction equalises (conditional) expected surplus among participants. The result is a decentralised, enforcement-free procedure that achieves both optimal and fair risk sharing under heterogeneous priors."
http://arxiv.org/abs/2505.02327v2,Slope Consistency of Quasi-Maximum Likelihood Estimator for Binary Choice Models,2025-05-05 02:48:44+00:00,"['Yoosoon Chang', 'Joon Y. Park', 'Guo Yan']",econ.EM,"Although QMLE is generally inconsistent, logistic regression relying on the binary choice model (BCM) with logistic errors is widely used, especially in machine learning contexts with many covariates and high-dimensional slope coefficients. This paper revisits the slope consistency of QMLE for BCMs. Ruud (1983) introduced a set of conditions under which QMLE may yield a constant multiple of the slope coefficient of BCMs asymptotically. However, he did not fully establish slope consistency of QMLE, which requires the existence of a positive multiple of slope coefficient identified as an interior maximizer of the population QMLE likelihood function over an appropriately restricted parameter space. We fill this gap by providing a formal proof of slope consistency under the same set of conditions for any binary choice model identified as in Manski (1975, 1985). Our result implies that logistic regression yields a consistent estimate for the slope coefficient of BCMs under suitable conditions."
http://arxiv.org/abs/2506.01525v1,"Effect of Insecurity on Agricultural Output in Benue State, Nigeria",2025-06-02 10:38:54+00:00,"['Victor Ushahemba Ijirshar', 'Isaiah Iortyom Udaah', 'Bridget Ngodoo Mile', 'Joyce Seember Vershima', 'Abba Adaudu']",econ.GN,"This study examined the effect of insecurity on agricultural output in Benue state. A descriptive survey design was employed, and 400 respondents were purposively selected from insecurity-prone local government areas, namely, Guma LGA, Agatu LGA, Gwer LGA, Gwer-West LGA, Katsina-Ala LGA, Logo LGA, Ukum LGA and Kwande LGA. The data were collected through the administration of a questionnaire and were analysed using t tests and structural equation modelling (SEM). The t-test was used to compare farmers' incomes before and after the insecurity in the study area to assess if the differences were statistically significant, while Structural Equation Modelling analysed the complex relationships among multiple variables, employing regression and factor analysis to model both direct and indirect effects. The results revealed that the monetary value of crop and livestock output decreased during periods of insecurity. Furthermore, the study showed that insecurity has an adverse effect on crop and livestock output. This means that a one percent increase in insecurity leads to a 0.211% and 0.311% decrease in crop and livestock output respectively. The study concluded that insecurity reduced agricultural output in Benue state. Based on the findings, it was recommended that the government deploy more security personnel, establish community policing initiatives, and employ modern surveillance technologies to deter criminal activities in insecure areas. Additionally, for places experiencing farmer-herder conflict, the government should provide grazing reserves for herdsmen and further strengthen the state law on open grazing prohibition and the establishment of ranch law."
http://arxiv.org/abs/2505.13809v3,Characterization of Efficient Influence Function for Off-Policy Evaluation Under Optimal Policies,2025-05-20 01:41:44+00:00,['Haoyu Wei'],math.ST,"Off-policy evaluation (OPE) provides a powerful framework for estimating the value of a counterfactual policy using observational data, without the need for additional experimentation. Despite recent progress in robust and efficient OPE across various settings, rigorous efficiency analysis of OPE under an estimated optimal policy remains limited. In this paper, we establish a concise characterization of the efficient influence function (EIF) for the value function under optimal policy within canonical Markov decision process models. Specifically, we provide the sufficient conditions for the existence of the EIF and characterize its expression. We also give the conditions under which the EIF does not exist."
http://arxiv.org/abs/2505.21787v1,Optimal Pricing Strategies for Heterogeneous Customers in Dual-Channel Closed-Loop Supply Chains: A Modeling Approach,2025-05-27 21:38:29+00:00,"['Yang Xiao', 'Hisashi Kurata', 'Ting Wang']",math.OC,"Dual-channel closed-loop supply chains (DCCLSCs) play a vital role in attaining both sustainability and profitability. This paper introduces a game-theoretic model to analyze optimal pricing strategies for primary and replacement customers within three distinct recycling frameworks: manufacturer-led, retailer-led, and collaborative recycling. The model identifies equilibrium pricing and subsidy decisions for each scenario, considering the primary customer's preference for the direct channel and the specific roles in recycling. The findings indicate that manufacturers tend to set lower prices in direct channels compared to retailers, aiming to stimulate demand and promote trade-ins. Manufacturer-led recycling initiatives result in stable pricing, whereas retailer-led recycling necessitates higher subsidies. Collaborative recycling strategies yield lower prices and an increase in trade-ins. Primary customers' preference for the direct channel significantly impacts pricing strategies, with a stronger preference leading to lower direct-channel prices and higher manufacturer subsidies. This paper contributes to the field by incorporating primary customer channel preferences and diverse recycling frameworks into DCCLSC pricing models. These insights assist manufacturers and retailers in adjusting pricing strategies and trade-in incentives according to primary customer preferences and associated costs, thereby enhancing profitability and recycling efficiency within DCCLSCs."
http://arxiv.org/abs/2505.23513v1,An Analysis of Pseudo-Goodwin Cycles in a Wage-Led Minsky Model,2025-05-29 14:55:36+00:00,['Johannes Buchner'],math.DS,"The goal of these notes is to make the concept of ""pseudo goodwin cycles"" mathematically more precise. At first the title seems like a contradiction to have a wage-led model and still find goodwin cycles in it, but the point we try to make in the paper is that those are only `pseudo-goodwin' cycles, and not real goodwin cycles."
http://arxiv.org/abs/2505.20527v2,"Financial literacy, robo-advising, and the demand for human financial advice: Evidence from Italy",2025-05-26 21:09:54+00:00,"['David Aristei', 'Manuela Gallo']",econ.GN,"This paper investigates the impact of objective financial knowledge, confidence in one's financial skills, and digital financial literacy on individuals' decisions to seek financial advice from robo-advice platforms. Using microdata from the Bank of Italy's 2023 survey on Italian adults' financial literacy, we find that individuals with greater financial knowledge are less inclined to rely on online services for automated financial advice. Conversely, confidence in one's financial abilities and digital financial literacy enhance the likelihood of utilising robo-advice services. Trust in financial innovation, the use of digital financial services, and the propensity to take risks and save also emerge as significant predictors of an individual's use of robo-advice. We also provide evidence of a significant complementary relationship between the adoption of robo-advisory services and the demand for independent professional human advice. By contrast, a substitution effect is found for non-independent human advice. These findings highlight the importance of hybrid solutions in professional financial consulting, where robo-advisory services complement human financial advice."
http://arxiv.org/abs/2505.22873v1,"Forecasting Residential Heating and Electricity Demand with Scalable, High-Resolution, Open-Source Models",2025-05-28 21:16:27+00:00,"['Stephen J. Lee', 'Cailinn Drouin']",econ.GN,"We present a novel framework for high-resolution forecasting of residential heating and electricity demand using probabilistic deep learning models. We focus specifically on providing hourly building-level electricity and heating demand forecasts for the residential sector. Leveraging multimodal building-level information -- including data on building footprint areas, heights, nearby building density, nearby building size, land use patterns, and high-resolution weather data -- and probabilistic modeling, our methods provide granular insights into demand heterogeneity. Validation at the building level underscores a step change improvement in performance relative to NREL's ResStock model, which has emerged as a research community standard for residential heating and electricity demand characterization. In building-level heating and electricity estimation backtests, our probabilistic models respectively achieve RMSE scores 18.3\% and 35.1\% lower than those based on ResStock. By offering an open-source, scalable, high-resolution platform for demand estimation and forecasting, this research advances the tools available for policymakers and grid planners, contributing to the broader effort to decarbonize the U.S. building stock and meeting climate objectives."
http://arxiv.org/abs/2505.23333v1,Evaluating financial tail risk forecasts: Testing Equal Predictive Ability,2025-05-29 10:48:56+00:00,['Lukas Bauer'],econ.EM,"This paper provides comprehensive simulation results on the finite sample properties of the Diebold-Mariano (DM) test by Diebold and Mariano (1995) and the model confidence set (MCS) testing procedure by Hansen et al. (2011) applied to the asymmetric loss functions specific to financial tail risk forecasts, such as Value-at-Risk (VaR) and Expected Shortfall (ES). We focus on statistical loss functions that are strictly consistent in the sense of Gneiting (2011a). We find that the tests show little power against models that underestimate the tail risk at the most extreme quantile levels, while the finite sample properties generally improve with the quantile level and the out-of-sample size. For the small quantile levels and out-of-sample sizes of up to two years, we observe heavily skewed test statistics and non-negligible type III errors, which implies that researchers should be cautious about using standard normal or bootstrapped critical values. We demonstrate both empirically and theoretically how these unfavorable finite sample results relate to the asymmetric loss functions and the time varying volatility inherent in financial return data."
http://arxiv.org/abs/2506.12604v1,"Selling Certification, Content Moderation, and Attention",2025-06-14 18:56:43+00:00,"['Heski Bar-Isaac', 'Rahul Deb', 'Matthew Mitchell']",econ.TH,"We introduce a model of content moderation for sale, where a platform can channel attention in two ways: direct steering that makes content visible to consumers and certification that controls what consumers know about the content. The platform optimally price discriminates using both instruments. Content from higher willingness-to-pay providers enjoys higher quality certification and more views. The platform cross-subsidizes content: the same certificate is assigned to content from low willingness-to-pay providers that appeals to consumers and content from higher willingness-to-pay providers that does not. Cross-subsidization can benefit consumers by making content more diverse; regulation enforcing accurate certification may be harmful."
http://arxiv.org/abs/2506.13016v1,Inequality's Economic and Social Roots: the Role of Social Networks and Homophily,2025-06-16 00:56:59+00:00,['Matthew O. Jackson'],econ.GN,"I discuss economic and social sources of inequality and elaborate on the role of social networks in inequality, economic immobility, and economic inefficiencies. The lens of social networks clarifies how the entanglement of people's information, opportunities, and behaviors with those of their friends and family leads to persistent differences across communities, resulting in inequality in education, employment, income, health, and wealth. The key role of homophily in separating groups within the network is highlighted. A network perspective's policy implications differ substantially from a narrower economic perspective that ignores social structure. I discuss the importance of ``policy cocktails'' that include aspects that are aimed at both the economic and social forces driving inequality."
http://arxiv.org/abs/2506.12416v1,Perfect Secrecy in the Wild: A Characterization,2025-06-14 09:19:26+00:00,"['Costas Cavounidis', 'Massimiliano Furlan', 'Alkis Georgiadis-Harris']",econ.TH,"Alice wishes to reveal the state $X$ to Bob, if he knows some other information $Y$ also known to her. If Bob does not, she wishes to reveal nothing about $X$ at all. When can Alice accomplish this? We provide a simple necessary and sufficient condition on the joint distribution of $X$ and $Y$. Shannon's result on the perfect secrecy of the one-time pad follows as a special case."
http://arxiv.org/abs/2505.21280v1,The Families that Stay Together: A Network Analysis of Dynastic Power in Philippine Politics,2025-05-27 14:49:47+00:00,"['Rafael Acuna', 'Aldie Alejandro', 'Robert Leung']",econ.GN,"Dynasties have long dominated Philippine politics. Despite the theoretical consensus that dynastic rule erodes democratic accountability, there is limited empirical evidence establishing dynasties' true impact on development. A key challenge has been developing robust metrics for characterizing dynasties that facilitate meaningful comparisons across geographies and election cycles. Using election data from 2004 to 2022, we leverage methods from graph theory to develop four indicators to investigate dynastic evolution: Political Herfindahl-Hirschman Index (HHI), measuring dynastic power concentration; Centrality Gini Coefficient (CGC), reflecting inequalities of influence between clan members; Connected Component Density (CCD), representing the degree of inter-clan connection; and Average Community Connectivity (ACC), quantifying intra-clan cohesion. Our analysis reveals three key findings. Firstly, dynasties have grown stronger and more interconnected, occupying an increasing share of elected positions. Dominant clans have also remained tightly knit, but with great power imbalances between members. Secondly, we examine variations in party-hopping between dynastic and non-dynastic candidates. Across every election cycle, party-hopping rates are significantly higher (p<0.01) among dynastic candidates than non-dynasts, suggesting that the dominance of dynasties may weaken institutional trust within parties. Finally, applying a Linear Mixed Model regression, controlling for geographic random-effects and time fixed-effects, we observe that provinces with high power asymmetries within clans (high CGCs) and with deeply interconnected clans (high CCDs) record significantly lower (p<0.05) Human Development Index scores. These findings suggest that clan structure, rather than power concentration alone--may be the chief determinant of a ruling dynasty's developmental impact."
http://arxiv.org/abs/2505.20787v1,Debiased Ill-Posed Regression,2025-05-27 06:47:33+00:00,"['AmirEmad Ghassami', 'James M. Robins', 'Andrea Rotnitzky']",stat.ME,"In various statistical settings, the goal is to estimate a function which is restricted by the statistical model only through a conditional moment restriction. Prominent examples include the nonparametric instrumental variable framework for estimating the structural function of the outcome variable, and the proximal causal inference framework for estimating the bridge functions. A common strategy in the literature is to find the minimizer of the projected mean squared error. However, this approach can be sensitive to misspecification or slow convergence rate of the estimators of the involved nuisance components. In this work, we propose a debiased estimation strategy based on the influence function of a modification of the projected error and demonstrate its finite-sample convergence rate. Our proposed estimator possesses a second-order bias with respect to the involved nuisance functions and a desirable robustness property with respect to the misspecification of one of the nuisance functions. The proposed estimator involves a hyper-parameter, for which the optimal value depends on potentially unknown features of the underlying data-generating process. Hence, we further propose a hyper-parameter selection approach based on cross-validation and derive an error bound for the resulting estimator. This analysis highlights the potential rate loss due to hyper-parameter selection and underscore the importance and advantages of incorporating debiasing in this setting. We also study the application of our approach to the estimation of regular parameters in a specific parameter class, which are linear functionals of the solutions to the conditional moment restrictions and provide sufficient conditions for achieving root-n consistency using our debiased estimator."
http://arxiv.org/abs/2505.21122v1,Union Shapley Value: Quantifying Group Impact via Collective Removal,2025-05-27 12:41:49+00:00,"['Piotr Kępczyński', 'Oskar Skibski']",cs.GT,"We perform a comprehensive analysis of extensions of the Shapley value to groups. We propose a new, natural extension called the Union Shapley Value, which assesses a group's contribution by examining the impact of its removal from the game. This intuition is formalized through two axiomatic characterizations, closely related to existing axiomatizations of the Shapley value. Furthermore, we characterize the class of group semivalues and identify a dual approach that measures synergy instead of the value of a coalition. Our analysis reveals a novel connection between several group values previously proposed in the literature."
http://arxiv.org/abs/2506.10690v1,Inference on panel data models with a generalized factor structure,2025-06-12 13:40:36+00:00,"['Juan M. Rodriguez-Poo', 'Alexandra Soberon', 'Stefan Sperlich']",econ.EM,"We consider identification, inference and validation of linear panel data models when both factors and factor loadings are accounted for by a nonparametric function. This general specification encompasses rather popular models such as the two-way fixed effects and the interactive fixed effects ones. By applying a conditional mean independence assumption between unobserved heterogeneity and the covariates, we obtain consistent estimators of the parameters of interest at the optimal rate of convergence, for fixed and large $T$. We also provide a specification test for the modeling assumption based on the methodology of conditional moment tests and nonparametric estimation techniques. Using degenerate and nondegenerate theories of U-statistics we show its convergence and asymptotic distribution under the null, and that it diverges under the alternative at a rate arbitrarily close to $\sqrt{NT}$. Finite sample inference is based on bootstrap. Simulations reveal an excellent performance of our methods and an empirical application is conducted."
http://arxiv.org/abs/2509.05354v1,Characterizing Optimality in Dynamic Settings: A Monotonicity-based Approach,2025-09-03 05:12:32+00:00,"['Zhuokai Huang', 'Demian Pouzo', 'Andrés Rodríguez-Clare']",econ.TH,"We develop a novel analytical method for studying optimal paths in dynamic optimization problems under general monotonicity conditions. The method centers on a locator function -- a simple object constructed directly from the model's primitives -- whose roots identify interior steady states and whose slope determines their local stability. Under strict concavity of the payoff function, the locator function also characterizes basins of attraction, yielding a complete description of qualitative dynamics. Without concavity, it can still deliver sharp results: if the function is single crossing from above, its root identifies a globally stable steady state; if the locator function is inverted-U-shaped with two interior roots (a typical case), only the higher root can be a locally stable interior steady state. The locator function further enables comparative statics of steady states with respect to parameters through direct analysis of its derivatives. These results are obtained without solving the full dynamic program. We illustrate the approach using a generalized neoclassical growth model, a rational (un)fitness model, and a learning-by-doing economy."
http://arxiv.org/abs/2509.08472v2,On the Identification of Diagnostic Expectations: Econometric Insights from DSGE Models,2025-09-10 10:20:06+00:00,['Jinting Guo'],econ.EM,"This paper provides the first econometric evidence for diagnostic expectations (DE) in DSGE models. Using the identification framework of Qu and Tkachenko (2017), I show that DE generate dynamics unreplicable under rational expectations (RE), with no RE parameterization capable of matching the autocovariance implied by DE. Consequently, DE are not observationally equivalent to RE and constitute an endogenous source of macroeconomic fluctuations, distinct from both structural frictions and exogenous shocks. From an econometric perspective, DE preserve overall model identification but weaken the identification of shock variances. To ensure robust conclusions across estimation methods and equilibrium conditions, I extend Bayesian estimation with Sequential Monte Carlo sampling to the indeterminacy domain. These findings advance the econometric study of expectations and highlight the macroeconomic relevance of diagnostic beliefs."
http://arxiv.org/abs/2507.05287v1,Increasing Systemic Resilience to Socioeconomic Challenges: Modeling the Dynamics of Liquidity Flows and Systemic Risks Using Navier-Stokes Equations,2025-07-05 08:33:53+00:00,['Davit Gondauri'],econ.GN,"Modern economic systems face unprecedented socioeconomic challenges, making systemic resilience and effective liquidity flow management essential. Traditional models such as CAPM, VaR, and GARCH often fail to reflect real market fluctuations and extreme events. This study develops and validates an innovative mathematical model based on the Navier-Stokes equations, aimed at the quantitative assessment, forecasting, and simulation of liquidity flows and systemic risks. The model incorporates 13 macroeconomic and financial parameters, including liquidity velocity, market pressure, internal stress, stochastic fluctuations, and risk premiums, all based on real data and formally included in the modified equation. The methodology employs econometric testing, Fourier analysis, stochastic simulation, and AI-based calibration to enable dynamic testing and forecasting. Simulation-based sensitivity analysis evaluates the impact of parameter changes on financial balance. The model is empirically tested using Georgian macroeconomic and financial data from 2010-2024, including GDP, inflation, the Gini index, CDS spreads, and LCR metrics. Results show that the model effectively describes liquidity dynamics, systemic risk, and extreme scenarios, while also offering a robust framework for multifactorial analysis, crisis prediction, and countercyclical policy planning."
http://arxiv.org/abs/2509.20634v1,Recidivism and Peer Influence with LLM Text Embeddings in Low Security Correctional Facilities,2025-09-25 00:14:57+00:00,"['Shanjukta Nath', 'Jiwon Hong', 'Jae Ho Chang', 'Keith Warren', 'Subhadeep Paul']",econ.EM,"We find AI embeddings obtained using a pre-trained transformer-based Large Language Model (LLM) of 80,000-120,000 written affirmations and correction exchanges among residents in low-security correctional facilities to be highly predictive of recidivism. The prediction accuracy is 30\% higher with embedding vectors than with only pre-entry covariates. However, since the text embedding vectors are high-dimensional, we perform Zero-Shot classification of these texts to a low-dimensional vector of user-defined classes to aid interpretation while retaining the predictive power. To shed light on the social dynamics inside the correctional facilities, we estimate peer effects in these LLM-generated numerical representations of language with a multivariate peer effect model, adjusting for network endogeneity. We develop new methodology and theory for peer effect estimation that accommodate sparse networks, multivariate latent variables, and correlated multivariate outcomes. With these new methods, we find significant peer effects in language usage for interaction and feedback."
http://arxiv.org/abs/2507.13124v2,"Heterogeneous Bribery, Technology Choice, and Capital Accumulation",2025-07-17 13:42:21+00:00,"['Jafar M. Olimov', 'Yi-Chan Tsai', 'Hao-Yu Yang']",econ.GN,"We study the production, entry, and technological decisions of firms in the presence of bribery. We find that bribery can be justified even in the absence of bureaucratic inefficiencies. We document substantial technology-specific heterogeneity in bribery in 148 countries and incorporate it into a general equilibrium model, where firms use capital-intensive or labor-intensive technology. When bribery more heavily affects less efficient labor-intensive firms, resources move toward more efficient capital-intensive firms, resulting in higher capital accumulation and aggregate output. In poorer countries, the elimination of bribery only for capital-intensive firms increases the capital stock by 18.7% more and the aggregate output by 3.4% more than the complete elimination of bribery. In wealthier countries, the elimination of bribery only for capital-intensive firms increases the capital stock by 44.4% more and the aggregate output by 15.4% more than the complete elimination of bribery. Our findings challenge the established view of bribery as uniformly harmful and demonstrate how the within-country heterogeneity in bribery can explain cross-country differences in income."
http://arxiv.org/abs/2507.13552v1,Combining stated and revealed preferences,2025-07-17 22:00:17+00:00,"['Romuald Meango', 'Marc Henry', 'Ismael Mourifie']",econ.EM,"Can stated preferences inform counterfactual analyses of actual choice? This research proposes a novel approach to researchers who have access to both stated choices in hypothetical scenarios and actual choices, matched or unmatched. The key idea is to use stated choices to identify the distribution of individual unobserved heterogeneity. If this unobserved heterogeneity is the source of endogeneity, the researcher can correct for its influence in a demand function estimation using actual choices and recover causal effects. Bounds on causal effects are derived in the case, where stated choice and actual choices are observed in unmatched data sets. These data combination bounds are of independent interest. We derive a valid bootstrap inference for the bounds and show its good performance in a simulation experiment."
http://arxiv.org/abs/2507.14391v1,Policy relevance of causal quantities in networks,2025-07-18 22:39:49+00:00,"['Sahil Loomba', 'Dean Eckles']",stat.ME,"In settings where units' outcomes are affected by others' treatments, there has been a proliferation of ways to quantify effects of treatments on outcomes. Here we describe how many proposed estimands can be represented as involving one of two ways of averaging over units and treatment assignments. The more common representation often results in quantities that are irrelevant, or at least insufficient, for optimal choice of policies governing treatment assignment. The other representation often yields quantities that lack an interpretation as summaries of unit-level effects, but that we argue may still be relevant to policy choice. Among various estimands, the expected average outcome -- or its contrast between two different policies -- can be represented both ways and, we argue, merits further attention."
http://arxiv.org/abs/2507.13274v2,"Analysis Theory of Data Economy: Dataization, Technological Progress and Dynamic General Equilibrium",2025-07-17 16:29:25+00:00,['Yongheng Hu'],econ.TH,"This paper constructs a clean and efficient representative agent model of the data economy from a macroeconomics perspective, in order to analyze the impact of dataization and technological progress on the dynamic general equilibrium of 'consumption-capital', and the catalysis effect of dataization on technological progress. We first set the data in production comes from dataization of the total output of the society and is exponentially functionally related to the technology. Secondly, the data production function is used to solve the optimization problem for firms and households and to construct a dynamic general equilibrium of 'consumption-capital' based on the endogenous interest rate solved by maximizing the returns of firms. Finally, by using numerical simulation and phase diagram analysis, we find that the effects of increasing dataization and encouraging technological progress each exhibit different nonlinear characteristics for equilibrium capital and equilibrium consumption, we thus conclude that dataization enables the positive effects of technological progress on economic development to be more rapid and persistent. We select two types of Chinese policies regarding data openness to represent the role of dataization, and demonstrate the catalysis role of datatization for the development of the digital economy by setting up difference in difference (DID) experiments, which provide persuasive evidence for the theoretical interpretation of the paper."
http://arxiv.org/abs/2507.17599v1,A general randomized test for Alpha,2025-07-23 15:30:50+00:00,"['Daniele Massacci', 'Lucio Sarno', 'Lorenzo Trapani', 'Pierluigi Vallarino']",econ.EM,"We propose a methodology to construct tests for the null hypothesis that the pricing errors of a panel of asset returns are jointly equal to zero in a linear factor asset pricing model -- that is, the null of ""zero alpha"". We consider, as a leading example, a model with observable, tradable factors, but we also develop extensions to accommodate for non-tradable and latent factors. The test is based on equation-by-equation estimation, using a randomized version of the estimated alphas, which only requires rates of convergence. The distinct features of the proposed methodology are that it does not require the estimation of any covariance matrix, and that it allows for both N and T to pass to infinity, with the former possibly faster than the latter. Further, unlike extant approaches, the procedure can accommodate conditional heteroskedasticity, non-Gaussianity, and even strong cross-sectional dependence in the error terms. We also propose a de-randomized decision rule to choose in favor or against the correct specification of a linear factor pricing model. Monte Carlo simulations show that the test has satisfactory properties and it compares favorably to several existing tests. The usefulness of the testing procedure is illustrated through an application of linear factor pricing models to price the constituents of the S&P 500."
http://arxiv.org/abs/2508.13233v1,A Category Theory Framework for Macroeconomic Modeling: The Case of Argentina's Bimonetary Economy,2025-08-17 23:31:27+00:00,['Luciano Pollicino'],econ.GN,"Traditional macroeconomic models, based on static algebraic systems, fail to capture the dynamics of a bimonetary economy like Argentina's. This paper proposes a framework based on category theory to develop a more flexible and structured model that represents the evolving relationships between key variables such as inflation expectations, interest rates, and currency demand. Using concepts like objects, morphisms, learning/forgetful functors, limits, and colimits, the model is applied to empirical data from 2018-2023. The findings reveal a significant structural misalignment between the equilibrium and observed exchange rates and propose a new aggregate indicator to measure devaluation risk. The framework demonstrates a strong synergy with modern computational tools like machine learning, offering a more robust approach to policy analysis and forecasting in complex economies."
http://arxiv.org/abs/2508.13946v1,Partial Identification of Causal Effects for Endogenous Continuous Treatments,2025-08-19 15:36:29+00:00,"['Abhinandan Dalal', 'Eric J. Tchetgen Tchetgen']",stat.ME,"No unmeasured confounding is a common assumption when reasoning about counterfactual outcomes, but such an assumption may not be plausible in observational studies. Sensitivity analysis is often employed to assess the robustness of causal conclusions to unmeasured confounding, but existing methods are predominantly designed for binary treatments. In this paper, we provide natural extensions of two extensively used sensitivity frameworks -- the Rosenbaum and Marginal sensitivity models -- to the setting of continuous exposures. Our generalization replaces scalar sensitivity parameters with sensitivity functions that vary with exposure level, enabling richer modeling and sharper identification bounds. We develop a unified pseudo-outcome regression formulation for bounding the counterfactual dose-response curve under both models, and propose corresponding nonparametric estimators which have second order bias. These estimators accommodate modern machine learning methods for obtaining nuisance parameter estimators, which are shown to achieve $L^2$- consistency, minimax rates of convergence under suitable conditions. Our resulting estimators of bounds for the counterfactual dose-response curve are shown to be consistent and asymptotic normal allowing for a user-specified bound on the degree of uncontrolled exposure endogeneity. We also offer a geometric interpretation that relates the Rosenbaum and Marginal sensitivity model and guides their practical usage in global versus targeted sensitivity analysis. The methods are validated through simulations and a real-data application on the effect of second-hand smoke exposure on blood lead levels in children."
http://arxiv.org/abs/2508.13366v1,Monotonic Path-Specific Effects: Application to Estimating Educational Returns,2025-08-18 21:17:11+00:00,['Aleksei Opacic'],stat.AP,"Conventional research on educational effects typically either employs a ""years of schooling"" measure of education, or dichotomizes attainment as a point-in-time treatment. Yet, such a conceptualization of education is misaligned with the sequential process by which individuals make educational transitions. In this paper, I propose a causal mediation framework for the study of educational effects on outcomes such as earnings. The framework considers the effect of a given educational transition as operating indirectly, via progression through subsequent transitions, as well as directly, net of these transitions. I demonstrate that the average treatment effect (ATE) of education can be additively decomposed into mutually exclusive components that capture these direct and indirect effects. The decomposition has several special properties which distinguish it from conventional mediation decompositions of the ATE, properties which facilitate less restrictive identification assumptions as well as identification of all causal paths in the decomposition. An analysis of the returns to high school completion in the NLSY97 cohort suggests that the payoff to a high school degree stems overwhelmingly from its direct labor market returns. Mediation via college attendance, completion and graduate school attendance is small because of individuals' low counterfactual progression rates through these subsequent transitions."
http://arxiv.org/abs/2508.12457v1,"From fields to fuel: analyzing the global economic and emissions potential of agricultural pellets, informed by a case study",2025-08-17 18:05:31+00:00,"['Sebastian G. Nosenzo', 'Rafael Kelman']",econ.GN,"Agricultural residues represent a vast, underutilized resource for renewable energy. This study combines empirical analysis from 179 countries with a case study of a pelletization facility to evaluate the global potential of agricultural pelletization for fossil fuel replacement. The findings estimate a technical availability of 1.44 billion tons of crop residues suitable for pellet production, translating to a 4.5% potential displacement of global fossil fuel energy use, equating to 22 million TJ and equivalent to 917 million tons of coal annually. The economically optimized scenario projects annual savings of $163 billion and a reduction of 1.35 billion tons of CO2 equivalent in emissions. Utilizing the custom-developed CLASP-P and RECOP models, the study further demonstrates that agricultural pellets can achieve competitive pricing against conventional fossil fuels in many markets. Despite logistical and policy challenges, agricultural pelletization emerges as a scalable, market-driven pathway to support global decarbonization goals while fostering rural economic development. These results reinforce the need for targeted investment, technological advancement, and supportive policy to unlock the full potential of agricultural pellets in the renewable energy mix."
http://arxiv.org/abs/2508.14498v1,The invisible hand as an emergent property: a gradient flow approach,2025-08-20 07:38:46+00:00,"['Giorgio Fabbri', 'Davide Fiaschi', 'Cristiano Ricci']",econ.TH,"We develop a general equilibrium model in which, at each instant, a short-run competitive equilibrium arises. Heterogeneity in factor allocation generates differential profit rates across sectors, prompting firms to move between them under myopic profit-seeking behaviour, subject to quadratic reallocation costs. The aggregate dynamics of the economy can be formalised as a gradient flow in a Wasserstein space, starting from a partial differential equation that describes the reallocation of firms across sectors. Two key emergent properties arise: (i) decentralised and uncoordinated decisions can be reinterpreted as the solution to a sequence of global optimisation problems, involving a function of aggregate consumption, which increases monotonically along the dynamic path; (ii) the long-run competitive equilibrium is efficient, as the distribution of firms maximises aggregate consumption and profit rates are equalised across sectors. We extend the baseline model to incorporate non-symmetric preferences, intrasectoral externalities, a fixed cost of reallocation, and labour immobility. These extensions reveal conditions under which the efficiency and uniqueness of the long-run equilibrium may fail, but also highlight the surprising result that the decentralised equilibrium can remain efficient even in the presence of externalities. Finally, using a large sample of EU firms from the period 2018-2023, we empirically document convergence in sectoral profit rates, but not in labour productivity, pointing to a certain degree of labour immobility. We also find evidence suggesting the absence of significant fixed costs of reallocation at the sectoral level, the presence of positive but limited intrasectoral externalities, and a moderate degree of substitutability among goods."
http://arxiv.org/abs/2509.01265v2,Self-Employment as a Signal: Career Concerns with Hidden Firm Performance,2025-09-01 08:54:00+00:00,"['Georgy Lukyanov', 'Konstantin Popov', 'Shubh Lashkery']",econ.TH,"We analyze a dynamic labor market in which a worker with career concerns chooses each period between (i) self-employment that makes output publicly observable and (ii) employment at a firm that pays a flat wage but keeps individual performance hidden from outside observers. Output is binary and the worker is risk averse. The worker values future opportunities through a reputation for talent; firms may be benchmark (myopic) (ignoring the informational content of an application) or equilibrium (updating beliefs from the very act of applying). Three forces shape equilibrium: an insurance - information trade-off, selection by reputation, and inference from application decisions. We show that (i) an absorbing employment region exists in which low-reputation workers strictly prefer the firm's insurance and optimally cease producing public information; (ii) sufficiently strong reputation triggers self-employment in order to generate public signals and preserve future outside options; and (iii) with equilibrium firms, application choices act as signals that shift hiring thresholds and wages even when in-firm performance remains opaque. Comparative statics deliver sharp, testable predictions for the prevalence of self-employment, the cyclicality of switching, and wage dynamics across markets with different degrees of performance transparency. The framework links classic career-concerns models to contemporary environments in which some tasks generate portable, public histories while firm tasks remain unobserved by the outside market (e.g., open-source contributions, freelancing platforms, or sales roles with standardized public metrics). Our results rationalize recent empirical findings on the value of public performance records and illuminate when opacity inside firms dampens or amplifies reputational incentives."
http://arxiv.org/abs/2508.20539v2,Endogenous Quality in Social Learning,2025-08-28 08:28:30+00:00,"['Georgy Lukyanov', 'Konstantin Shamruk', 'Ekaterina Logina']",econ.TH,"We study a dynamic reputation model with a fixed posted price where only purchases are public. A long-lived seller chooses costly quality; each buyer observes the purchase history and a private signal. Under a Markov selection, beliefs split into two cascades - where actions are unresponsive and investment is zero - and an interior region where the seller invests. The policy is inverse-U in reputation and produces two patterns: Early Resolution (rapid absorption at the optimistic cascade) and Double Hump (two investment episodes). Higher signal precision at fixed prices enlarges cascades and can reduce investment. We compare welfare and analyze two design levers: flexible pricing, which can keep actions informative and remove cascades for patient sellers, and public outcome disclosure, which makes purchases more informative and expands investment."
http://arxiv.org/abs/2509.04855v1,The Paradox of Doom: Acknowledging Extinction Risk Reduces the Incentive to Prevent It,2025-09-05 07:14:24+00:00,"['Jakub Growiec', 'Klaus Prettner']",econ.GN,"We investigate the salience of extinction risk as a source of impatience. Our framework distinguishes between human extinction risk and individual mortality risk while allowing for various degrees of intergenerational altruism. Additionally, we consider the evolutionarily motivated ""selfish gene"" perspective. We find that the risk of human extinction is an indispensable component of the discount rate, whereas individual mortality risk can be hedged against - partially or fully, depending on the setup - through human reproduction. Overall, we show that in the face of extinction risk, people become more impatient rather than more farsighted. Thus, the greater the threat of extinction, the less incentive there is to invest in avoiding it. Our framework can help explain why humanity consistently underinvests in mitigation of catastrophic risks, ranging from climate change mitigation, via pandemic prevention, to addressing the emerging risks of transformative artificial intelligence."
http://arxiv.org/abs/2509.05076v1,Randomization and ambiguity perception,2025-09-05 13:15:53+00:00,"['Yutaro Akita', 'Kensei Nakamura']",econ.TH,"We provide a model of preferences over lotteries of acts in which a decision maker behaves as if optimally filtering her ambiguity perception. She has a set of plausible ambiguity perceptions and a cost function over them, and chooses multiple priors to maximize the minimum expected utility minus the cost. We characterize the model by axioms on attitude toward randomization and its timing, uniquely identify the filtering cost from observable data, and conduct several comparatives. Our model can explain Machina's (2009) two paradoxes, which are incompatible with many standard ambiguity models."
http://arxiv.org/abs/2508.08166v1,Relative Income and Gender Norms: Evidence from Latin America,2025-08-11 16:41:30+00:00,"['Ercio Muñoz', 'Dario Sansone', 'João Tampellini']",econ.GN,"Using data from over 500,000 dual-earner households in Mexico, we provide evidence of discontinuities in the distribution of relative income within households in Latin America. Similar to high-income countries, we observe a sharp drop at the 50% threshold, where the wife earns more than the husband, but the discontinuity is up to five times larger and has increased over time. These patterns are robust to excluding equal earners, self-employed individuals, or couples in the same occupation/industry. Discontinuities persist across subgroups, including couples with or without children, married or unmarried partners, and those with older wives or female household heads. We also find comparable discontinuities in Brazil and Panama, as well as among some same-sex couples. Moreover, women who are primary earners continue to supply more non-market labor than their male partners, although the gap is narrower than in households where the woman is the secondary earner."
http://arxiv.org/abs/2508.06824v1,The impact of brand equity on vertical integration in franchise systems,2025-08-09 04:45:28+00:00,"['Mohammad Kayed', 'Manish Kacker', 'Ruhai Wu', 'Farhad Sadeh']",econ.GN,"Brand equity and vertical integration are focal, strategic elements of a franchise system that can profoundly influence franchise performance. Despite the recognized importance of these two strategic levers and the longstanding research interest in the topic, our understanding of the interplay between brand equity and vertical integration (company ownership of outlets) in a franchise system remains incomplete. In this study, we revisit the five-decade-old question of how brand equity affects vertical integration in a franchise system and present some novel, nuanced insights into the topic. Evidence from a Bayesian Panel Vector Autoregressive model on a large panel data set shows that brand equity has a powerful, lagging inverse effect on vertical integration, such that higher brand equity leads to less downstream vertical integration in a franchise system. Reverse causality analyses identify a less pronounced but present reciprocal effect. Boundary conditions analyses reveal that the negative effect of brand equity on vertical integration is weaker in franchise systems with international presence and in retail-focused (vs. service-focused) franchises, and stronger in franchise systems with more financial resources. These findings (a) challenge traditional views (e.g., transaction cost theory, resource-based view, ownership redirection hypothesis) on the topic by demonstrating a negative effect for brand equity on vertical integration in franchise systems and showing that greater financial resources amplify this effect, and (b) shed new light on the intricate dynamics (temporal causation, reverse causation) and contingencies of this debated effect. Managerially, this research draws attention to the underrecognized strategic benefit of brand equity in mitigating channel governance issues and advise against unnecessary vertical integration, especially when brand equity is robust."
http://arxiv.org/abs/2508.09079v1,The shape of economics before and after the financial crisis,2025-08-12 16:58:23+00:00,"['Alberto Baccini', 'Lucio Barabesi', 'Carlo Debernardi']",econ.GN,"This paper investigates the impact of the global financial crisis on the shape of economics as a discipline by analyzing EconLit-indexed journals from 2006 to 2020 using a multilayer network approach. We consider two types of social relationships among journals, based on shared editors (interlocking editorship) and shared authors (interlocking authorship), as well as two forms of intellectual proximity, derived from bibliographic coupling and textual similarity. These four dimensions are integrated using Similarity Network Fusion to produce a unified similarity network from which journal communities are identified. Comparing the field in 2006, 2012, and 2019 reveals a high degree of structural continuity. Our findings suggest that, despite changes in research topics after the crisis, fundamental social and intellectual relationships among journals have remained remarkably stable. Editorial networks, in particular, continue to shape hierarchies and legitimize knowledge production."
http://arxiv.org/abs/2508.08980v1,Existence of Richter-Peleg Representation for General Preferences,2025-08-12 14:47:02+00:00,"['Leandro Gorno', 'Paulo Klinger Monteiro']",econ.TH,"This paper provides a general characterization of preferences that admit a Richter-Peleg representation without imposing completeness or transitivity. We establish that a binary relation on a nonempty set admits a Richter-Peleg representation if and only if it is ""strongly acyclic"" and its transitive closure is ""separable"". Strong acyclicity rules out problematic cycles among indifference classes, while separability limits the structural complexity of the relation. Our main result has two significant corollaries. First, when the set of alternatives is countable, a binary relation admits a Richter-Peleg representation if and only if it is strongly acyclic. Second, a preorder admits a Richter-Peleg representation if and only if it is separable. These findings have important implications for decision theory, particularly for obtaining maximal elements through scalar maximization in the presence of indecisiveness."
http://arxiv.org/abs/2508.08914v1,The impact of the European Union's enlargement with the Western Balkans and the Association Trio on the power of member states in the Council,2025-08-12 13:07:25+00:00,"['Tímea Kovács', 'Dóra Gréta Petróczy', 'Gábor Pásztor']",econ.GN,"As of 2022, the European Union has taken several steps regarding enlargement. We focus on the accession of countries with which the Union is actively negotiating membership. This is examined under two enlargement scenarios: first, the enlargement along the lines of the Western Balkan countries, and second, the accession of a trio (Ukraine, Moldova, and Georgia) to the already enlarged Union. We determine the a priori power of the member states based on the Banzhaf and Shapley--Shubik indices. Various coalitions are also assumed to assess the power and influence of member states, considering both pre- and post-enlargement scenarios. We found a rare case when the two indices give different rankings. In the case of the Western Balkan countries' accession, the smaller population member states gain power, presenting an example of the new member paradox. While in a Union of 36 members, every member state loses some of their current power. However, some coalitions are better off with the EU36 enlargement than a EU33 one."
http://arxiv.org/abs/2509.02800v1,Too Noisy to Collude? Algorithmic Collusion Under Laplacian Noise,2025-09-02 20:02:17+00:00,['Niuniu Zhang'],econ.GN,"The rise of autonomous pricing systems has sparked growing concern over algorithmic collusion in markets from retail to housing. This paper examines controlled information quality as an ex ante policy lever: by reducing the fidelity of data that pricing algorithms draw on, regulators can frustrate collusion before supracompetitive prices emerge. We show, first, that information quality is the central driver of competitive outcomes, shaping prices, profits, and consumer welfare. Second, we demonstrate that collusion can be slowed or destabilized by injecting carefully calibrated noise into pooled market data, yielding a feasibility region where intervention disrupts cartels without undermining legitimate pricing. Together, these results highlight information control as a lightweight yet practical lever to blunt digital collusion at its source."
http://arxiv.org/abs/2509.01310v1,Gender Differences in Healthcare Utilisation -- Evidence from Unexpected Adverse Health Shocks,2025-09-01 09:50:09+00:00,"[""Nadja van 't Hoff"", 'Giovanni Mellace', 'Seetha Menon']",econ.GN,"This paper is the first to provide causal evidence of gender differences in healthcare utilisation to better understand the male-female health-survival paradox, where women live longer but experience worse health outcomes. Using rich Danish administrative healthcare data, we apply a staggered difference-in-differences approach that exploits the randomness in treatment timing to estimate the causal impact of adverse health shocks, such as non-fatal heart attacks or strokes, on healthcare use. Our findings suggest that men consistently use more healthcare than women, highlighting the underlying factors driving gender disparities in health outcomes. These insights contribute to the broader discourse on healthcare equity and inform policy interventions aimed at addressing these imbalances."
http://arxiv.org/abs/2509.04928v1,A Bayesian Gaussian Process Dynamic Factor Model,2025-09-05 08:51:02+00:00,"['Tony Chernis', 'Niko Hauzenberger', 'Haroon Mumtaz', 'Michael Pfarrhofer']",econ.EM,"We propose a dynamic factor model (DFM) where the latent factors are linked to observed variables with unknown and potentially nonlinear functions. The key novelty and source of flexibility of our approach is a nonparametric observation equation, specified via Gaussian Process (GP) priors for each series. Factor dynamics are modeled with a standard vector autoregression (VAR), which facilitates computation and interpretation. We discuss a computationally efficient estimation algorithm and consider two empirical applications. First, we forecast key series from the FRED-QD dataset and show that the model yields improvements in predictive accuracy relative to linear benchmarks. Second, we extract driving factors of global inflation dynamics with the GP-DFM, which allows for capturing international asymmetries."
http://arxiv.org/abs/2509.05828v1,What Slips the Mind Stalls the Deal: Delay in Bargaining with Absentmindedness,2025-09-06 20:45:48+00:00,['Cole Wittbrodt'],econ.TH,"In finite-horizon bargaining, deals are often made ""on the courthouse steps"", just before the deadline. Most classic finite-horizon bargaining models fail to generate deadline effects, or even delay, in equilibrium. Players foresee the future path of play, and come to a deal immediately to circumvent bargaining frictions. We propose a novel source of bargaining delay: absentmindedness. A bargainer who does not know the calendar time may rationally reject an ""ultimatum offer"" as the trade deadline looms. Rational confusion is a source of bargaining power for the absentminded player, as it induces the other party to make fair offers near the trade deadline to prevent negotiations from breaking down. The absentminded party may reject greedier offers in hope of receiving a fair offer closer to the deadline. If any offer is feasible, there are equilibria which feature delay if and only if players are patient. Such equilibria always involve history-dependent strategies. I provide a necessary and sufficient condition for there to exist a Markov perfect equilibrium with delay: the space of feasible offers must be sufficiently disconnected."
http://arxiv.org/abs/2507.13798v1,Choosing and Using Information in Evaluation Decisions,2025-07-18 10:20:03+00:00,"['Katherine B. Coffman', 'Scott Kostyshak', 'Perihan O. Saygin']",econ.GN,"We use a controlled experiment to study how information acquisition impacts candidate evaluations. We provide evaluators with group-level information on performance and the opportunity to acquire additional, individual-level performance information before making a final evaluation. We find that, on average, evaluators under-acquire individual-level information, leading to more stereotypical evaluations of candidates. Consistent with stereotyping, we find that (irrelevant) group-level comparisons have a significant impact on how candidates are evaluated; group-level comparisons bias initial assessments, responses to information, and final evaluations. This leads to under-recognition of talented candidates from comparatively weaker groups and over-selection of untalented candidates from comparatively stronger groups."
http://arxiv.org/abs/2507.12690v1,NA-DiD: Extending Difference-in-Differences with Capabilities,2025-07-16 23:49:04+00:00,['Stanisław M. S. Halkiewicz'],econ.EM,"This paper introduces the Non-Additive Difference-in-Differences (NA-DiD) framework, which extends classical DiD by incorporating non-additive measures the Choquet integral for effect aggregation. It serves as a novel econometric tool for impact evaluation, particularly in settings with non-additive treatment effects. First, we introduce the integral representation of the classial DiD model, and then extend it to non-additive measures, therefore deriving the formulae for NA-DiD estimation. Then, we give its theoretical properties. Applying NA-DiD to a simulated hospital hygiene intervention, we find that classical DiD can overestimate treatment effects, f.e. failing to account for compliance erosion. In contrast, NA-DiD provides a more accurate estimate by incorporating non-linear aggregation. The Julia implementation of the techniques used and introduced in this article is provided in the appendices."
http://arxiv.org/abs/2507.12616v1,Third-Party Credit Guarantees and the Cost of Debt: Evidence from Corporate Loans,2025-07-16 20:25:07+00:00,['Mehdi Beyhaghi'],econ.GN,"Using a comprehensive dataset collected by the Federal Reserve, I find that over one-third of corporate loans issued by US banks are fully guaranteed by legal entities separate from borrowing firms. Using an empirical strategy that accounts for time-varying firm and lender effects, I find that the existence of a third-party credit guarantee is negatively related to loan risk, loan rate, and loan delinquency. Third party credit guarantees alleviate the effect of collateral constraints in credit market. Firms (particularly smaller firms) that experience a negative shock to their asset values are less likely to use collateral and more likely to use credit guarantees in new borrowings."
http://arxiv.org/abs/2507.13084v1,Do Governments React to Public Debt Accumulation? A Cross-Country Analysis,2025-07-17 12:55:44+00:00,"['Paolo Canofari', 'Alessandro Piergallini', 'Marco Tedeschi']",econ.GN,"Do governments adjust budgetary policy to rising public debt, precluding fiscal unsustainability? Using budget data for 52 industrial and emerging economies since 1990, we apply panel methods accounting for cross-sectional dependence and heterogeneous fiscal conduct. We find that a primary-balance rule with tax-smoothing motives and responsiveness to debt has robust explanatory power in describing fiscal behavior. Controlling for temporary output, temporary spending, and the current account balance, a 10-percentage-point increase in the debt-to-GDP ratio raises the long-run primary surplus-to-GDP ratio by 0.5 percentage points on average. Corrective adjustments hold across high- and low-debt countries and across industrial and emerging economies. Our results imply many governments pursue Ricardian policy designs, avoiding Ponzi-type financing."
http://arxiv.org/abs/2507.15048v1,Central Bank Digital Currency: Demand Shocks and Optimal Monetary Policy,2025-07-20 17:05:43+00:00,"['Hanfeng Chen', 'Maria Elena Filippin']",econ.TH,"We study the implications of a central bank digital currency (CBDC) for the transmission of household preference shocks and for welfare in a New Keynesian framework where the CBDC competes with bank deposits for household resources and banks have market power. We show that an increase in the benefit of CBDC has a mildly expansionary effect, weakening bank market power and significantly reducing the deposit spread. As households economize on liquid asset holdings, they reduce both CBDC and deposit balances. However, the degree of bank disintermediation is low, as deposit outflows remain modest. We then examine the welfare implications of CBDC rate setting and find that, compared to a non-interest-bearing CBDC, the gains with standard coefficients for a CBDC interest rate Taylor rule are modest, but they become considerable when the coefficients are optimized. Welfare gains are higher when the CBDC provides a higher benefit."
http://arxiv.org/abs/2507.21824v1,Markowitz Variance May Vastly Undervalue or Overestimate Portfolio Variance and Risks,2025-07-29 14:00:26+00:00,['Victor Olkhov'],econ.GN,"We consider the investor who doesn't trade shares of his portfolio. The investor only observes the current trades made in the market with his securities to estimate the current return, variance, and risks of his unchanged portfolio. We show how the time series of consecutive trades made in the market with the securities of the portfolio can determine the time series that model the trades with the portfolio as with a single security. That establishes the equal description of the market-based variance of the securities and of the portfolio composed of these securities that account for the fluctuations of the volumes of the consecutive trades. We show that Markowitz's (1952) variance describes only the approximation when all volumes of the consecutive trades with securities are assumed constant. The market-based variance depends on the coefficient of variation of fluctuations of volumes of trades. To emphasize this dependence and to estimate possible deviation from Markowitz variance, we derive the Taylor series of the market-based variance up to the 2nd term by the coefficient of variation, taking Markowitz variance as a zero approximation. We consider three limiting cases with low and high fluctuations of the portfolio returns, and with a zero covariance of trade values and volumes and show that the impact of the coefficient of variation of trade volume fluctuations can cause Markowitz's assessment to highly undervalue or overestimate the market-based variance of the portfolio. Incorrect assessments of the variances of securities and of the portfolio cause wrong risk estimates, disturb optimal portfolio selection, and result in unexpected losses. The major investors, portfolio managers, and developers of macroeconomic models like BlackRock, JP Morgan, and the U.S. Fed should use market-based variance to adjust their predictions to the randomness of market trades."
http://arxiv.org/abs/2507.21559v1,A Bayesian Ensemble Projection of Climate Change and Technological Impacts on Future Crop Yields,2025-07-29 07:45:14+00:00,"['Dan Li', 'Vassili Kitsios', 'David Newth', ""Terence John O'Kane""]",stat.AP,"This paper introduces a Bayesian hierarchical modeling framework within a fully probabilistic setting for crop yield estimation, model selection, and uncertainty forecasting under multiple future greenhouse gas emission scenarios. By informing on regional agricultural impacts, this approach addresses broader risks to global food security. Extending an established multivariate econometric crop-yield model to incorporate country-specific error variances, the framework systematically relaxes restrictive homogeneity assumptions and enables transparent decomposition of predictive uncertainty into contributions from climate models, emission scenarios, and crop model parameters. In both in-sample and out-of-sample analyses focused on global wheat production, the results demonstrate significant improvements in calibration and probabilistic accuracy of yield projections. These advances provide policymakers and stakeholders with detailed, risk-sensitive information to support the development of more resilient and adaptive agricultural and climate strategies in response to escalating climate-related risks."
http://arxiv.org/abs/2507.21360v1,Efficacy of AI RAG Tools for Complex Information Extraction and Data Annotation Tasks: A Case Study Using Banks Public Disclosures,2025-07-28 22:06:11+00:00,"['Nicholas Botti', 'Flora Haberkorn', 'Charlotte Hoopes', 'Shaun Khan']",cs.AI,"We utilize a within-subjects design with randomized task assignments to understand the effectiveness of using an AI retrieval augmented generation (RAG) tool to assist analysts with an information extraction and data annotation task. We replicate an existing, challenging real-world annotation task with complex multi-part criteria on a set of thousands of pages of public disclosure documents from global systemically important banks (GSIBs) with heterogeneous and incomplete information content. We test two treatment conditions. First, a ""naive"" AI use condition in which annotators use only the tool and must accept the first answer they are given. And second, an ""interactive"" AI treatment condition where annotators use the tool interactively, and use their judgement to follow-up with additional information if necessary. Compared to the human-only baseline, the use of the AI tool accelerated task execution by up to a factor of 10 and enhanced task accuracy, particularly in the interactive condition. We find that when extrapolated to the full task, these methods could save up to 268 hours compared to the human-only approach. Additionally, our findings suggest that annotator skill, not just with the subject matter domain, but also with AI tools, is a factor in both the accuracy and speed of task performance."
http://arxiv.org/abs/2507.20796v1,Aligning Large Language Model Agents with Rational and Moral Preferences: A Supervised Fine-Tuning Approach,2025-07-28 13:05:04+00:00,"['Wei Lu', 'Daniel L. Chen', 'Christian B. Hansen']",econ.GN,"Understanding how large language model (LLM) agents behave in strategic interactions is essential as these systems increasingly participate autonomously in economically and morally consequential decisions. We evaluate LLM preferences using canonical economic games, finding substantial deviations from human behavior. Models like GPT-4o show excessive cooperation and limited incentive sensitivity, while reasoning models, such as o3-mini, align more consistently with payoff-maximizing strategies. We propose a supervised fine-tuning pipeline that uses synthetic datasets derived from economic reasoning to align LLM agents with economic preferences, focusing on two stylized preference structures. In the first, utility depends only on individual payoffs (homo economicus), while utility also depends on a notion of Kantian universalizability in the second preference structure (homo moralis). We find that fine-tuning based on small datasets shifts LLM agent behavior toward the corresponding economic agent. We further assess the fine-tuned agents' behavior in two applications: Moral dilemmas involving autonomous vehicles and algorithmic pricing in competitive markets. These examples illustrate how different normative objectives embedded via realizations from structured preference structures can influence market and moral outcomes. This work contributes a replicable, cost-efficient, and economically grounded pipeline to align AI preferences using moral-economic principles."
http://arxiv.org/abs/2508.00658v1,Multi-Band Variable-Lag Granger Causality: A Unified Framework for Causal Time Series Inference across Frequencies,2025-08-01 14:22:51+00:00,"['Chakattrai Sookkongwaree', 'Tattep Lakmuang', 'Chainarong Amornbunchornvej']",cs.AI,"Understanding causal relationships in time series is fundamental to many domains, including neuroscience, economics, and behavioral science. Granger causality is one of the well-known techniques for inferring causality in time series. Typically, Granger causality frameworks have a strong fix-lag assumption between cause and effect, which is often unrealistic in complex systems. While recent work on variable-lag Granger causality (VLGC) addresses this limitation by allowing a cause to influence an effect with different time lags at each time point, it fails to account for the fact that causal interactions may vary not only in time delay but also across frequency bands. For example, in brain signals, alpha-band activity may influence another region with a shorter delay than slower delta-band oscillations. In this work, we formalize Multi-Band Variable-Lag Granger Causality (MB-VLGC) and propose a novel framework that generalizes traditional VLGC by explicitly modeling frequency-dependent causal delays. We provide a formal definition of MB-VLGC, demonstrate its theoretical soundness, and propose an efficient inference pipeline. Extensive experiments across multiple domains demonstrate that our framework significantly outperforms existing methods on both synthetic and real-world datasets, confirming its broad applicability to any type of time series data. Code and datasets are publicly available."
http://arxiv.org/abs/2507.20403v2,A General Framework for Estimating Preferences Using Response Time Data,2025-07-27 20:13:02+00:00,"['Federico Echenique', 'Alireza Fallah', 'Michael I. Jordan']",econ.TH,"We propose a general methodology for recovering preference parameters from data on choices and response times. Our methods yield estimates with fast ($1/n$ for $n$ data points) convergence rates when specialized to the popular Drift Diffusion Model (DDM), but are broadly applicable to generalizations of the DDM as well as to alternative models of decision making that make use of response time data. The paper develops an empirical application to an experiment on intertemporal choice, showing that the use of response times delivers predictive accuracy and matters for the estimation of economically relevant parameters."
http://arxiv.org/abs/2508.00294v1,Formal Power Series Representations in Probability and Expected Utility Theory,2025-08-01 03:34:39+00:00,"['Arthur Paul Pedersen', 'Samuel Allen Alexander']",math.PR,"We advance a general theory of coherent preference that surrenders restrictions embodied in orthodox doctrine. This theory enjoys the property that any preference system admits extension to a complete system of preferences, provided it satisfies a certain coherence requirement analogous to the one de Finetti advanced for his foundations of probability. Unlike de Finetti's theory, the one we set forth requires neither transitivity nor Archimedeanness nor boundedness nor continuity of preference. This theory also enjoys the property that any complete preference system meeting the standard of coherence can be represented by utility in an ordered field extension of the reals. Representability by utility is a corollary of this paper's central result, which at once extends Hölder's Theorem and strengthens Hahn's Embedding Theorem."
http://arxiv.org/abs/2508.00078v1,Evaluating COVID 19 Feature Contributions to Bitcoin Return Forecasting: Methodology Based on LightGBM and Genetic Optimization,2025-07-31 18:12:33+00:00,"['Imen Mahmoud', 'Andrei Velichko']",cs.LG,"This study proposes a novel methodological framework integrating a LightGBM regression model and genetic algorithm (GA) optimization to systematically evaluate the contribution of COVID-19-related indicators to Bitcoin return prediction. The primary objective was not merely to forecast Bitcoin returns but rather to determine whether including pandemic-related health data significantly enhances prediction accuracy. A comprehensive dataset comprising daily Bitcoin returns and COVID-19 metrics (vaccination rates, hospitalizations, testing statistics) was constructed. Predictive models, trained with and without COVID-19 features, were optimized using GA over 31 independent runs, allowing robust statistical assessment. Performance metrics (R2, RMSE, MAE) were statistically compared through distribution overlaps and Mann-Whitney U tests. Permutation Feature Importance (PFI) analysis quantified individual feature contributions. Results indicate that COVID-19 indicators significantly improved model performance, particularly in capturing extreme market fluctuations (R2 increased by 40%, RMSE decreased by 2%, both highly significant statistically). Among COVID-19 features, vaccination metrics, especially the 75th percentile of fully vaccinated individuals, emerged as dominant predictors. The proposed methodology extends existing financial analytics tools by incorporating public health signals, providing investors and policymakers with refined indicators to navigate market uncertainty during systemic crises."
http://arxiv.org/abs/2508.05491v1,Deconstructing the Crystal Ball: From Ad-Hoc Prediction to Principled Startup Evaluation with the SAISE Framework,2025-08-07 15:26:32+00:00,"['Seyed Mohammad Ali Jafari', 'Ali Mobini Dehkordi', 'Ehsan Chitsaz', 'Yadollah Yaghoobzadeh']",cs.CE,"The integration of Artificial Intelligence (AI) into startup evaluation represents a significant technological shift, yet the academic research underpinning this transition remains methodologically fragmented. Existing studies often employ ad-hoc approaches, leading to a body of work with inconsistent definitions of success, atheoretical features, and a lack of rigorous validation. This fragmentation severely limits the comparability, reliability, and practical utility of current predictive models.
  To address this critical gap, this paper presents a comprehensive systematic literature review of 57 empirical studies. We deconstruct the current state-of-the-art by systematically mapping the features, algorithms, data sources, and evaluation practices that define the AI-driven startup prediction landscape. Our synthesis reveals a field defined by a central paradox: a strong convergence on a common toolkit -- venture databases and tree-based ensembles -- but a stark divergence in methodological rigor. We identify four foundational weaknesses: a fragmented definition of ""success,"" a divide between theory-informed and data-driven feature engineering, a chasm between common and best-practice model validation, and a nascent approach to data ethics and explainability.
  In response to these findings, our primary contribution is the proposal of the Systematic AI-driven Startup Evaluation (SAISE) Framework. This novel, five-stage prescriptive roadmap is designed to guide researchers from ad-hoc prediction toward principled evaluation. By mandating a coherent, end-to-end methodology that emphasizes stage-aware problem definition, theory-informed data synthesis, principled feature engineering, rigorous validation, and risk-aware interpretation, the SAISE framework provides a new standard for conducting more comparable, robust, and practically relevant research in this rapidly maturing domain"
http://arxiv.org/abs/2507.22869v1,Inference on Common Trends in a Cointegrated Nonlinear SVAR,2025-07-30 17:44:07+00:00,"['James A. Duffy', 'Xiyu Jiao']",econ.EM,"We consider the problem of performing inference on the number of common stochastic trends when data is generated by a cointegrated CKSVAR (a two-regime, piecewise-linear SVAR; Mavroeidis, 2021), using a modified version of the Breitung (2002) multivariate variance ratio test that is robust to the presence of nonlinear cointegration (of a known form). To derive the asymptotics of our test statistic, we prove a fundamental LLN-type result for a class of stable but nonstationary autoregressive processes, using a novel dual linear process approximation. We show that our modified test yields correct inferences regarding the number of common trends in such a system, whereas the unmodified test tends to infer a higher number of common trends than are actually present, when cointegrating relations are nonlinear."
http://arxiv.org/abs/2507.22852v1,Robust Contract with Career Concerns,2025-07-30 17:22:58+00:00,"['Tan Gan', 'Hongcheng Li']",econ.TH,"An employer contracts with a worker to incentivize efforts whose productivity depends on ability; the worker then enters a market that pays him contingent on ability evaluation. With non-additive monitoring technology, the interdependence between market expectations and worker efforts can lead to multiple equilibria (contrasting Holmstrom (1982/1999); Gibbons and Murphy (1992)). We identify a sufficient and necessary criterion for the employer to face such strategic uncertainty--one linked to skill-effort complementarity, a pervasive feature of labor markets. To fully implement work, the employer optimally creates private wage discrimination to iteratively eliminate pessimistic market expectations and low worker efforts. Our result suggests that present contractual privacy, employers' coordination motives generate within-group pay inequality. The comparative statics further explain several stylized facts about residual wage dispersion."
http://arxiv.org/abs/2508.04259v1,High-Dimensional Matrix-Variate Diffusion Index Models for Time Series Forecasting,2025-08-06 09:44:40+00:00,"['Zhiren Ma', 'Qian Zhao', 'Riquan Zhang', 'Zhaoxing Gao']",econ.EM,"This paper proposes a novel diffusion-index model for forecasting when predictors are high-dimensional matrix-valued time series. We apply an $α$-PCA method to extract low-dimensional matrix factors and build a bilinear regression linking future outcomes to these factors, estimated via iterative least squares. To handle weak factor structures, we introduce a supervised screening step to select informative rows and columns. Theoretical properties, including consistency and asymptotic normality, are established. Simulations and real data show that our method significantly improves forecast accuracy, with the screening procedure providing additional gains over standard benchmarks in out-of-sample mean squared forecast error."
http://arxiv.org/abs/2508.03855v1,"Exports, Labor Markets, and the Environment: Evidence from Brazil",2025-08-05 19:13:27+00:00,"['Carlos Góes', 'Otavio Conceição', 'Gabriel Lara Ibarra', 'Gladys Lopez-Acevedo']",econ.GN,"What is the environmental impact of exports? Focusing on 2000-20, this paper combines customs, administrative, and census microdata to estimate employment elasticities with respect to exports. The findings show that municipalities that faced increased exports experienced faster growth in formal employment. The elasticities were 0.25 on impact, peaked at 0.4, and remained positive and significant even 10 years after the shock, pointing to a long and protracted labor market adjustment. In the long run, informal employment responds negatively to export shocks. Using a granular taxonomy for economic activities based on their environmental impact, the paper documents that environmentally risky activities have a larger share of employment than environmentally sustainable ones, and that the relationship between these activities and exports is nuanced. Over the short run, environmentally risky employment responds more strongly to exports relative to environmentally sustainable employment. However, over the long run, this pattern reverses, as the impact of exports on environmentally sustainable employment is more persistent."
http://arxiv.org/abs/2508.00263v1,Robust Econometrics for Growth-at-Risk,2025-08-01 02:10:16+00:00,"['Tobias Adrian', 'Yuya Sasaki', 'Yulong Wang']",econ.EM,"The Growth-at-Risk (GaR) framework has garnered attention in recent econometric literature, yet current approaches implicitly assume a constant Pareto exponent. We introduce novel and robust econometrics to estimate the tails of GaR based on a rigorous theoretical framework and establish validity and effectiveness. Simulations demonstrate consistent outperformance relative to existing alternatives in terms of predictive accuracy. We perform a long-term GaR analysis that provides accurate and insightful predictions, effectively capturing financial anomalies better than current methods."
http://arxiv.org/abs/2507.22211v1,Uniqueness of Inflection Points in Binomial Exceedance Function Compositions,2025-07-29 20:17:35+00:00,"['Srinivas Arigapudi', 'Yuval Heller', 'Amnon Schreiber']",econ.TH,"We examine functions representing the cumulative probability of a binomial random variable exceeding a threshold, expressed in terms of the success probability per trial. These functions are known to exhibit a unique inflection point. We generalize this property to their compositions and highlight its applications."
http://arxiv.org/abs/2508.03540v1,An Evolutionary Analysis of Narrative Selection,2025-08-05 15:09:57+00:00,"['Federico Innocenti', 'Roberto Rozzi']",econ.TH,"We study the performance of different methods for processing information, incorporating narrative selection within an evolutionary model. All agents update their beliefs according to Bayes' Rule, but some strategically choose the narrative they use in updating according to heterogeneous criteria. We simulate the endogenous composition of the population, considering different laws of motion for the underlying state of the world. We find that conformists -- that is, agents that choose the narrative to conform to the average belief in the population -- have an evolutionary advantage over other agents across all specifications. The survival chances of the remaining types depend on the uncertainty regarding the state of the world. Agents who tend to develop mild beliefs perform better when the uncertainty is high, whereas agents who tend to develop extreme beliefs perform better when the uncertainty is low."
http://arxiv.org/abs/2508.00844v1,Exploring Agentic Artificial Intelligence Systems: Towards a Typological Framework,2025-07-07 14:05:30+00:00,"['Christopher Wissuchek', 'Patrick Zschech']",cs.AI,"Artificial intelligence (AI) systems are evolving beyond passive tools into autonomous agents capable of reasoning, adapting, and acting with minimal human intervention. Despite their growing presence, a structured framework is lacking to classify and compare these systems. This paper develops a typology of agentic AI systems, introducing eight dimensions that define their cognitive and environmental agency in an ordinal structure. Using a multi-phase methodological approach, we construct and refine this typology, which is then evaluated through a human-AI hybrid approach and further distilled into constructed types. The framework enables researchers and practitioners to analyze varying levels of agency in AI systems. By offering a structured perspective on the progression of AI capabilities, the typology provides a foundation for assessing current systems and anticipating future developments in agentic AI."
http://arxiv.org/abs/2508.01677v1,Anchoring-Based Causal Design (ABCD): Estimating the Effects of Beliefs,2025-08-03 09:07:58+00:00,"['Raanan Sulitzeanu-Kenan', 'Micha Mandel', 'Yosef Rinott']",econ.GN,"A central challenge in any study of the effects of beliefs on outcomes, such as decisions and behavior, is the risk of omitted variables bias. Omitted variables, frequently unmeasured or even unknown, can induce correlations between beliefs and decisions that are not genuinely causal, in which case the omitted variables are referred to as confounders. To address the challenge of causal inference, researchers frequently rely on information provision experiments to randomly manipulate beliefs. The information supplied in these experiments can serve as an instrumental variable (IV), enabling causal inference, so long as it influences decisions exclusively through its impact on beliefs. However, providing varying information to participants to shape their beliefs can raise both methodological and ethical concerns. Methodological concerns arise from potential violations of the exclusion restriction assumption. Such violations may stem from information source effects, when attitudes toward the source affect the outcome decision directly, thereby introducing a confounder. An ethical concern arises from manipulating the provided information, as it may involve deceiving participants. This paper proposes and empirically demonstrates a new method for treating beliefs and estimating their effects, the Anchoring-Based Causal Design (ABCD), which avoids deception and source influences. ABCD combines the cognitive mechanism known as anchoring with instrumental variable (IV) estimation. Instead of providing substantive information, the method employs a deliberately non-informative procedure in which participants compare their self-assessment of a concept to a randomly assigned anchor value. We present the method and the results of eight experiments demonstrating its application, strengths, and limitations. We conclude by discussing the potential of this design for advancing experimental social science."
http://arxiv.org/abs/2508.02686v1,Adaptive Market Intelligence: A Mixture of Experts Framework for Volatility-Sensitive Stock Forecasting,2025-07-22 19:37:13+00:00,['Diego Vallarino'],q-fin.ST,"This study develops and empirically validates a Mixture of Experts (MoE) framework for stock price prediction across heterogeneous volatility regimes using real market data. The proposed model combines a Recurrent Neural Network (RNN) optimized for high-volatility stocks with a linear regression model tailored to stable equities. A volatility-aware gating mechanism dynamically weights the contributions of each expert based on asset classification. Using a dataset of 30 publicly traded U.S. stocks spanning diverse sectors, the MoE approach consistently outperforms both standalone models.
  Specifically, it achieves up to 33% improvement in MSE for volatile assets and 28% for stable assets relative to their respective baselines. Stratified evaluation across volatility classes demonstrates the model's ability to adapt complexity to underlying market dynamics. These results confirm that no single model suffices across market regimes and highlight the advantage of adaptive architectures in financial prediction. Future work should explore real-time gate learning, dynamic volatility segmentation, and applications to portfolio optimization."
http://arxiv.org/abs/2508.00159v2,Model-Based Soft Maximization of Suitable Metrics of Long-Term Human Power,2025-07-31 20:56:43+00:00,"['Jobst Heitzig', 'Ram Potham']",cs.AI,"Power is a key concept in AI safety: power-seeking as an instrumental goal, sudden or gradual disempowerment of humans, power balance in human-AI interaction and international AI governance. At the same time, power as the ability to pursue diverse goals is essential for wellbeing.
  This paper explores the idea of promoting both safety and wellbeing by forcing AI agents explicitly to empower humans and to manage the power balance between humans and AI agents in a desirable way. Using a principled, partially axiomatic approach, we design a parametrizable and decomposable objective function that represents an inequality- and risk-averse long-term aggregate of human power. It takes into account humans' bounded rationality and social norms, and, crucially, considers a wide variety of possible human goals.
  We derive algorithms for computing that metric by backward induction or approximating it via a form of multi-agent reinforcement learning from a given world model. We exemplify the consequences of (softly) maximizing this metric in a variety of paradigmatic situations and describe what instrumental sub-goals it will likely imply. Our cautious assessment is that softly maximizing suitable aggregate metrics of human power might constitute a beneficial objective for agentic AI systems that is safer than direct utility-based objectives."
http://arxiv.org/abs/2508.06469v1,A Geometric Analysis of Gains from Trade,2025-08-08 17:19:36+00:00,"['Jason Hartline', 'Kangning Wang']",cs.GT,We provide a geometric proof that the random proposer mechanism is a $4$-approximation to the first-best gains from trade in bilateral exchange. We then refine this geometric analysis to recover the state-of-the-art approximation ratio of $3.15$.
http://arxiv.org/abs/2507.02293v1,Large-Scale Estimation under Unknown Heteroskedasticity,2025-07-03 04:00:14+00:00,['Sheng Chao Ho'],econ.EM,"This paper studies nonparametric empirical Bayes methods in a heterogeneous parameters framework that features unknown means and variances. We provide extended Tweedie's formulae that express the (infeasible) optimal estimators of heterogeneous parameters, such as unit-specific means or quantiles, in terms of the density of certain sufficient statistics. These are used to propose feasible versions with nearly parametric regret bounds of the order of $(\log n)^κ/ n$. The estimators are employed in a study of teachers' value-added, where we find that allowing for heterogeneous variances across teachers is crucial for delivery optimal estimates of teacher quality and detecting low-performing teachers."
http://arxiv.org/abs/2507.00763v1,Comparing Misspecified Models with Big Data: A Variational Bayesian Perspective,2025-07-01 14:05:24+00:00,"['Yong Li', 'Sushanta K. Mallick', 'Tao Zeng', 'Junxing Zhang']",econ.EM,"Optimal data detection in massive multiple-input multiple-output (MIMO) systems often requires prohibitively high computational complexity. A variety of detection algorithms have been proposed in the literature, offering different trade-offs between complexity and detection performance. In recent years, Variational Bayes (VB) has emerged as a widely used method for addressing statistical inference in the context of massive data. This study focuses on misspecified models and examines the risk functions associated with predictive distributions derived from variational posterior distributions. These risk functions, defined as the expectation of the Kullback-Leibler (KL) divergence between the true data-generating density and the variational predictive distributions, provide a framework for assessing predictive performance. We propose two novel information criteria for predictive model comparison based on these risk functions. Under certain regularity conditions, we demonstrate that the proposed information criteria are asymptotically unbiased estimators of their respective risk functions. Through comprehensive numerical simulations and empirical applications in economics and finance, we demonstrate the effectiveness of these information criteria in comparing misspecified models in the context of massive data."
http://arxiv.org/abs/2507.00913v1,Local Strategy-Proofness and Dictatorship,2025-07-01 16:18:28+00:00,"['Abinash Panda', 'Anup Pramanik', 'Ragini Saxena']",econ.TH,"We investigate preference domains where every unanimous and locally strategy-proof social choice function (scf) satisfies dictatorship. We identify a condition on domains called connected with two distinct neighbours which is necessary for unanimous and locally strategy-proof scfs to satisfy dictatorship. Further, we show that this condition is sufficient within the class of domains where every unanimous and locally strategy-proof scf satisfies tops-onlyness. While a complete characterization remains open, we make significant progress by showing that on connected with two distinct neighbours domains, unanimity and strategy-proofness (a stronger requirement) guarantee dictatorship."
http://arxiv.org/abs/2507.01365v1,Consumption Stimulus with Digital Coupons,2025-07-02 05:08:33+00:00,"['Ying Chen', 'Mingyi Li', 'Jiaming Mao', 'Jingyi Zhou']",econ.GN,"We study consumption stimulus with digital coupons, which provide time-limited subsidies contingent on minimum spending. We analyze a large-scale program in China and present five main findings: (1) the program generates large short-term effects, with each $\yen$1 of government subsidy inducing $\yen$3.4 in consumer spending; (2) consumption responses vary substantially, driven by both demand-side factors (e.g., wealth) and supply-side factors (e.g., local consumption amenities); (3) The largest spending increases occur among consumers whose baseline spending already exceeds coupon thresholds and for whom coupon subsidies should be equivalent to cash, suggesting behavioral motivations; (4) high-response consumers disproportionately direct their spending toward large businesses, leading to a regressive allocation of stimulus benefits; and (5) targeting the most responsive consumers can double total stimulus effects. A hybrid design combining targeted distribution with direct support to small businesses improves both the efficiency and equity of the program."
http://arxiv.org/abs/2507.02439v2,Introducing a New Brexit-Related Uncertainty Index: Its Evolution and Economic Consequences,2025-07-03 08:52:55+00:00,"['Ismet Gocer', 'Julia Darby', 'Serdar Ongan']",econ.GN,"Important game-changer economic events and transformations cause uncertainties that may affect investment decisions, capital flows, international trade, and macroeconomic variables. One such major transformation is Brexit, which refers to the multiyear process through which the UK withdrew from the EU. This study develops and uses a new Brexit-Related Uncertainty Index (BRUI). In creating this index, we apply Text Mining, Context Window, Natural Language Processing (NLP), and Large Language Models (LLMs) from Deep Learning techniques to analyse the monthly country reports of the Economist Intelligence Unit from May 2012 to January 2025. Additionally, we employ a standard vector autoregression (VAR) analysis to examine the model-implied responses of various macroeconomic variables to BRUI shocks. While developing the BRUI, we also create a complementary COVID-19 Related Uncertainty Index (CRUI) to distinguish the uncertainties stemming from these distinct events. Empirical findings and comparisons of BRUI with other earlier-developed uncertainty indexes demonstrate the robustness of the new index. This new index can assist British policymakers in measuring and understanding the impacts of Brexit-related uncertainties, enabling more effective policy formulation."
http://arxiv.org/abs/2507.01804v1,Meta-emulation: An application to the social cost of carbon,2025-07-02 15:23:54+00:00,['Richard S. J. Tol'],econ.EM,"A large database of published model results is used to estimate the distribution of the social cost of carbon as a function of the underlying assumptions. The literature on the social cost of carbon deviates in its assumptions from the literatures on the impacts of climate change, discounting, and risk aversion. The proposed meta-emulator corrects this. The social cost of carbon is higher than reported in the literature."
http://arxiv.org/abs/2507.13767v2,Navigating the Lobbying Landscape: Insights from Opinion Dynamics Models,2025-07-18 09:23:51+00:00,"['Daniele Giachini', 'Leonardo Ciambezi', 'Verdiana Del Rosso', 'Fabrizio Fornari', 'Valentina Pansanella', 'Lilit Popoyan', 'Alina Sîrbu']",econ.GN,"While lobbying has been demonstrated to have an important effect on public opinion and policy making, existing models of opinion formation do not specifically include its effect. In this work we introduce a new model of opinion dynamics where lobbyists can implement complex strategies and are characterised by a finite budget. Individuals update their opinions through a learning process resembling Bayesian learning, but influenced by cognitive biases such as under-reaction and confirmation bias. We study the model numerically and demonstrate rich dynamics both with and without lobbyists. In the presence of lobbying, we observe two regimes: one in which lobbyists can have full influence on the agent network, and another where the peer-effect generates polarisation. When symmetric lobbyists are present, the lobbyist influence regime is characterised by long opinion oscillations, while in the transition area between the two regimes we observe convergence to the optimistic model when the lobbying influence is long enough. These rich dynamics pave the way for studying real lobbying strategies to validate the model in practice."
http://arxiv.org/abs/2507.19934v1,Greening Schoolyards and Urban Property Values: A Systematic Review of Geospatial and Statistical Evidence,2025-07-26 12:39:24+00:00,['Mahshid Gorjian'],physics.soc-ph,"1.1 Background Parks and the greening of schoolyards are examples of urban green spaces that have been praised for their environmental, social, and economic benefits in cities all over the world. More studies show that living near green spaces is good for property values. However, there is still disagreement about how strong and consistent these effects are in different cities (Browning et al., 2023; Grunewald et al., 2024; Teo et al., 2023). 1.2 Purpose This systematic review is the first to bring together a lot of geographical and statistical information that links greening schoolyards to higher property prices, as opposed to just green space in general. By focusing on schoolyard-specific interventions, we find complex spatial, economic, and social effects that are often missed in larger studies of green space. 1.3 Methods This review followed the PRISMA guidelines and did a systematic search and review of papers that were published in well-known journals for urban studies, the environment, and real estate. The criteria for inclusion stressed the use of hedonic pricing or spatial econometric models to look at the relationship between urban green space and home values in a quantitative way. Fifteen studies from North America, Europe, and Asia met the requirements for inclusion (Anthamatten et al., 2022; Wen et al., 2019; Li et al., 2019; Mansur & Yusuf, 2022)."
http://arxiv.org/abs/2507.19603v1,Uniform Critical Values for Likelihood Ratio Tests in Boundary Problems,2025-07-25 18:18:56+00:00,"['Giuseppe Cavaliere', 'Adam McCloskey', 'Rasmus S. Pedersen', 'Anders Rahbek']",econ.EM,"Limit distributions of likelihood ratio statistics are well-known to be discontinuous in the presence of nuisance parameters at the boundary of the parameter space, which lead to size distortions when standard critical values are used for testing. In this paper, we propose a new and simple way of constructing critical values that yields uniformly correct asymptotic size, regardless of whether nuisance parameters are at, near or far from the boundary of the parameter space. Importantly, the proposed critical values are trivial to compute and at the same time provide powerful tests in most settings. In comparison to existing size-correction methods, the new approach exploits the monotonicity of the two components of the limiting distribution of the likelihood ratio statistic, in conjunction with rectangular confidence sets for the nuisance parameters, to gain computational tractability. Uniform validity is established for likelihood ratio tests based on the new critical values, and we provide illustrations of their construction in two key examples: (i) testing a coefficient of interest in the classical linear regression model with non-negativity constraints on control coefficients, and, (ii) testing for the presence of exogenous variables in autoregressive conditional heteroskedastic models (ARCH) with exogenous regressors. Simulations confirm that the tests have desirable size and power properties. A brief empirical illustration demonstrates the usefulness of our proposed test in relation to testing for spill-overs and ARCH effects."
http://arxiv.org/abs/2507.16462v1,Binary Response Forecasting under a Factor-Augmented Framework,2025-07-22 11:05:19+00:00,"['Tingting Cheng', 'Jiachen Cong', 'Fei Liu', 'Xuanbin Yang']",econ.EM,"In this paper, we propose a novel factor-augmented forecasting regression model with a binary response variable. We develop a maximum likelihood estimation method for the regression parameters and establish the asymptotic properties of the resulting estimators. Monte Carlo simulation results show that the proposed estimation method performs very well in finite samples. Finally, we demonstrate the usefulness of the proposed model through an application to U.S. recession forecasting. The proposed model consistently outperforms conventional Probit regression across both in-sample and out-of-sample exercises, by effectively utilizing high-dimensional information through latent factors."
http://arxiv.org/abs/2507.15735v1,The Root of Revenue Continuity,2025-07-21 15:42:41+00:00,"['Sergiu Hart', 'Noam Nisan']",cs.GT,"In the setup of selling one or more goods, various papers have shown, in various forms and for various purposes, that a small change in the distribution of a buyer's valuations may cause only a small change in the possible revenue that can be extracted. We prove a simple, clean, convenient, and general statement to this effect: let X and Y be random valuations on k additive goods, and let W(X,Y) be the Wasserstein (or ""earth mover's"") distance between them; then sqrt(Rev(X))-sqrt(Rev(Y)) <= sqrt(W(X,Y)). This further implies that a simple explicit modification of any optimal mechanism for X, namely, ""uniform discounting"", is guaranteed to be almost optimal for any Y that is close to X in the Wasserstein distance."
http://arxiv.org/abs/2507.08764v1,Propensity score with factor loadings: the effect of the Paris Agreement,2025-07-11 17:20:29+00:00,"['Angelo Forino', 'Andrea Mercatanti', 'Giacomo Morelli']",econ.EM,"Factor models for longitudinal data, where policy adoption is unconfounded with respect to a low-dimensional set of latent factor loadings, have become increasingly popular for causal inference. Most existing approaches, however, rely on a causal finite-sample approach or computationally intensive methods, limiting their applicability and external validity. In this paper, we propose a novel causal inference method for panel data based on inverse propensity score weighting where the propensity score is a function of latent factor loadings within a framework of causal inference from super-population. The approach relaxes the traditional restrictive assumptions of causal panel methods, while offering advantages in terms of causal interpretability, policy relevance, and computational efficiency. Under standard assumptions, we outline a three-step estimation procedure for the ATT and derive its large-sample properties using Mestimation theory. We apply the method to assess the causal effect of the Paris Agreement, a policy aimed at fostering the transition to a low-carbon economy, on European stock returns. Our empirical results suggest a statistically significant and negative short-run effect on the stock returns of firms that issued green bonds."
http://arxiv.org/abs/2507.12281v1,International promotion patterns in the smart city literature: Exploring the role of geography in affecting local drivers and smart cities' outcomes,2025-07-16 14:29:45+00:00,"['Filippo Marchesani', 'Francesca Masciarelli', 'Andrea Bikfalvi']",econ.GN,"The rise of smart cities represents a significant trend in urban development. However, only in recent years has attention shifted toward the international promotion of these cities. Despite ongoing academic discussions on the impact of smart city development on urban environments, the global recognition of smart cities remains uncertain due to their multidisciplinary nature. To address this, we conducted a systematic literature review of articles published in top-tier peer-reviewed journals from 2008 to December 2021, offering a comprehensive analysis of the existing literature."
http://arxiv.org/abs/2507.18494v1,Partitioned Wild Bootstrap for Panel Data Quantile Regression,2025-07-24 15:08:26+00:00,"['Antonio F. Galvao', 'Carlos Lamarche', 'Thomas Parker']",econ.EM,"Practical inference procedures for quantile regression models of panel data have been a pervasive concern in empirical work, and can be especially challenging when the panel is observed over many time periods and temporal dependence needs to be taken into account. In this paper, we propose a new bootstrap method that applies random weighting to a partition of the data -- partition-invariant weights are used in the bootstrap data generating process -- to conduct statistical inference for conditional quantiles in panel data that have significant time-series dependence. We demonstrate that the procedure is asymptotically valid for approximating the distribution of the fixed effects quantile regression estimator. The bootstrap procedure offers a viable alternative to existing resampling methods. Simulation studies show numerical evidence that the novel approach has accurate small sample behavior, and an empirical application illustrates its use."
http://arxiv.org/abs/2507.18747v1,Financial Regulation and AI: A Faustian Bargain?,2025-07-24 18:57:04+00:00,"['Christopher Clayton', 'Antonio Coppola']",econ.GN,"We examine whether and how granular, real-time predictive models should be integrated into central banks' macroprudential toolkit. First, we develop a tractable framework that formalizes the tradeoff regulators face when choosing between implementing models that forecast systemic risk accurately but have uncertain causal content and models with the opposite profile. We derive the regulator's optimal policy in a setting in which private portfolios react endogenously to the regulator's model choice and policy rule. We show that even purely predictive models can generate welfare gains for a regulator, and that predictive precision and knowledge of causal impacts of policy interventions are complementary. Second, we introduce a deep learning architecture tailored to financial holdings data--a graph transformer--and we discuss why it is optimally suited to this problem. The model learns vector embedding representations for both assets and investors by explicitly modeling the relational structure of holdings, and it attains state-of-the-art predictive accuracy in out-of-sample forecasting tasks including trade prediction."
http://arxiv.org/abs/2507.17415v1,Green Transition with Dynamic Social Preferences,2025-07-23 11:16:07+00:00,"['Kirill Borissov', 'Nigar Hashimzade']",econ.TH,"We examine a green transition policy involving a tax on brown goods in an economy where preferences for green consumption consist of a constant intrinsic individual component and an evolving social component. We analyse equilibrium dynamics when social preferences exert a positive externality in green consumption, creating complementarity between policy and preferences. The results show that accounting for this externality allows for a lower tax rate compared to policy ignoring the social norm effects. Furthermore, stability conditions permit gradual tax reductions or even removal along the transition path, minimising welfare losses. Thus, incorporating policy-preference interactions improves green transition policy design."
http://arxiv.org/abs/2507.18092v1,Rethinking Indonesia's Public Debt in the Era of Negative Interest Rate-Growth Differentials,2025-07-24 05:00:59+00:00,['Mervin Goklas Hamonangan'],econ.GN,"This study contributes to the discussion about how higher public debt may not be costly because of the negative interest rate-growth differentials by simulating OLG models introduced by Blanchard (2019) under uncertainty, showing debt and welfare dynamics in two scenarios: intergenerational transfers and debt rollovers in the case of Indonesia. The simulation is done by modifying the model parameters based on interest rate-growth differentials historic data from 2004-2019. It is found that the fiscal consensus does not hold when implementing Blanchard (2019) analysis with Indonesian-based rate parameters. Increasing public debt makes the economy more volatile and high risk. Modifying other factors supports the initial finding, with lower initial endowment diminishing the benefits of public debt and higher capital share under Cobb-Douglas. When the threat of debt explosion appears, efforts to reduce debt share will reduce the welfare of the society. The policy implication is to be careful of the opportunity. Increasing public debt may not be the way to go, avoiding possible dire consequences."
http://arxiv.org/abs/2508.10302v1,Two-Way Mean Group Estimators for Heterogeneous Panel Models with Fixed T,2025-08-14 03:15:02+00:00,"['Xun Lu', 'Liangjun Su']",econ.EM,"We consider a correlated random coefficient panel data model with two-way fixed effects and interactive fixed effects in a fixed T framework. We propose a two-way mean group (TW-MG) estimator for the expected value of the slope coefficient and propose a leave-one-out jackknife method for valid inference. We also consider a pooled estimator and provide a Hausman-type test for poolability. Simulations demonstrate the excellent performance of our estimators and inference methods in finite samples. We apply our new methods to two datasets to examine the relationship between health-care expenditure and income, and estimate a production function."
http://arxiv.org/abs/2508.11033v2,Note on Selection Bias in Observational Estimates of Algorithmic Progress,2025-08-14 19:38:10+00:00,['Parker Whitfill'],econ.GN,"Ho et. al (2024) attempts to estimate the degree of algorithmic progress from language models. They collect observational data on language models' loss and compute over time, and argue that as time has passed, language models' algorithmic efficiency has been rising. That is, the loss achieved for fixed compute has been dropping over time. In this note, I raise one potential methodological problem with the estimation strategy. Intuitively, if part of algorithmic quality is latent, and compute choices are endogenous to algorithmic quality, then resulting estimates of algorithmic quality will be contaminated by selection bias."
http://arxiv.org/abs/2508.10724v2,Two-Instrument Screening under Soft Budget Constraints,2025-08-14 15:07:10+00:00,['Xinli Guo'],econ.TH,"We study soft budget constraints in multi-tier public finance when an upper-tier government uses two instruments: an ex-ante grant schedule and an ex-post rescue. Under convex rescue costs and standard primitives, the three-stage leader-follower problem collapses to one dimensional screening with a single allocation index: the cap on realized rescue. A hazard-based characterization delivers a unified rule that nests (i) no rescue, (ii) a threshold-cap with commitment, and (iii) a threshold--linear--cap without commitment. The knife-edge for eliminating bailouts compares the marginal cost at the origin to the supremum of a virtual weight, and the comparative statics show how greater curvature tightens caps while discretion shifts transfers toward front loading by lowering the effective grant weight. The framework provides a portable benchmark for mechanism design and yields testable implications for policy and empirical work on intergovernmental finance."
http://arxiv.org/abs/2508.17151v1,Integrative Experiments Identify How Punishment Impacts Welfare in Public Goods Games,2025-08-23 22:07:27+00:00,"['Mohammed Alsobay', 'David G. Rand', 'Duncan J. Watts', 'Abdullah Almaatouq']",econ.GN,"Punishment as a mechanism for promoting cooperation has been studied extensively for more than two decades, but its effectiveness remains a matter of dispute. Here, we examine how punishment's impact varies across cooperative settings through a large-scale integrative experiment. We vary 14 parameters that characterize public goods games, sampling 360 experimental conditions and collecting 147,618 decisions from 7,100 participants. Our results reveal striking heterogeneity in punishment effectiveness: while punishment consistently increases contributions, its impact on payoffs (i.e., efficiency) ranges from dramatically enhancing welfare (up to 43% improvement) to severely undermining it (up to 44% reduction) depending on the cooperative context. To characterize these patterns, we developed models that outperformed human forecasters (laypeople and domain experts) in predicting punishment outcomes in new experiments. Communication emerged as the most predictive feature, followed by contribution framing (opt-out vs. opt-in), contribution type (variable vs. all-or-nothing), game length (number of rounds), peer outcome visibility (whether participants can see others' earnings), and the availability of a reward mechanism. Interestingly, however, most of these features interact to influence punishment effectiveness rather than operating independently. For example, the extent to which longer games increase the effectiveness of punishment depends on whether groups can communicate. Together, our results refocus the debate over punishment from whether or not it ""works"" to the specific conditions under which it does and does not work. More broadly, our study demonstrates how integrative experiments can be combined with machine learning to uncover generalizable patterns, potentially involving interactions between multiple features, and help generate novel explanations in complex social phenomena."
http://arxiv.org/abs/2508.07384v2,Conceptual winsorizing: An application to the social cost of carbon,2025-08-10 15:18:22+00:00,['Richard S. J. Tol'],econ.EM,"There are many published estimates of the social cost of carbon. Some are clear outliers, the result of poorly constrained models. Percentile winsorizing is an option, but I here propose conceptual winsorizing: The social cost of carbon is either a willingness to pay, which cannot exceed the ability to pay, or a proposed carbon tax, which cannot raise more revenue than all other taxes combined. Conceptual winsorizing successfully removes high outliers. It slackens as economies decarbonize, slowly without climate policy, faster with."
http://arxiv.org/abs/2508.09046v1,Real Preferences Under Arbitrary Norms,2025-08-12 16:09:02+00:00,"['Joshua Zeitlin', 'Corinna Coupette']",econ.TH,"Whether the goal is to analyze voting behavior, locate facilities, or recommend products, the problem of translating between (ordinal) rankings and (numerical) utilities arises naturally in many contexts. This task is commonly approached by representing both the individuals doing the ranking (voters) and the items to be ranked (alternatives) in a shared metric space, where ordinal preferences are translated into relationships between pairwise distances. Prior work has established that any collection of rankings with $n$ voters and $m$ alternatives (preference profile) can be embedded into $d$-dimensional Euclidean space for $d \geq \min\{n,m-1\}$ under the Euclidean norm and the Manhattan norm. We show that this holds for all $p$-norms and establish that any pair of rankings can be embedded into $R^2$ under arbitrary norms, significantly expanding the reach of spatial preference models."
http://arxiv.org/abs/2508.09040v1,Bias correction for Chatterjee's graph-based correlation coefficient,2025-08-12 16:01:44+00:00,"['Mona Azadkia', 'Leihao Chen', 'Fang Han']",stat.ME,"Azadkia and Chatterjee (2021) recently introduced a simple nearest neighbor (NN) graph-based correlation coefficient that consistently detects both independence and functional dependence. Specifically, it approximates a measure of dependence that equals 0 if and only if the variables are independent, and 1 if and only if they are functionally dependent. However, this NN estimator includes a bias term that may vanish at a rate slower than root-$n$, preventing root-$n$ consistency in general. In this article, we propose a bias correction approach that overcomes this limitation, yielding an NN-based estimator that is both root-$n$ consistent and asymptotically normal."
http://arxiv.org/abs/2508.12716v1,"Bivariate Distribution Regression; Theory, Estimation and an Application to Intergenerational Mobility",2025-08-18 08:27:12+00:00,"['Victor Chernozhukov', 'Iván Fernández-Val', 'Jonas Meier', 'Aico van Vuuren', 'Francis Vella']",econ.EM,"We employ distribution regression (DR) to estimate the joint distribution of two outcome variables conditional on chosen covariates. While Bivariate Distribution Regression (BDR) is useful in a variety of settings, it is particularly valuable when some dependence between the outcomes persists after accounting for the impact of the covariates. Our analysis relies on a result from Chernozhukov et al. (2018) which shows that any conditional joint distribution has a local Gaussian representation. We describe how BDR can be implemented and present some associated functionals of interest. As modeling the unexplained dependence is a key feature of BDR, we focus on functionals related to this dependence. We decompose the difference between the joint distributions for different groups into composition, marginal and sorting effects. We provide a similar decomposition for the transition matrices which describe how location in the distribution in one of the outcomes is associated with location in the other. Our theoretical contributions are the derivation of the properties of these estimated functionals and appropriate procedures for inference. Our empirical illustration focuses on intergenerational mobility. Using the Panel Survey of Income Dynamics data, we model the joint distribution of parents' and children's earnings. By comparing the observed distribution with constructed counterfactuals, we isolate the impact of observable and unobservable factors on the observed joint distribution. We also evaluate the forces responsible for the difference between the transition matrices of sons' and daughters'."
http://arxiv.org/abs/2508.14132v1,Macroeconomic Foundation of Monetary Accounting by Diagrams of Categorical Universals,2025-08-19 10:54:50+00:00,"['Renée Menéndez', 'Viktor Winschel']",econ.GN,"We present a category theoretical formulation of the Monetary Macroeconomic Accounting Theory (MoMaT) of Menéndez and Winschel [2025]. We take macroeconomic (national) accounting systems to be composed from microeconomic double-entry systems with real and monetary units of accounts. Category theory is the compositional grammar and module system of mathematics which we use to lift micro accounting consistency to the macro level. The main function of money in MoMaT is for the repayment of loans and not for the exchange of goods, bridging the desynchronisation of input and output payments of producers. Accordingly, temporal accounting consistency is at the macroeconomic level. We show that the accounting for macroeconomies organised by a division of labor can be consistent and stable as a prerequisite for risk and GDP sharing of societies. We exemplify the theory by five sectoral agents of Labor and Resource owners, a Company as the productive sector, a Capitalist for profits, and a Bank as the financial sector providing loans to synchronise the micro and the macro levels of an economy. The dynamics is described by eight sectoral macroeconomic bookings in each period demonstrating stable convergence of the MoMaT in numerical simulations. The categorical program implements a consistent evolution of hierarchical loan repayment contracts by an endofunctor. The universal constructions of a limit verify all constraints as the sectoral investment and learning function at the macroeconomic level. The dual colimit computes the aggregated informations at the macro level as usual in the mathematics of transitions from local to global structures. We use visual diagrams to make complex economic relationships intuitive. This paper is meant to map economic to categorical concepts to enable interdisciplinary collaboration for digital twins of monetary accounting systems."
http://arxiv.org/abs/2507.22748v2,How Exposed Are UK Jobs to Generative AI? Developing and Applying a Novel Task-Based Index,2025-07-30 15:05:05+00:00,"['Golo Henseke', 'Rhys Davies', 'Alan Felstead', 'Duncan Gallie', 'Francis Green', 'Ying Zhou']",econ.GN,"We draw on Eloundou et al. (2024) to develop the Generative AI Susceptibility Index (GAISI), a task-based measure of UK job exposure to large language models (LLMs), such as ChatGPT. GAISI is derived from probabilistic task ratings by LLMs and linked to worker-reported task data from the Skills and Employment Surveys. It reflects the share of job activities where an LLM or LLM-powered system can reduce task completion time by at least 25% beyond existing productivity tools. The index demonstrates high reliability, strong alignment with AI capabilities, and superior predictive power compared to existing exposure measures. By 2023-24, nearly all UK jobs exhibited some exposure, yet only a minority were heavily affected. Aggregate exposure has risen since 2017, primarily due to occupational shifts rather than changes in task profiles. The price premium for AI-exposed tasks declined relative to 2017, measuring approximately 12% lower in 2023-24. Job postings fell following the release of ChatGPT, with job postings 5.5% lower in 2025-Q2 than if pre-GPT hiring patterns had persisted. GAISI offers a robust framework for assessing AI's impact on work, providing early evidence that displacement effects may already outweigh productivity gains."
http://arxiv.org/abs/2508.11619v1,Approximate Factor Model with S-vine Copula Structure,2025-08-15 17:38:22+00:00,"['Jialing Han', 'Yu-Ning Li']",stat.ME,"We propose a novel framework for approximate factor models that integrates an S-vine copula structure to capture complex dependencies among common factors. Our estimation procedure proceeds in two steps: first, we apply principal component analysis (PCA) to extract the factors; second, we employ maximum likelihood estimation that combines kernel density estimation for the margins with an S-vine copula to model the dependence structure. Jointly fitting the S-vine copula with the margins yields an oblique factor rotation without resorting to ad hoc restrictions or traditional projection pursuit methods. Our theoretical contributions include establishing the consistency of the rotation and copula parameter estimators, developing asymptotic theory for the factor-projected empirical process under dependent data, and proving the uniform consistency of the projected entropy estimators. Simulation studies demonstrate convergence with respect to both the dimensionality and the sample size. We further assess model performance through Value-at-Risk (VaR) estimation via Monte Carlo methods and apply our methodology to the daily returns of S&P 500 Index constituents to forecast the VaR of S&P 500 index."
http://arxiv.org/abs/2508.11395v1,Banking 2.0: The Stablecoin Banking Revolution -- How Digital Assets Are Reshaping Global Finance,2025-08-15 11:05:16+00:00,"['Kevin McNamara', 'Rhea Pritham Marpu']",cs.ET,"The global financial system stands at an inflection point. Stablecoins represent the most significant evolution in banking since the abandonment of the gold standard, positioned to enable ""Banking 2.0"" by seamlessly integrating cryptocurrency innovation with traditional finance infrastructure. This transformation rivals artificial intelligence as the next major disruptor in the financial sector. Modern fiat currencies derive value entirely from institutional trust rather than physical backing, creating vulnerabilities that stablecoins address through enhanced stability, reduced fraud risk, and unified global transactions that transcend national boundaries. Recent developments demonstrate accelerating institutional adoption: landmark U.S. legislation including the GENIUS Act of 2025, strategic industry pivots from major players like JPMorgan's crypto-backed loan initiatives, and PayPal's comprehensive ""Pay with Crypto"" service. Widespread stablecoin implementation addresses critical macroeconomic imbalances, particularly the inflation-productivity gap plaguing modern monetary systems, through more robust and diversified backing mechanisms. Furthermore, stablecoins facilitate deregulation and efficiency gains, paving the way for a more interconnected international financial system. This whitepaper comprehensively explores how stablecoins are poised to reshape banking, supported by real-world examples, current market data, and analysis of their transformative potential."
http://arxiv.org/abs/2508.11151v1,The core in the housing market model with fractional endowments,2025-08-15 01:49:14+00:00,"['Jingsheng Yu', 'Jun Zhang']",econ.TH,"We explore the core concept in a generalization of the housing market model where agents own fractional endowments while maintaining ordinal preferences. Recognizing that individuals are easier than coalitions to block an allocation, we adopt a definition in which individuals block an allocation if their received assignments do not first-order stochastically dominate their endowment, while a non-singleton coalition blocks an allocation if they can reallocate their endowments to obtain new assignments that first-order stochastically dominate their original assignments. Our findings show that, unlike the original model, the strong core may be empty, while the weak core is nonempty. The weak core always contains elements that satisfy equal treatment of equals, but it may not contain elements satisfying equal-endowment no envy."
http://arxiv.org/abs/2508.08152v1,Optimal Fees for Liquidity Provision in Automated Market Makers,2025-08-11 16:30:02+00:00,"['Steven Campbell', 'Philippe Bergault', 'Jason Milionis', 'Marcel Nutz']",q-fin.TR,"Passive liquidity providers (LPs) in automated market makers (AMMs) face losses due to adverse selection (LVR), which static trading fees often fail to offset in practice. We study the key determinants of LP profitability in a dynamic reduced-form model where an AMM operates in parallel with a centralized exchange (CEX), traders route their orders optimally to the venue offering the better price, and arbitrageurs exploit price discrepancies. Using large-scale simulations and real market data, we analyze how LP profits vary with market conditions such as volatility and trading volume, and characterize the optimal AMM fee as a function of these conditions. We highlight the mechanisms driving these relationships through extensive comparative statics, and confirm the model's relevance through market data calibration. A key trade-off emerges: fees must be low enough to attract volume, yet high enough to earn sufficient revenues and mitigate arbitrage losses. We find that under normal market conditions, the optimal AMM fee is competitive with the trading cost on the CEX and remarkably stable, whereas in periods of very high volatility, a high fee protects passive LPs from severe losses. These findings suggest that a threshold-type dynamic fee schedule is both robust enough to market conditions and improves LP outcomes."
http://arxiv.org/abs/2508.09615v1,The asymmetrical Acquisition of information about the range of asset value in market,2025-08-13 08:46:54+00:00,"['Jianhao Su', 'Yanliang Zhang']",econ.TH,"The information investors acquire in asset markets has various forms. We refer to range information as information about the upper and lower bound which the payoff of an asset may reach in the future. This paper explores the market impacts of investors' asymmetrical acquisition of range information. Uninformed traders are inherently unable to directly obtain the private signal held by informed traders. This study shows that when range information is released to investors asymmetrically, uninformed traders who can only obtain rougher range information will not trade assets under the max-min ambiguity aversion criterion. Investors' asymmetrical acquisition of range information can cause that market liquidity and the sensitivity of market price to private signal vary continuously with the signal and noise trading volume. We also reveal that investors' asymmetrical acquisition of range information can increase market liquidity and the sensitivity of price under some conditions and decrease them under some other conditions."
http://arxiv.org/abs/2508.16195v1,Strategyproof Randomized Social Choice for Restricted Sets of Utility Functions,2025-08-22 08:15:11+00:00,['Patrick Lederer'],cs.GT,"Social decision schemes (SDSs) map the voters' preferences over multiple alternatives to a probability distribution over these alternatives. In a seminal result, Gibbard (1977) has characterized the set of SDSs that are strategyproof with respect to all utility functions and his result implies that all such SDSs are either unfair to the voters or alternatives, or they require a significant amount of randomization. To circumvent this negative result, we propose the notion of $U$-strategyproofness which postulates that only voters with a utility function in a predefined set $U$ cannot manipulate. We then analyze the tradeoff between $U$-strategyproofness and various decisiveness notions that restrict the amount of randomization of SDSs. In particular, we show that if the utility functions in the set $U$ value the best alternative much more than other alternatives, there are $U$-strategyproof SDSs that choose an alternative with probability $1$ whenever all but $k$ voters rank it first. On the negative side, we demonstrate that $U$-strategyproofness is incompatible with Condorcet-consistency if the set $U$ satisfies minimal symmetry conditions. Finally, we show that no ex post efficient and $U$-strategyproof SDS can be significantly more decisive than the uniform random dictatorship if the voters are close to indifferent between their two favorite alternatives."
http://arxiv.org/abs/2508.15675v1,Large-dimensional Factor Analysis with Weighted PCA,2025-08-21 15:55:37+00:00,"['Zhongyuan Lyu', 'Ming Yuan']",stat.ME,"Principal component analysis (PCA) is arguably the most widely used approach for large-dimensional factor analysis. While it is effective when the factors are sufficiently strong, it can be inconsistent when the factors are weak and/or the noise has complex dependence structure. We argue that the inconsistency often stems from bias and introduce a general approach to restore consistency. Specifically, we propose a general weighting scheme for PCA and show that with a suitable choice of weighting matrices, it is possible to deduce consistent and asymptotic normal estimators under much weaker conditions than the usual PCA. While the optimal weight matrix may require knowledge about the factors and covariance of the idiosyncratic noise that are not known a priori, we develop an agnostic approach to adaptively choose from a large class of weighting matrices that can be viewed as PCA for weighted linear combinations of auto-covariances among the observations. Theoretical and numerical results demonstrate the merits of our methodology over the usual PCA and other recently developed techniques for large-dimensional approximate factor models."
http://arxiv.org/abs/2508.20249v1,Exponential Discounting under Partial Efficiency,2025-08-27 20:13:35+00:00,['Charles Gauthier'],econ.TH,This paper derives a novel representation of the exponential discounting model that allows one to assess departures from the model via a measure of efficiency. The approach uses a revealed preference methodology that does not make any parametric assumption on the utility function and allows for unrestricted heterogeneity. The method is illustrated using longitudinal data from checkout scanners and gives insights into sources of departure from exponential discounting.
http://arxiv.org/abs/2509.01063v1,An Economy of AI Agents,2025-09-01 02:07:39+00:00,"['Gillian K. Hadfield', 'Andrew Koh']",econ.GN,"In the coming decade, artificially intelligent agents with the ability to plan and execute complex tasks over long time horizons with little direct oversight from humans may be deployed across the economy. This chapter surveys recent developments and highlights open questions for economists around how AI agents might interact with humans and with each other, shape markets and organizations, and what institutions might be required for well-functioning markets."
http://arxiv.org/abs/2509.01499v1,When Do Consumers Lose from Variable Electricity Pricing?,2025-09-01 14:20:27+00:00,"['Nathan Engelman Lado', 'Richard Chen', 'Saurabh Amin']",econ.GN,"Time-varying electricity pricing better reflects the varying cost of electricity compared to flat-rate pricing. Variations between peak and off-peak costs are increasing due to weather variation, renewable intermittency, and increasing electrification of demand. Empirical and theoretical studies suggest that variable pricing can lower electricity supply costs and reduce grid stress. However, the distributional impacts, particularly on low-income consumers, remain understudied. This paper develops a theoretical framework to analyze how consume heterogeneity affects welfare outcomes when electricity markets transition from flat-rate to time-varying pricing, considering realistic assumptions about heterogeneous consumer demand, supply costs, and utility losses from unmet consumption.
  We derive sufficient conditions for identifying when consumers lose utility from pricing reforms and compare welfare effects across consumer types. Our findings reveal that consumer vulnerability depends on the interaction of consumption timing, demand flexibility capabilities, and price sensitivity levels. Consumers with high peak-period consumption and inflexible demand, characteristics often associated with low-income households, are most vulnerable to welfare losses. Critically, we demonstrate that demand flexibility provides welfare protection only when coincident with large price changes. Our equilibrium analysis reveals that aggregate flexibility patterns generate spillover effects through pricing mechanisms, with peak periods experiencing greater price changes when they have less aggregate flexibility, potentially concentrating larger price increases among vulnerable populations that have a limited ability to respond. These findings suggest that variable pricing policies should be accompanied by targeted policies ensuring equitable access to demand response capabilities and pricing benefits."
http://arxiv.org/abs/2509.01478v1,Handling Sparse Non-negative Data in Finance,2025-09-01 13:46:27+00:00,"['Agostino Capponi', 'Zhaonan Qu']",econ.EM,"We show that Poisson regression, though often recommended over log-linear regression for modeling count and other non-negative variables in finance and economics, can be far from optimal when heteroskedasticity and sparsity -- two common features of such data -- are both present. We propose a general class of moment estimators, encompassing Poisson regression, that balances the bias-variance trade-off under these conditions. A simple cross-validation procedure selects the optimal estimator. Numerical simulations and applications to corporate finance data reveal that the best choice varies substantially across settings and often departs from Poisson regression, underscoring the need for a more flexible estimation framework."
http://arxiv.org/abs/2509.02513v1,Bayesian Polarization,2025-09-02 17:06:29+00:00,['Tuval Danenberg'],econ.TH,"We study belief polarization among Bayesian agents observing public information about a multidimensional state. Baliga et al. (2013) show that divergence in the sense of first-order stochastic dominance is impossible for one dimensional beliefs, but we find that in multidimensional settings it can occur for all marginal beliefs, even with infinitely many signals. At the same time, we extend their impossibility result: divergence in the sense of multidimensional stochastic dominance is impossible. For an intermediate stochastic order, polarization may arise only in the short-run. We provide necessary and sufficient conditions on signal structures for persistent polarization and discuss implications for polarization in actions."
http://arxiv.org/abs/2507.10148v1,A Folk Theorem for Indefinitely Repeated Network Games,2025-07-14 10:50:40+00:00,['Andrea Benso'],econ.TH,"We consider a repeated game in which players, considered as nodes of a network, are connected. Each player observes her neighbors' moves only. Thus, monitoring is private and imperfect. Players can communicate with their neighbors at each stage; each player, for any subset of her neighbors, sends the same message to any player of that subset. Thus, communication is local and both public and private. Both communication and monitoring structures are given by the network. The solution concept is perfect Bayesian equilibrium. In this paper we show that a folk theorem holds if and only if the network is 2-connected for any number of players."
http://arxiv.org/abs/2507.10052v1,Analyzing the Crowding-Out Effect of Investment Herding on Consumption: An Optimal Control Theory Approach,2025-07-14 08:33:20+00:00,"['Huisheng Wang', 'H. Vicky Zhao']",q-fin.PM,"Investment herding, a phenomenon where households mimic the decisions of others rather than relying on their own analysis, has significant effects on financial markets and household behavior. Excessive investment herding may reduce investments and lead to a depletion of household consumption, which is called the crowding-out effect. While existing research has qualitatively examined the impact of investment herding on consumption, quantitative studies in this area remain limited. In this work, we investigate the optimal investment and consumption decisions of households under the impact of investment herding. We formulate an optimization problem to model how investment herding influences household decisions over time. Based on the optimal control theory, we solve for the analytical solutions of optimal investment and consumption decisions. We theoretically analyze the impact of investment herding on household consumption decisions and demonstrate the existence of the crowding-out effect. We further explore how parameters, such as interest rate, excess return rate, and volatility, influence the crowding-out effect. Finally, we conduct a real data test to validate our theoretical analysis of the crowding-out effect. This study is crucial to understanding the impact of investment herding on household consumption and offering valuable insights for policymakers seeking to stimulate consumption and mitigate the negative effects of investment herding on economic growth."
http://arxiv.org/abs/2507.07477v1,Electricity Market Predictability: Virtues of Machine Learning and Links to the Macroeconomy,2025-07-10 07:05:03+00:00,"['Jinbo Cai', 'Wenze Li', 'Wenjie Wang']",econ.GN,"With stakeholder-level in-market data, we conduct a comparative analysis of machine learning (ML) for forecasting electricity prices in Singapore, spanning 15 individual models and 4 ensemble approaches. Our empirical findings justify the three virtues of ML models: (1) the virtue of capturing non-linearity, (2) the complexity (Kelly et al., 2024) and (3) the l2-norm and bagging techniques in a weak factor environment (Shen and Xiu, 2024). Simulation also supports the first virtue. Penalizing prediction correlation improves ensemble performance when individual models are highly correlated. The predictability can be translated into sizable economic gains under the mean-variance framework. We also reveal significant patterns of time-series heterogeneous predictability across macro regimes: predictability is clustered in expansion, volatile market and extreme geopolitical risk periods. Our feature importance results agree with the complex dynamics of Singapore's electricity market after de regulation, yet highlight its relatively supply-driven nature with the continued presence of strong regulatory influences."
http://arxiv.org/abs/2507.07300v1,No Midcost Democracy,2025-07-09 21:50:11+00:00,"['Hans Gersbach', 'Arthur Schichl', 'Oriol Tejada']",econ.TH,"Which level of voting costs is optimal in a democracy? This paper argues that intermediate voting costs - what we term a ""Midcost democracy"" - should be avoided, as they fail to ensure that electoral outcomes reflect the preferences of the majority. We study a standard binary majority decision in which a majority of the electorate prefers alternative A over alternative B. The population consists of partisan voters, who always participate, and non-partisan voters, who vote only when they believe their participation could be pivotal, given that voting entails a cost. We show that the probability of the majority-preferred alternative A winning is non-monotonic in the level of voting costs. Specifically, when voting costs are either high or negligible, alternative A wins in all equilibria. However, at intermediate cost levels, this alignment breaks down. These findings suggest that democratic systems should avoid institutional arrangements that lead to moderate voting costs, as they may undermine the majority principle."
http://arxiv.org/abs/2507.07286v1,Identifying Present-Biased Discount Functions in Dynamic Discrete Choice Models,2025-07-09 21:10:25+00:00,"['Jaap H. Abbring', 'Øystein Daljord', 'Fedor Iskhakov']",econ.EM,"We study the identification of dynamic discrete choice models with sophisticated, quasi-hyperbolic time preferences under exclusion restrictions. We consider both standard finite horizon problems and empirically useful infinite horizon ones, which we prove to always have solutions. We reduce identification to finding the present-bias and standard discount factors that solve a system of polynomial equations with coefficients determined by the data and use this to bound the cardinality of the identified set. The discount factors are usually identified, but hard to precisely estimate, because exclusion restrictions do not capture the defining feature of present bias, preference reversals, well."
http://arxiv.org/abs/2507.04668v1,Forward Variable Selection in Ultra-High Dimensional Linear Regression Using Gram-Schmidt Orthogonalization,2025-07-07 05:15:48+00:00,"['Jialuo Chen', 'Zhaoxing Gao', 'Ruey S. Tsay']",stat.ME,"We investigate forward variable selection for ultra-high dimensional linear regression using a Gram-Schmidt orthogonalization procedure. Unlike the commonly used Forward Regression (FR) method, which computes regression residuals using an increasing number of selected features, or the Orthogonal Greedy Algorithm (OGA), which selects variables based on their marginal correlations with the residuals, our proposed Gram-Schmidt Forward Regression (GSFR) simplifies the selection process by evaluating marginal correlations between the residuals and the orthogonalized new variables. Moreover, we introduce a new model size selection criterion that determines the number of selected variables by detecting the most significant change in their unique contributions, effectively filtering out redundant predictors along the selection path. While GSFR is theoretically equivalent to FR except for the stopping rule, our refinement and the newly proposed stopping rule significantly improve computational efficiency. In ultra-high dimensional settings, where the dimensionality far exceeds the sample size and predictors exhibit strong correlations, we establish that GSFR achieves a convergence rate comparable to OGA and ensures variable selection consistency under mild conditions. We demonstrate the proposed method {using} simulations and real data examples. Extensive numerical studies show that GSFR outperforms commonly used methods in ultra-high dimensional variable selection."
http://arxiv.org/abs/2507.04663v1,A General Class of Model-Free Dense Precision Matrix Estimators,2025-07-07 05:07:17+00:00,['Mehmet Caner Agostino Capponi Mihailo Stojnic'],econ.EM,"We introduce prototype consistent model-free, dense precision matrix estimators that have broad application in economics. Using quadratic form concentration inequalities and novel algebraic characterizations of confounding dimension reductions, we are able to: (i) obtain non-asymptotic bounds for precision matrix estimation errors and also (ii) consistency in high dimensions; (iii) uncover the existence of an intrinsic signal-to-noise -- underlying dimensions tradeoff; and (iv) avoid exact population sparsity assumptions. In addition to its desirable theoretical properties, a thorough empirical study of the S&P 500 index shows that a tuning parameter-free special case of our general estimator exhibits a doubly ascending Sharpe Ratio pattern, thereby establishing a link with the famous double descent phenomenon dominantly present in recent statistical and machine learning literature."
http://arxiv.org/abs/2507.05782v1,Branding through responsibility: the advertising impact of CSR activities in the Korean instant noodles market,2025-07-08 08:41:02+00:00,"['Youngjin Hong', 'In Kyung Kim', 'Kyoo il Kim']",econ.GN,"This paper empirically examines the extent to which a favorable view of a firm, shaped by its social contributions, influences consumer choices and firm sales. Using a favorability rating that reflects media exposure of each firm's corporate social responsibility (CSR) activities in the Korean instant noodles market during the 2010s, we find evidence that improvements in the corporate image of Ottogi - one of the country's largest instant noodle producers - positively affected consumer utility for the firm's products. Notably, Ottogi's annual sales of its major brands increased by an average of 23.7 million packages, or 6.7%, as a result of CSR activities and the associated rise in consumer favorability. This effect is comparable in magnitude to that of a nearly 60% increase in advertising spending. Our findings suggest that CSR can foster firm growth by boosting product sales."
http://arxiv.org/abs/2510.09590v2,Ranking Policies Under Loss Aversion and Inequality Aversion,2025-10-10 17:47:25+00:00,"['Martyna Kobus', 'Radosław Kurek', 'Thomas Parker']",econ.TH,"Strong empirical evidence from laboratory experiments, and more recently from population surveys, shows that individuals, when evaluating their situations, pay attention to whether they experience gains or losses, with losses weighing more heavily than gains. The electorate's loss aversion, in turn, influences politicians' choices. We propose a new framework for welfare analysis of policy outcomes that, in addition to the traditional focus on post-policy incomes, also accounts for individuals' gains and losses resulting from policies. We develop several bivariate stochastic dominance criteria for ranking policy outcomes that are sensitive to features of the joint distribution of individuals' income changes and absolute incomes. The main social objective assumes that individuals are loss averse with respect to income gains and losses, inequality averse with respect to absolute incomes, and hold varying preferences regarding the association between incomes and income changes. We translate these and other preferences into functional inequalities that can be tested using sample data. The concepts and methods are illustrated using data from an income support experiment conducted in Connecticut."
http://arxiv.org/abs/2510.00754v2,A Unified Framework for Spatial and Temporal Treatment Effect Boundaries: Theory and Identification,2025-10-01 10:43:05+00:00,['Tatsuru Kikuchi'],econ.EM,"This paper develops a unified theoretical framework for detecting and estimating boundaries in treatment effects across both spatial and temporal dimensions. We formalize the concept of treatment effect boundaries as structural parameters characterizing regime transitions where causal effects cease to operate. Building on reaction-diffusion models of information propagation, we establish conditions under which spatial and temporal boundaries share common dynamics governed by diffusion parameters (delta, lambda), yielding the testable prediction d^*/tau^* = 3.32 lambda sqrt{delta} for standard detection thresholds. We derive formal identification results under staggered treatment adoption and develop a three-stage estimation procedure implementable with standard panel data. Monte Carlo simulations demonstrate excellent finite-sample performance, with boundary estimates achieving RMSE below 10% in realistic configurations. We apply the framework to two empirical settings: EU broadband diffusion (2006-2021) and US wildfire economic impacts (2017-2022). The broadband application reveals a scope limitation -- our framework assumes depreciation dynamics and fails when effects exhibit increasing returns through network externalities. The wildfire application provides strong validation: estimated boundaries satisfy d^* = 198 km and tau^* = 2.7 years, with the empirical ratio (72.5) exactly matching the theoretical prediction 3.32 lambda sqrt{delta} = 72.5. The framework provides practical tools for detecting when localized treatments become systemic and identifying critical thresholds for policy intervention."
http://arxiv.org/abs/2510.19630v2,Network Contagion Dynamics in European Banking: A Navier-Stokes Framework for Systemic Risk Assessment,2025-10-22 14:27:12+00:00,['Tatsuru Kikuchi'],econ.EM,"This paper develops a continuous functional framework for analyzing contagion dynamics in financial networks, extending the Navier-Stokes-based approach to network-structured spatial processes. We model financial distress propagation as a diffusion process on weighted networks, deriving a network diffusion equation from first principles that predicts contagion decay depends on the network's algebraic connectivity through the relation $κ= \sqrt{λ_2/D}$, where $λ_2$ is the second-smallest eigenvalue of the graph Laplacian and $D$ is the diffusion coefficient. Applying this framework to European banking data from the EBA stress tests (2018, 2021, 2023), we estimate interbank exposure networks using maximum entropy methods and track the evolution of systemic risk through the COVID-19 crisis. Our key finding is that network connectivity declined by 45\% from 2018 to 2023, implying a 26\% reduction in the contagion decay parameter. Difference-in-differences analysis reveals this structural change was driven by regulatory-induced deleveraging of systemically important banks, which experienced differential asset reductions of 17\% relative to smaller institutions. The networks exhibit lognormal rather than scale-free degree distributions, suggesting greater resilience than previously assumed in the literature. Extensive robustness checks across parametric and non-parametric estimation methods confirm declining systemic risk, with cross-method correlations exceeding 0.95. These findings demonstrate that post-COVID-19 regulatory reforms effectively reduced network interconnectedness and systemic vulnerability in the European banking system."
http://arxiv.org/abs/2510.14909v1,"The Impact of Medicaid Coverage on Mental Health, Why Insurance Makes People Happier in OHIE: by Spending Less or by Spending More?",2025-10-16 17:28:43+00:00,['Yangyang Li'],econ.GN,"The Oregon Health Insurance Experiment (OHIE) offers a unique opportunity to examine the causal relationship between Medicaid coverage and happiness among low-income adults, using an experimental design. This study leverages data from comprehensive surveys conducted at 0 and 12 months post-treatment. Previous studies based on OHIE have shown that individuals receiving Medicaid exhibited a significant improvement in mental health compared to those who did not receive coverage. The primary objective is to explore how Medicaid coverage impacts happiness, specifically analyzing in which direction variations in healthcare spending significantly improve mental health: higher spending or lower spending after Medicaid. Utilizing instrumental variable (IV) regression, I conducted six separate regressions across subgroups categorized by expenditure levels and happiness ratings, and the results reveal distinct patterns. Enrolling in OHP has significantly decreased the probability of experiencing unhappiness, regardless of whether individuals had high or low medical spending. Additionally, it decreased the probability of being pretty happy and having high medical expenses, while increasing the probability among those with lower expenses. Concerning the probability of being very happy, the OHP only had a positive effect on being very happy and spending less, and its effect on those with high expenses was insignificant. These findings align with the benefit of Medicaid: alleviating financial burden, contributing to the well-being of distinct subgroups."
http://arxiv.org/abs/2510.13148v2,Nonparametric Identification of Spatial Treatment Effect Boundaries: Evidence from Bank Branch Consolidation,2025-10-15 04:57:31+00:00,['Tatsuru Kikuchi'],econ.EM,"I develop a nonparametric framework for identifying spatial boundaries of treatment effects without imposing parametric functional form restrictions. The method employs local linear regression with data-driven bandwidth selection to flexibly estimate spatial decay patterns and detect treatment effect boundaries. Monte Carlo simulations demonstrate that the nonparametric approach exhibits lower bias and correctly identifies the absence of boundaries when none exist, unlike parametric methods that may impose spurious spatial patterns. I apply this framework to bank branch openings during 2015--2020, matching 5,743 new branches to 5.9 million mortgage applications across 14,209 census tracts. The analysis reveals that branch proximity significantly affects loan application volume (8.5\% decline per 10 miles) but not approval rates, consistent with branches stimulating demand through local presence while credit decisions remain centralized. Examining branch survival during the digital transformation era (2010--2023), I find a non-monotonic relationship with area income: high-income areas experience more closures despite conventional wisdom. This counterintuitive pattern reflects strategic consolidation of redundant branches in over-banked wealthy urban areas rather than discrimination against poor neighborhoods. Controlling for branch density, urbanization, and competition, the direct income effect diminishes substantially, with branch density emerging as the primary determinant of survival. These findings demonstrate the necessity of flexible nonparametric methods for detecting complex spatial patterns that parametric models would miss, and challenge simplistic narratives about banking deserts by revealing the organizational complexity underlying spatial consolidation decisions."
http://arxiv.org/abs/2510.12289v2,Nonparametric Identification and Estimation of Spatial Treatment Effect Boundaries: Evidence from 42 Million Pollution Observations,2025-10-14 08:49:40+00:00,['Tatsuru Kikuchi'],econ.EM,"This paper develops a nonparametric framework for identifying and estimating spatial boundaries of treatment effects in settings with geographic spillovers. While atmospheric dispersion theory predicts exponential decay of pollution under idealized assumptions, these assumptions -- steady winds, homogeneous atmospheres, flat terrain -- are systematically violated in practice. I establish nonparametric identification of spatial boundaries under weak smoothness and monotonicity conditions, propose a kernel-based estimator with data-driven bandwidth selection, and derive asymptotic theory for inference. Using 42 million satellite observations of NO$_2$ concentrations near coal plants (2019-2021), I find that nonparametric kernel regression reduces prediction errors by 1.0 percentage point on average compared to parametric exponential decay assumptions, with largest improvements at policy-relevant distances: 2.8 percentage points at 10 km (near-source impacts) and 3.7 percentage points at 100 km (long-range transport). Parametric methods systematically underestimate near-source concentrations while overestimating long-range decay. The COVID-19 pandemic provides a natural experiment validating the framework's temporal sensitivity: NO$_2$ concentrations dropped 4.6\% in 2020, then recovered 5.7\% in 2021. These results demonstrate that flexible, data-driven spatial methods substantially outperform restrictive parametric assumptions in environmental policy applications."
http://arxiv.org/abs/2510.14409v2,Dynamic Spatial Treatment Effect Boundaries: A Continuous Functional Framework from Navier-Stokes Equations,2025-10-16 08:08:31+00:00,['Tatsuru Kikuchi'],econ.EM,"I develop a comprehensive theoretical framework for dynamic spatial treatment effect boundaries using continuous functional definitions grounded in Navier-Stokes partial differential equations. Rather than discrete treatment effect estimators, the framework characterizes treatment intensity as a continuous function $τ(\mathbf{x}, t)$ over space-time, enabling rigorous analysis of propagation dynamics, boundary evolution, and cumulative exposure patterns. Building on exact self-similar solutions expressible through Kummer confluent hypergeometric and modified Bessel functions, I establish that treatment effects follow scaling laws $τ(d, t) = t^{-α} f(d/t^β)$ where exponents characterize diffusion mechanisms. Empirical validation using 42 million TROPOMI satellite observations of NO$_2$ pollution from U.S. coal-fired power plants demonstrates strong exponential spatial decay ($κ_s = 0.004$ per km, $R^2 = 0.35$) with detectable boundaries at 572 km. Monte Carlo simulations confirm superior performance over discrete parametric methods in boundary detection and false positive avoidance (94\% vs 27\% correct rejection). Regional heterogeneity analysis validates diagnostic capability: positive decay parameters within 100 km confirm coal plant dominance; negative parameters beyond 100 km correctly signal when urban sources dominate. The continuous functional perspective unifies spatial econometrics with mathematical physics, providing theoretically grounded methods for boundary detection, exposure quantification, and policy evaluation across environmental economics, banking, and healthcare applications."
http://arxiv.org/abs/2510.16003v1,"Rethinking Arrow--Debreu: A New Framework for Exchange, Time, and Uncertainty",2025-10-14 13:29:02+00:00,['Nizar Riane'],econ.TH,"This paper revisits the Arrow-Debreu general equilibrium framework through the lens of effective trade, emphasizing the distinction between theoretical and realizable market interactions. We develop the Effective Trade Model (ETM), where transactions arise from bilateral feasibility rather than aggregate supply and demand desires. Within this framework, we establish the main properties of the price-demand correspondence and prove the existence of Nash equilibria, incorporating production, money, and network topology. The analysis extends to time, uncertainty, and open economies, revealing how loanable funds and exchange rates emerge endogenously. Our results show that equilibrium is shaped by transaction constraints, subjective pricing, and decentralized negotiation, rather than by universal market-clearing conditions, and thereby call into question the foundations of welfare theory. Anticipation is modeled via the conditional mode, capturing bounded rationality and information limitations in contrast to the rational expectations hypothesis. The ETM thus offers a behaviorally and structurally grounded alternative to classical general equilibrium, bridging microfoundations, monetary dynamics, and temporal consistency within a unified framework."
http://arxiv.org/abs/2510.24781v1,Dual-Channel Technology Diffusion: Spatial Decay and Network Contagion in Supply Chain Networks,2025-10-25 16:46:34+00:00,['Tatsuru Kikuchi'],econ.EM,"This paper develops a dual-channel framework for analyzing technology diffusion that integrates spatial decay mechanisms from continuous functional analysis with network contagion dynamics from spectral graph theory. Building on our previous studies, which establish Navier-Stokes-based approaches to spatial treatment effects and financial network fragility, we demonstrate that technology adoption spreads simultaneously through both geographic proximity and supply chain connections. Using comprehensive data on six technologies adopted by 500 firms over 2010-2023, we document three key findings. First, technology adoption exhibits strong exponential geographic decay with spatial decay rate $κ\approx 0.043$ per kilometer, implying a spatial boundary of $d^* \approx 69$ kilometers beyond which spillovers are negligible (R-squared = 0.99). Second, supply chain connections create technology-specific networks whose algebraic connectivity ($λ_2$) increases 300-380 percent as adoption spreads, with correlation between $λ_2$ and adoption exceeding 0.95 across all technologies. Third, traditional difference-in-differences methods that ignore spatial and network structure exhibit 61 percent bias in estimated treatment effects. An event study around COVID-19 reveals that network fragility increased 24.5 percent post-shock, amplifying treatment effects through supply chain spillovers in a manner analogous to financial contagion documented in our recent study. Our framework provides micro-foundations for technology policy: interventions have spatial reach of 69 kilometers and network amplification factor of 10.8, requiring coordinated geographic and supply chain targeting for optimal effectiveness."
http://arxiv.org/abs/2511.03813v1,Price-Based Attention and Welfare,2025-11-05 19:25:49+00:00,['Kaushil Patel'],econ.TH,"To choose between two discrete goods, a consumer pays attention to only those with prices below a threshold. From these, she chooses her most preferred good. We assume consumers in a population have the same preference but may have different thresholds. Similar models of bounded rationality have been studied in the empirical marketing literature. We fully characterize the model, and using observational choice data alone, we identify the welfare implications of a price change. The behavioral content of our model overlaps with an important class of random utility models, but the welfare implications are meaningfully different. The distribution of equivalent variation under our model first-order stochastically dominates that under the random utility model."
http://arxiv.org/abs/2510.11013v2,Spatial and Temporal Boundaries in Difference-in-Differences: A Framework from Navier-Stokes Equation,2025-10-13 05:08:35+00:00,['Tatsuru Kikuchi'],econ.EM,"This paper develops a unified framework for identifying spatial and temporal boundaries of treatment effects in difference-in-differences designs. Starting from fundamental fluid dynamics equations (Navier-Stokes), we derive conditions under which treatment effects decay exponentially in space and time, enabling researchers to calculate explicit boundaries beyond which effects become undetectable. The framework encompasses both linear (pure diffusion) and nonlinear (advection-diffusion with chemical reactions) regimes, with testable scope conditions based on dimensionless numbers from physics (Péclet and Reynolds numbers). We demonstrate the framework's diagnostic capability using air pollution from coal-fired power plants. Analyzing 791 ground-based PM$_{2.5}$ monitors and 189,564 satellite-based NO$_2$ grid cells in the Western United States over 2019-2021, we find striking regional heterogeneity: within 100 km of coal plants, both pollutants show positive spatial decay (PM$_{2.5}$: $κ_s = 0.00200$, $d^* = 1,153$ km; NO$_2$: $κ_s = 0.00112$, $d^* = 2,062$ km), validating the framework. Beyond 100 km, negative decay parameters correctly signal that urban sources dominate and diffusion assumptions fail. Ground-level PM$_{2.5}$ decays approximately twice as fast as satellite column NO$_2$, consistent with atmospheric transport physics. The framework successfully diagnoses its own validity in four of eight analyzed regions, providing researchers with physics-based tools to assess whether their spatial difference-in-differences setting satisfies diffusion assumptions before applying the estimator. Our results demonstrate that rigorous boundary detection requires both theoretical derivation from first principles and empirical validation of underlying physical assumptions."
http://arxiv.org/abs/2510.08709v2,Blackwell without Priors,2025-10-09 18:12:50+00:00,['Maxwell Rosenthal'],econ.TH,"This paper proposes a fully prior-free model of experimentation in which the decision maker observes the entire distribution of signals generated by a known experiment under an unknown distribution of the state of the world. One experiment is robustly more informative than another if the decision maker's maxmin expected utility after observing the output of the former is always at least her maxmin expected utility after observing the latter. We show that this ranking holds if and only if the less informative experiment is a linear transformation of the more informative experiment; equivalently, the null space of the more informative experiment is a subset of the null space of the less informative experiment. Our criterion is implied by Blackwell's order but does not imply it, and we show by example that our ranking admits strictly more comparable pairs of experiments than the classical ranking."
http://arxiv.org/abs/2510.15324v3,Dynamic Spatial Treatment Effects as Continuous Functionals: Theory and Evidence from Healthcare Access,2025-10-17 05:19:49+00:00,['Tatsuru Kikuchi'],econ.EM,"I develop a continuous functional framework for spatial treatment effects grounded in Navier-Stokes partial differential equations. Rather than discrete treatment parameters, the framework characterizes treatment intensity as continuous functions $τ(\mathbf{x}, t)$ over space-time, enabling rigorous analysis of boundary evolution, spatial gradients, and cumulative exposure. Empirical validation using 32,520 U.S. ZIP codes demonstrates exponential spatial decay for healthcare access ($κ= 0.002837$ per km, $R^2 = 0.0129$) with detectable boundaries at 37.1 km. The framework successfully diagnoses when scope conditions hold: positive decay parameters validate diffusion assumptions near hospitals, while negative parameters correctly signal urban confounding effects. Heterogeneity analysis reveals 2-13 $\times$ stronger distance effects for elderly populations and substantial education gradients. Model selection strongly favors logarithmic decay over exponential ($Δ\text{AIC} > 10,000$), representing a middle ground between exponential and power-law decay. Applications span environmental economics, banking, and healthcare policy. The continuous functional framework provides predictive capability ($d^*(t) = ξ^* \sqrt{t}$), parameter sensitivity ($\partial d^*/\partial ν$), and diagnostic tests unavailable in traditional difference-in-differences approaches."
http://arxiv.org/abs/2511.05512v1,Estimating the Impact of the Bitcoin Halving on Its Price Using Synthetic Control,2025-10-25 05:00:23+00:00,['Vladislav Virtonen'],econ.GN,"The third Bitcoin halving that took place in May 2020 cut down the mining reward from 12.5 to 6.25 BTC per block and thus slowed down the rate of issuance of new Bitcoins, making it more scarce. The fourth and most recent halving happened in April 2024, cutting the block reward further to 3.125 BTC. If the demand did not decrease simultaneously after these halvings, then the neoclassical economic theory posits that the price of Bitcoin should have increased due to the halving. But did it, in fact, increase for that reason, or is this a post hoc fallacy? This paper uses synthetic control to construct a weighted Bitcoin that is different from its counterpart in one aspect - it did not undergo halving. Comparing the price trajectory of the actual and the simulated Bitcoins, I find evidence of a positive effect of the 2024 Bitcoin halving on its price three months later. The magnitude of this effect is one fifth of the total percentage change in the price of Bitcoin during the study period - from April 2, 2023, to July 21, 2024 (17 months). The second part of the study fails to obtain a statistically significant and robust causal estimate of the effect of the 2020 Bitcoin halving on Bitcoin's price. This is the first paper analyzing the effect of halving causally, building on the existing body of correlational research."
http://arxiv.org/abs/2510.01115v1,Exploring Network-Knowledge Graph Duality: A Case Study in Agentic Supply Chain Risk Analysis,2025-10-01 17:02:14+00:00,"['Evan Heus', 'Rick Bookstaber', 'Dhruv Sharma']",cs.AI,"Large Language Models (LLMs) struggle with the complex, multi-modal, and network-native data underlying financial risk. Standard Retrieval-Augmented Generation (RAG) oversimplifies relationships, while specialist models are costly and static. We address this gap with an LLM-centric agent framework for supply chain risk analysis. Our core contribution is to exploit the inherent duality between networks and knowledge graphs (KG). We treat the supply chain network as a KG, allowing us to use structural network science principles for retrieval. A graph traverser, guided by network centrality scores, efficiently extracts the most economically salient risk paths. An agentic architecture orchestrates this graph retrieval alongside data from numerical factor tables and news streams. Crucially, it employs novel ``context shells'' -- descriptive templates that embed raw figures in natural language -- to make quantitative data fully intelligible to the LLM. This lightweight approach enables the model to generate concise, explainable, and context-rich risk narratives in real-time without costly fine-tuning or a dedicated graph database."
http://arxiv.org/abs/2510.01351v1,Does Adoption of Zero Tillage Reduce Crop Residue Burning? Evidence from Satellite Remote Sensing and Household Survey Data in India,2025-10-01 18:27:19+00:00,"['Dominik Naeher', 'Virginia Ziulu']",econ.GN,"Previous research indicates that zero tillage technology offers a profitable alternative to crop residue burning, with significant potential to reduce agricultural emissions and contribute to improvements in air quality and public health. Yet, empirical evidence on the link between zero tillage adoption and residue burning remains scarce, adding to the difficulties policy makers face in this context. This study addresses this gap by integrating high-resolution satellite imagery with household survey data from India to examine the empirical relationship between zero tillage and residue burning. We compare different methods for constructing burn indicators from remote-sensing data and assess their predictive power against survey-based measures. Our findings reveal a robust negative association between zero tillage and crop residue burning, with reductions in the incidence of burning of 50% or more across both survey data and satellite-derived indicators. By providing insights into optimal geospatial data integration methods, our study also makes a methodological contribution that can inform future research and support evidence-based policy interventions for more sustainable agricultural practices."
http://arxiv.org/abs/2510.07204v1,Beyond the Oracle Property: Adaptive LASSO in Cointegrating Regressions,2025-10-08 16:38:30+00:00,"['Karsten Reichold', 'Ulrike Schneider']",econ.EM,"This paper establishes new asymptotic results for the adaptive LASSO estimator in cointegrating regression models. We study model selection probabilities, estimator consistency, and limiting distributions under both standard and moving-parameter asymptotics. We also derive uniform convergence rates and the fastest local-to-zero rates that can still be detected by the estimator, complementing and extending the results of Lee, Shi, and Gao (2022, Journal of Econometrics, 229, 322--349). Our main findings include that under conservative tuning, the adaptive LASSO estimator is uniformly $T$-consistent and the cut-off rate for local-to-zero coefficients that can be detected by the procedure is $1/T$. Under consistent tuning, however, both rates are slower and depend on the tuning parameter. The theoretical results are complemented by a detailed simulation study showing that the finite-sample distribution of the adaptive LASSO estimator deviates substantially from what is suggested by the oracle property, whereas the limiting distributions derived under moving-parameter asymptotics provide much more accurate approximations. Finally, we show that our results also extend to models with local-to-unit-root regressors and to predictive regressions with unit-root predictors."
http://arxiv.org/abs/2510.11360v1,A mathematical model for pricing perishable goods for quick-commerce applications,2025-10-13 12:59:58+00:00,['Milon Bhattacharya'],cs.CE,"Quick commerce (q-commerce) is one of the fastest growing sectors in India. It provides informal employment to approximately 4,50,000 workers, and it is estimated to become a USD 200 Billion industry by 2026. A significant portion of this industry deals with perishable goods. (e.g. milk, dosa batter etc.) These are food items which are consumed relatively fresh by the consumers and therefore their order volume is high and repetitive even when the average basket size is relatively small. The fundamental challenge for the retailer is that, increasing selling price would hamper sales and would lead to unsold inventory. On the other hand setting a price less, would lead to forgoing of potential revenue. This paper attempts to propose a mathematical model which formalizes this dilemma. The problem statement is not only important for improving the unit economics of the perennially loss making quick commerce firms, but also would lead to a trickle-down effect in improving the conditions of the gig workers as observed in [4]. The sections below describe the mathematical formulation. The results from the simulation would be published in a follow-up study."
http://arxiv.org/abs/2510.10527v1,Denoised IPW-Lasso for Heterogeneous Treatment Effect Estimation in Randomized Experiments,2025-10-12 09:54:23+00:00,"['Mingqian Guan', 'Komei Fujita', 'Naoya Sueishi', 'Shota Yasui']",econ.EM,"This paper proposes a new method for estimating conditional average treatment effects (CATE) in randomized experiments. We adopt inverse probability weighting (IPW) for identification; however, IPW-transformed outcomes are known to be noisy, even when true propensity scores are used. To address this issue, we introduce a noise reduction procedure and estimate a linear CATE model using Lasso, achieving both accuracy and interpretability. We theoretically show that denoising reduces the prediction error of the Lasso. The method is particularly effective when treatment effects are small relative to the variability of outcomes, which is often the case in empirical applications. Applications to the Get-Out-the-Vote dataset and Criteo Uplift Modeling dataset demonstrate that our method outperforms fully nonparametric machine learning methods in identifying individuals with higher treatment effects. Moreover, our method uncovers informative heterogeneity patterns that are consistent with previous empirical findings."
http://arxiv.org/abs/2510.06903v1,When Machines Meet Each Other: Network Effects and the Strategic Role of History in Multi-Agent AI,2025-10-08 11:39:16+00:00,"['Yu Liu', 'Wenwen Li', 'Yifan Dou', 'Guangnan Ye']",econ.GN,"As artificial intelligence (AI) enters the agentic era, large language models (LLMs) are increasingly deployed as autonomous agents that interact with one another rather than operate in isolation. This shift raises a fundamental question: how do machine agents behave in interdependent environments where outcomes depend not only on their own choices but also on the coordinated expectations of peers? To address this question, we study LLM agents in a canonical network-effect game, where economic theory predicts convergence to a fulfilled expectation equilibrium (FEE). We design an experimental framework in which 50 heterogeneous GPT-5-based agents repeatedly interact under systematically varied network-effect strengths, price trajectories, and decision-history lengths. The results reveal that LLM agents systematically diverge from FEE: they underestimate participation at low prices, overestimate at high prices, and sustain persistent dispersion. Crucially, the way history is structured emerges as a design lever. Simple monotonic histories-where past outcomes follow a steady upward or downward trend-help stabilize coordination, whereas nonmonotonic histories amplify divergence and path dependence. Regression analyses at the individual level further show that price is the dominant driver of deviation, history moderates this effect, and network effects amplify contextual distortions. Together, these findings advance machine behavior research by providing the first systematic evidence on multi-agent AI systems under network effects and offer guidance for configuring such systems in practice."
http://arxiv.org/abs/2510.08415v1,Stochastic Volatility-in-mean VARs with Time-Varying Skewness,2025-10-09 16:35:12+00:00,"['Leonardo N. Ferreira', 'Haroon Mumtaz', 'Ana Skoblar']",econ.EM,"This paper introduces a Bayesian vector autoregression (BVAR) with stochastic volatility-in-mean and time-varying skewness. Unlike previous approaches, the proposed model allows both volatility and skewness to directly affect macroeconomic variables. We provide a Gibbs sampling algorithm for posterior inference and apply the model to quarterly data for the US and the UK. Empirical results show that skewness shocks have economically significant effects on output, inflation and spreads, often exceeding the impact of volatility shocks. In a pseudo-real-time forecasting exercise, the proposed model outperforms existing alternatives in many cases. Moreover, the model produces sharper measures of tail risk, revealing that standard stochastic volatility models tend to overstate uncertainty. These findings highlight the importance of incorporating time-varying skewness for capturing macro-financial risks and improving forecast performance."
http://arxiv.org/abs/2510.01551v1,The centripetal pull of climate: Evidence from European Parliament elections (1989-2019),2025-10-02 00:49:12+00:00,"['Marco Dueñas', 'Hector Galindo-Silva', 'Antoine Mandel']",econ.GN,"This paper examines the impact of temperature shocks on European Parliament elections. We combine high-resolution climate data with results from parliamentary elections between 1989 and 2019, aggregated at the NUTS-2 regional level. Exploiting exogenous variation in unusually warm and hot days during the months preceding elections, we identify the effect of short-run temperature shocks on voting behaviour. We find that temperature shocks reduce ideological polarisation and increase vote concentration, as voters consolidate around larger, more moderate parties. This aggregated pattern is explained by a gain in support of liberal and, to a lesser extent, social democratic parties, while right-wing parties lose vote share. Consistent with a salience mechanism, complementary analysis of party manifestos shows greater emphasis on climate-related issues in warmer pre-electoral contexts. Overall, our findings indicate that climate shocks can shift party systems toward the centre and weaken political extremes."
http://arxiv.org/abs/2510.01535v1,Cautions on Tail Index Regressions,2025-10-02 00:11:27+00:00,['Thomas T. Yang'],econ.EM,"We revisit tail-index regressions. For linear specifications, we find that the usual full-rank condition can fail because conditioning on extreme outcomes causes regressors to degenerate to constants. More generally, the conditional distribution of the covariates in the tails concentrates on the values at which the tail index is minimized. Away from those points, the conditional density tends to zero. For local nonparametric tail index regression, the convergence rate can be very slow. We conclude with practical suggestions for applied work."
http://arxiv.org/abs/2510.02408v1,Can GenAI Improve Academic Performance? Evidence from the Social and Behavioral Sciences,2025-10-02 07:32:47+00:00,"['Dragan Filimonovic', 'Christian Rutzer', 'Conny Wunsch']",econ.GN,"This paper estimates the effect of Generative AI (GenAI) adoption on scientific productivity and quality in the social and behavioral sciences. Using matched author-level panel data and a difference-in-differences design, we find that GenAI adoption is associated with sizable increases in research productivity, measured by the number of published papers. It also leads to moderate gains in publication quality, based on journal impact factors. These effects are most pronounced among early-career researchers, authors working in technically complex subfields, and those from non-English-speaking countries. The results suggest that GenAI tools may help lower some structural barriers in academic publishing and promote more inclusive participation in research."
http://arxiv.org/abs/2510.02705v1,Does FOMC Tone Really Matter? Statistical Evidence from Spectral Graph Network Analysis,2025-10-03 03:49:30+00:00,"['Jaeho Choi', 'Jaewon Kim', 'Seyoung Chung', 'Chae-shick Chung', 'Yoonsoo Lee']",econ.GN,"This study examines the relationship between Federal Open Market Committee (FOMC) announcements and financial market network structure through spectral graph theory. Using hypergraph networks constructed from S\&P 100 stocks around FOMC announcement dates (2011--2024), we employ the Fiedler value -- the second eigenvalue of the hypergraph Laplacian -- to measure changes in market connectivity and systemic stability. Our event study methodology reveals that FOMC announcements significantly alter network structure across multiple time horizons. Analysis of policy tone, classified using natural language processing, reveals heterogeneous effects: hawkish announcements induce network fragmentation at short horizons ($k=6$) followed by reconsolidation at medium horizons ($k=14$), while neutral statements show limited immediate impact but exhibit delayed fragmentation. These findings suggest that monetary policy communication affects market architecture through a network structural transmission, with effects varying by announcement timing and policy stance."
http://arxiv.org/abs/2510.04785v1,A behavioral reinvestigation of the effect of long ties on social contagions,2025-10-06 13:05:23+00:00,"['Luca Lazzaro', 'Manuel S. Mariani', 'René Algesheimer', 'Radu Tanase']",physics.soc-ph,"Faced with uncertainty in decision making, individuals often turn to their social networks to inform their decisions. In consequence, these networks become central to how new products and behaviors spread. A key structural feature of networks is the presence of long ties, which connect individuals who share few mutual contacts. Under what conditions do long ties facilitate or hinder diffusion? The literature provides conflicting results, largely due to differing assumptions about individual decision-making. We reinvestigate the role of long ties by experimentally measuring adoption decisions under social influence for products with uncertain payoffs and embedding these decisions in network simulations. At the individual level, we find that higher payoff uncertainty increases the average reliance on social influence. However, personal traits such as risk preferences and attitudes toward uncertainty lead to substantial heterogeneity in how individuals respond to social influence. At the collective level, the observed individual heterogeneity ensures that long ties consistently promote diffusion, but their positive effect weakens as uncertainty increases. Our results reveal that the effect of long ties is not determined by whether the aggregate process is a simple or complex contagion, but by the extent of heterogeneity in how individuals respond to social influence."
http://arxiv.org/abs/2510.05783v1,The role of work-life balance in effective business management,2025-10-07 10:57:19+00:00,"['Anna Kasperczuk', 'Michał Ćwiąkała', 'Ernest Górka', 'Piotr Ręczajski', 'Piotr Mrzygłód', 'Maciej Frasunkiewicz', 'Agnieszka Darcińska-Głębocka', 'Jan Piwnik', 'Grzegorz Gardocki']",econ.GN,"This study examines the role of work-life balance (WLB) as a strategic component of effective business management and its influence on employee motivation, job satisfaction, and organizational performance. Drawing on a quantitative survey of 102 economically active individuals, the research investigates the effectiveness of various WLB initiatives, including flexible working hours, private medical care, and additional employee benefits. The results reveal that flexible working arrangements are the most impactful tool for enhancing work-life balance and significantly contribute to higher levels of employee motivation. A statistically significant positive correlation was observed between perceived work-life balance and motivation, indicating that improving WLB directly strengthens commitment, reduces burnout, and increases job satisfaction. Moreover, the findings highlight differences in WLB perceptions across demographic groups, suggesting the need for tailored policies. The study emphasizes that organizations actively supporting WLB achieve greater employee loyalty, improved productivity, and enhanced employer branding. These results have practical implications for human resource strategies, showing that integrating WLB initiatives can improve overall organizational performance and societal well-being. The paper also identifies research gaps and recommends exploring cultural, occupational, and remote work contexts in future studies to better understand how WLB strategies shape workforce engagement in dynamic labor markets."
http://arxiv.org/abs/2510.04388v1,REMIND-PyPSA-Eur: Integrating power system flexibility into sector-coupled energy transition pathways,2025-10-05 22:42:44+00:00,"['Adrian Odenweller', 'Falko Ueckerdt', 'Johannes Hampp', 'Ivan Ramirez', 'Felix Schreyer', 'Robin Hasse', 'Jarusch Muessel', 'Chen Chris Gong', 'Robert Pietzcker', 'Tom Brown', 'Gunnar Luderer']",econ.GN,"The rapid expansion of low-cost renewable electricity combined with end-use electrification in transport, industry, and buildings offers a promising path to deep decarbonisation. However, aligning variable supply with demand requires strategies for daily and seasonal balancing. Existing models either lack the wide scope required for long-term transition pathways or the spatio-temporal detail to capture power system variability and flexibility. Here, we combine the complementary strengths of REMIND, a long-term integrated assessment model, and PyPSA-Eur, an hourly energy system model, through a bi-directional, price-based and iterative soft coupling. REMIND provides pathway variables such as sectoral electricity demand, installed capacities, and costs to PyPSA-Eur, which returns optimised operational variables such as capacity factors, storage requirements, and relative prices. After sufficient convergence, this integrated approach jointly optimises long-term investment and short-term operation. We demonstrate the coupling for two Germany-focused scenarios, with and without demand-side flexibility, reaching climate neutrality by 2045. Our results confirm that a sector-coupled energy system with nearly 100\% renewable electricity is technically possible and economically viable. Power system flexibility influences long-term pathways through price differentiation: supply-side market values vary by generation technology, while demand-side prices vary by end-use sector. Flexible electrolysers and smart-charging electric vehicles benefit from below-average prices, whereas less flexible heat pumps face almost twice the average price due to winter peak loads. Without demand-side flexibility, electricity prices increase across all end-users, though battery deployment partially compensates. Our approach therefore fully integrates power system dynamics into multi-decadal energy transition pathways."
http://arxiv.org/abs/2510.07047v1,Analysis of managerial behaviors in business management,2025-10-08 14:16:07+00:00,"['Ernest Górka', 'Dariusz Baran', 'Michał Ćwiąkała', 'Gabriela Wojak', 'Robert Marszczuk', 'Katarzyna Olszyńska', 'Piotr Mrzygłód', 'Maciej Frasunkiewicz', 'Piotr Ręczajski', 'Kamil Saługa', 'Maciej Ślusarczyk', 'Jan Piwnik']",econ.GN,"This study explores how different managerial behaviors influence team effectiveness and organizational outcomes, using Kenneth Blanchard's situational leadership model as a diagnostic tool. Conducted across ten companies, the research evaluates leadership adaptability through a scenario-based questionnaire identifying instructional, teaching, supportive, and delegating styles. Results show that the supportive (affiliative) style is dominant, present in 60 percent of surveyed companies, with delegating being second. Correlation analysis reveals strong negative relationships between certain styles, particularly instructional and supportive, indicating that flexibility in leadership is crucial. The findings suggest that over-reliance on any one style may lead to inefficiencies, while a balanced, situational approach enhances decision-making, morale, and adaptability. The research contributes to leadership theory by demonstrating how behavioral combinations, not static traits, influence outcomes. It offers practical implications for managerial training, recommending the integration of diagnostic tools like the Blanchard test to improve style awareness and behavioral flexibility. Limitations include reliance on self-assessment data and a small sample size. Future research should explore longitudinal and cross-industry analyses to assess how leadership behaviors evolve over time or under pressure."
http://arxiv.org/abs/2510.07004v1,The importance of emotional intelligence in leadership for building an effective team,2025-10-08 13:27:46+00:00,"['Joanna Ćwiąkała', 'Waldemar Gajda', 'Michał Ćwiąkała', 'Ernest Górka', 'Dariusz Baran', 'Gabriela Wojak', 'Piotr Mrzygłód', 'Maciej Frasunkiewicz', 'Piotr Ręczajski', 'Jan Piwnik']",econ.GN,"This study investigates the significance of emotional intelligence (EI) as a fundamental component of effective leadership and its impact on building cohesive, motivated, and high-performing teams. Drawing on data from a survey of 100 professionals, the research examines how EI competencies including self-awareness, self-regulation, empathy, and social skills shape leadership effectiveness, team collaboration, conflict resolution, and workplace motivation. The results demonstrate strong correlations between EI and key leadership traits such as empathy, ethical conduct, social competence, and motivational effectiveness. Leaders with higher levels of EI are perceived as more empathetic, ethical, and capable of fostering trust, resolving conflicts, and inspiring commitment, thereby improving team dynamics and overall organizational performance. The study also highlights that ethical leadership significantly enhances motivation and that social competence is essential for engaging and aligning teams toward common goals. While the findings are exploratory due to the limited sample size, they provide valuable insights for leadership development programs, emphasizing the importance of integrating EI-focused training, coaching, and assessment tools into organizational strategies. The research contributes to leadership theory by demonstrating that emotional intelligence is not an isolated skill but a central driver of interpersonal effectiveness, employee engagement, and sustainable business success."
http://arxiv.org/abs/2510.10437v1,Women's inheritance rights reforms and impact on women's empowerment: evidence from India,2025-10-12 04:07:52+00:00,"['Minali Grover', 'Ajay Sharma']",econ.GN,"This paper explores the influence of inheritance rights on women' empowerment in India. We employ the quasi-natural experiment framework wherein; five states amended the Hindu Succession Act (HSA) from 1976 to 1994 before it was federally amended in 2005. Further, we apply difference-in-difference (DID) strategy and consider triangulation approach to identify women empowerment indicators namely: access to resources, agency, and outcomes to measure varying dimensions of empowerment. Using the India Human Development Survey (IHDS-I), our results indicate a positive impact on marriage choice, intimate partner violence, physical, and civil autonomy. However, negative impact on household autonomy and no significant on economic participation for women exposed to state amendments. Further, exploring the heterogeneities in terms of socio-economic status, location, level of patriarchy in a state, gender of the head of the household. Overall, the study highlights that the impact of inheritance law is not unfirm across different groups."
http://arxiv.org/abs/2510.12183v1,L2-relaxation for Economic Prediction,2025-10-14 06:27:11+00:00,"['Zhentao Shi', 'Yishu Wang']",econ.EM,"We leverage an ensemble of many regressors, the number of which can exceed the sample size, for economic prediction. An underlying latent factor structure implies a dense regression model with highly correlated covariates. We propose the L2-relaxation method for estimating the regression coefficients and extrapolating the out-of-sample (OOS) outcomes. This framework can be applied to policy evaluation using the panel data approach (PDA), where we further establish inference for the average treatment effect. In addition, we extend the traditional single unit setting in PDA to allow for many treated units with a short post-treatment period. Monte Carlo simulations demonstrate that our approach exhibits excellent finite sample performance for both OOS prediction and policy evaluation. We illustrate our method with two empirical examples: (i) predicting China's producer price index growth rate and evaluating the effect of real estate regulations, and (ii) estimating the impact of Brexit on the stock returns of British and European companies."
http://arxiv.org/abs/2510.12911v1,Beyond Returns: A Candlestick-Based Approach to Spot Covariance Estimation,2025-10-14 18:34:53+00:00,['Yasin Simsek'],econ.EM,"Spot covariance estimation is commonly based on high-frequency open-to-close return data over short time windows, but such approaches face a trade-off between statistical accuracy and localization. In this paper, I introduce a new estimation framework using high-frequency candlestick data, which include open, high, low, and close prices, effectively addressing this trade-off. By exploiting the information contained in candlesticks, the proposed method improves estimation accuracy relative to benchmarks while preserving local structure. I further develop a test for spot covariance inference based on candlesticks that demonstrates reasonable size control and a notable increase in power, particularly in small samples. Motivated by recent work in the finance literature, I empirically test the market neutrality of the iShares Bitcoin Trust ETF (IBIT) using 1-minute candlestick data for the full year of 2024. The results show systematic deviations from market neutrality, especially in periods of market stress. An event study around FOMC announcements further illustrates the new method's ability to detect subtle shifts in response to relatively mild information events."
http://arxiv.org/abs/2510.14415v1,Evaluating Policy Effects under Network Interference without Network Information: A Transfer Learning Approach,2025-10-16 08:19:26+00:00,['Tadao Hoshino'],stat.ME,"This paper develops a sensitivity analysis framework that transfers the average total treatment effect (ATTE) from source data with a fully observed network to target data whose network is completely unknown. The ATTE represents the average social impact of a policy that assigns the treatment to every individual in the dataset. We postulate a covariate-shift type assumption that both source and target datasets share the same conditional mean outcome. However, because the target network is unobserved, this assumption alone is not sufficient to pin down the ATTE for the target data. To address this issue, we consider a sensitivity analysis based on the uncertainty of the target network's degree distribution, where the extent of uncertainty is measured by the Wasserstein distance from a given reference degree distribution. We then construct bounds on the target ATTE using a linear programming-based estimator. The limiting distribution of the bound estimator is derived via the functional delta method, and we develop a wild bootstrap approach to approximate the distribution. As an empirical illustration, we revisit the social network experiment on farmers' weather insurance adoption in China by Cai et al. (2015)."
http://arxiv.org/abs/2510.20863v1,"State capacity, innovation, and endogenous development in Chile",2025-10-22 22:40:39+00:00,['Rodrigo Barra Novoa'],econ.GN,"The study explores the evolution of Chile's industrial policy from 1990 to 2022 through the lens of state capacity, innovation and endogenous development. In a global context where governments are reasserting their role as active agents of innovation, Chile presents a paradox. It is a stable and open economy that has expanded investment in science and technology but still struggles to transform this effort into sustainable capabilities. Drawing on the works of Mazzucato, Aghion, Howitt, Mokyr, Samuelson and Sampedro, the study integrates evolutionary economics, public policy and humanist ethics. Using a longitudinal case study approach and official data, it finds that Chile has improved its innovation institutions but continues to experience weak coordination, regional inequality and a fragile culture of knowledge. The research concludes that achieving inclusive innovation requires adaptive governance and an ethical vision of innovation as a public good."
http://arxiv.org/abs/2510.07682v3,From tug-of-war to Brownian Boost: explicit ODE solutions for player-funded stochastic-differential games,2025-10-09 02:14:25+00:00,['Alan Hammond'],math.PR,"Brownian Boost is a one-parameter family of stochastic differential games played on the real line in which players spend at rates of their choosing in an ongoing effort to influence the drift of a randomly diffusing point particle~$X$. One or other player is rewarded, at time infinity, according to whether~$X$ tends to plus or minus infinity. Each player's net receipt is the final reward (only for the victor) minus the player's total spend. We characterise and explicitly compute the time-homogeneous Markov-perfect Nash equilibria of Brownian Boost, finding the derivatives of the players' expected payoffs to solve a pair of coupled first-order non-linear ODE. Brownian Boost is a high-noise limit of a two-dimensional family of player-funded tug-of-war games, one of which was studied in~\cite{LostPennies}. We analyse the discrete games, finding them, and Brownian Boost, to exemplify key features studied in the economics literature of tug-of-war initiated by~\cite{HarrisVickers87}: a battlefield region where players spend heavily;
  stakes that decay rapidly but asymmetrically in distance to the battlefield; and an effect of discouragement that makes equilibria fragile under asymmetric perturbation of incentive.
  Tug-of-war has a parallel mathematical literature derived from~\cite{PSSW09}, which solved the scaled fair-coin game in a Euclidean domain via the infinity Laplacian PDE. By offering an analytic solution to Brownian Boost, a game that models strategic interaction and resource allocation, we seek to build a bridge between the two tug-of-war literatures."
http://arxiv.org/abs/2510.22086v1,Social preferences or moral concerns: What drives rejections in the Ultimatum game?,2025-10-24 23:54:06+00:00,"['Pau Juan-Bartroli', 'José Ignacio Rivero-Wildemauwe']",econ.TH,"Rejections of positive offers in the Ultimatum Game have been attributed to different motivations. We show that a model combining social preferences and moral concerns provides a unifying explanation for these rejections while accounting for additional evidence. Under the preferences considered, a positive degree of spite is a necessary and sufficient condition for rejecting positive offers. This indicates that social preferences, rather than moral concerns, drive rejection behavior. This does not imply that moral concerns do not matter. We show that rejection thresholds increase with individuals' moral concerns, suggesting that morality acts as an amplifier of social preferences. Using data from van Leeuwen and Alger (2024), we estimate individuals' social preferences and moral concerns using a finite mixture approach. Consistent with previous evidence, we identify two types of individuals who reject positive offers in the Ultimatum Game, but that differ in their Dictator Game behavior."
http://arxiv.org/abs/2510.22232v1,Rational Adversaries and the Maintenance of Fragility: A Game-Theoretic Theory of Rational Stagnation,2025-10-25 09:28:15+00:00,['Daisuke Hirota'],cs.GT,"Cooperative systems often remain in persistently suboptimal yet stable states. This paper explains such ""rational stagnation"" as an equilibrium sustained by a rational adversary whose utility follows the principle of potential loss, $u_{D} = U_{ideal} - U_{actual}$. Starting from the Prisoner's Dilemma, we show that the transformation $u_{i}' = a\,u_{i} + b\,u_{j}$ and the ratio of mutual recognition $w = b/a$ generate a fragile cooperation band $[w_{\min},\,w_{\max}]$ where both (C,C) and (D,D) are equilibria. Extending to a dynamic model with stochastic cooperative payoffs $R_{t}$ and intervention costs $(C_{c},\,C_{m})$, a Bellman-style analysis yields three strategic regimes: immediate destruction, rational stagnation, and intervention abandonment. The appendix further generalizes the utility to a reference-dependent nonlinear form and proves its stability under reference shifts, ensuring robustness of the framework. Applications to social-media algorithms and political trust illustrate how adversarial rationality can deliberately preserve fragility."
http://arxiv.org/abs/2510.15121v1,A physically extended EEIO framework for material efficiency assessment in United States manufacturing supply chains,2025-10-16 20:26:41+00:00,"['Heather Liddell', 'Beth Kelley', 'Liz Wachs', 'Alberta Carpenter', 'Joe Cresko']",econ.GN,"A physical assessment of material flows in an economy (e.g., material flow quantification) can support the development of sustainable decarbonization and circularity strategies by providing the tangible physical context of industrial production quantities and supply chain relationships. However, completing a physical assessment is challenging due to the scarcity of high-quality raw data and poor harmonization across industry classification systems used in data reporting. Here we describe a new physical extension for the U.S. Department of Energy's (DOE's) EEIO for Industrial Decarbonization (EEIO-IDA) model, yielding an expanded EEIO model that is both physically and environmentally extended. In the model framework, the U.S. economy is divided into goods-producing and service-producing subsectors, and mass flows are quantified for each goods-producing subsector using a combination of trade data (e.g., UN Comtrade) and physical production data (e.g., U.S. Geological Survey). Given that primary-source production data are not available for all subsectors, price-imputation and mass-balance assumptions are developed and used to complete the physical flows dataset with high-quality estimations. The resulting dataset, when integrated with the EEIO-IDA tool, enables the quantification of environmental impact intensity metrics on a mass basis (e.g., CO$_2$eq/kg)) for each industrial subsector. This work is designed to align with existing DOE frameworks and tools, including the EEIO-IDA tool, the DOE Industrial Decarbonization Roadmap (2022), and Pathways for U.S. Industrial Transformations study (2025)."
http://arxiv.org/abs/2510.15420v1,"Heterogeneity among migrants, education-occupation mis-match and returns to education: Evidence from India",2025-10-17 08:14:41+00:00,"['Shweta Bahl', 'Ajay Sharma']",econ.GN,"Using nationally representative data for India, this paper examines the incidence of education occupation mismatch and returns to education and EOM for internal migrants while considering the heterogeneity among them. In particular, this study considers heterogeneity arising because of the reason to migrate, demographic characteristics, spatial factors, migration experience, and type of migration. The analysis reveals that there exists variation in the incidence and returns to EOM depending on the reason to migrate, demographic characteristics, and spatial factors. The study highlights the need of focusing on EOM to increase the productivity benefits of migration. It also provides the framework for minimizing migrants' likelihood of being mismatched while maximizing their returns to education."
http://arxiv.org/abs/2510.15399v1,International migration and dietary diversity of left-behind households: evidence from India,2025-10-17 07:51:26+00:00,"['Pooja Batra', 'Ajay Sharma']",econ.GN,"In this paper, we analyse the impact of international migration on the food consumption and dietary diversity of left-behind households. Using the Kerala migration survey 2011, we study whether households with emigrants (on account of international migration) have higher consumption expenditure and improved dietary diversity than their non-migrating counterparts. We use ordinary least square and instrumental variable approach to answer this question. The key findings are that: a) emigrant households have higher overall consumption expenditure as well as higher expenditure on food; b) we find that international migration leads to increase in the dietary diversity of left behind households. Further, we explore the effect on food sub-group expenditure for both rural and urban households. We find that emigrant households spend more on protein (milk, pulses and egg, fish and meat), at the same time there is higher spending on non-healthy food habits (processed and ready to eat food items) among them."
http://arxiv.org/abs/2510.15214v1,How to Sell High-Dimensional Data Optimally,2025-10-17 00:49:03+00:00,"['Andrew Li', 'R. Ravi', 'Karan Singh', 'Zihong Yi', 'Weizhong Zhang']",cs.GT,"Motivated by the problem of selling large, proprietary data, we consider an information pricing problem proposed by Bergemann et al. that involves a decision-making buyer and a monopolistic seller. The seller has access to the underlying state of the world that determines the utility of the various actions the buyer may take. Since the buyer gains greater utility through better decisions resulting from more accurate assessments of the state, the seller can therefore promise the buyer supplemental information at a price. To contend with the fact that the seller may not be perfectly informed about the buyer's private preferences (or utility), we frame the problem of designing a data product as one where the seller designs a revenue-maximizing menu of statistical experiments.
  Prior work by Cai et al. showed that an optimal menu can be found in time polynomial in the state space, whereas we observe that the state space is naturally exponential in the dimension of the data. We propose an algorithm which, given only sampling access to the state space, provably generates a near-optimal menu with a number of samples independent of the state space. We then analyze a special case of high-dimensional Gaussian data, showing that (a) it suffices to consider scalar Gaussian experiments, (b) the optimal menu of such experiments can be found efficiently via a semidefinite program, and (c) full surplus extraction occurs if and only if a natural separation condition holds on the set of potential preferences of the buyer."
http://arxiv.org/abs/2510.22750v1,Information-Credible Stability in Matching with Incomplete Information,2025-10-26 16:53:18+00:00,['Kaibalyapati Mishra'],econ.TH,"In this paper, I develop a refinement of stability for matching markets with incomplete information. I introduce Information-Credible Pairwise Stability (ICPS), a solution concept in which deviating pairs can use credible, costly tests to reveal match-relevant information before deciding whether to block. By leveraging the option value of information, ICPS strictly refines Bayesian stability, rules out fear-driven matchings, and connects belief-based and information-based notions of stability. ICPS collapses to Bayesian stability when testing is uninformative or infeasible and coincides with complete-information stability when testing is perfect and free. I show that any ICPS-blocking deviation strictly increases total expected surplus, ensuring welfare improvement. I also prove that ICPS-stable allocations always exist, promote positive assortative matching, and are unique when the test power is sufficiently strong. The framework extends to settings with non-transferable utility, correlated types, and endogenous or sequential testing."
http://arxiv.org/abs/2510.22714v1,Pairwise Difference Representations of Moments: Gini and Generalized Lagrange identities,2025-10-26 15:16:54+00:00,"['Jean-Marie Dufour', 'Abderrahim Taamouti', 'Meilin Tong']",stat.ME,"We provide pairwise-difference (Gini-type) representations of higher-order central moments for both general random variables and empirical moments. Such representations do not require a measure of location. For third and fourth moments, this yields pairwise-difference representations of skewness and kurtosis coefficients. We show that all central moments possess such representations, so no reference to the mean is needed for moments of any order. This is done by considering i.i.d. replications of the random variables considered, by observing that central moments can be interpreted as covariances between a random variable and powers of the same variable, and by giving recursions which link the pairwise-difference representation of any moment to lower order ones. Numerical summation identities are deduced. Through a similar approach, we give analogues of the Lagrange and Binet-Cauchy identities for general random variables, along with a simple derivation of the classic Cauchy-Schwarz inequality for covariances. Finally, an application to unbiased estimation of centered moments is discussed."
http://arxiv.org/abs/2510.22884v1,"Identification, Estimation, and Inference in Two-Sided Interaction Models",2025-10-27 00:21:43+00:00,['Federico Crippa'],econ.EM,"This paper studies a class of models for two-sided interactions, where outcomes depend on latent characteristics of two distinct agent types. Models in this class have two core elements: the matching network, which records which agent pairs interact, and the interaction function, which maps latent characteristics of these agents to outcomes and determines the role of complementarities. I introduce the Tukey model, which captures complementarities with a single interaction parameter, along with two extensions that allow richer complementarity patterns. First, I establish an identification trade-off between the flexibility of the interaction function and the density of the matching network: the Tukey model is identified under mild conditions, whereas the more flexible extensions require dense networks that are rarely observed in applications. Second, I propose a cycle-based estimator for the Tukey interaction parameter and show that it is consistent and asymptotically normal even when the network is sparse. Third, I use its asymptotic distribution to construct a formal test of no complementarities. Finally, an empirical illustration shows that the Tukey model recovers economically meaningful complementarities."
http://arxiv.org/abs/2510.26503v1,The sustainability of contribution norms with income dynamics,2025-10-30 13:55:56+00:00,"['Pau Juan-Bartroli', 'Esteban Muñoz-Sobrado']",econ.GN,"The sustainability of cooperation is crucial for understanding the progress of societies. We study a repeated game in which individuals decide the share of their income to transfer to other group members. A central feature of our model is that individuals may, with some probability, switch incomes across periods, our measure of income mobility, while the overall income distribution remains constant over time. We analyze how income mobility and income inequality affect the sustainability of contribution norms, informal agreements about how much each member should transfer to the group. We find that greater income mobility facilitates cooperation. In contrast, the effect of inequality is ambiguous and depends on the progressivity of the contribution norm and the degree of mobility. We apply our framework to an optimal taxation problem to examine the interaction between public and private redistribution."
http://arxiv.org/abs/2510.26051v1,Estimation and Inference in Boundary Discontinuity Designs: Distance-Based Methods,2025-10-30 01:03:57+00:00,"['Matias D. Cattaneo', 'Rocio Titiunik', 'Ruiqi', 'Yu']",econ.EM,"We study the statistical properties of nonparametric distance-based (isotropic) local polynomial regression estimators of the boundary average treatment effect curve, a key causal functional parameter capturing heterogeneous treatment effects in boundary discontinuity designs. We present necessary and/or sufficient conditions for identification, estimation, and inference in large samples, both pointwise and uniformly along the boundary. Our theoretical results highlight the crucial role played by the ``regularity'' of the boundary (a one-dimensional manifold) over which identification, estimation, and inference are conducted. Our methods are illustrated with simulated data. Companion general-purpose software is provided."
http://arxiv.org/abs/2510.24496v1,Panel data models with randomly generated groups,2025-10-28 15:12:15+00:00,"['Jean-Pierre Florens', 'Anna Simoni']",econ.EM,"We develop a structural framework for modeling and inferring unobserved heterogeneity in dynamic panel-data models. Unlike methods treating clustering as a descriptive device, we model heterogeneity as arising from a latent clustering mechanism, where the number of clusters is unknown and estimated. Building on the mixture of finite mixtures (MFM) approach, our method avoids the clustering inconsistency issues of Dirichlet process mixtures and provides an interpretable representation of the population clustering structure. We extend the Telescoping Sampler of Fruhwirth-Schnatter et al. (2021) to dynamic panels with covariates, yielding an efficient MCMC algorithm that delivers full Bayesian inference and credible sets. We show that asymptotically the posterior distribution of the mixing measure contracts around the truth at parametric rates in Wasserstein distance, ensuring recovery of clustering and structural parameters. Simulations demonstrate strong finite-sample performance. Finally, an application to the income-democracy relationship reveals latent heterogeneity only when controlling for additional covariates."
http://arxiv.org/abs/2510.24433v1,Nearest Neighbor Matching as Least Squares Density Ratio Estimation and Riesz Regression,2025-10-28 14:01:51+00:00,['Masahiro Kato'],econ.EM,"This study proves that Nearest Neighbor (NN) matching can be interpreted as an instance of Riesz regression for automatic debiased machine learning. Lin et al. (2023) shows that NN matching is an instance of density-ratio estimation with their new density-ratio estimator. Chernozhukov et al. (2024) develops Riesz regression for automatic debiased machine learning, which directly estimates the Riesz representer (or equivalently, the bias-correction term) by minimizing the mean squared error. In this study, we first prove that the density-ratio estimation method proposed in Lin et al. (2023) is essentially equivalent to Least-Squares Importance Fitting (LSIF) proposed in Kanamori et al. (2009) for direct density-ratio estimation. Furthermore, we derive Riesz regression using the LSIF framework. Based on these results, we derive NN matching from Riesz regression. This study is based on our work Kato (2025a) and Kato (2025b)."
http://arxiv.org/abs/2510.25275v1,New methods to compensate artists in music streaming platforms,2025-10-29 08:36:25+00:00,"['Gustavo Bergantiños', 'Juan D. Moreno-Ternero']",econ.TH,"We study the problem of measuring the popularity of artists in music streaming platforms and the ensuing methods to compensate them (from the revenues platforms raise by charging users). We uncover the space of popularity indices upon exploring the implications of several axioms capturing principles with normative appeal. As a result, we characterize several families of indices. Some of them are intimately connected to the Shapley value, the central tool in cooperative game theory. Our characterizations might help to address the rising concern in the music industry to explore new methods that reward artists more appropriately. We actually connect our families to the new royalties models, recently launched by Spotify and Deezer."
http://arxiv.org/abs/2510.24916v1,Productivity Beliefs and Efficiency in Science,2025-10-28 19:36:59+00:00,"['Fabio Bertolotti', 'Kyle Myers', 'Wei Yang Tham']",econ.GN,"We develop a method to estimate producers' productivity beliefs when output quantities and input prices are unobservable, and we use it to evaluate the market for science. Our model of researchers' labor supply shows how their willingness to pay for inputs reveals their productivity beliefs. We estimate the model's parameters using data from a nationally representative survey of researchers and find the distribution of productivity to be very skewed. Our counterfactuals indicate that a more efficient allocation of the current budget could be worth billions of dollars. There are substantial gains from developing new ways of identifying talented scientists."
http://arxiv.org/abs/2510.24899v1,Estimating Nationwide High-Dosage Tutoring Expenditures: A Predictive Model Approach,2025-10-28 19:02:07+00:00,"['Jason Godfrey', 'Trisha Banerjee']",econ.GN,"This study applies an optimized XGBoost regression model to estimate district-level expenditures on high-dosage tutoring from incomplete administrative data. The COVID-19 pandemic caused unprecedented learning loss, with K-12 students losing up to half a grade level in certain subjects. To address this, the federal government allocated \$190 billion in relief. We know from previous research that small-group tutoring, summer and after school programs, and increased support staff were all common expenditures for districts. We don't know how much was spent in each category. Using a custom scraped dataset of over 7,000 ESSER (Elementary and Secondary School Emergency Relief) plans, we model tutoring allocations as a function of district characteristics such as enrollment, total ESSER funding, urbanicity, and school count. Extending the trained model to districts that mention tutoring but omit cost information yields an estimated aggregate allocation of approximately \$2.2 billion. The model achieved an out-of-sample $R^2$=0.358, demonstrating moderate predictive accuracy given substantial reporting heterogeneity. Methodologically, this work illustrates how gradient-boosted decision trees can reconstruct large-scale fiscal patterns where structured data are sparse or missing. The framework generalizes to other domains where policy evaluation depends on recovering latent financial or behavioral variables from semi-structured text and sparse administrative sources."
http://arxiv.org/abs/2510.24735v1,Learning to Unlearn: Education as a Remedy for Misspecified Beliefs,2025-10-14 15:35:30+00:00,"['Daria Fedyaeva', 'Georgy Lukyanov', 'Hannah Tollié']",econ.TH,"We study education as a remedy for misspecified beliefs in a canonical sequential social-learning model. Uneducated agents misinterpret action histories - treating actions as if they were independent signals and, potentially, overstating signal precision - while educated agents use the correct likelihoods (and may also enjoy higher private precision). We define a misspecified-belief PBE and show existence with a simple structure: education is a cutoff in the realized cost and actions are threshold rules in a single log-likelihood index. A closed-form value-of-education statistic compares the accuracy of the educated versus uneducated decision at any history; this yields transparent conditions for self-education. When a misspecified process sustains an incorrect cascade, uniformly positive private value and a positive flip probability imply that education breaks the cascade almost surely in finite time, with an explicit bound on expected break time. We quantify welfare gains from making education available and show how small per-education subsidies sharply raise de-cascading probabilities and improve discounted welfare. Extensions cover imperfect observability of education choices and a planner who deploys history-dependent subsidies."
http://arxiv.org/abs/2510.26030v1,World personal income distribution evolution measured by purchasing power parity exchange rates,2025-10-30 00:10:35+00:00,"['J. D. A. Islas-García', 'M. del Castillo-Mussot', 'Marcelo B. Ribeiro']",econ.GN,"The evolution of global income distribution from 1988 to 2018 is analyzed using purchasing power parity exchange rates and well-established statistical distributions. This research proposes the use of two separate distributions to more accurately represent the overall data, rather than relying on a single distribution. The global income distribution was fitted to log-normal and gamma functions, which are standard tools in econophysics. Despite limitations in data completeness during the early years, the available information covered the vast majority of the world's population. Probability density function (PDF) curves enabled the identification of key peaks in the distribution, while complementary cumulative distribution function (CCDF) curves highlighted general trends in inequality. Initially, the global income distribution exhibited a bimodal pattern; however, the growth of middle classes in highly populated countries such as China and India has driven the transition to a unimodal distribution in recent years. While single-function fits with gamma or log-normal distributions provided reasonable accuracy, the bimodal approach constructed as a sum of log-normal distributions yielded near-perfect fits."
http://arxiv.org/abs/2510.26810v1,Emergent Dynamical Spatial Boundaries in Emergency Medical Services: A Navier-Stokes Framework from First Principles,2025-10-23 11:04:32+00:00,['Tatsuru Kikuchi'],stat.AP,"Emergency medical services (EMS) response times are critical determinants of patient survival, yet existing approaches to spatial coverage analysis rely on discrete distance buffers or ad-hoc geographic information system (GIS) isochrones without theoretical foundation. This paper derives continuous spatial boundaries for emergency response from first principles using fluid dynamics (Navier-Stokes equations), demonstrating that response effectiveness decays exponentially with time: $τ(t) = τ_0 \exp(-κt)$, where $τ_0$ is baseline effectiveness and $κ$ is the temporal decay rate. Using 10,000 simulated emergency incidents from the National Emergency Medical Services Information System (NEMSIS), I estimate decay parameters and calculate critical boundaries $d^*$ where response effectiveness falls below policy-relevant thresholds. The framework reveals substantial demographic heterogeneity: elderly populations (85+) experience 8.40-minute average response times versus 7.83 minutes for younger adults (18-44), with 33.6\% of poor-access incidents affecting elderly populations despite representing 5.2\% of the sample. Non-parametric kernel regression validation confirms exponential decay is appropriate (mean squared error 8-12 times smaller than parametric), while traditional difference-in-differences analysis validates treatment effect existence (DiD coefficient = -1.35 minutes, $p < 0.001$). The analysis identifies vulnerable populations--elderly, rural, and low-income communities--facing systematically longer response times, informing optimal EMS station placement and resource allocation to reduce health disparities."
http://arxiv.org/abs/2510.20066v1,A Multi-Layer Machine Learning and Econometric Pipeline for Forecasting Market Risk: Evidence from Cryptoasset Liquidity Spillovers,2025-10-22 22:36:34+00:00,"['Yimeng Qiu', 'Feihuang Fang']",cs.LG,"We study whether liquidity and volatility proxies of a core set of cryptoassets generate spillovers that forecast market-wide risk. Our empirical framework integrates three statistical layers: (A) interactions between core liquidity and returns, (B) principal-component relations linking liquidity and returns, and (C) volatility-factor projections that capture cross-sectional volatility crowding. The analysis is complemented by vector autoregression impulse responses and forecast error variance decompositions (see Granger 1969; Sims 1980), heterogeneous autoregressive models with exogenous regressors (HAR-X, Corsi 2009), and a leakage-safe machine learning protocol using temporal splits, early stopping, validation-only thresholding, and SHAP-based interpretation. Using daily data from 2021 to 2025 (1462 observations across 74 assets), we document statistically significant Granger-causal relationships across layers and moderate out-of-sample predictive accuracy. We report the most informative figures, including the pipeline overview, Layer A heatmap, Layer C robustness analysis, vector autoregression variance decompositions, and the test-set precision-recall curve. Full data and figure outputs are provided in the artifact repository."
http://arxiv.org/abs/2510.26723v2,Bridging the Gap between Empirical Welfare Maximization and Conditional Average Treatment Effect Estimation in Policy Learning,2025-10-30 17:23:40+00:00,['Masahiro Kato'],stat.ML,"The goal of policy learning is to train a policy function that recommends a treatment given covariates to maximize population welfare. There are two major approaches in policy learning: the empirical welfare maximization (EWM) approach and the plug-in approach. The EWM approach is analogous to a classification problem, where one first builds an estimator of the population welfare, which is a functional of policy functions, and then trains a policy by maximizing the estimated welfare. In contrast, the plug-in approach is based on regression, where one first estimates the conditional average treatment effect (CATE) and then recommends the treatment with the highest estimated outcome. This study bridges the gap between the two approaches by showing that both are based on essentially the same optimization problem. In particular, we prove an exact equivalence between EWM and least squares over a reparameterization of the policy class. As a consequence, the two approaches are interchangeable in several respects and share the same theoretical guarantees under common conditions. Leveraging this equivalence, we propose a regularization method for policy learning. The reduction to least squares yields a smooth surrogate that is typically easier to optimize in practice. At the same time, for many natural policy classes the inherent combinatorial hardness of exact EWM generally remains, so the reduction should be viewed as an optimization aid rather than a universal bypass of NP-hardness."
http://arxiv.org/abs/2511.03424v1,The moment is here: a generalised class of estimators for fuzzy regression discontinuity designs,2025-11-05 12:40:34+00:00,['Stuart Lane'],econ.EM,"The standard fuzzy regression discontinuity (FRD) estimator is a ratio of differences of local polynomial estimators. I show that this estimator does not have finite moments of any order in finite samples, regardless of the choice of kernel function, bandwidth, or order of polynomial. This leads to an imprecise estimator with a heavy-tailed sampling distribution, and inaccurate inference with small sample sizes or when the discontinuity in the probability of treatment assignment at the cutoff is small. I present a generalised class of computationally simple FRD estimators, which contains a continuum of estimators with finite moments of all orders in finite samples, and nests both the standard FRD and sharp (SRD) estimators. The class is indexed by a single tuning parameter, and I provide simple values that lead to substantial improvements in median bias, median absolute deviation and root mean squared error. These new estimators remain very stable in small samples, or when the discontinuity in the probability of treatment assignment at the cutoff is small. Simple confidence intervals that have strong coverage and length properties in small samples are also developed. The improvements are seen across a wide range of models and using common bandwidth selection algorithms in extensive Monte Carlo simulations. The improved stability and performance of the estimators and confidence intervals is also demonstrated using data on class size effects on educational attainment."
http://arxiv.org/abs/2511.04348v1,Regime Changes and Real-Financial Cycles: Searching Minsky's Hypothesis in a Nonlinear Setting,2025-11-06 13:28:24+00:00,"['Domenico delli Gatti', 'Filippo Gusella', 'Giorgio Ricchiuti']",econ.GN,"This paper investigates Minsky's cycles by extending the paper of stockhammer et al. (2019) with a nonlinear model to capture possible local real-financial endogenous cycles. We trace nonlinear regime changes and check the presence of Minsky cycles from the 1970s to 2020 for the USA, France, Germany, Canada, Australia, and the UK, linking the GDP with corporate debt, interest rate, and household debt. When considering corporate debt, the results reveal real-financial endogenous cycles in all countries, except Australia, and across all countries when interest rates are included. We find evidence for an interaction mechanism between household debt and GDP only for the USA and the UK. These findings underscore the importance of nonlinear regime transitions in empirically assessing Minsky's theory."
http://arxiv.org/abs/2511.00944v1,On the estimation of leverage effect and volatility of volatility in the presence of jumps,2025-11-02 14:05:48+00:00,"['Qiang Liu', 'Zhi Liu', 'Wang Zhou']",stat.ME,"We study the estimation of leverage effect and volatility of volatility by using high-frequency data with the presence of jumps. We first construct spot volatility estimator by using the empirical characteristic function of the high-frequency increments to deal with the effect of jumps, based on which the estimators of leverage effect and volatility of volatility are proposed. Compared with existing estimators, our method is valid under more general jumps, making it a better alternative for empirical applications. Under some mild conditions, the asymptotic normality of the estimators is established and consistent estimators of the limiting variances are proposed based on the estimation of volatility functionals. We conduct extensive simulation study to verify the theoretical results. The results demonstrate that our estimators have relative better performance than the existing ones, especially when the jump is of infinite variation. Besides, we apply our estimators to a real high-frequency dataset, which reveals nonzero leverage effect and volatility of volatility in the market."
http://arxiv.org/abs/2511.01565v1,Gendered Responses to Subtle Social Pressure: Experimental Evidence from Survey Results,2025-11-03 13:32:16+00:00,['Sevgi Çolak'],econ.GN,"This study analyzes whether subtle variations in the survey questionnaire phrasing influence participant engagement and whether these effects differ by gender. Building on theories of social pressure and politeness norms, it is hypothesized that presumptive phrasing would reduce engagement compared to appreciative phrasing and baseline phrasing (H1), and this effect would be more pronounced among women (H2). Mixed-effects regression models showed no significant treatment effects on any outcome and no evidence of gender moderation for 164 participants and 492 observations. The only robust finding was a small negative baseline sentiment across all participants, independent of any treatment or gender. The findings contribute to refining theoretical expectations about the conditions in which linguistic framing and gender norms shape behaviour."
http://arxiv.org/abs/2511.01133v1,"Liquidity Shocks, Homeownership, and Income Inequality: Impact of Early Pension Withdrawals and Reduced Deposit",2025-11-03 00:52:08+00:00,"['Hamza Hanbali', 'Gaurav Khemka', 'Himasha Warnakulasooriya']",econ.GN,"The paper analyzes two government policies affecting housing demand: early withdrawal from pension savings (EW), and reduction of loan deposit (RD). A model incorporating demand feedback on housing prices using Australian data shows both policies raise prices in the short run. RD delays or prevents access for low-income households, particularly in supply-constrained markets. EW improves accessibility across groups and is most efficient when full withdrawal is permitted, but can reduce retirement security if pension grows faster than property prices. The results also indicate that unequal outcomes stem not from price surges themselves but from pre-existing market disparities."
http://arxiv.org/abs/2511.03091v1,The Economics of Spatial Coordination in Critical Infrastructure Investment,2025-11-05 00:36:15+00:00,"['L Kaili Diamond', 'Benjamin Gilbert']",econ.EM,"We develop a hybrid approach to estimate spatial coordination mechanisms in structural dynamic discrete choice models by combining nested fixed-point (NFXP) dynamic programming with method of simulated moments (MSM), achieving computational tractability in spatial settings while preserving structural interpretation. Applying this framework to GPU replacement data from 12,915 equipment locations in Oak Ridge National Laboratory's Titan supercomputer, we identify two distinct coordination mechanisms: sequential replacement cascades (gamma_lag = -0.793) and contemporaneous failure batching (gamma_fail = -0.265). Sequential coordination dominates - approximately three times stronger than failure batching - indicating that operators engage in deliberate strategic behavior rather than purely reactive responses. Spatial interdependencies account for 5.3% of variation unexplained by independent-decision models, with coordination concentrated in high-risk thermal environments exhibiting effects more than 10 times stronger than cool zones. Formal tests decisively reject spatial independence (chi-squared(2) = 685.38, p < 0.001), demonstrating that infrastructure policies ignoring spatial coordination will systematically mistime interventions and forgo available coordination gains."
http://arxiv.org/abs/2511.03145v1,"Balanced contributions, consistency, and value for games with externalities",2025-11-05 03:08:37+00:00,"['André Casajus', 'Yukihiko Funaki', 'Frank Huettner']",econ.TH,"We consider fair and consistent extensions of the Shapley value for games with externalities. Based on the restriction identified by Casajus et al. (2024, Games Econ. Behavior 147, 88-146), we define balanced contributions, Sobolev's consistency, and Hart and Mas-Colell's consistency for games with externalities, and we show that these properties lead to characterizations of the generalization of the Shapley value introduced by Macho-Stadler et al. (2007, J. Econ. Theory 135, 339-356), that parallel important characterizations of the Shapley value."
http://arxiv.org/abs/2510.09859v2,Token Is All You Price,2025-10-10 20:49:31+00:00,['Weijie Zhong'],econ.TH,"We build a mechanism design framework where a platform designs GenAI models to screen users who obtain instrumental value from the generated conversation and privately differ in their preference for latency. We show that the revenue-optimal mechanism is simple: deploy a single aligned (user-optimal) model and use token cap as the only instrument to screen the user. The design decouples model training from pricing, is readily implemented with token metering, and mitigates misalignment pressures."
http://arxiv.org/abs/2510.22864v1,Unifying regression-based and design-based causal inference in time-series experiments,2025-10-26 23:12:17+00:00,"['Zhexiao Lin', 'Peng Ding']",stat.ME,"Time-series experiments, also called switchback experiments or N-of-1 trials, play increasingly important roles in modern applications in medical and industrial areas. Under the potential outcomes framework, recent research has studied time-series experiments from the design-based perspective, relying solely on the randomness in the design to drive the statistical inference. Focusing on simpler statistical methods, we examine the design-based properties of regression-based methods for estimating treatment effects in time-series experiments. We demonstrate that the treatment effects of interest can be consistently estimated using ordinary least squares with an appropriately specified working model and transformed regressors. Our analysis allows for estimating a diverging number of treatment effects simultaneously, and establishes the consistency and asymptotic normality of the regression-based estimators. Additionally, we show that asymptotically, the heteroskedasticity and autocorrelation consistent variance estimators provide conservative estimates of the true, design-based variances. Importantly, although our approach relies on regression, our design-based framework allows for misspecification of the regression model."
http://arxiv.org/abs/2511.02458v1,Prompting for Policy: Forecasting Macroeconomic Scenarios with Synthetic LLM Personas,2025-11-04 10:38:10+00:00,"['Giulia Iadisernia', 'Carolina Camassa']",cs.CL,"We evaluate whether persona-based prompting improves Large Language Model (LLM) performance on macroeconomic forecasting tasks. Using 2,368 economics-related personas from the PersonaHub corpus, we prompt GPT-4o to replicate the ECB Survey of Professional Forecasters across 50 quarterly rounds (2013-2025). We compare the persona-prompted forecasts against the human experts panel, across four target variables (HICP, core HICP, GDP growth, unemployment) and four forecast horizons. We also compare the results against 100 baseline forecasts without persona descriptions to isolate its effect. We report two main findings. Firstly, GPT-4o and human forecasters achieve remarkably similar accuracy levels, with differences that are statistically significant yet practically modest. Our out-of-sample evaluation on 2024-2025 data demonstrates that GPT-4o can maintain competitive forecasting performance on unseen events, though with notable differences compared to the in-sample period. Secondly, our ablation experiment reveals no measurable forecasting advantage from persona descriptions, suggesting these prompt components can be omitted to reduce computational costs without sacrificing accuracy. Our results provide evidence that GPT-4o can achieve competitive forecasting accuracy even on out-of-sample macroeconomic events, if provided with relevant context data, while revealing that diverse prompts produce remarkably homogeneous forecasts compared to human panels."
http://arxiv.org/abs/2511.01877v1,Overprocurement of balancing capacity may increase the welfare in the cross-zonal energy-reserve coallocation problem,2025-10-22 12:01:55+00:00,"['Dávid Csercsik', 'Ádám Sleisz']",q-fin.GN,"When the traded energy and reserve products between zones are co-allocated to optimize the infrastructure usage, both deterministic and stochastic flows have to be accounted for on interconnector lines. We focus on allocation models, which guarantee deliverability in the context of the portfolio bidding European day-ahead market framework, assuming a flow-based description of network constraints. In such models, as each unit of allocated reserve supply implies additional cost, it is straightforward to assume that the amount of allocated reserve is equal to the accepted reserve demand quantity. However, as it is illustrated by the proposed work, overprocurement of reserves may imply counterintuitive benefits. Reserve supplies not used for balancing may be used for congestion management, thus allowing valuable additional flows in the network."
http://arxiv.org/abs/2511.02816v1,Sufficient Statistics for Markovian Feedback Processes and Unobserved Heterogeneity in Dynamic Panel Logit Models,2025-11-04 18:40:35+00:00,['Sukgyu Shin'],econ.EM,"In this paper, we examine identification in a dynamic panel logit model with state dependence, first-order Markov feedback processes, and individual unobserved heterogeneity by introducing sufficient statistics for the feedback process and unobserved heterogeneity. If a sequentially exogenous discrete covariate follows a first-order Markov process, identification of the coefficient on the covariate via conditional likelihood is infeasible, whereas identification of the coefficient on the lagged dependent variable is feasible when there are at least three periods after the initial-condition period. If the feedback depends only on the lagged dependent variable, the coefficient on the covariate is identified with at least two periods, and the coefficient on the lagged dependent variable is identified with at least three periods."
http://arxiv.org/abs/2511.02792v1,The Bias-Variance Tradeoff in Long-Term Experimentation,2025-11-04 18:15:12+00:00,"['Daniel Ting', 'Kenneth Hung']",stat.ME,"As we exhaust methods that reduces variance without introducing bias, reducing variance in experiments often requires accepting some bias, using methods like winsorization or surrogate metrics. While this bias-variance tradeoff can be optimized for individual experiments, bias may accumulate over time, raising concerns for long-term optimization. We analyze whether bias is ever acceptable when it can accumulate, and show that a bias-variance tradeoff persists in long-term settings. Improving signal-to-noise remains beneficial, even if it introduces bias. This implies we should shift from thinking there is a single ``correct'', unbiased metric to thinking about how to make the best estimates and decisions when better precision can be achieved at the expense of bias.
  Furthermore, our model adds nuance to previous findings that suggest less stringent launch criterion leads to improved gains. We show while this is beneficial when the system is far from the optimum, more stringent launch criterion is preferable as the system matures."
http://arxiv.org/abs/2511.03572v1,Leniency Designs: An Operator's Manual,2025-11-05 15:53:19+00:00,"['Paul Goldsmith-Pinkham', 'Peter Hull', 'Michal Kolesár']",econ.EM,"We develop a step-by-step guide to leniency (a.k.a. judge or examiner instrument) designs, drawing on recent econometric literatures. The unbiased jackknife instrumental variables estimator (UJIVE) is purpose-built for leveraging exogenous leniency variation, avoiding subtle biases even in the presence of many decision-makers or controls. We show how UJIVE can also be used to assess key assumptions underlying leniency designs, including quasi-random assignment and average first-stage monotonicity, and to probe the external validity of treatment effect estimates. We further discuss statistical inference, arguing that non-clustered standard errors are often appropriate. A reanalysis of Farre-Mensa et al. (2020), using quasi-random examiner assignment to estimate the value of patents to startups, illustrates our checklist."
http://arxiv.org/abs/2510.05551v2,Correcting sample selection bias with categorical outcomes,2025-10-07 03:38:24+00:00,['Onil Boussim'],econ.EM,"In this paper, I propose a method for correcting sample selection bias when the outcome of interest is categorical, such as occupational choice, health status, or field of study. Classical approaches to sample selection rely on strong parametric distributional assumptions, which may be restrictive in practice. I develop a local representation that decomposes each joint probability into marginal probabilities and a category-specific association parameter that captures how selection differentially affects each outcome. Under some exclusion restrictions, I establish nonparametric point identification of the latent categorical distribution. Building on this identification result, I introduce a semiparametric multinomial logit model with sample selection, propose a computationally tractable two-step estimator, and derive its asymptotic properties. I illustrate the method by studying the determinants of healthcare utilization in Côte d'Ivoire."
http://arxiv.org/abs/2511.05870v1,Synthetic Parallel Trends,2025-11-08 06:10:24+00:00,['Yiqi Liu'],econ.EM,"Popular empirical strategies for policy evaluation in the panel data literature -- including difference-in-differences (DID), synthetic control (SC) methods, and their variants -- rely on key identifying assumptions that can be expressed through a specific choice of weights $ω$ relating pre-treatment trends to the counterfactual outcome. While each choice of $ω$ may be defensible in empirical contexts that motivate a particular method, it relies on fundamentally untestable and often fragile assumptions. I develop an identification framework that allows for all weights satisfying a Synthetic Parallel Trends assumption: the treated unit's trend is parallel to a weighted combination of control units' trends for a general class of weights. The framework nests these existing methods as special cases and is by construction robust to violations of their respective assumptions. I construct a valid confidence set for the identified set of the treatment effect, which admits a linear programming representation with estimated coefficients and nuisance parameters that are profiled out. In simulations where the assumptions underlying DID or SC-based methods are violated, the proposed confidence set remains robust and attains nominal coverage, while existing methods suffer severe undercoverage."
http://arxiv.org/abs/2510.23434v2,Choosing What to Learn: Experimental Design when Combining Experimental with Observational Evidence,2025-10-27 15:37:23+00:00,"['Aristotelis Epanomeritakis', 'Davide Viviano']",econ.EM,"Experiments deliver credible but often localized effects, tied to specific sites, populations, or mechanisms. When such estimates are insufficient to extrapolate effects for broader policy questions, such as external validity and general-equilibrium (GE) effects, researchers combine trials with external evidence from reduced-form or structural observational estimates, or prior experiments. We develop a unified framework for designing experiments in this setting: the researcher selects which parameters (or moments) to identify experimentally from a feasible set (e.g., which treatment arms and/or individuals to include in the experiment), allocates sample size, and specifies how to weight experimental and observational estimators. Because observational inputs may be biased in ways unknown ex ante, we develop a minimax proportional regret objective that evaluates any candidate design relative to an oracle that knows the bias and jointly chooses the design and estimator. This yields a transparent bias-variance trade-off that requires no prespecified bias bound and depends only on information about the precision of the estimators and the estimand's sensitivity to the underlying parameters. We illustrate the framework by (i) designing small-scale cash transfer experiments aimed at estimating GE effects and (ii) optimizing site selection for microfinance interventions."
http://arxiv.org/abs/2511.05128v1,Do Test Scores Help Teachers Give Better Track Advice to Students? A Principal Stratification Analysis,2025-11-07 10:14:03+00:00,"['Andrea Ichino', 'Fabrizia Mealli', 'Javier Viviens']",econ.EM,"We study whether access to standardized test scores improves the quality of teachers' secondary school track recommendations, using Dutch data and a metric based on Principal Stratification in a quasi-randomized setting. Allowing teachers to revise their recommendations when test results exceed expectations increases the share of students successfully placed in more demanding tracks by at least 6%, but misplaces 7% of weaker students. However, only implausibly high weights on the short-term losses of students who must change track because of misplacement would justify prohibiting test-score-based upgrades. Access to test scores also induces fairer recommendations for immigrant and low-SES students."
http://arxiv.org/abs/2510.27112v1,Targeted Advertising Platforms: Data Sharing and Customer Poaching,2025-10-31 02:25:36+00:00,['Klajdi Hoxha'],econ.TH,"E-commerce platforms are rolling out ambitious targeted advertising initiatives that rely on merchants sharing customer data with each other via the platform. Yet current platform designs fail to address participating merchants' concerns about customer poaching. This paper proposes a model of designing targeted advertising platforms that incentivizes merchants to voluntarily share customer data despite poaching concerns. I characterize the optimal mechanism that maximizes a weighted sum of platform's revenues, customer engagement and merchants' surplus. In sufficiently large platforms, the optimal mechanism can be implemented through the design of three markets: $i)$ selling market, where merchants can sell all their data at a posted price $p$, $ii)$ exchange market, where merchants share all their data in exchange for high click-through rate (CTR) ads, and $iii)$ buying market, where high-value merchants buy high CTR ads at the full price. The model is broad in scope with applications in other market design settings like the greenhouse gas credit markets and reallocating public resources, and points toward new directions in combinatorial market exchange designs."
http://arxiv.org/abs/2510.12049v2,Generative AI and Firm Productivity: Field Experiments in Online Retail,2025-10-14 01:17:09+00:00,"['Lu Fang', 'Zhe Yuan', 'Kaifu Zhang', 'Dante Donati', 'Miklos Sarvary']",econ.GN,"We quantify the impact of Generative Artificial Intelligence (GenAI) on firm productivity through a series of large-scale randomized field experiments involving millions of users and products at a leading cross-border online retail platform. Over six months in 2023-2024, GenAI-based enhancements were integrated into seven consumer-facing business workflows. We find that GenAI adoption significantly increases sales, with treatment effects ranging from $0\%$ to $16.3\%$, depending on GenAI's marginal contribution relative to existing firm practices. Because inputs and prices were held constant across experimental arms, these gains map directly into total factor productivity improvements. Across the four GenAI applications with positive effects, the implied annual incremental value is approximately $\$ 5$ per consumer-an economically meaningful impact given the retailer's scale and the early stage of GenAI adoption. The primary mechanism operates through higher conversion rates, consistent with GenAI reducing frictions in the marketplace and improving consumer experience. We also document substantial heterogeneity: smaller and newer sellers, as well as less experienced consumers, exhibit disproportionately larger gains. Our findings provide novel, large-scale causal evidence on the productivity effects of GenAI in online retail, highlighting both its immediate value and broader potential."
http://arxiv.org/abs/2511.05599v1,0.001% and Counting: Revisiting the Price Rounding Tax,2025-11-05 22:14:58+00:00,"['Doron Sayag', 'Avichai Snir', 'Daniel Levy']",econ.GN,"In 1991 and 2008, Israel abolished the equivalents of 1-cent and 5-cent coins, respectively, effectively eliminating low-denomination coins and introducing rounding in cash transactions. When totals were rounded up, shoppers incurred a small rounding tax. Using detailed data on price endings and basket sizes across supermarkets, drugstores, small groceries, and convenience stores, we estimate that the magnitude of the rounding tax borne by Israeli consumers averaged only between 0.001 percent and 0.002 percent of revenues in the fast-moving consumer goods markets. These findings have implications for the ongoing debate regarding the desirability and viability of abolishing the 1-cent and 5-cent coins in the US."
http://arxiv.org/abs/2511.05515v1,The Breadth Premium: Measuring the Firm-level Impact of CEO Career Breadth,2025-10-26 14:52:59+00:00,['T. Alexander Puutio'],q-fin.GN,"Prevailing career and education systems continue to reward early specialization and deep expertise within narrow domains. While such depth promotes efficiency, it may also limit adaptability in complex and rapidly changing environments. Building on research showing that variability in training inputs enhances learning outcomes across cognitive and behavioral domains, this study explores whether the same principle applies to executive performance.
  Using an original dataset of 650 CEOs leading firms that together represent roughly 85% of US market capitalization, we construct a composite Breadth Index capturing cross-domain educational and professional breadth. Preliminary analyses reveal that firms led by higher-breadth CEOs outperform their industry peers by an average of 9.8 percentage points over a three-year window. Regression results indicate that each one-point increase on the five-point Range Index corresponds to a 1.8-point gain in abnormal returns (p < 0.03), with effects remaining robust across industries, firm sizes, and CEO age groups.
  These early findings suggest that leadership breadth, defined as experience spanning multiple functions, disciplines, and sectors, is positively associated with firm-level performance. While the dataset remains under validation, the pattern observed supports the emerging view that as specialization deepens, the marginal value of lateral insight rises. Breadth, in this light, functions as a form of adaptive capital; it enhances leaders' capacity for integrative reasoning, organizational translation, and strategic flexibility in uncertain environments."
http://arxiv.org/abs/2511.07218v1,"The Long Shadow of Superstars: Effects on Opportunities, Careers, and Team Production",2025-11-10 15:40:19+00:00,['Masaya Nishihata'],econ.GN,"Superstars often dominate key tasks because of their exceptional abilities, but this concentration of responsibility may unintentionally limit on-the-job learning opportunities for others. Using panel data from Major League Baseball (MLB), this study examines how superstar presence affects teammates' opportunities and career outcomes. To address potential endogeneity in team composition, we exploit plausibly exogenous variation in superstar availability caused by injuries. When a superstar is active in the same team-position unit, non-star teammates play significantly less. These short-term reductions in playing time extend to longer horizons: players who begin their careers alongside a superstar who remains active for a full season (i.e., not on the injured list) are about 1.7 times more likely to exit MLB earlier than comparable peers. A key mechanism is reduced skill development -- limited playing opportunities hinder subsequent growth in offensive performance. At the team level, greater dependence on superstars raises immediate productivity but magnifies performance declines after their departure, indicating a trade-off between short-term success and long-term adaptability. Overall, the findings suggest that while concentrating key roles in top performers boosts output in the short run, it can restrict others' development and retention. Similar dynamics may arise in other organizations that rely heavily on a few exceptional individuals."
http://arxiv.org/abs/2511.07014v1,Diffolio: A Diffusion Model for Multivariate Probabilistic Financial Time-Series Forecasting and Portfolio Construction,2025-11-10 12:05:32+00:00,"['So-Yoon Cho', 'Jin-Young Kim', 'Kayoung Ban', 'Hyeng Keun Koo', 'Hyun-Gyoon Kim']",cs.CE,"Probabilistic forecasting is crucial in multivariate financial time-series for constructing efficient portfolios that account for complex cross-sectional dependencies. In this paper, we propose Diffolio, a diffusion model designed for multivariate financial time-series forecasting and portfolio construction. Diffolio employs a denoising network with a hierarchical attention architecture, comprising both asset-level and market-level layers. Furthermore, to better reflect cross-sectional correlations, we introduce a correlation-guided regularizer informed by a stable estimate of the target correlation matrix. This structure effectively extracts salient features not only from historical returns but also from asset-specific and systematic covariates, significantly enhancing the performance of forecasts and portfolios. Experimental results on the daily excess returns of 12 industry portfolios show that Diffolio outperforms various probabilistic forecasting baselines in multivariate forecasting accuracy and portfolio performance. Moreover, in portfolio experiments, portfolios constructed from Diffolio's forecasts show consistently robust performance, thereby outperforming those from benchmarks by achieving higher Sharpe ratios for the mean-variance tangency portfolio and higher certainty equivalents for the growth-optimal portfolio. These results demonstrate the superiority of our proposed Diffolio in terms of not only statistical accuracy but also economic significance."
http://arxiv.org/abs/2510.11261v2,Mean-Field Price Formation on Trees: with Multi-Population and Non-Rational Agents,2025-10-13 10:51:56+00:00,['Masaaki Fujii'],q-fin.MF,"This work solves the equilibrium price formation problem for the risky stock by combining mean-field game theory with the binomial tree framework, following the classic approach of Cox, Ross & Rubinstein. For agents with exponential and recursive utilities of exponential-type, we prove the existence of a unique mean-field market-clearing equilibrium and derive an explicit analytic formula for equilibrium transition probabilities of the stock price on the binomial lattice. The agents face stochastic terminal liabilities and incremental endowments that depend on unhedgeable common and idiosyncratic factors, in addition to the stock price path. We also incorporate an external order flow. Furthermore, the analytic tractability of the proposed approach allows us to extend the framework in two important directions: First, we incorporate multi-population heterogeneity, allowing agents to differ in functional forms for their liabilities, endowments, and risk coefficients. Second, we relax the rational expectations hypothesis by modeling agents operating under subjective probability measures which induce stochastically biased views on the stock transition probabilities. Our numerical examples illustrate the qualitative effects of these components on the equilibrium price distribution."
http://arxiv.org/abs/2511.00080v1,Closing the SNAP Gap: Identifying Under-Enrollment in High-Poverty ZIP Codes,2025-10-29 18:25:49+00:00,['Auyona Ray'],econ.GN,"This project began by constructing an index of economic insecurity using multiple socioeconomic indicators. Although poverty alone predicted SNAP participation more accurately than the composite index, its explanatory power was weaker than anticipated, echoing past findings that enrollment cannot be explained by income alone. This led to a shift in focus: identifying ZIP codes with high poverty but unexpectedly low SNAP participation, areas defined here as having a SNAP Gap, where ZIPs fall in the top 30 percent of family poverty and the bottom 10 percent of SNAP enrollment. Using nationally available ZIP level data from 2014 to 2023, I trained logistic classification models on four interpretable structural indicators: lack of vehicle, lack of internet access, lack of computer access, and percentage of adults with only a high school diploma. The most effective model relies on just two predictors, vehicle access and education, and outperforms tree based classifiers in both precision and calibration. Results show that economic insecurity is consistently concentrated in rural ZIP codes, with transportation access emerging as the most stable barrier to program take up. This study provides a nationwide diagnostic framework that can inform the development of scalable screening tools for targeting outreach and improving benefit access in underserved communities."
http://arxiv.org/abs/2511.00068v1,"Hope, Signals, and Silicon: A Game-Theoretic Model of the Pre-Doctoral Academic Labor Market in the Age of AI",2025-10-29 09:03:05+00:00,['Shaohui Wang'],econ.TH,"This paper develops a unified game-theoretic account of how generative AI reshapes the pre-doctoral ""hope-labor"" market linking Principal Investigators (PIs), Research Assistants (RAs), and PhD admissions. We integrate (i) a PI-RA relational-contract stage, (ii) a task-based production technology in which AI is both substitute (automation) and complement (augmentation/leveling), and (iii) a capacity-constrained admissions tournament that converts absolute output into relative rank. The model yields four results. First, AI has a dual and thresholded effect on RA demand: when automation dominates, AI substitutes for RA labor; when augmentation dominates, small elite teams become more valuable. Second, heterogeneous PI objectives endogenously segment the RA market: quantity-maximizing PIs adopt automation and scale ""project-manager"" RAs, whereas quality-maximizing PIs adopt augmentation and cultivate ""idea-generator"" RAs. Third, a symmetric productivity shock triggers a signaling arms race: more ""strong"" signals flood a fixed-slot tournament, depressing the admission probability attached to any given signal and potentially lowering RA welfare despite higher productivity. Fourth, AI degrades the informational content of polished routine artifacts, creating a novel moral-hazard channel (""effort laundering"") that shifts credible recommendations toward process-visible, non-automatable creative contributions. We discuss welfare and equity implications, including over-recruitment with thin mentoring, selectively misleading letters, and opaque pipelines, and outline light-touch governance (process visibility, AI-use disclosure, and limited viva/replication checks) that preserves efficiency while reducing unethical supervision and screening practices."
http://arxiv.org/abs/2511.00597v1,Concentration Inequalities for Suprema of Empirical Processes with Dependent Data via Generic Chaining with Applications to Statistical Learning,2025-11-01 15:39:29+00:00,"['Chiara Amorino', 'Christian Brownlees', 'Ankita Ghosh']",econ.EM,"This paper develops a general concentration inequality for the suprema of empirical processes with dependent data. The concentration inequality is obtained by combining generic chaining with a coupling-based strategy. Our framework accommodates high-dimensional and heavy-tailed (sub-Weibull) data. We demonstrate the usefulness of our result by deriving non-asymptotic predictive performance guarantees for empirical risk minimization in regression problems with dependent data. In particular, we establish an oracle inequality for a broad class of nonlinear regression models and, as a special case, a single-layer neural network model. Our results show that empirical risk minimzaton with dependent data attains a prediction accuracy comparable to that in the i.i.d. setting for a wide range of nonlinear regression models."
http://arxiv.org/abs/2511.00718v1,Persuasive Selection in Signaling Games,2025-11-01 21:53:08+00:00,['Haoyuan Zeng'],econ.TH,"This paper introduces a novel criterion, persuasiveness, to select equilibria in signaling games. In response to the Stiglitz critique, persuasiveness focuses on the comparison across equilibria. An equilibrium is more persuasive than an alternative if the set of types of the sender who prefer the alternative would sequentially deviate to the former once other types have done so -- that is, if an unraveling occurs. Persuasiveness has strong selective power: it uniquely selects an equilibrium outcome in monotone signaling games. Moreover, in non-monotone signaling games, persuasiveness refines predictions beyond existing selection criteria. Notably, it can also select equilibria in cheap-talk games, where standard equilibrium refinements for signaling games have no selective power."
http://arxiv.org/abs/2511.00715v1,Mechanism Design with Information Leakage,2025-11-01 21:40:39+00:00,"['Samuel Häfner', 'Marek Pycia', 'Haoyuan Zeng']",econ.TH,"We study the design of mechanisms -- e.g., auctions -- when the designer does not control information flows between mechanism participants. A mechanism equilibrium is leakage-proof if no player conditions their actions on leaked information; a property distinct from ex-post incentive compatibility. Only leakage-proof mechanisms can implement social choice functions in environments with leakage. Efficient auctions need to be leakage-proof, while revenue-maximizing ones not necessarily so. Second-price and ascending auctions are leakage-proof; first-price auctions are not; while whether descending auctions are leakage-proof depends on tie-breaking."
http://arxiv.org/abs/2511.02120v1,Evaluating Factor Contributions for Sold Homes,2025-11-03 23:21:09+00:00,"['Jason R. Bailey', 'W. Brent Lindquist', 'Svetlozar T. Rachev']",econ.GN,"We evaluate the contributions of ten intrinsic and extrinsic factors, including ESG (environmental, social, and governance) factors readily available from website data to individual home sale prices using a P-spline generalized additive model (GAM). We identify the relative significance of each factor by evaluating the change in adjusted R^2 value resulting from its removal from the model. We combine this with information from correlation matrices to identify the added predictive value of a factor. Based on data from 2022 through 2024 for three major U.S. cities, the GAM consistently achieved higher adjusted R^2 values across all cities (compared to a benchmark generalized linear model) and identified all factors as statistically significant at the 0.5% level. The tests revealed that living area and location (latitude, longitude) were the most significant factors; each independently adds predictive value. The ESG-related factors exhibited limited significance; two of them each adding independent predictive value. The elderly/disabled accessibility factor was much more significant in one retirement-oriented city. In all cities, the accessibility factor showed moderate correlation with one intrinsic factor. Despite the granularity of the ESG data, this study also represents a pivotal step toward integrating sustainability-related factors into predictive models for real estate valuation."
http://arxiv.org/abs/2511.02099v1,AI Spillover is Different: Flat and Lean Firms as Engines of AI Diffusion and Productivity Gain,2025-11-03 22:19:31+00:00,"['Xiaoning Wang', 'Chun Feng', 'Tianshu Sun']",econ.GN,"Labor mobility is a critical source of technology acquisition for firms. This paper examines how artificial intelligence (AI) knowledge is disseminated across firms through labor mobility and identifies the organizational conditions that facilitate productive spillovers. Using a comprehensive dataset of over 460 million job records from Revelio Labs (2010 to 2023), we construct an inter-firm mobility network of AI workers among over 16,000 U.S. companies. Estimating a Cobb Douglas production function, we find that firms benefit substantially from the AI investments of other firms from which they hire AI talents, with productivity spillovers two to three times larger than those associated with traditional IT after accounting for labor scale. Importantly, these spillovers are contingent on organizational context: hiring from flatter and more lean startup method intensive firms generates significant productivity gains, whereas hiring from firms lacking these traits yields little benefit. Mechanism tests indicate that ""flat and lean"" organizations cultivate more versatile AI generalists who transfer richer knowledge across firms. These findings reveal that AI spillovers differ fundamentally from traditional IT spillovers: while IT spillovers primarily arise from scale and process standardization, AI spillovers critically depend on the experimental and integrative environments in which AI knowledge is produced. Together, these results underscore the importance of considering both labor mobility and organizational context in understanding the full impact of AI-driven productivity spillovers."
http://arxiv.org/abs/2511.08617v1,"The effects of International Monetary Fund programs: a systematic review with narrative synthesis on poverty, inequality, and social indicators",2025-11-06 22:36:32+00:00,['Ricardo Alonzo Fernández Salguero'],econ.GN,"This systematic review with narrative synthesis examines the social impacts of International Monetary Fund (IMF) programs. We systematically searched five academic databases and grey literature following PRISMA guidelines and included 53 empirical studies that met predefined eligibility criteria. For each study we assessed risk of bias, with particular attention to how endogeneity, selection bias, and confounding were handled. Because of substantial heterogeneity in outcomes and research designs, results were synthesized narratively rather than through meta analysis. We find that a minority of studies, often using methods with higher risk of bias such as propensity score matching, report no systematic adverse social effects. By contrast, a large body of work using stronger quasi experimental designs, especially instrumental variable strategies, links IMF conditionality to higher income inequality, worse health outcomes (notably tuberculosis and child mortality), and growth of the informal economy. Overall, the best available evidence indicates that IMF programs, particularly those centered on fiscal austerity and structural reforms, impose significant social costs and that a redesign of conditionality is needed to protect social spending and advance the Sustainable Development Goals."
http://arxiv.org/abs/2511.08785v1,Making Talk Cheap: Generative AI and Labor Market Signaling,2025-11-11 21:11:57+00:00,"['Anais Galdin', 'Jesse Silbert']",econ.GN,"Large language models (LLMs) like ChatGPT have significantly lowered the cost of producing written content. This paper studies how LLMs, through lowering writing costs, disrupt markets that traditionally relied on writing as a costly signal of quality (e.g., job applications, college essays). Using data from Freelancer.com, a major digital labor platform, we explore the effects of LLMs' disruption of labor market signaling on equilibrium market outcomes. We develop a novel LLM-based measure to quantify the extent to which an application is tailored to a given job posting. Taking the measure to the data, we find that employers have a high willingness to pay for workers with more customized applications in the period before LLMs are introduced, but not after. To isolate and quantify the effect of LLMs' disruption of signaling on equilibrium outcomes, we develop and estimate a structural model of labor market signaling, in which workers invest costly effort to produce noisy signals that predict their ability in equilibrium. We use the estimated model to simulate a counterfactual equilibrium in which LLMs render written applications useless in signaling workers' ability. Without costly signaling, employers are less able to identify high-ability workers, causing the market to become significantly less meritocratic: compared to the pre-LLM equilibrium, workers in the top quintile of the ability distribution are hired 19% less often, workers in the bottom quintile are hired 14% more often."
http://arxiv.org/abs/2511.05927v1,Artificial intelligence and the Gulf Cooperation Council workforce adapting to the future of work,2025-11-08 08:42:14+00:00,"['Mohammad Rashed Albous', 'Melodena Stephens', 'Odeh Rashed Al-Jayyousi']",cs.CY,"The rapid expansion of artificial intelligence (AI) in the Gulf Cooperation Council (GCC) raises a central question: are investments in compute infrastructure matched by an equally robust build-out of skills, incentives, and governance? Grounded in socio-technical systems (STS) theory, this mixed-methods study audits workforce preparedness across Kingdom of Saudi Arabia (KSA), the United Arab Emirates (UAE), Qatar, Kuwait, Bahrain, and Oman. We combine term frequency--inverse document frequency (TF--IDF) analysis of six national AI strategies (NASs), an inventory of 47 publicly disclosed AI initiatives (January 2017--April 2025), paired case studies, the Mohamed bin Zayed University of Artificial Intelligence (MBZUAI) and the Saudi Data & Artificial Intelligence Authority (SDAIA) Academy, and a scenario matrix linking oil-revenue slack (technical capacity) to regulatory coherence (social alignment). Across the corpus, 34/47 initiatives (0.72; 95% Wilson CI 0.58--0.83) exhibit joint social--technical design; country-level indices span 0.57--0.90 (small n; intervals overlap). Scenario results suggest that, under our modeled conditions, regulatory convergence plausibly binds outcomes more than fiscal capacity: fragmented rules can offset high oil revenues, while harmonized standards help preserve progress under austerity. We also identify an emerging two-track talent system, research elites versus rapidly trained practitioners, that risks labor-market bifurcation without bridging mechanisms. By extending STS inquiry to oil-rich, state-led economies, the study refines theory and sets a research agenda focused on longitudinal coupling metrics, ethnographies of coordination, and outcome-based performance indicators."
http://arxiv.org/abs/2511.04449v2,Comparison of Oracles: Part II,2025-11-06 15:21:31+00:00,"['David Lagziel', 'Ehud Lehrer', 'Tao Wang']",econ.TH,"This paper studies incomplete-information games in which an information provider, an oracle, publicly discloses information to the players. One oracle is said to dominate another if, in every game, it can replicate the equilibrium outcomes induced by the latter. The companion Part I characterizes dominance under deterministic signaling and under stochastic signaling with a unique common knowledge component. The present paper extends the analysis to general environments and provides a characterization of equivalence (mutual dominance) among oracles. To this end, we develop a theory of information loops, thereby extending the seminal work of Blackwell (1951) to strategic environments and Aumann (1976)'s theory of common knowledge."
http://arxiv.org/abs/2510.03792v3,"Gas supply shocks, uncertainty and price setting: evidences from Italian firms",2025-10-04 12:00:34+00:00,['Giuseppe Pagano Giorgianni'],econ.GN,"This paper examines how natural gas supply shocks affect Italian firms' pricing decisions and inflation expectations using quarterly survey data from the Bank of Italy's Survey on Inflation and Growth Expectations (SIGE). We identify natural gas supply shocks through an external IV-VAR approach exploiting likely unexpected news about interruption to gas supplies to Europe. Our findings show that although gas supply shocks do not have huge effects on gas quantity and only modest effect on gas inventories, they are quickly transmitted to spot electricity prices with persistent effects. We then estimate a proxy internalizing BVAR incorporating firm-level variables from SIGE, documenting that gas supply shocks raise firms' current and expected prices as well as inflation uncertainty. Finally, we uncover substantial nonlinearities using state-dependent local projections: under high inflation uncertainty, firms successfully pass cost increases on to consumers, sustaining elevated prices; under low uncertainty, recessionary effects dominate, leading firms to cut prices below baseline."
http://arxiv.org/abs/2510.05454v1,Estimating Treatment Effects Under Bounded Heterogeneity,2025-10-06 23:39:46+00:00,"['Soonwoo Kwon', 'Liyang Sun']",econ.EM,"Researchers often use specifications that correctly estimate the average treatment effect under the assumption of constant effects. When treatment effects are heterogeneous, however, such specifications generally fail to recover this average effect. Augmenting these specifications with interaction terms between demeaned covariates and treatment eliminates this bias, but often leads to imprecise estimates and becomes infeasible under limited overlap. We propose a generalized ridge regression estimator, $\texttt{regulaTE}$, that penalizes the coefficients on the interaction terms to achieve an optimal trade-off between worst-case bias and variance in estimating the average effect under limited treatment effect heterogeneity. Building on this estimator, we construct confidence intervals that remain valid under limited overlap and can also be used to assess sensitivity to violations of the constant effects assumption. We illustrate the method in empirical applications under unconfoundedness and staggered adoption, providing a practical approach to inference under limited overlap."
http://arxiv.org/abs/2510.07180v1,Bayesian Portfolio Optimization by Predictive Synthesis,2025-10-08 16:18:11+00:00,"['Masahiro Kato', 'Kentaro Baba', 'Hibiki Kaibuchi', 'Ryo Inokuchi']",econ.EM,"Portfolio optimization is a critical task in investment. Most existing portfolio optimization methods require information on the distribution of returns of the assets that make up the portfolio. However, such distribution information is usually unknown to investors. Various methods have been proposed to estimate distribution information, but their accuracy greatly depends on the uncertainty of the financial markets. Due to this uncertainty, a model that could well predict the distribution information at one point in time may perform less accurately compared to another model at a different time. To solve this problem, we investigate a method for portfolio optimization based on Bayesian predictive synthesis (BPS), one of the Bayesian ensemble methods for meta-learning. We assume that investors have access to multiple asset return prediction models. By using BPS with dynamic linear models to combine these predictions, we can obtain a Bayesian predictive posterior about the mean rewards of assets that accommodate the uncertainty of the financial markets. In this study, we examine how to construct mean-variance portfolios and quantile-based portfolios based on the predicted distribution information."
http://arxiv.org/abs/2510.07016v1,The role of communication in effective business management,2025-10-08 13:41:56+00:00,"['Dariusz Baran', 'Ernest Górka', 'Michał Ćwiąkała', 'Gabriela Wojak', 'Mateusz Grzelak', 'Katarzyna Olszyńska', 'Piotr Mrzygłód', 'Maciej Frasunkiewicz', 'Piotr Ręczajski', 'Maciej Ślusarczyk', 'Jan Piwnik']",econ.GN,"This paper examines the impact of internal communication on effective business management through a comparative analysis of two medium-sized car rental companies operating in Poland. Using a structured survey completed by 220 employees, the study evaluates 15 communication-related factors, including feedback culture, managerial accessibility, message clarity, and interdepartmental coordination. The findings indicate that Company X significantly outperforms Company Y across all evaluated dimensions, largely due to its use of advanced communication technologies, participatory models, and clear feedback mechanisms. The research highlights the strategic role of two-way communication in fostering employee engagement, organizational transparency, and operational efficiency. It contributes to the field by offering a rare, data-driven comparison within one industry and supports existing models that link internal communication to job satisfaction and motivation. Limitations include reliance on self-reported data and focus on a single industry and country. Future studies are recommended to explore cross-sector and longitudinal perspectives, especially in the context of digital and hybrid work environments."
http://arxiv.org/abs/2510.04464v2,Identification in Auctions with Truncated Transaction Prices,2025-10-06 03:38:53+00:00,['Tonghui Qi'],econ.EM,"I establish nonparametric identification results in first- and second-price auctions when transaction prices are truncated by a binding reserve price under a range of information structures. When the number of potential bidders is fixed and known across all auctions, if only the transaction price is observed, the bidders' private-value distribution is identified in second-price auctions but not in first-price auctions. Identification in first-price auctions can be achieved if either the number of active bidders or the number of auctions with no sales is observed. When the number of potential bidders varies across auctions and is unknown, the bidders' private-value distribution is identified in first-price auctions but not in second-price auctions, provided that both the transaction price and the number of active bidders are observed. I derive analogous results to auctions with entry costs, which face a similar truncation issue when data on potential bidders who do not enter are missing."
http://arxiv.org/abs/2510.02154v1,A Computational Approach to Sustainable Policies Evaluation of the Italian Wheat Production System,2025-10-02 16:05:07+00:00,"['Gianfranco Giuloni', 'Edmondo Di Giuseppe', 'Arianna Di Paola']",econ.GN,"This work outlines the modeling steps for developing a tool aimed at supporting policymakers in guiding policies toward more sustainable wheat production. In the agricultural sector,policies affect a highly diverse set of farms, which differ across several dimensions such as size,land composition, local climate, and irrigation availability. To address this significant heterogeneity, we construct an Agent-Based Model (ABM). The model is initialized using a representative survey of Italian farms, which captures their heterogeneity. The ABM is then scaled to include a number of farms comparable to those operating nationwide. To capture broader dynamics, the ABM is integrated with two additional components:a global model of international wheat markets and a tool for assessing the environmental impacts of wheat production. This integrated framework enables us to account for the feedback loop between global prices and local production while evaluating the environmental implications of policy measures."
http://arxiv.org/abs/2510.01387v1,Learning to Play Multi-Follower Bayesian Stackelberg Games,2025-10-01 19:20:35+00:00,"['Gerson Personnat', 'Tao Lin', 'Safwan Hossain', 'David C. Parkes']",cs.GT,"In a multi-follower Bayesian Stackelberg game, a leader plays a mixed strategy over $L$ actions to which $n\ge 1$ followers, each having one of $K$ possible private types, best respond. The leader's optimal strategy depends on the distribution of the followers' private types. We study an online learning version of this problem: a leader interacts for $T$ rounds with $n$ followers with types sampled from an unknown distribution every round. The leader's goal is to minimize regret, defined as the difference between the cumulative utility of the optimal strategy and that of the actually chosen strategies. We design learning algorithms for the leader under different feedback settings. Under type feedback, where the leader observes the followers' types after each round, we design algorithms that achieve $\mathcal O\big(\sqrt{\min\{L\log(nKA T), nK \} \cdot T} \big)$ regret for independent type distributions and $\mathcal O\big(\sqrt{\min\{L\log(nKA T), K^n \} \cdot T} \big)$ regret for general type distributions. Interestingly, those bounds do not grow with $n$ at a polynomial rate. Under action feedback, where the leader only observes the followers' actions, we design algorithms with $\mathcal O( \min\{\sqrt{ n^L K^L A^{2L} L T \log T}, K^n\sqrt{ T } \log T \} )$ regret. We also provide a lower bound of $Ω(\sqrt{\min\{L, nK\}T})$, almost matching the type-feedback upper bounds."
