id,title,published_date,authors,primary_category,summary,year
http://arxiv.org/abs/2012.15716v1,Assessing Sensitivity to Unconfoundedness: Estimation and Inference,2020-12-31 17:14:38+00:00,"['Matthew A. Masten', 'Alexandre Poirier', 'Linqi Zhang']",econ.EM,"This paper provides a set of methods for quantifying the robustness of
treatment effects estimated using the unconfoundedness assumption (also known
as selection on observables or conditional independence). Specifically, we
estimate and do inference on bounds on various treatment effect parameters,
like the average treatment effect (ATE) and the average effect of treatment on
the treated (ATT), under nonparametric relaxations of the unconfoundedness
assumption indexed by a scalar sensitivity parameter c. These relaxations allow
for limited selection on unobservables, depending on the value of c. For large
enough c, these bounds equal the no assumptions bounds. Using a non-standard
bootstrap method, we show how to construct confidence bands for these bound
functions which are uniform over all values of c. We illustrate these methods
with an empirical application to effects of the National Supported Work
Demonstration program. We implement these methods in a companion Stata module
for easy use in practice.",2020
http://arxiv.org/abs/2101.01093v1,Breaking Ties: Regression Discontinuity Design Meets Market Design,2020-12-31 09:00:13+00:00,"['Atila Abdulkadiroglu', 'Joshua D. Angrist', 'Yusuke Narita', 'Parag Pathak']",econ.EM,"Many schools in large urban districts have more applicants than seats.
Centralized school assignment algorithms ration seats at over-subscribed
schools using randomly assigned lottery numbers, non-lottery tie-breakers like
test scores, or both. The New York City public high school match illustrates
the latter, using test scores and other criteria to rank applicants at
``screened'' schools, combined with lottery tie-breaking at unscreened
``lottery'' schools. We show how to identify causal effects of school
attendance in such settings. Our approach generalizes regression discontinuity
methods to allow for multiple treatments and multiple running variables, some
of which are randomly assigned. The key to this generalization is a local
propensity score that quantifies the school assignment probabilities induced by
lottery and non-lottery tie-breakers. The local propensity score is applied in
an empirical assessment of the predictive value of New York City's school
report cards. Schools that receive a high grade indeed improve SAT math scores
and increase graduation rates, though by much less than OLS estimates suggest.
Selection bias in OLS estimates is egregious for screened schools.",2020
http://arxiv.org/abs/2012.15435v3,Transitional Dynamics of the Saving Rate and Economic Growth,2020-12-31 04:01:13+00:00,"['Markus Brueckner', 'Tomoo Kikuchi', 'George Vachadze']",econ.GN,"We estimate the relationship between GDP per capita growth and the growth
rate of the national savings rate using a panel of 130 countries over the
period 1960-2017. We find that GDP per capita growth increases (decreases) the
growth rate of the national savings rate in poor countries (rich countries),
and a higher credit-to-GDP ratio decreases the national savings rate as well as
the income elasticity of the national savings rate. We develop a model with a
credit constraint to explain the growth-saving relationship by the saving
behavior of entrepreneurs at both the intensive and extensive margins. We
further present supporting evidence for our theoretical findings by utilizing
cross-country time series data of the number of new businesses registered and
the corporate savings rate.",2020
http://arxiv.org/abs/2012.15371v1,The Economics of Variable Renewables and Electricity Storage,2020-12-30 23:54:19+00:00,"['Javier López Prol', 'Wolf-Peter Schill']",econ.GN,"The transformation of the electricity sector is a main element of the
transition to a decarbonized economy. Conventional generators powered by fossil
fuels have to be replaced by variable renewable energy (VRE) sources in
combination with electricity storage and other options for providing temporal
flexibility. We discuss the market dynamics of increasing VRE penetration and
their integration in the electricity system. We describe the merit-order effect
(decline of wholesale electricity prices as VRE penetration increases) and the
cannibalization effect (decline of VRE value as their penetration increases).
We further review the role of electricity storage and other flexibility options
for integrating variable renewables, and how storage can contribute to
mitigating the two mentioned effects. We also use a stylized open-source model
to provide some graphical intuition on this. While relatively high shares of
VRE are achievable with moderate amounts of electricity storage, the role of
long-term storage increases as the VRE share approaches 100%.",2020
http://arxiv.org/abs/2012.15367v3,Assessing the Sensitivity of Synthetic Control Treatment Effect Estimates to Misspecification Error,2020-12-30 23:35:30+00:00,"['Billy Ferguson', 'Brad Ross']",econ.EM,"We propose a sensitivity analysis for Synthetic Control (SC) treatment effect
estimates to interrogate the assumption that the SC method is well-specified,
namely that choosing weights to minimize pre-treatment prediction error yields
accurate predictions of counterfactual post-treatment outcomes. Our data-driven
procedure recovers the set of treatment effects consistent with the assumption
that the misspecification error incurred by the SC method is at most the
observable misspecification error incurred when using the SC estimator to
predict the outcomes of some control unit. We show that under one definition of
misspecification error, our procedure provides a simple, geometric motivation
for comparing the estimated treatment effect to the distribution of placebo
residuals to assess estimate credibility. When we apply our procedure to
several canonical studies that report SC estimates, we broadly confirm the
conclusions drawn by the source papers.",2020
http://arxiv.org/abs/2101.00009v3,Adversarial Estimation of Riesz Representers,2020-12-30 19:46:57+00:00,"['Victor Chernozhukov', 'Whitney Newey', 'Rahul Singh', 'Vasilis Syrgkanis']",econ.EM,"Many causal parameters are linear functionals of an underlying regression.
The Riesz representer is a key component in the asymptotic variance of a
semiparametrically estimated linear functional. We propose an adversarial
framework to estimate the Riesz representer using general function spaces. We
prove a nonasymptotic mean square rate in terms of an abstract quantity called
the critical radius, then specialize it for neural networks, random forests,
and reproducing kernel Hilbert spaces as leading cases. Our estimators are
highly compatible with targeted and debiased machine learning with sample
splitting; our guarantees directly verify general conditions for inference that
allow mis-specification. We also use our guarantees to prove inference without
sample splitting, based on stability or complexity. Our estimators achieve
nominal coverage in highly nonlinear simulations where some previous methods
break down. They shed new light on the heterogeneous effects of matching
grants.",2020
http://arxiv.org/abs/2012.15158v4,Testing the effectiveness of unconventional monetary policy in Japan and the United States,2020-12-30 13:58:56+00:00,"['Daisuke Ikeda', 'Shangshang Li', 'Sophocles Mavroeidis', 'Francesco Zanetti']",econ.GN,"Unconventional monetary policy (UMP) may make the effective lower bound (ELB)
on the short-term interest rate irrelevant. We develop a theoretical model that
underpins our empirical test of this `irrelevance hypothesis' based on the
simple idea that under the hypothesis, the short rate can be excluded in any
empirical model that accounts for alternative measures of monetary policy. We
test the hypothesis for Japan and the United States using a structural vector
autoregressive model with the ELB. We firmly reject the hypothesis but find
that UMP has had strong delayed effects.",2020
http://arxiv.org/abs/2012.15144v1,What is the impact of labor displacement on management consulting services?,2020-12-30 13:12:34+00:00,['Edouard Ribes'],econ.GN,"Labor displacement off-or nearshore is a performance improvement instrument
that currently sparks a lot of interest in the service sector. This article
proposes a model to understand the consequences of such a decision on
management consulting firms. Its calibration on the market of consulting
services for the German transportation industry highlights that, under
realistic assumptions, labor displacement translates in price decrease by-0.5%
on average per year and that for MC practices to remain competitive/profitable
they have to at least increase the amount of work they off/nears shore by +0.7%
a year.",2020
http://arxiv.org/abs/2012.15035v3,Measuring Human Adaptation to AI in Decision Making: Application to Evaluate Changes after AlphaGo,2020-12-30 04:34:46+00:00,"['Minkyu Shin', 'Jin Kim', 'Minkyung Kim']",cs.HC,"Across a growing number of domains, human experts are expected to learn from
and adapt to AI with superior decision making abilities. But how can we
quantify such human adaptation to AI? We develop a simple measure of human
adaptation to AI and test its usefulness in two case studies. In Study 1, we
analyze 1.3 million move decisions made by professional Go players and find
that a positive form of adaptation to AI (learning) occurred after the players
could observe the reasoning processes of AI, rather than mere actions of AI.
These findings based on our measure highlight the importance of explainability
for human learning from AI. In Study 2, we test whether our measure is
sufficiently sensitive to capture a negative form of adaptation to AI (cheating
aided by AI), which occurred in a match between professional Go players. We
discuss our measure's applications in domains other than Go, especially in
domains in which AI's decision making ability will likely surpass that of human
experts.",2020
http://arxiv.org/abs/2012.15007v4,Evolutionarily Stable (Mis)specifications: Theory and Applications,2020-12-30 02:33:15+00:00,"['Kevin He', 'Jonathan Libgober']",econ.TH,"Toward explaining the persistence of biased inferences, we propose a
framework to evaluate competing (mis)specifications in strategic settings.
Agents with heterogeneous (mis)specifications coexist and draw Bayesian
inferences about their environment through repeated play. The relative
stability of (mis)specifications depends on their adherents' equilibrium
payoffs. A key mechanism is the learning channel: the endogeneity of perceived
best replies due to inference. We characterize when a rational society is only
vulnerable to invasion by some misspecification through the learning channel.
The learning channel leads to new stability phenomena, and can confer an
evolutionary advantage to otherwise detrimental biases in economically relevant
applications.",2020
http://arxiv.org/abs/2012.14999v1,The Involution of Industrial Life Cycle on Atlantic City Gambling Industry,2020-12-30 01:31:46+00:00,"['Jin Quan Zhou', 'Wen Jin He']",econ.GN,"The industrial life cycle theory has proved to be helpful for describing the
evolution of industries from birth to maturity. This paper is to highlight the
historical evolution stage of Atlantic City's gambling industry in a structural
framework covered by industrial market, industrial organization, industrial
policies and innovation. Data mining was employed to obtain from local official
documents, to verify the module of industrial life cycle in differential phases
as introduction, development, maturity and decline. The trajectory of Atlantic
City's gambling sector evolution reveals the process from the stages of
introduction to decline via a set of variables describing structural properties
of this industry such as product, market and organization of industry under a
special industry environment in which industry recession as a result of theory
of industry life cycle is a particular evidence be proved again. Innovation of
the gambling industry presents the ongoing recovering process of the Atlantic
City gambling industry enriches the theory of industrial life cycle in service
sectors.",2020
http://arxiv.org/abs/2012.14976v1,"REME -- Renewable Energy and Materials Economy -- The Path to Energy Security, Prosperity and Climate Stability",2020-12-29 23:34:44+00:00,['Peter Eisenberger'],physics.soc-ph,"A Renewable Energy and Materials Economy (REME) is proposed as the solution
to the climate change threat. REME mimics nature to produce carbon neutral
liquid fuels and chemicals as well as carbon negative materials by using water,
CO$_2$ from the atmosphere and renewable energy as inputs. By being in harmony
with nature REME has a positive feedback between economic development and
climate change protection. In REME the feedback driven accelerated rate of
economic growth enables the climate change threat to be addressed in a timely
manner. It is also cost-effective protection because it sequesters by
monetizing the carbon removed from the air in carbon-based building materials.
Thus, addressing the climate change threat is not a cost to the economy but a
result of REME driven prosperity.",2020
http://arxiv.org/abs/2012.14962v5,The Impact of Corona Populism: Empirical Evidence from Austria and Theory,2020-12-29 22:36:09+00:00,['Patrick Mellacher'],econ.GN,"I study the co-evolution between public opinion and party policy in
situations of crises by investigating a policy U-turn of a major Austrian
right-wing party (FPOE) during the Covid-19 pandemic. My analysis suggests the
existence of both i) a ""Downsian"" effect, which causes voters to adapt their
party preferences based on policy congruence and ii) a ""party identification""
effect, which causes partisans to realign their policy preferences based on
""their"" party's platform. Specifically, I use individual-level panel data to
show that i) ""corona skeptical"" voters who did not vote for the FPOE in the
pre-Covid-19 elections of 2019 were more likely to vote for the party after it
embraced ""corona populism"", and ii) beliefs of respondents who declared that
they voted for the FPOE in 2019 diverged from the rest of the population in
three out of four health-dimensions only after the turn, causing them to
underestimate the threat posed by Covid-19 compared to the rest of the
population. Using aggregate-level panel data, I study whether the turn has
produced significant behavioral differences which could be observed in terms of
reported cases and deaths per capita. Paradoxically, after the turn the FPOE
vote share is significantly positively correlated with deaths per capita, but
not with the reported number of infections. I hypothesize that this due to a
self-selection bias in testing, which causes a correlation between the number
of ""corona skeptics"" and the share of unreported cases after the turn. I find
empirical support for this hypothesis in individual-level data from a Covid-19
prevalence study that involves information about participants' true vs.
reported infection status. I finally study a simple heterogeneous mixing
epidemiological model and show that a testing bias can indeed explain the
apparent paradox of an increase in deaths without an increase in reported
cases.",2020
http://arxiv.org/abs/2012.14941v1,The wealth of nations and the health of populations: A quasi-experimental design of the impact of sovereign debt crises on child mortality,2020-12-29 20:57:59+00:00,['Adel Daoud'],econ.GN,"The wealth of nations and the health of populations are intimately strongly
associated, yet the extent to which economic prosperity (GDP per capita) causes
improved health remains disputed. The purpose of this article is to analyze the
impact of sovereign debt crises (SDC) on child mortality, using a sample of 57
low- and middle-income countries surveyed by the Demographic and Health Survey
between the years 1990 and 2015. These surveys supply 229 household data and
containing about 3 million childbirth history records. This focus on SDC
instead of GDP provides a quasi-experimental moment in which the influence of
unobserved confounding is less than a moment analyzing the normal fluctuations
of GDP. This study measures child mortality at six thresholds: neonatal,
under-one (infant), under-two, under-three, under-four, and under-five
mortality. Using a machine-learning (ML) model for causal inference, this study
finds that while an SDC causes an adverse yet statistically insignificant
effect on neonatal mortality, all other child mortality group samples are
adversely affected between a probability of 0.12 to 0.14 (all statistically
significant at the 95-percent threshold). Through this ML, this study also
finds that the most important treatment heterogeneity moderator, in the entire
adjustment set, is whether a child is born in a low-income country.",2020
http://arxiv.org/abs/2012.14898v2,Exponential Communication Separations between Notions of Selfishness,2020-12-29 18:41:44+00:00,"['Aviad Rubinstein', 'Raghuvansh R. Saxena', 'Clayton Thomas', 'S. Mathew Weinberg', 'Junyao Zhao']",cs.GT,"We consider the problem of implementing a fixed social choice function
between multiple players (which takes as input a type $t_i$ from each player
$i$ and outputs an outcome $f(t_1,\ldots, t_n)$), in which each player must be
incentivized to follow the protocol. In particular, we study the communication
requirements of a protocol which: (a) implements $f$, (b) implements $f$ and
computes payments that make it ex-post incentive compatible (EPIC) to follow
the protocol, and (c) implements $f$ and computes payments in a way that makes
it dominant-strategy incentive compatible (DSIC) to follow the protocol.
  We show exponential separations between all three of these quantities,
already for just two players. That is, we first construct an $f$ such that $f$
can be implemented in communication $c$, but any EPIC implementation of $f$
(with any choice of payments) requires communication $\exp(c)$. This answers an
open question of [FS09, BBS13]. Second, we construct an $f$ such that an EPIC
protocol implements $f$ with communication $C$, but all DSIC implementations of
$f$ require communication $\exp(C)$.",2020
http://arxiv.org/abs/2012.14886v2,A Pairwise Strategic Network Formation Model with Group Heterogeneity: With an Application to International Travel,2020-12-29 18:24:35+00:00,['Tadao Hoshino'],econ.EM,"In this study, we consider a pairwise network formation model in which each
dyad of agents strategically determines the link status between them. Our model
allows the agents to have unobserved group heterogeneity in the propensity of
link formation. For the model estimation, we propose a three-step maximum
likelihood (ML) method. First, we obtain consistent estimates for the
heterogeneity parameters at individual level using the ML estimator. Second, we
estimate the latent group structure using the binary segmentation algorithm
based on the results obtained from the first step. Finally, based on the
estimated group membership, we re-execute the ML estimation. Under certain
regularity conditions, we show that the proposed estimator is asymptotically
unbiased and distributed as normal at the parametric rate. As an empirical
illustration, we focus on the network data of international visa-free travels.
The results indicate the presence of significant strategic complementarity and
a certain level of degree heterogeneity in the network formation behavior.",2020
http://arxiv.org/abs/2012.14823v2,Bias-Aware Inference in Regularized Regression Models,2020-12-29 16:06:43+00:00,"['Timothy B. Armstrong', 'Michal Kolesár', 'Soonwoo Kwon']",econ.EM,"We consider inference on a scalar regression coefficient under a constraint
on the magnitude of the control coefficients. A class of estimators based on a
regularized propensity score regression is shown to exactly solve a tradeoff
between worst-case bias and variance. We derive confidence intervals (CIs)
based on these estimators that are bias-aware: they account for the possible
bias of the estimator. Under homoskedastic Gaussian errors, these estimators
and CIs are near-optimal in finite samples for MSE and CI length. We also
provide conditions for asymptotic validity of the CI with unknown and possibly
heteroskedastic error distribution, and derive novel optimal rates of
convergence under high-dimensional asymptotics that allow the number of
regressors to increase more quickly than the number of observations. Extensive
simulations and an empirical application illustrate the performance of our
methods.",2020
http://arxiv.org/abs/2012.14820v2,Bayesian analysis of seasonally cointegrated VAR model,2020-12-29 15:57:54+00:00,['Justyna Wróblewska'],econ.EM,"The paper aims at developing the Bayesian seasonally cointegrated model for
quarterly data. We propose the prior structure, derive the set of full
conditional posterior distributions, and propose the sampling scheme. The
identification of cointegrating spaces is obtained \emph{via} orthonormality
restrictions imposed on vectors spanning them. In the case of annual frequency,
the cointegrating vectors are complex, which should be taken into account when
identifying them. The point estimation of the cointegrating spaces is also
discussed. The presented methods are illustrated by a simulation experiment and
are employed in the analysis of money and prices in the Polish economy.",2020
http://arxiv.org/abs/2012.14693v1,The impact of Climate on Economic and Financial Cycles: A Markov-switching Panel Approach,2020-12-29 10:09:46+00:00,"['Monica Billio', 'Roberto Casarin', 'Enrica De Cian', 'Malcolm Mistry', 'Anthony Osuntuyi']",econ.EM,"This paper examines the impact of climate shocks on 13 European economies
analysing jointly business and financial cycles, in different phases and
disentangling the effects for different sector channels. A Bayesian Panel
Markov-switching framework is proposed to jointly estimate the impact of
extreme weather events on the economies as well as the interaction between
business and financial cycles. Results from the empirical analysis suggest that
extreme weather events impact asymmetrically across the different phases of the
economy and heterogeneously across the EU countries. Moreover, we highlight how
the manufacturing output, a component of the industrial production index,
constitutes the main channel through which climate shocks impact the EU
economies.",2020
http://arxiv.org/abs/2012.14557v3,Twofold Multiprior Preferences and Failures of Contingent Reasoning,2020-12-29 01:36:26+00:00,"['Federico Echenique', 'Masaki Miyashita', 'Yuta Nakamura', 'Luciano Pomatto', 'Jamie Vinson']",econ.TH,"We propose a model of incomplete \textit{twofold multiprior preferences}, in
which an act $f$ is ranked above an act $g$ only when $f$ provides higher
utility in a worst-case scenario than what $g$ provides in a best-case
scenario. The model explains failures of contingent reasoning, captured through
a weakening of the state-by-state monotonicity (or dominance) axiom. Our model
gives rise to rich comparative statics results, as well as extension exercises,
and connections to choice theory. We present an application to second-price
auctions.",2020
http://arxiv.org/abs/2012.14503v1,"Growth, development, and structural change at the firm-level: The example of the PR China",2020-12-28 22:05:40+00:00,"['Torsten Heinrich', 'Jangho Yang', 'Shuanping Dai']",econ.GN,"Understanding the microeconomic details of technological catch-up processes
offers great potential for informing both innovation economics and development
policy. We study the economic transition of the PR China from an agrarian
country to a high-tech economy as one example for such a case. It is clear from
past literature that rapidly rising productivity levels played a crucial role.
However, the distribution of labor productivity in Chinese firms has not been
comprehensively investigated and it remains an open question if this can be
used to guide economic development. We analyze labor productivity and the
dynamic change of labor productivity in firm-level data for the years 1998-2013
from the Chinese Industrial Enterprise Database. We demonstrate that both
variables are conveniently modeled as L\'evy alpha-stable distributions,
provide parameter estimates and analyze dynamic changes to this distribution.
We find that the productivity gains were not due to super-star firms, but due
to a systematic shift of the entire distribution with otherwise mostly
unchanged characteristics. We also found an emerging right-skew in the
distribution of labor productivity change. While there are significant
differences between the 31 provinces and autonomous regions of the P.R. China,
we also show that there are systematic relations between micro-level and
province-level variables. We conclude with some implications of these findings
for development policy.",2020
http://arxiv.org/abs/2012.14372v1,On a Japanese Subjective Well-Being Indicator Based on Twitter data,2020-12-28 17:28:30+00:00,"['Tiziana Carpi', 'Airo Hino', 'Stefano Maria Iacus', 'Giuseppe Porro']",stat.AP,"This study presents for the first time the SWB-J index, a subjective
well-being indicator for Japan based on Twitter data. The index is composed by
eight dimensions of subjective well-being and is estimated relying on Twitter
data by using human supervised sentiment analysis. The index is then compared
with the analogous SWB-I index for Italy, in order to verify possible analogies
and cultural differences. Further, through structural equation models, a causal
assumption is tested to see whether the economic and health conditions of the
country influence the well-being latent variable and how this latent dimension
affects the SWB-J and SWB-I indicators. It turns out that, as expected, the
economic and health welfare is only one aspect of the multidimensional
well-being that is captured by the Twitter-based indicator.",2020
http://arxiv.org/abs/2012.14432v5,Local Dominance,2020-12-28 15:34:55+00:00,"['Emiliano Catonini', 'Jingyi Xue']",econ.TH,"We define notions of dominance between two actions in a dynamic game. Local
dominance considers players who have a blurred view of the future and compare
the two actions by first focusing on the outcomes that may realize at the
current stage. When considering the possibility that the game may continue,
they can only check that the local comparison is not overturned under the
assumption of ""continuing in the same way"" after the two actions (in a newly
defined sense). Despite the lack of forward planning, local dominance solves
dynamic mechanisms that were found easy to play and implements social choice
functions that cannot be implemented in obviously-dominant strategies.",2020
http://arxiv.org/abs/2012.14153v1,Succeeding at home and abroad -- Accounting for the international spillovers of cities' SDG actions,2020-12-28 09:08:17+00:00,"['Rebecka Ericsdotter Engstrom', 'David Collste', 'Sarah E. Cornell', 'Francis X Johnson', 'Henrik Carlsen', 'Fernando Jaramillo', 'Goran Finnveden', 'Georgia Destouni', 'Mark Howells', 'Nina Weitz', 'Viveka Palm', 'Francesco Fuso-Nerini']",physics.soc-ph,"Local SDG action is imperative to reach the 2030 Agenda, but different
strategies for progressing on one SDG locally may cause different 'spillovers'
on the same and other SDGs beyond local and national borders. We call for
research efforts to empower local authorities to 'account globally' when acting
locally.",2020
http://arxiv.org/abs/2012.13937v2,Time-Transformed Test for the Explosive Bubbles under Non-stationary Volatility,2020-12-27 13:20:48+00:00,"['Eiji Kurozumi', 'Anton Skrobotov', 'Alexey Tsarev']",econ.EM,"This paper is devoted to testing for the explosive bubble under time-varying
non-stationary volatility. Because the limiting distribution of the seminal
Phillips et al. (2011) test depends on the variance function and usually
requires a bootstrap implementation under heteroskedasticity, we construct the
test based on a deformation of the time domain. The proposed test is
asymptotically pivotal under the null hypothesis and its limiting distribution
coincides with that of the standard test under homoskedasticity, so that the
test does not require computationally extensive methods for inference.
Appealing finite sample properties are demonstrated through Monte-Carlo
simulations. An empirical application demonstrates that the upsurge behavior of
cryptocurrency time series in the middle of the sample is partially explained
by the volatility change.",2020
http://arxiv.org/abs/2012.13830v1,Calculated Boldness: Optimizing Financial Decisions with Illiquid Assets,2020-12-26 23:33:33+00:00,"['Stanislav Shalunov', 'Alexei Kitaev', 'Yakov Shalunov', 'Arseniy Akopyan']",q-fin.PM,"We consider games of chance played by someone with external capital that
cannot be applied to the game and determine how this affects risk-adjusted
optimal betting. Specifically, we focus on Kelly optimization as a metric,
optimizing the expected logarithm of total capital including both capital in
play and the external capital. For games with multiple rounds, we determine the
optimal strategy through dynamic programming and construct a close
approximation through the WKB method. The strategy can be described in terms of
short-term utility functions, with risk aversion depending on the ratio of the
amount in the game to the external money. Thus, a rational player's behavior
varies between conservative play that approaches Kelly strategy as they are
able to invest a larger fraction of total wealth and extremely aggressive play
that maximizes linear expectation when a larger portion of their capital is
locked away. Because you always have expected future productivity to account
for as external resources, this goes counter to the conventional wisdom that
super-Kelly betting is a ruinous proposition.",2020
http://arxiv.org/abs/2012.13816v1,"Value of agreement in decision analysis: Concept, measures and application",2020-12-26 21:26:39+00:00,['Tom Pape'],econ.GN,"In multi-criteria decision analysis workshops, participants often appraise
the options individually before discussing the scoring as a group. The
individual appraisals lead to score ranges within which the group then seeks
the necessary agreement to identify their preferred option. Preference
programming enables some options to be identified as dominated even before the
group agrees on a precise scoring for them. Workshop participants usually face
time pressure to make a decision. Decision support can be provided by flagging
options for which further agreement on their scores seems particularly
valuable. By valuable, we mean the opportunity to identify other options as
dominated (using preference programming) without having their precise scores
agreed beforehand. The present paper quantifies this Value of Agreement and
extends the concept to portfolio decision analysis and criterion weights. The
new concept is validated through a case study in recruitment.",2020
http://arxiv.org/abs/2012.13813v1,Prioritising data items for business analytics: Framework and application to human resources,2020-12-26 21:19:39+00:00,['Tom Pape'],econ.GN,"The popularity of business intelligence (BI) systems to support business
analytics has tremendously increased in the last decade. The determination of
data items that should be stored in the BI system is vital to ensure the
success of an organisation's business analytic strategy. Expanding conventional
BI systems often leads to high costs of internally generating, cleansing and
maintaining new data items whilst the additional data storage costs are in many
cases of minor concern -- what is a conceptual difference to big data systems.
Thus, potential additional insights resulting from a new data item in the BI
system need to be balanced with the often high costs of data creation. While
the literature acknowledges this decision problem, no model-based approach to
inform this decision has hitherto been proposed. The present research describes
a prescriptive framework to prioritise data items for business analytics and
applies it to human resources. To achieve this goal, the proposed framework
captures core business activities in a comprehensive process map and assesses
their relative importance and possible data support with multi-criteria
decision analysis.",2020
http://arxiv.org/abs/2012.13805v4,Weighting-Based Treatment Effect Estimation via Distribution Learning,2020-12-26 20:15:44+00:00,"['Dongcheng Zhang', 'Kunpeng Zhang']",cs.LG,"Existing weighting methods for treatment effect estimation are often built
upon the idea of propensity scores or covariate balance. They usually impose
strong assumptions on treatment assignment or outcome model to obtain unbiased
estimation, such as linearity or specific functional forms, which easily leads
to the major drawback of model mis-specification. In this paper, we aim to
alleviate these issues by developing a distribution learning-based weighting
method. We first learn the true underlying distribution of covariates
conditioned on treatment assignment, then leverage the ratio of covariates'
density in the treatment group to that of the control group as the weight for
estimating treatment effects. Specifically, we propose to approximate the
distribution of covariates in both treatment and control groups through
invertible transformations via change of variables. To demonstrate the
superiority, robustness, and generalizability of our method, we conduct
extensive experiments using synthetic and real data. From the experiment
results, we find that our method for estimating average treatment effect on
treated (ATT) with observational data outperforms several cutting-edge
weighting-only benchmarking methods, and it maintains its advantage under a
doubly-robust estimation framework that combines weighting with some advanced
outcome modeling methods.",2020
http://arxiv.org/abs/2012.13710v1,Analysis of Randomized Experiments with Network Interference and Noncompliance,2020-12-26 10:11:20+00:00,['Bora Kim'],econ.EM,"Randomized experiments have become a standard tool in economics. In analyzing
randomized experiments, the traditional approach has been based on the Stable
Unit Treatment Value (SUTVA: \cite{rubin}) assumption which dictates that there
is no interference between individuals. However, the SUTVA assumption fails to
hold in many applications due to social interaction, general equilibrium,
and/or externality effects. While much progress has been made in relaxing the
SUTVA assumption, most of this literature has only considered a setting with
perfect compliance to treatment assignment. In practice, however, noncompliance
occurs frequently where the actual treatment receipt is different from the
assignment to the treatment. In this paper, we study causal effects in
randomized experiments with network interference and noncompliance. Spillovers
are allowed to occur at both treatment choice stage and outcome realization
stage. In particular, we explicitly model treatment choices of agents as a
binary game of incomplete information where resulting equilibrium treatment
choice probabilities affect outcomes of interest. Outcomes are further
characterized by a random coefficient model to allow for general unobserved
heterogeneity in the causal effects. After defining our causal parameters of
interest, we propose a simple control function estimator and derive its
asymptotic properties under large-network asymptotics. We apply our methods to
the randomized subsidy program of \cite{dupas} where we find evidence of
spillover effects on both short-run and long-run adoption of
insecticide-treated bed nets. Finally, we illustrate the usefulness of our
methods by analyzing the impact of counterfactual subsidy policies.",2020
http://arxiv.org/abs/2012.13657v1,Negative votes to depolarize politics,2020-12-26 01:05:24+00:00,['Karthik H. Shankar'],econ.TH,"The controversies around the 2020 US presidential elections certainly casts
serious concerns on the efficiency of the current voting system in representing
the people's will. Is the naive Plurality voting suitable in an extremely
polarized political environment? Alternate voting schemes are gradually gaining
public support, wherein the voters rank their choices instead of just voting
for their first preference. However they do not capture certain crucial aspects
of voter preferences like disapprovals and negativities against candidates. I
argue that these unexpressed negativities are the predominant source of
polarization in politics. I propose a voting scheme with an explicit expression
of these negative preferences, so that we can simultaneously decipher the
popularity as well as the polarity of each candidate. The winner is picked by
an optimal tradeoff between the most popular and the least polarizing
candidate. By penalizing the candidates for their polarization, we can
discourage the divisive campaign rhetorics and pave way for potential third
party candidates.",2020
http://arxiv.org/abs/2012.13650v1,A Theory of Updating Ambiguous Information,2020-12-26 00:12:59+00:00,['Rui Tang'],econ.TH,"We introduce a new updating rule, the conditional maximum likelihood rule
(CML) for updating ambiguous information. The CML formula replaces the
likelihood term in Bayes' rule with the maximal likelihood of the given signal
conditional on the state. We show that CML satisfies a new axiom, increased
sensitivity after updating, while other updating rules do not. With CML, a
decision maker's posterior is unaffected by the order in which independent
signals arrive. CML also accommodates recent experimental findings on updating
signals of unknown accuracy and has simple predictions on learning with such
signals. We show that an information designer can almost achieve her maximal
payoff with a suitable ambiguous information structure whenever the agent
updates according to CML.",2020
http://arxiv.org/abs/2012.13614v1,Quantile regression with generated dependent variable and covariates,2020-12-25 19:12:18+00:00,['Jayeeta Bhattacharya'],econ.EM,"We study linear quantile regression models when regressors and/or dependent
variable are not directly observed but estimated in an initial first step and
used in the second step quantile regression for estimating the quantile
parameters. This general class of generated quantile regression (GQR) covers
various statistical applications, for instance, estimation of endogenous
quantile regression models and triangular structural equation models, and some
new relevant applications are discussed. We study the asymptotic distribution
of the two-step estimator, which is challenging because of the presence of
generated covariates and/or dependent variable in the non-smooth quantile
regression estimator. We employ techniques from empirical process theory to
find uniform Bahadur expansion for the two step estimator, which is used to
establish the asymptotic results. We illustrate the performance of the GQR
estimator through simulations and an empirical application based on auctions.",2020
http://arxiv.org/abs/2012.13514v1,Using social recognition to address the gender difference in volunteering for low-promotability tasks,2020-12-25 04:41:57+00:00,"['Ritwik Banerjee', 'Priyoma Mustafi']",econ.GN,"Research shows that women volunteer significantly more for tasks that people
prefer others to complete. Such tasks carry little monetary incentives because
of their very nature. We use a modified version of the volunteer's dilemma game
to examine if non-monetary interventions, particularly, social recognition can
be used to change the gender norms associated with such tasks. We design three
treatments, where a) a volunteer receives positive social recognition, b) a
non-volunteer receives negative social recognition, and c) a volunteer receives
positive, but a non-volunteer receives negative social recognition. Our results
indicate that competition for social recognition increases the overall
likelihood that someone in a group has volunteered. Positive social recognition
closes the gender gap observed in the baseline treatment, so does the
combination of positive and negative social recognition. Our results,
consistent with the prior literature on gender differences in competition,
suggest that public recognition of volunteering can change the default gender
norms in organizations and increase efficiency at the same time.",2020
http://arxiv.org/abs/2012.13331v2,Computation of Convex Hull Prices in Electricity Markets with Non-Convexities using Dantzig-Wolfe Decomposition,2020-12-24 17:06:26+00:00,"['Panagiotis Andrianesis', 'Dimitris Bertsimas', 'Michael C. Caramanis', 'William W. Hogan']",math.OC,"The presence of non-convexities in electricity markets has been an active
research area for about two decades. The -- inevitable under current marginal
cost pricing -- problem of guaranteeing that no market participant incurs
losses in the day-ahead market is addressed in current practice through
make-whole payments a.k.a. uplift. Alternative pricing rules have been studied
to deal with this problem. Among them, Convex Hull (CH) prices associated with
minimum uplift have attracted significant attention. Several US Independent
System Operators (ISOs) have considered CH prices but resorted to
approximations, mainly because determining exact CH prices is computationally
challenging, while providing little intuition about the price formation
rationale. In this paper, we describe the CH price estimation problem by
relying on Dantzig-Wolfe decomposition and Column Generation, as a tractable,
highly paralellizable, and exact method -- i.e., yielding exact, not
approximate, CH prices -- with guaranteed finite convergence. Moreover, the
approach provides intuition on the underlying price formation rationale. A test
bed of stylized examples provide an exposition of the intuition in the CH price
formation. In addition, a realistic ISO dataset is used to support scalability
and validate the proof-of-concept.",2020
http://arxiv.org/abs/2012.13267v1,Filtering the intensity of public concern from social media count data with jumps,2020-12-24 14:18:04+00:00,"['Matteo Iacopini', 'Carlo R. M. A. Santagiustina']",stat.AP,"Count time series obtained from online social media data, such as Twitter,
have drawn increasing interest among academics and market analysts over the
past decade. Transforming Web activity records into counts yields time series
with peculiar features, including the coexistence of smooth paths and sudden
jumps, as well as cross-sectional and temporal dependence. Using Twitter posts
about country risks for the United Kingdom and the United States, this paper
proposes an innovative state space model for multivariate count data with
jumps. We use the proposed model to assess the impact of public concerns in
these countries on market systems. To do so, public concerns inferred from
Twitter data are unpacked into country-specific persistent terms, risk social
amplification events, and co-movements of the country series. The identified
components are then used to investigate the existence and magnitude of
country-risk spillovers and social amplification effects on the volatility of
financial markets.",2020
http://arxiv.org/abs/2101.01065v1,Predicting the Performance of a Future United Kingdom Grid and Wind Fleet When Providing Power to a Fleet of Battery Electric Vehicles,2020-12-24 03:40:35+00:00,"['Anthony D Stephens', 'David R Walwyn']",econ.GN,"Sales of new petrol and diesel passenger vehicles may not be permitted in the
United Kingdom (UK) post-2030. Should this happen, it is likely that vehicles
presently powered by hydrocarbons will be progressively replaced by Battery
Electric Vehicles (BEVs). This paper describes the use of mathematical
modelling, drawing on real time records of the UK electricity grid, to
investigate the likely performance of the grid when supplying power to a fleet
of up to 35 million BEVs. The model highlights the importance of understanding
how the grid will cope when powering a BEV fleet under conditions similar to
those experienced during an extended wind lull during the 3rd week of January
2017. Allowing a two-way flow of electricity between the BEVs and the grid,
known as the vehicle-to-grid (V2G) configuration, turns out to be of key
importance in minimising the need for additional gas turbine generation or
energy storage during wind lulls. This study has shown that with the use of
V2G, it should be possible to provide power to about 15 million BEVs with the
gas turbine capacity currently available. Without V2G, it is likely that the
current capacity of the gas turbines and associated gas infrastructure might be
overwhelmed by even a relatively small BEV fleet. Since it is anticipated that
80% of BEV owners will be able to park the vehicles at their residences,
widespread V2G will enable both the powering of residences when supply from the
grid is constrained and the charging of BEVs when supply is in excess. The
model shows that this configuration will maintain a constant load on the grid
and avoid the use of either expensive alternative storage or hydrogen obtained
by reforming methane. There should be no insuperable problem in providing power
to the 20% of BEV owners who do not have parking at their residences; their
power could come directly from the grid.",2020
http://arxiv.org/abs/2012.12982v1,Awareness Logic: A Kripke-based Rendition of the Heifetz-Meier-Schipper Model,2020-12-23 21:24:06+00:00,"['Gaia Belardinelli', 'Rasmus K. Rendsvig']",cs.AI,"Heifetz, Meier and Schipper (HMS) present a lattice model of awareness. The
HMS model is syntax-free, which precludes the simple option to rely on formal
language to induce lattices, and represents uncertainty and unawareness with
one entangled construct, making it difficult to assess the properties of
either. Here, we present a model based on a lattice of Kripke models, induced
by atom subset inclusion, in which uncertainty and unawareness are separate. We
show the models to be equivalent by defining transformations between them which
preserve formula satisfaction, and obtain completeness through our and HMS'
results.",2020
http://arxiv.org/abs/2012.12802v3,Machine Learning Advances for Time Series Forecasting,2020-12-23 17:01:56+00:00,"['Ricardo P. Masini', 'Marcelo C. Medeiros', 'Eduardo F. Mendes']",econ.EM,"In this paper we survey the most recent advances in supervised machine
learning and high-dimensional models for time series forecasting. We consider
both linear and nonlinear alternatives. Among the linear methods we pay special
attention to penalized regressions and ensemble of models. The nonlinear
methods considered in the paper include shallow and deep neural networks, in
their feed-forward and recurrent versions, and tree-based methods, such as
random forests and boosted trees. We also consider ensemble and hybrid models
by combining ingredients from different alternatives. Tests for superior
predictive ability are briefly reviewed. Finally, we discuss application of
machine learning in economics and finance and provide an illustration with
high-frequency financial data.",2020
http://arxiv.org/abs/2012.12704v1,Estimating The Effect Of Subscription based Streaming Services On The Demand For Game Consoles,2020-12-23 14:30:55+00:00,"['Tung Yu Marco Chan', 'Yue Zhang', 'Tsun Yi Yeung']",econ.GN,"In this paper, we attempt to estimate the effect of the implementation of
subscription-based streaming services on the demand of the associated game
consoles. We do this by applying the BLP demand estimation model proposed by
Berry (1994). This results in a linear demand specification which can be
identified using conventional identification methods such as instrumental
variables estimation and fixed-effects models. We find that given our dataset,
the two-stage least squares (2SLS) regression provides us with convincing
estimates that subscription-based streaming services does have a positive
effect on the demand of game consoles as proposed by the general principle of
complementary goods.",2020
http://arxiv.org/abs/2012.12550v3,Invidious Comparisons: Ranking and Selection as Compound Decisions,2020-12-23 09:21:38+00:00,"['Jiaying Gu', 'Roger Koenker']",econ.EM,"There is an innate human tendency, one might call it the ""league table
mentality,"" to construct rankings. Schools, hospitals, sports teams, movies,
and myriad other objects are ranked even though their inherent
multi-dimensionality would suggest that -- at best -- only partial orderings
were possible. We consider a large class of elementary ranking problems in
which we observe noisy, scalar measurements of merit for $n$ objects of
potentially heterogeneous precision and are asked to select a group of the
objects that are ""most meritorious."" The problem is naturally formulated in the
compound decision framework of Robbins's (1956) empirical Bayes theory, but it
also exhibits close connections to the recent literature on multiple testing.
The nonparametric maximum likelihood estimator for mixture models (Kiefer and
Wolfowitz (1956)) is employed to construct optimal ranking and selection rules.
Performance of the rules is evaluated in simulations and an application to
ranking U.S kidney dialysis centers.",2020
http://arxiv.org/abs/2012.12503v2,Cities in a world of diminishing transport costs,2020-12-23 06:03:31+00:00,"['Tomoya Mori', 'Minoru Osawa']",econ.GN,"Economic activities favor mutual geographical proximity and concentrate
spatially to form cities. In a world of diminishing transport costs, however,
the advantage of physical proximity is fading, and the role of cities in the
economy may be declining. To provide insights into the long-run evolution of
cities, we analyzed Japan's census data over the 1970--2015 period. We found
that fewer and larger cities thrived at the national scale, suggesting an
eventual mono-centric economy with a single megacity; simultaneously, each
larger city flattened out at the local scale, suggesting an eventual extinction
of cities. We interpret this multi-scale phenomenon as an instance of pattern
formation by self-organization, which is widely studied in mathematics and
biology. However, cities' dynamics are distinct from mathematical or biological
mechanisms because they are governed by economic interactions mediated by
transport costs between locations. Our results call for the synthesis of
knowledge in mathematics, biology, and economics to open the door for a general
pattern formation theory that is applicable to socioeconomic phenomena.",2020
http://arxiv.org/abs/2012.12422v1,Topological data analysis and UNICEF Multiple Indicator Cluster Surveys,2020-12-23 00:04:42+00:00,"['Jun Ru Anderson', 'Fahrudin Memic', 'Ismar Volic']",physics.soc-ph,"Multiple Indicator Cluster Surveys (MICS), supported by UNICEF, are one of
the most important global household survey programs that provide data on health
and education of women and children. We analyze the Serbia 2014-15 MICS dataset
using topological data analysis which treats the data cloud as a topological
space and extracts information about its intrinsic geometric properties. In
particular, our analysis uses the Mapper algorithm, a dimension-reduction and
clustering method which produces a graph from the data cloud. The resulting
Mapper graph provides insight into various relationships between household
wealth - as expressed by the wealth index, an important indicator extracted
from the MICS data - and other parameters such as urban/rural setting,
ownership of items, and prioritization of possessions. Among other uses, these
findings can serve to inform policy by providing a hierarchy of essential
amenities. They can also potentially be used to refine the wealth index or
deepen our understanding of what it captures.",2020
http://arxiv.org/abs/2101.01531v1,"Predicting Residential Property Value in Catonsville, Maryland: A Comparison of Multiple Regression Techniques",2020-12-22 20:27:35+00:00,"['Lee Whieldon', 'Huthaifa Ashqar']",stat.AP,"Predicting Residential Property Value in Catonsville, Maryland: A Comparison
of Multiple Regression Techniques",2020
http://arxiv.org/abs/2012.12118v4,Social distancing in networks: A web-based interactive experiment,2020-12-22 15:56:33+00:00,"['Edoardo Gallo', 'Darija Barak', 'Alastair Langtry']",econ.GN,"Governments have used social distancing to stem the spread of COVID-19, but
lack evidence on the most effective policy to ensure compliance. We examine the
effectiveness of fines and informational messages (nudges) in promoting social
distancing in a web-based interactive experiment conducted during the first
wave of the pandemic on a near-representative sample of the US population.
Fines promote distancing, but nudges only have a marginal impact. Individuals
do more social distancing when they are aware they are a superspreader. Using
an instrumental variable approach, we argue progressives are more likely to
practice distancing, and they are marginally more responsive to fines.",2020
http://arxiv.org/abs/2012.12060v3,Information Leakage Games: Exploring Information as a Utility Function,2020-12-22 14:51:30+00:00,"['Mário S. Alvim', 'Konstantinos Chatzikokolakis', 'Yusuke Kawamoto', 'Catuscia Palamidessi']",cs.CR,"A common goal in the areas of secure information flow and privacy is to build
effective defenses against unwanted leakage of information. To this end, one
must be able to reason about potential attacks and their interplay with
possible defenses. In this paper, we propose a game-theoretic framework to
formalize strategies of attacker and defender in the context of information
leakage, and provide a basis for developing optimal defense methods. A novelty
of our games is that their utility is given by information leakage, which in
some cases may behave in a non-linear way. This causes a significant deviation
from classic game theory, in which utility functions are linear with respect to
players' strategies. Hence, a key contribution of this paper is the
establishment of the foundations of information leakage games. We consider two
kinds of games, depending on the notion of leakage considered. The first kind,
the QIF-games, is tailored for the theory of quantitative information flow
(QIF). The second one, the DP-games, corresponds to differential privacy (DP).",2020
http://arxiv.org/abs/2012.12020v1,How many people are infected? A case study on SARS-CoV-2 prevalence in Austria,2020-12-22 14:12:06+00:00,['Gabriel Ziegler'],econ.GN,"Using recent data from voluntary mass testing, I provide credible bounds on
prevalence of SARS-CoV-2 for Austrian counties in early December 2020. When
estimating prevalence, a natural missing data problem arises: no test results
are generated for non-tested people. In addition, tests are not perfectly
predictive for the underlying infection. This is particularly relevant for mass
SARS-CoV-2 testing as these are conducted with rapid Antigen tests, which are
known to be somewhat imprecise. Using insights from the literature on partial
identification, I propose a framework addressing both issues at once. I use the
framework to study differing selection assumptions for the Austrian data.
Whereas weak monotone selection assumptions provide limited identification
power, reasonably stronger assumptions reduce the uncertainty on prevalence
significantly.",2020
http://arxiv.org/abs/2012.11935v1,Split-then-Combine simplex combination and selection of forecasters,2020-12-22 11:19:38+00:00,"['Antonio Martin Arroyo', 'Aranzazu de Juan Fernandez']",econ.EM,"This paper considers the Split-Then-Combine (STC) approach (Arroyo and de
Juan, 2014) to combine forecasts inside the simplex space, the sample space of
positive weights adding up to one. As it turns out, the simplicial statistic
given by the center of the simplex compares favorably against the fixed-weight,
average forecast. Besides, we also develop a Combine-After-Selection (CAS)
method to get rid of redundant forecasters. We apply these two approaches to
make out-of-sample one-step ahead combinations and subcombinations of forecasts
for several economic variables. This methodology is particularly useful when
the sample size is smaller than the number of forecasts, a case where other
methods (e.g., Least Squares (LS) or Principal Component Analysis (PCA)) are
not applicable.",2020
http://arxiv.org/abs/2012.11900v2,Expanding on Repeated Consumer Search Using Multi-Armed Bandits and Secretaries,2020-12-22 09:53:55+00:00,['Tung Yu Marco Chan'],econ.TH,"We seek to take a different approach in deriving the optimal search policy
for the repeated consumer search model found in Fishman and Rob (1995) with the
main motivation of dropping the assumption of prior knowledge of the price
distribution $F(p)$ in each period. We will do this by incorporating the famous
multi-armed bandit problem (MAB). We start by modifying the MAB framework to
fit the setting of the repeated consumer search model and formulate the
objective as a dynamic optimization problem. Then, given any sequence of
exploration, we assign a value to each store in that sequence using Bellman
equations. We then proceed to break down the problem into individual optimal
stopping problems for each period which incidentally coincides with the
framework of the famous secretary problem where we proceed to derive the
optimal stopping policy. We will see that implementing the optimal stopping
policy in each period solves the original dynamic optimization by `forward
induction' reasoning.",2020
http://arxiv.org/abs/2101.02628v2,Analyzing the response to TV serials retelecast during COVID19 lockdown in India,2020-12-22 07:05:32+00:00,['Sandeep Ranjan'],econ.GN,"TV serials are a popular source of entertainment. The ongoing COVID19
lockdown has a high probability of degrading the publics mental health. The
Government of India started the retelecast of yesteryears popular TV serials on
public broadcaster Doordarshan from 28th March 2020 to 31st July 2020. Tweets
corresponding to the Doordarshan hashtag were mined to create a dataset. The
experiment aims to analyze the publics response to the retelecast of TV serials
by calculating the sentiment score of the tweet dataset. Datasets mean
sentiment score of 0.65 and high share 64.58% of positive tweets signifies the
acceptance of Doordarshans retelecast decision. The sentiment analysis result
also reflects the positive state of mind of the public.",2020
http://arxiv.org/abs/2012.15753v1,"The Role of Referrals in Immobility, Inequality, and Inefficiency in Labor Markets",2020-12-22 06:34:59+00:00,"['Lukas Bolte', 'Nicole Immorlica', 'Matthew O. Jackson']",econ.GN,"We study the consequences of job markets' heavy reliance on referrals.
Referrals screen candidates and lead to better matches and increased
productivity, but disadvantage job-seekers who have few or no connections to
employed workers, leading to increased inequality. Coupled with homophily,
referrals also lead to immobility: a demographic group's low current employment
rate leads that group to have relatively low future employment as well. We
identify conditions under which distributing referrals more evenly across a
population not only reduces inequality, but also improves future productivity
and economic mobility. We use the model to examine optimal policies, showing
that one-time affirmative action policies involve short-run production losses,
but lead to long-term improvements in equality, mobility, and productivity due
to induced changes in future referrals. We also examine how the possibility of
firing workers changes the effects of referrals.",2020
http://arxiv.org/abs/2012.12861v2,"Credit Freezes, Equilibrium Multiplicity, and Optimal Bailouts in Financial Networks",2020-12-22 06:02:09+00:00,"['Matthew O. Jackson', 'Agathe Pernoud']",cs.GT,"We analyze how interdependencies between organizations in financial networks
can lead to multiple possible equilibrium outcomes. A multiplicity arises if
and only if there exists a certain type of dependency cycle in the network that
allows for self-fulfilling chains of defaults. We provide necessary and
sufficient conditions for banks' solvency in any equilibrium. Building on these
conditions, we characterize the minimum bailout payments needed to ensure
systemic solvency, as well as how solvency can be ensured by guaranteeing a
specific set of debt payments. Bailout injections needed to eliminate
self-fulfilling cycles of defaults (credit freezes) are fully recoverable,
while those needed to prevent cascading defaults outside of cycles are not. We
show that the minimum bailout problem is computationally hard, but provide an
upper bound on optimal payments and show that the problem has intuitive
solutions in specific network structures such as those with disjoint cycles or
a core-periphery structure.",2020
http://arxiv.org/abs/2012.11768v3,Estimating the Impact of Weather on Agriculture,2020-12-22 00:41:06+00:00,"['Jeffrey D. Michler', 'Anna Josephson', 'Talip Kilic', 'Siobhan Murray']",econ.GN,"This paper quantifies the significance and magnitude of the effect of
measurement error in remote sensing weather data in the analysis of smallholder
agricultural productivity. The analysis leverages 17 rounds of
nationally-representative, panel household survey data from six countries in
Sub-Saharan Africa. These data are spatially-linked with a range of geospatial
weather data sources and related metrics. We provide systematic evidence on
measurement error introduced by 1) different methods used to obfuscate the
exact GPS coordinates of households, 2) different metrics used to quantify
precipitation and temperature, and 3) different remote sensing measurement
technologies. First, we find no discernible effect of measurement error
introduced by different obfuscation methods. Second, we find that simple
weather metrics, such as total seasonal rainfall and mean daily temperature,
outperform more complex metrics, such as deviations in rainfall from the
long-run average or growing degree days, in a broad range of settings. Finally,
we find substantial amounts of measurement error based on remote sensing
product. In extreme cases, data drawn from different remote sensing products
result in opposite signs for coefficients on weather metrics, meaning that
precipitation or temperature draw from one product purportedly increases crop
output while the same metrics drawn from a different product purportedly
reduces crop output. We conclude with a set of six best practices for
researchers looking to combine remote sensing weather data with socioeconomic
survey data.",2020
http://arxiv.org/abs/2012.12951v1,"If Global or Local Investor Sentiments are Prone to Developing an Impact on Stock Returns, is there an Industry Effect?",2020-12-21 22:51:42+00:00,"['Jing Shi', 'Marcel Ausloos', 'Tingting Zhu']",econ.GN,"This paper investigates the heterogeneous impacts of either Global or Local
Investor Sentiments on stock returns. We study 10 industry sectors through the
lens of 6 (so called) emerging countries: China, Brazil, India, Mexico,
Indonesia and Turkey, over the 2000 to 2014 period. Using a panel data
framework, our study sheds light on a significant effect of Local Investor
Sentiments on expected returns for basic materials, consumer goods, industrial,
and financial industries. Moreover, our results suggest that from Global
Investor Sentiments alone, one cannot predict expected stock returns in these
markets.",2020
http://arxiv.org/abs/2012.11679v5,Discordant Relaxations of Misspecified Models,2020-12-21 21:04:14+00:00,"['Lixiong Li', 'Désiré Kédagni', 'Ismaël Mourifié']",econ.EM,"In many set-identified models, it is difficult to obtain a tractable
characterization of the identified set. Therefore, researchers often rely on
non-sharp identification conditions, and empirical results are often based on
an outer set of the identified set. This practice is often viewed as
conservative yet valid because an outer set is always a superset of the
identified set. However, this paper shows that when the model is refuted by the
data, two sets of non-sharp identification conditions derived from the same
model could lead to disjoint outer sets and conflicting empirical results. We
provide a sufficient condition for the existence of such discordancy, which
covers models characterized by conditional moment inequalities and the Artstein
(1983) inequalities. We also derive sufficient conditions for the non-existence
of discordant submodels, therefore providing a class of models for which
constructing outer sets cannot lead to misleading interpretations. In the case
of discordancy, we follow Masten and Poirier (2021) by developing a method to
salvage misspecified models, but unlike them, we focus on discrete relaxations.
We consider all minimum relaxations of a refuted model that restores
data-consistency. We find that the union of the identified sets of these
minimum relaxations is robust to detectable misspecifications and has an
intuitive empirical interpretation.",2020
http://arxiv.org/abs/2012.11649v3,On the Aggregation of Probability Assessments: Regularized Mixtures of Predictive Densities for Eurozone Inflation and Real Interest Rates,2020-12-21 19:22:29+00:00,"['Francis X. Diebold', 'Minchul Shin', 'Boyuan Zhang']",econ.EM,"We propose methods for constructing regularized mixtures of density
forecasts. We explore a variety of objectives and regularization penalties, and
we use them in a substantive exploration of Eurozone inflation and real
interest rate density forecasts. All individual inflation forecasters (even the
ex post best forecaster) are outperformed by our regularized mixtures. From the
Great Recession onward, the optimal regularization tends to move density
forecasts' probability mass from the centers to the tails, correcting for
overconfidence.",2020
http://arxiv.org/abs/2012.11542v1,Uncertainty on the Reproduction Ratio in the SIR Model,2020-12-21 18:21:36+00:00,"['Sean Elliott', 'Christian Gourieroux']",stat.ME,"The aim of this paper is to understand the extreme variability on the
estimated reproduction ratio $R_0$ observed in practice. For expository purpose
we consider a discrete time stochastic version of the
Susceptible-Infected-Recovered (SIR) model, and introduce different approximate
maximum likelihood (AML) estimators of $R_0$. We carefully discuss the
properties of these estimators and illustrate by a Monte-Carlo study the width
of confidence intervals on $R_0$.",2020
http://arxiv.org/abs/2012.11342v2,A Nearly Similar Powerful Test for Mediation,2020-12-21 13:46:09+00:00,"['Kees Jan van Garderen', 'Noud van Giersbergen']",econ.EM,"This paper derives a new powerful test for mediation that is easy to use.
Testing for mediation is empirically very important in psychology, sociology,
medicine, economics and business, generating over 100,000 citations to a single
key paper. The no-mediation hypothesis $H_{0}:\theta_{1}\theta _{2}=0$ also
poses a theoretically interesting statistical problem since it defines a
manifold that is non-regular in the origin where rejection probabilities of
standard tests are extremely low. We prove that a similar test for mediation
only exists if the size is the reciprocal of an integer. It is unique, but has
objectionable properties. We propose a new test that is nearly similar with
power close to the envelope without these abject properties and is easy to use
in practice. Construction uses the general varying $g$-method that we propose.
We illustrate the results in an educational setting with gender role beliefs
and in a trade union sentiment application.",2020
http://arxiv.org/abs/2012.11286v1,An Empirical Evaluation On The Effectiveness Of Medicaid Expansion Across 49 States,2020-12-21 12:27:16+00:00,['Tung Yu Marco Chan'],econ.GN,"In 2014 the Patient Protection and Affordable Care Act (ACA) introduced the
expansion of Medicaid where states can opt to expand the eligibility for those
in need of free health insurance. In this paper, we attempt to assess the
effectiveness of Medicaid expansion on health outcomes of state populations
using Difference-in-Difference (DD) regressions to seek for causal impacts of
expanding Medicaid on health outcomes in 49 states. We find that in the time
frame of 2013 to 2016, Medicaid expansion seems to have had no significant
impact on the health outcomes of states that have chosen to expand.",2020
http://arxiv.org/abs/2012.11282v1,"National Accounts as a Stock-Flow Consistent System, Part 1: The Real Accounts",2020-12-21 12:22:21+00:00,"['Matti Estola', 'Kristian Vepsäläinen']",econ.GN,"The 2008 economic crisis was not forecastable by at that time existing models
of macroeconomics. Thus macroeconomics needs new tools. We introduce a model
based on National Accounts that shows how macroeconomic sectors are
interconnected. These connections explain the spread of business cycles from
one industry to another and from financial sector to the real economy. These
lingages cannot be explained by General Equilibrium type of models. Our model
describes the real part of National Accounts (NA) of an economy. The accounts
are presented in the form of a money flow diagram between the following
macro-sectors: Non-financial firms, financial firms, households, government,
and rest of the world. The model contains all main items in NA and the
corresponding simulation model creates time paths for 59 key macroeconomic
quantities for an unlimited future. Finnish data of NA from time period
1975-2012 is used in calibrating the parameters of the model, and the model
follows the historical data with sufficient accuracy. Our study serves as a
basis for systems analytic macro-models that can explain the positive and
negative feed-backs in the production system of an economy. These feed-backs
are born from interactions between economic units and between real and
financial markets. JEL E01, E10.
  Key words: Stock-Flow Models, National Accounts, Simulation model.",2020
http://arxiv.org/abs/2012.11222v5,Weak Identification with Bounds in a Class of Minimum Distance Models,2020-12-21 10:05:21+00:00,['Gregory Fletcher Cox'],econ.EM,"When parameters are weakly identified, bounds on the parameters may provide a
valuable source of information. Existing weak identification estimation and
inference results are unable to combine weak identification with bounds. Within
a class of minimum distance models, this paper proposes identification-robust
inference that incorporates information from bounds when parameters are weakly
identified. This paper demonstrates the value of the bounds and
identification-robust inference in a simple latent factor model and a simple
GARCH model. This paper also demonstrates the identification-robust inference
in an empirical application, a factor model for parental investments in
children.",2020
http://arxiv.org/abs/2012.11215v3,"Binary Classification Tests, Imperfect Standards, and Ambiguous Information",2020-12-21 09:51:30+00:00,['Gabriel Ziegler'],econ.EM,"New binary classification tests are often evaluated relative to a
pre-established test. For example, rapid Antigen tests for the detection of
SARS-CoV-2 are assessed relative to more established PCR tests. In this paper,
I argue that the new test can be described as producing ambiguous information
when the pre-established is imperfect. This allows for a phenomenon called
dilation -- an extreme form of non-informativeness. As an example, I present
hypothetical test data satisfying the WHO's minimum quality requirement for
rapid Antigen tests which leads to dilation. The ambiguity in the information
arises from a missing data problem due to imperfection of the established test:
the joint distribution of true infection and test results is not observed.
Using results from Copula theory, I construct the (usually non-singleton) set
of all these possible joint distributions, which allows me to assess the new
test's informativeness. This analysis leads to a simple sufficient condition to
make sure that a new test is not a dilation. I illustrate my approach with
applications to data from three COVID-19 related tests. Two rapid Antigen tests
satisfy my sufficient condition easily and are therefore informative. However,
less accurate procedures, like chest CT scans, may exhibit dilation.",2020
http://arxiv.org/abs/2012.11046v1,Policy Transforms and Learning Optimal Policies,2020-12-20 23:09:27+00:00,['Thomas M. Russell'],econ.EM,"We study the problem of choosing optimal policy rules in uncertain
environments using models that may be incomplete and/or partially identified.
We consider a policymaker who wishes to choose a policy to maximize a
particular counterfactual quantity called a policy transform. We characterize
learnability of a set of policy options by the existence of a decision rule
that closely approximates the maximin optimal value of the policy transform
with high probability. Sufficient conditions are provided for the existence of
such a rule. However, learnability of an optimal policy is an ex-ante notion
(i.e. before observing a sample), and so ex-post (i.e. after observing a
sample) theoretical guarantees for certain policy rules are also provided. Our
entire approach is applicable when the distribution of unobservables is not
parametrically specified, although we discuss how semiparametric restrictions
can be used. Finally, we show possible applications of the procedure to a
simultaneous discrete choice example and a program evaluation example.",2020
http://arxiv.org/abs/2012.11028v1,The Probabilistic Serial and Random Priority Mechanisms with Minimum Quotas,2020-12-20 21:38:24+00:00,['Marek Bojko'],econ.TH,"Consider the problem of assigning indivisible objects to agents with strict
ordinal preferences over objects, where each agent is interested in consuming
at most one object, and objects have integer minimum and maximum quotas. We
define an assignment to be feasible if it satisfies all quotas and assume such
an assignment always exists. The Probabilistic Serial (PS) and Random Priority
(RP) mechanisms are generalised based on the same intuitive idea: Allow agents
to consume their most preferred available object until the total mass of agents
yet to be allocated is exactly equal to the remaining amount of unfilled lower
quotas; in this case, we restrict agents' menus to objects which are yet to
fill their minimum quotas. We show the mechanisms satisfy the same criteria as
their classical counterparts: PS is ordinally efficient, envy-free and weakly
strategy-proof; RP is strategy-proof, weakly envy-free but not ordinally
efficient.",2020
http://arxiv.org/abs/2012.10790v1,Achieving Reliable Causal Inference with Data-Mined Variables: A Random Forest Approach to the Measurement Error Problem,2020-12-19 21:48:23+00:00,"['Mochen Yang', 'Edward McFowland III', 'Gordon Burtch', 'Gediminas Adomavicius']",econ.EM,"Combining machine learning with econometric analysis is becoming increasingly
prevalent in both research and practice. A common empirical strategy involves
the application of predictive modeling techniques to 'mine' variables of
interest from available data, followed by the inclusion of those variables into
an econometric framework, with the objective of estimating causal effects.
Recent work highlights that, because the predictions from machine learning
models are inevitably imperfect, econometric analyses based on the predicted
variables are likely to suffer from bias due to measurement error. We propose a
novel approach to mitigate these biases, leveraging the ensemble learning
technique known as the random forest. We propose employing random forest not
just for prediction, but also for generating instrumental variables to address
the measurement error embedded in the prediction. The random forest algorithm
performs best when comprised of a set of trees that are individually accurate
in their predictions, yet which also make 'different' mistakes, i.e., have
weakly correlated prediction errors. A key observation is that these properties
are closely related to the relevance and exclusion requirements of valid
instrumental variables. We design a data-driven procedure to select tuples of
individual trees from a random forest, in which one tree serves as the
endogenous covariate and the other trees serve as its instruments. Simulation
experiments demonstrate the efficacy of the proposed approach in mitigating
estimation biases and its superior performance over three alternative methods
for bias correction.",2020
http://arxiv.org/abs/2012.10735v1,The role of time estimation in decreased impatience in Intertemporal Choice,2020-12-19 16:33:07+00:00,"['Camila S. Agostino Peter M. E. Claessens', 'Fuat Balci', 'Yossi Zana']",econ.TH,"The role of specific cognitive processes in deviations from constant
discounting in intertemporal choice is not well understood. We evaluated
decreased impatience in intertemporal choice tasks independent of discounting
rate and non-linearity in long-scale time representation; nonlinear time
representation was expected to explain inconsistencies in discounting rate.
Participants performed temporal magnitude estimation and intertemporal choice
tasks. Psychophysical functions for time intervals were estimated by fitting
linear and power functions, while discounting functions were estimated by
fitting exponential and hyperbolic functions. The temporal magnitude estimates
of 65% of the participants were better fit with power functions (mostly
compression). 63% of the participants had intertemporal choice patterns
corresponding best to hyperbolic functions. Even when the perceptual bias in
the temporal magnitude estimations was compensated in the discounting rate
computation, the data of 8 out of 14 participants continued exhibiting temporal
inconsistency. The results suggest that temporal inconsistency in discounting
rate can be explained to different degrees by the bias in temporal
representations. Non-linearity in temporal representation and discounting rate
should be evaluated on an individual basis. Keywords: Intertemporal choice,
temporal magnitude, model comparison, impatience, time inconsistency",2020
http://arxiv.org/abs/2012.11595v1,Valuation Models Applied to Value-Based Management. Application to the Case of UK Companies with Problems,2020-12-19 14:09:14+00:00,['Marcel Ausloos'],q-fin.GN,"Many still rightly wonder whether accounting numbers affect business value.
Basic questions are why? and how? I aim at promoting an objective choice on how
optimizing the most suitable valuation methods under a value-based management
framework through some performance measurement systems. First, I present a
comprehensive review of valuation methods. Three valuations methods, (i) Free
Cash Flow Valuation Model (FCFVM), (ii) Residual Earning Valuation Model (REVM)
and (iii) Abnormal Earning Growth Model (AEGM), are presented. I point out to
advantages and limitations. As applications, the proofs of the findings are
illustrated on three study cases: Marks & Spencer's business pattern (size and
growth prospect), which had a recently advertised valuation problem, and two
comparable companies, Tesco and Sainsbury's, all three chosen for
multiple-based valuation. For the purpose, two value drivers are chosen,
EnV/EBIT (entity value/earnings before interests and taxes) and the
corresponding EnV/Sales. Thus, the question whether accounting numbers through
models based on mathematical economics truly affect business value has an
answer: Maybe, yes.",2020
http://arxiv.org/abs/2012.11594v1,Insider trading in the run-up to merger announcements. Before and after the UK's Financial Services Act 2012,2020-12-19 11:37:20+00:00,"['Rebecaa Pham', 'Marcel Ausloos']",q-fin.GN,"After the 2007/2008 financial crisis, the UK government decided that a change
in regulation was required to amend the poor control of financial markets. The
Financial Services Act 2012 was developed as a result in order to give more
control and authority to the regulators of financial markets. Thus, the
Financial Conduct Authority (FCA) succeeded the Financial Services Authority
(FSA). An area requiring an improvement in regulation was insider trading. Our
study examines the effectiveness of the FCA in its duty of regulating insider
trading through utilising the event study methodology to assess abnormal
returns in the run-up to the first announcement of mergers. Samples of abnormal
returns are examined on periods, under regulation either by the FSA or by the
FCA. Practically, stock price data on the London Stock Exchange from 2008-2012
and 2015-2019 is investigated. The results from this study determine that
abnormal returns are reduced after the implementation of the Financial Services
Act 2012; prices are also found to be noisier in the period before the 2012
Act. Insignificant abnormal returns are found in the run-up to the first
announcement of mergers in the 2015-2019 period. This concludes that the FCA is
efficient in regulating insider trading.",2020
http://arxiv.org/abs/2012.10475v2,Minority games played by arbitrageurs on the energy market,2020-12-18 19:29:47+00:00,"['Tim Ritmeester', 'Hildegard Meyer-Ortmanns']",econ.TH,"Along with the energy transition, the energy markets change their
organization toward more decentralized and self-organized structures, striving
for locally optimal profits. These tendencies may endanger the physical grid
stability. One realistic option is the exhaustion of reserve energy due to an
abuse by arbitrageurs. We map the energy market to different versions of a
minority game and determine the expected amount of arbitrage as well as its
fluctuations as a function of the model parameters. Of particular interest are
the impact of heterogeneous contributions of arbitrageurs, the interplay
between external stochastic events and nonlinear price functions of reserve
power, and the effect of risk aversion due to suspected penalties. The
non-monotonic dependence of arbitrage on the control parameters reveals an
underlying phase transition that is the counterpart to replica symmetry
breaking in spin glasses. As conclusions from our results we propose economic
and statutory measures to counteract a detrimental effect of arbitrage.",2020
http://arxiv.org/abs/2012.10315v5,"Kernel Methods for Unobserved Confounding: Negative Controls, Proxies, and Instruments",2020-12-18 16:00:08+00:00,['Rahul Singh'],stat.ML,"Negative control is a strategy for learning the causal relationship between
treatment and outcome in the presence of unmeasured confounding. The treatment
effect can nonetheless be identified if two auxiliary variables are available:
a negative control treatment (which has no effect on the actual outcome), and a
negative control outcome (which is not affected by the actual treatment). These
auxiliary variables can also be viewed as proxies for a traditional set of
control variables, and they bear resemblance to instrumental variables. I
propose a family of algorithms based on kernel ridge regression for learning
nonparametric treatment effects with negative controls. Examples include dose
response curves, dose response curves with distribution shift, and
heterogeneous treatment effects. Data may be discrete or continuous, and low,
high, or infinite dimensional. I prove uniform consistency and provide finite
sample rates of convergence. I estimate the dose response curve of cigarette
smoking on infant birth weight adjusting for unobserved confounding due to
household income, using a data set of singleton births in the state of
Pennsylvania between 1989 and 1991.",2020
http://arxiv.org/abs/2012.10077v8,Two-way Fixed Effects and Differences-in-Differences Estimators with Several Treatments,2020-12-18 07:11:36+00:00,"['Clément de Chaisemartin', ""Xavier D'Haultfœuille""]",econ.EM,"We study two-way-fixed-effects regressions (TWFE) with several treatment
variables. Under a parallel trends assumption, we show that the coefficient on
each treatment identifies a weighted sum of that treatment's effect, with
possibly negative weights, plus a weighted sum of the effects of the other
treatments. Thus, those estimators are not robust to heterogeneous effects and
may be contaminated by other treatments' effects. We further show that omitting
a treatment from the regression can actually reduce the estimator's bias,
unlike what would happen under constant treatment effects. We propose an
alternative difference-in-differences estimator, robust to heterogeneous
effects and immune to the contamination problem. In the application we
consider, the TWFE regression identifies a highly non-convex combination of
effects, with large contamination weights, and one of its coefficients
significantly differs from our heterogeneity-robust estimator.",2020
http://arxiv.org/abs/2012.09906v4,Green governments,2020-12-17 19:59:55+00:00,"['Niklas Potrafke', 'Kaspar Wuthrich']",econ.GN,"We examine how Green governments influence environmental, macroeconomic, and
education outcomes. We exploit that the Fukushima nuclear disaster in Japan
gave rise to an unanticipated change in government in the German state
Baden-Wuerttemberg in 2011. Using the synthetic control method, we find no
evidence that the Green government influenced CO2 emissions or increased
renewable energy usage overall. The share of wind power usage even decreased.
Intra-ecological conflicts prevented the Green government from implementing
drastic changes in environmental policies. The results do not suggest that the
Green government influenced macroeconomic outcomes. Inclusive education
policies caused comprehensive schools to become larger.",2020
http://arxiv.org/abs/2012.09541v1,Hiring from a pool of workers,2020-12-17 12:27:42+00:00,"['Azar Abizada', 'Inácio Bó']",econ.TH,"In many countries and institutions around the world, the hiring of workers is
made through open competitions. In them, candidates take tests and are ranked
based on scores in exams and other predetermined criteria. Those who satisfy
some eligibility criteria are made available for hiring from a ""pool of
workers."" In each of an ex-ante unknown number of rounds, vacancies are
announced, and workers are then hired from that pool. When the scores are the
only criterion for selection, the procedure satisfies desired fairness and
independence properties. We show that when affirmative action policies are
introduced, the established methods of reserves and procedures used in Brazil,
France, and Australia, fail to satisfy those properties. We then present a new
rule, which we show to be the unique rule that extends static notions of
fairness to problems with multiple rounds while satisfying aggregation
independence, a consistency requirement. Finally, we show that if multiple
institutions hire workers from a single pool, even minor consistency
requirements are incompatible with variations in the institutions' rules.",2020
http://arxiv.org/abs/2012.09493v1,Optimal switch from a fossil-fueled to an electric vehicle,2020-12-17 10:34:44+00:00,"['Paolo Falbo', 'Giorgio Ferrari', 'Giorgio Rizzini', 'Maren Diane Schmeck']",econ.GN,"In this paper we propose and solve a real options model for the optimal
adoption of an electric vehicle. A policymaker promotes the abeyance of
fossil-fueled vehicles through an incentive, and the representative
fossil-fueled vehicle's owner decides the time at which buying an electric
vehicle, while minimizing a certain expected cost. This involves a combination
of various types of costs: the stochastic opportunity cost of driving one unit
distance with a traditional fossil-fueled vehicle instead of an electric one,
the cost associated to traffic bans, and the net purchase cost. After
determining the optimal switching time and the minimal cost function for a
general diffusive opportunity cost, we specialize to the case of a
mean-reverting process. In such a setting, we provide a model calibration on
real data from Italy, and we study the dependency of the optimal switching time
with respect to the model's parameters. Moreover, we study the effect of
traffic bans and incentive on the expected optimal switching time. We observe
that incentive and traffic bans on fossil-fueled transport can be used as
effective tools in the hand of the policymaker to encourage the adoption of
electric vehicles, and hence to reduce air pollution.",2020
http://arxiv.org/abs/2012.09445v3,"Self-Fulfilling Prophecies, Quasi Non-Ergodicity and Wealth Inequality",2020-12-17 08:40:26+00:00,"['Jean-Philippe Bouchaud', 'Roger Farmer']",econ.TH,"We construct a model of an exchange economy in which agents trade assets
contingent on an observable signal, the probability of which depends on public
opinion. The agents in our model are replaced occasionally and each person
updates beliefs in response to observed outcomes. We show that the distribution
of the observed signal is described by a quasi-non-ergodic process and that
people continue to disagree with each other forever. These disagreements
generate large wealth inequalities that arise from the multiplicative nature of
wealth dynamics which make successful bold bets highly profitable.",2020
http://arxiv.org/abs/2012.09422v4,The Variational Method of Moments,2020-12-17 07:21:06+00:00,"['Andrew Bennett', 'Nathan Kallus']",cs.LG,"The conditional moment problem is a powerful formulation for describing
structural causal parameters in terms of observables, a prominent example being
instrumental variable regression. A standard approach reduces the problem to a
finite set of marginal moment conditions and applies the optimally weighted
generalized method of moments (OWGMM), but this requires we know a finite set
of identifying moments, can still be inefficient even if identifying, or can be
theoretically efficient but practically unwieldy if we use a growing sieve of
moment conditions. Motivated by a variational minimax reformulation of OWGMM,
we define a very general class of estimators for the conditional moment
problem, which we term the variational method of moments (VMM) and which
naturally enables controlling infinitely-many moments. We provide a detailed
theoretical analysis of multiple VMM estimators, including ones based on kernel
methods and neural nets, and provide conditions under which these are
consistent, asymptotically normal, and semiparametrically efficient in the full
conditional moment model. We additionally provide algorithms for valid
statistical inference based on the same kind of variational reformulations,
both for kernel- and neural-net-based varieties. Finally, we demonstrate the
strong performance of our proposed estimation and inference algorithms in a
detailed series of synthetic experiments.",2020
http://arxiv.org/abs/2012.09336v2,Levelling Down and the COVID-19 Lockdowns: Uneven Regional Recovery in UK Consumer Spending,2020-12-17 00:27:31+00:00,"['John Gathergood', 'Fabian Gunzinger', 'Benedict Guttman-Kenney', 'Edika Quispe-Torreblanca', 'Neil Stewart']",econ.GN,"We show the recovery in consumer spending in the United Kingdom through the
second half of 2020 is unevenly distributed across regions. We utilise Fable
Data: a real-time source of consumption data that is a highly correlated,
leading indicator of Bank of England and Office for National Statistics data.
The UK's recovery is heavily weighted towards the ""home counties"" around outer
London and the South. We observe a stark contrast between strong online
spending growth while offline spending contracts. The strongest recovery in
spending is seen in online spending in the ""commuter belt"" areas in outer
London and the surrounding localities and also in areas of high second home
ownership, where working from home (including working from second homes) has
significantly displaced the location of spending. Year-on-year spending growth
in November 2020 in localities facing the UK's new tighter ""Tier 3""
restrictions (mostly the midlands and northern areas) was 38.4% lower compared
with areas facing the less restrictive ""Tier 2"" (mostly London and the South).
These patterns had been further exacerbated during November 2020 when a second
national lockdown was imposed. To prevent such COVID-19-driven regional
inequalities from becoming persistent we propose governments introduce
temporary, regionally-targeted interventions in 2021. The availability of
real-time, regional data enables policymakers to efficiently decide when, where
and how to implement such regional interventions and to be able to rapidly
evaluate their effectiveness to consider whether to expand, modify or remove
them.",2020
http://arxiv.org/abs/2012.09306v1,"Decentralized Finance, Centralized Ownership? An Iterative Mapping Process to Measure Protocol Token Distribution",2020-12-16 22:52:10+00:00,"['Matthias Nadler', 'Fabian Schär']",econ.GN,"In this paper, we analyze various Decentralized Finance (DeFi) protocols in
terms of their token distributions. We propose an iterative mapping process
that allows us to split aggregate token holdings from custodial and escrow
contracts and assign them to their economic beneficiaries. This method accounts
for liquidity-, lending-, and staking-pools, as well as token wrappers, and can
be used to break down token holdings, even for high nesting levels. We compute
individual address balances for several snapshots and analyze intertemporal
distribution changes. In addition, we study reallocation and protocol usage
data, and propose a proxy for measuring token dependencies and ecosystem
integration. The paper offers new insights on DeFi interoperability as well as
token ownership distribution and may serve as a foundation for further
research.",2020
http://arxiv.org/abs/2012.08988v1,Exact Trend Control in Estimating Treatment Effects Using Panel Data with Heterogenous Trends,2020-12-16 14:33:53+00:00,['Chirok Han'],econ.EM,"For a panel model considered by Abadie et al. (2010), the counterfactual
outcomes constructed by Abadie et al., Hsiao et al. (2012), and Doudchenko and
Imbens (2017) may all be confounded by uncontrolled heterogenous trends. Based
on exact-matching on the trend predictors, I propose new methods of estimating
the model-specific treatment effects, which are free from heterogenous trends.
When applied to Abadie et al.'s (2010) model and data, the new estimators
suggest considerably smaller effects of California's tobacco control program.",2020
http://arxiv.org/abs/2012.09627v3,"United States FDA drug approvals are persistent and polycyclic: Insights into economic cycles, innovation dynamics, and national policy",2020-12-16 13:30:17+00:00,['Iraj Daizadeh'],econ.EM,"It is challenging to elucidate the effects of changes in external influences
(such as economic or policy) on the rate of US drug approvals. Here, a novel
approach, termed the Chronological Hurst Exponent (CHE), is proposed, which
hypothesizes that changes in the long-range memory latent within the dynamics
of time series data may be temporally associated with changes in such
influences. Using the monthly number the FDA Center for Drug Evaluation and
Research (CDER) approvals from 1939 to 2019 as the data source, it is
demonstrated that the CHE has a distinct S-shaped structure demarcated by an
8-year (1939-1947) Stagnation Period, a 27-year (1947-1974) Emergent
(time-varying Period, and a 45-year (1974-2019) Saturation Period. Further,
dominant periodicities (resolved via wavelet analyses) are identified during
the most recent 45-year CHE Saturation Period at 17, 8 and 4 years; thus, US
drug approvals have been following a Juglar-Kuznet mid-term cycle with
Kitchin-like bursts. As discussed, this work suggests that (1) changes in
extrinsic factors (e.g., of economic and/or policy origin ) during the Emergent
Period may have led to persistent growth in US drug approvals enjoyed since
1974, (2) the CHE may be a valued method to explore influences on time series
data, and (3) innovation-related economic cycles exist (as viewed via the proxy
metric of US drug approvals).",2020
http://arxiv.org/abs/2012.08864v1,"Development of cloud, digital technologies and the introduction of chip technologies",2020-12-16 11:05:41+00:00,['Ali R. Baghirzade'],econ.GN,"Hardly any other area of research has recently attracted as much attention as
machine learning (ML) through the rapid advances in artificial intelligence
(AI). This publication provides a short introduction to practical concepts and
methods of machine learning, problems and emerging research questions, as well
as an overview of the participants, an overview of the application areas and
the socio-economic framework conditions of the research.
  In expert circles, ML is used as a key technology for modern artificial
intelligence techniques, which is why AI and ML are often used interchangeably,
especially in an economic context. Machine learning and, in particular, deep
learning (DL) opens up entirely new possibilities in automatic language
processing, image analysis, medical diagnostics, process management and
customer management. One of the important aspects in this article is
chipization. Due to the rapid development of digitalization, the number of
applications will continue to grow as digital technologies advance. In the
future, machines will more and more provide results that are important for
decision making. To this end, it is important to ensure the safety, reliability
and sufficient traceability of automated decision-making processes from the
technological side. At the same time, it is necessary to ensure that ML
applications are compatible with legal issues such as responsibility and
liability for algorithmic decisions, as well as technically feasible. Its
formulation and regulatory implementation is an important and complex issue
that requires an interdisciplinary approach. Last but not least, public
acceptance is critical to the continued diffusion of machine learning processes
in applications. This requires widespread public discussion and the involvement
of various social groups.",2020
http://arxiv.org/abs/2012.08840v1,Exploring Narrative Economics: An Agent-Based-Modeling Platform that Integrates Automated Traders with Opinion Dynamics,2020-12-16 10:31:08+00:00,"['Kenneth Lomas', 'Dave Cliff']",q-fin.TR,"In seeking to explain aspects of real-world economies that defy easy
understanding when analysed via conventional means, Nobel Laureate Robert
Shiller has since 2017 introduced and developed the idea of Narrative
Economics, where observable economic factors such as the dynamics of prices in
asset markets are explained largely as a consequence of the narratives (i.e.,
the stories) heard, told, and believed by participants in those markets.
Shiller argues that otherwise irrational and difficult-to-explain behaviors,
such as investors participating in highly volatile cryptocurrency markets, are
best explained and understood in narrative terms: people invest because they
believe, because they have a heartfelt opinions, about the future prospects of
the asset, and they tell to themselves and others stories (narratives) about
those beliefs and opinions. In this paper we describe what is, to the best of
our knowledge, the first ever agent-based modelling platform that allows for
the study of issues in narrative economics. We have created this by integrating
and synthesizing research in two previously separate fields: opinion dynamics
(OD), and agent-based computational economics (ACE) in the form of
minimally-intelligent trader-agents operating in accurately modelled financial
markets. We show here for the first time how long-established models in OD and
in ACE can be brought together to enable the experimental study of issues in
narrative economics, and we present initial results from our system. The
program-code for our simulation platform has been released as freely-available
open-source software on GitHub, to enable other researchers to replicate and
extend our work",2020
http://arxiv.org/abs/2012.08444v2,Minimax Risk and Uniform Convergence Rates for Nonparametric Dyadic Regression,2020-12-15 17:35:07+00:00,"['Bryan S. Graham', 'Fengshi Niu', 'James L. Powell']",math.ST,"Let $i=1,\ldots,N$ index a simple random sample of units drawn from some
large population. For each unit we observe the vector of regressors $X_{i}$
and, for each of the $N\left(N-1\right)$ ordered pairs of units, an outcome
$Y_{ij}$. The outcomes $Y_{ij}$ and $Y_{kl}$ are independent if their indices
are disjoint, but dependent otherwise (i.e., ""dyadically dependent""). Let
$W_{ij}=\left(X_{i}',X_{j}'\right)'$; using the sampled data we seek to
construct a nonparametric estimate of the mean regression function
$g\left(W_{ij}\right)\overset{def}{\equiv}\mathbb{E}\left[\left.Y_{ij}\right|X_{i},X_{j}\right].$
  We present two sets of results. First, we calculate lower bounds on the
minimax risk for estimating the regression function at (i) a point and (ii)
under the infinity norm. Second, we calculate (i) pointwise and (ii) uniform
convergence rates for the dyadic analog of the familiar Nadaraya-Watson (NW)
kernel regression estimator. We show that the NW kernel regression estimator
achieves the optimal rates suggested by our risk bounds when an appropriate
bandwidth sequence is chosen. This optimal rate differs from the one available
under iid data: the effective sample size is smaller and
$d_W=\mathrm{dim}(W_{ij})$ influences the rate differently.",2020
http://arxiv.org/abs/2012.08223v2,Long-term prediction intervals with many covariates,2020-12-15 11:26:08+00:00,"['Sayar Karmakar', 'Marek Chudy', 'Wei Biao Wu']",stat.ME,"Accurate forecasting is one of the fundamental focus in the literature of
econometric time-series. Often practitioners and policy makers want to predict
outcomes of an entire time horizon in the future instead of just a single
$k$-step ahead prediction. These series, apart from their own possible
non-linear dependence, are often also influenced by many external predictors.
In this paper, we construct prediction intervals of time-aggregated forecasts
in a high-dimensional regression setting. Our approach is based on quantiles of
residuals obtained by the popular LASSO routine. We allow for general
heavy-tailed, long-memory, and nonlinear stationary error process and
stochastic predictors. Through a series of systematically arranged consistency
results we provide theoretical guarantees of our proposed quantile-based method
in all of these scenarios. After validating our approach using simulations we
also propose a novel bootstrap based method that can boost the coverage of the
theoretical intervals. Finally analyzing the EPEX Spot data, we construct
prediction intervals for hourly electricity prices over horizons spanning 17
weeks and contrast them to selected Bayesian and bootstrap interval forecasts.",2020
http://arxiv.org/abs/2012.08970v1,Disentangling the socio-ecological drivers behind illegal fishing in a small-scale fishery managed by a TURF system,2020-12-15 09:38:22+00:00,"['Silvia de Juan', 'Maria Dulce Subida', 'Andres Ospina-Alvarez', 'Ainara Aguilar', 'Miriam Fernandez']",econ.GN,"A substantial increase in illegal extraction of the benthic resources in
central Chile is likely driven by an interplay of numerous socio-economic local
factors that threatens the success of the fisheries management areas (MA)
system. To assess this problem, the exploitation state of a commercially
important benthic resource (i.e., keyhole limpet) in the MAs was related with
socio-economic drivers of the small-scale fisheries. The potential drivers of
illegal extraction included rebound effect of fishing effort displacement by
MAs, level of enforcement, distance to surveillance authorities, wave exposure
and land-based access to the MA, and alternative economic activities in the
fishing village. The exploitation state of limpets was assessed by the
proportion of the catch that is below the minimum legal size, with high
proportions indicating a poor state, and by the relative median size of limpets
fished within the MAs in comparison with neighbouring OA areas, with larger
relative sizes in the MA indicating a good state. A Bayesian-Belief Network
approach was adopted to assess the effects of potential drivers of illegal
fishing on the status of the benthic resource in the MAs. Results evidenced the
absence of a direct link between the level of enforcement and the status of the
resource, with other socio-economic (e.g., alternative economic activities in
the village) and context variables (e.g., fishing effort or distance to
surveillance authorities) playing important roles. Scenario analysis explored
variables that are susceptible to be managed, evidencing that BBN is a powerful
approach to explore the role of multiple external drivers, and their impact on
marine resources, in complex small-scale fisheries.",2020
http://arxiv.org/abs/2012.08155v3,Real-time Inflation Forecasting Using Non-linear Dimension Reduction Techniques,2020-12-15 08:55:18+00:00,"['Niko Hauzenberger', 'Florian Huber', 'Karin Klieber']",econ.EM,"In this paper, we assess whether using non-linear dimension reduction
techniques pays off for forecasting inflation in real-time. Several recent
methods from the machine learning literature are adopted to map a large
dimensional dataset into a lower dimensional set of latent factors. We model
the relationship between inflation and the latent factors using constant and
time-varying parameter (TVP) regressions with shrinkage priors. Our models are
then used to forecast monthly US inflation in real-time. The results suggest
that sophisticated dimension reduction methods yield inflation forecasts that
are highly competitive to linear approaches based on principal components.
Among the techniques considered, the Autoencoder and squared principal
components yield factors that have high predictive power for one-month- and
one-quarter-ahead inflation. Zooming into model performance over time reveals
that controlling for non-linear relations in the data is of particular
importance during recessionary episodes of the business cycle or the current
COVID-19 pandemic.",2020
http://arxiv.org/abs/2012.08133v4,Kicking You When You're Already Down: The Multipronged Impact of Austerity on Crime,2020-12-15 07:39:39+00:00,"['Corrado Giulietti', 'Brendon McConnell']",econ.GN,"The UK Welfare Reform Act 2012 imposed a series of deep welfare cuts, which
disproportionately affected ex-ante poorer areas. In this paper, we provide the
first evidence of the impact of these austerity measures on two different but
complementary elements of crime -- the crime rate and the less-studied
concentration of crime -- over the period 2011-2015 in England and Wales, and
document four new facts. First, areas more exposed to the welfare reforms
experience increased levels of crime, an effect driven by a rise in violent
crime. Second, both violent and property crime become more concentrated within
an area due to the welfare reforms. Third, it is ex-ante more deprived
neighborhoods that bear the brunt of the crime increases over this period.
Fourth, we find no evidence that the welfare reforms increased recidivism,
suggesting that the changes in crime we find are likely driven by new
criminals. Combining these results, we document unambiguous evidence of a
negative spillover of the welfare reforms at the heart of the UK government's
austerity program on social welfare, which reinforced the direct
inequality-worsening effect of this program. Guided by a hedonic house price
model, we calculate the welfare effects implied by the cuts in order to provide
a financial quantification of the impact of the reform. We document an implied
welfare loss of the policy -- borne by the public -- that far exceeds the
savings made to government coffers.",2020
http://arxiv.org/abs/2012.08022v1,Identification of inferential parameters in the covariate-normalized linear conditional logit model,2020-12-15 00:37:52+00:00,['Philip Erickson'],econ.EM,"The conditional logit model is a standard workhorse approach to estimating
customers' product feature preferences using choice data. Using these models at
scale, however, can result in numerical imprecision and optimization failure
due to a combination of large-valued covariates and the softmax probability
function. Standard machine learning approaches alleviate these concerns by
applying a normalization scheme to the matrix of covariates, scaling all values
to sit within some interval (such as the unit simplex). While this type of
normalization is innocuous when using models for prediction, it has the side
effect of perturbing the estimated coefficients, which are necessary for
researchers interested in inference. This paper shows that, for two common
classes of normalizers, designated scaling and centered scaling, the
data-generating non-scaled model parameters can be analytically recovered along
with their asymptotic distributions. The paper also shows the numerical
performance of the analytical results using an example of a scaling normalizer.",2020
http://arxiv.org/abs/2012.07739v2,The economics of stop-and-go epidemic control,2020-12-14 17:38:50+00:00,"['Claudius Gros', 'Daniel Gros']",econ.TH,"We analyse 'stop-and-go' containment policies that produce infection cycles
as periods of tight lockdowns are followed by periods of falling infection
rates. The subsequent relaxation of containment measures allows cases to
increase again until another lockdown is imposed and the cycle repeats. The
policies followed by several European countries during the Covid-19 pandemic
seem to fit this pattern. We show that 'stop-and-go' should lead to lower
medical costs than keeping infections at the midpoint between the highs and
lows produced by 'stop-and-go'. Increasing the upper and reducing the lower
limits of a stop-and-go policy by the same amount would lower the average
medical load. But increasing the upper and lowering the lower limit while
keeping the geometric average constant would have the opposite effect. We also
show that with economic costs proportional to containment, any path that brings
infections back to the original level (technically a closed cycle) has the same
overall economic cost.",2020
http://arxiv.org/abs/2012.07680v1,Fair and Efficient Allocations under Lexicographic Preferences,2020-12-14 16:27:26+00:00,"['Hadi Hosseini', 'Sujoy Sikdar', 'Rohit Vaish', 'Lirong Xia']",cs.GT,"Envy-freeness up to any good (EFX) provides a strong and intuitive guarantee
of fairness in the allocation of indivisible goods. But whether such
allocations always exist or whether they can be efficiently computed remains an
important open question. We study the existence and computation of EFX in
conjunction with various other economic properties under lexicographic
preferences--a well-studied preference model in artificial intelligence and
economics. In sharp contrast to the known results for additive valuations, we
not only prove the existence of EFX and Pareto optimal allocations, but in fact
provide an algorithmic characterization of these two properties. We also
characterize the mechanisms that are, in addition, strategyproof, non-bossy,
and neutral. When the efficiency notion is strengthened to rank-maximality, we
obtain non-existence and computational hardness results, and show that
tractability can be restored when EFX is relaxed to another well-studied
fairness notion called maximin share guarantee (MMS).",2020
http://arxiv.org/abs/2012.07669v1,The structure of multiplex networks predicts play in economic games and real-world cooperation,2020-12-14 16:16:32+00:00,"['Curtis Atkisson', 'Monique Borgerhoff Mulder']",econ.GN,"Explaining why humans cooperate in anonymous contexts is a major goal of
human behavioral ecology, cultural evolution, and related fields. What predicts
cooperation in anonymous contexts is inconsistent across populations, levels of
analysis, and games. For instance, market integration is a key predictor across
ethnolinguistic groups but has inconsistent predictive power at the individual
level. We adapt an idea from 19th-century sociology: people in societies with
greater overlap in ties across domains among community members (Durkheim's
""mechanical"" solidarity) will cooperate more with their network partners and
less in anonymous contexts than people in societies with less overlap
(""organic"" solidarity). This hypothesis, which can be tested at the individual
and community level, assumes that these two types of societies differ in the
importance of keeping existing relationships as opposed to recruiting new
partners. Using multiplex networks, we test this idea by comparing cooperative
tendencies in both anonymous experimental games and real-life communal labor
tasks across 9 Makushi villages in Guyana that vary in the degree of
within-village overlap. Average overlap in a village predicts both real-world
cooperative and anonymous interactions in the predicted direction; individual
overlap also has effects in the expected direction. These results reveal a
consistent patterning of cooperative tendencies at both individual and local
levels and contribute to the debate over the emergence of norms for cooperation
among humans. Multiplex overlap can help us understand inconsistencies in
previous studies of cooperation in anonymous contexts and is an unexplored
dimension with explanatory power at multiple levels of analysis.",2020
http://arxiv.org/abs/2012.10400v1,Trademark filings and patent application count time series are structurally near-identical and cointegrated: Implications for studies in innovation,2020-12-14 16:09:15+00:00,['Iraj Daizadeh'],econ.EM,"Through time series analysis, this paper empirically explores, confirms and
extends the trademark/patent inter-relationship as proposed in the normative
intellectual-property (IP)-oriented Innovation Agenda view of the science and
technology (S&T) firm. Beyond simple correlation, it is shown that
trademark-filing (Trademarks) and patent-application counts (Patents) have
similar (if not, identical) structural attributes (including similar
distribution characteristics and seasonal variation, cross-wavelet
synchronicity/coherency (short-term cross-periodicity) and structural breaks)
and are cointegrated (integration order of 1) over a period of approximately 40
years (given the monthly observations). The existence of cointegration strongly
suggests a ""long-run"" equilibrium between the two indices; that is, there is
(are) exogenous force(s) restraining the two indices from diverging from one
another. Structural breakpoints in the chrono-dynamics of the indices supports
the existence of potentially similar exogeneous forces(s), as the break dates
are simultaneous/near-simultaneous (Trademarks: 1987, 1993, 1999, 2005, 2011;
Patents: 1988, 1994, 2000, and 2011). A discussion of potential triggers
(affecting both time series) causing these breaks, and the concept of
equilibrium in the context of these proxy measures are presented. The
cointegration order and structural co-movements resemble other macro-economic
variables, stoking the opportunity of using econometrics approaches to further
analyze these data. As a corollary, this work further supports the inclusion of
trademark analysis in innovation studies. Lastly, the data and corresponding
analysis tools (R program) are presented as Supplementary Materials for
reproducibility and convenience to conduct future work for interested readers.",2020
http://arxiv.org/abs/2012.08355v1,A mathematical model of national-level food system sustainability,2020-12-14 16:06:35+00:00,"['Conor Goold', 'Simone Pfuderer', 'William H. M. James', 'Nik Lomax', 'Fiona Smith', 'Lisa M. Collins']",econ.GN,"The global food system faces various endogeneous and exogeneous, biotic and
abiotic risk factors, including a rising human population, higher population
densities, price volatility and climate change. Quantitative models play an
important role in understanding food systems' expected responses to shocks and
stresses. Here, we present a stylised mathematical model of a national-level
food system that incorporates domestic supply of a food commodity,
international trade, consumer demand, and food commodity price. We derive a
critical compound parameter signalling when domestic supply will become
unsustainable and the food system entirely dependent on imports, which results
in higher commodity prices, lower consumer demand and lower inventory levels.
Using Bayesian estimation, we apply the dynamic food systems model to infer the
sustainability of the UK pork industry. We find that the UK pork industry is
currently sustainable but because the industry is dependent on imports to meet
demand, a decrease in self-sufficiency below 50% (current levels are 60-65%)
would lead it close to the critical boundary signalling its collapse. Our model
provides a theoretical foundation for future work to determine more complex
causal drivers of food system vulnerability.",2020
http://arxiv.org/abs/2012.07624v1,Welfare Analysis via Marginal Treatment Effects,2020-12-14 15:16:03+00:00,"['Yuya Sasaki', 'Takuya Ura']",econ.EM,"Consider a causal structure with endogeneity (i.e., unobserved
confoundedness) in empirical data, where an instrumental variable is available.
In this setting, we show that the mean social welfare function can be
identified and represented via the marginal treatment effect (MTE, Bjorklund
and Moffitt, 1987) as the operator kernel. This representation result can be
applied to a variety of statistical decision rules for treatment choice,
including plug-in rules, Bayes rules, and empirical welfare maximization (EWM)
rules as in Hirano and Porter (2020, Section 2.3). Focusing on the application
to the EWM framework of Kitagawa and Tetenov (2018), we provide convergence
rates of the worst case average welfare loss (regret) in the spirit of Manski
(2004).",2020
http://arxiv.org/abs/2012.07509v1,Decision Making under Uncertainty: A Game of Two Selves,2020-12-14 13:42:47+00:00,['Jianming Xia'],econ.TH,"In this paper we characterize the niveloidal preferences that satisfy the
Weak Order, Monotonicity, Archimedean, and Weak C-Independence Axioms from the
point of view of an intra-personal, leader-follower game. We also show that the
leader's strategy space can serve as an ambiguity aversion index.",2020
http://arxiv.org/abs/2012.07238v1,Misspecified Beliefs about Time Lags,2020-12-14 03:45:43+00:00,"['Yingkai Li', 'Harry Pei']",econ.TH,"We examine the long-term behavior of a Bayesian agent who has a misspecified
belief about the time lag between actions and feedback, and learns about the
payoff consequences of his actions over time. Misspecified beliefs about time
lags result in attribution errors, which have no long-term effect when the
agent's action converges, but can lead to arbitrarily large long-term
inefficiencies when his action cycles. Our proof uses concentration
inequalities to bound the frequency of action switches, which are useful to
study learning problems with history dependence. We apply our methods to study
a policy choice game between a policy-maker who has a correctly specified
belief about the time lag and the public who has a misspecified belief.",2020
http://arxiv.org/abs/2012.07027v1,Impact of Regional Reactions to War on Contemporary Chinese Trade,2020-12-13 10:49:34+00:00,['Xuejian Wang'],econ.GN,"Different regional reactions to war in 1894 and 1900 can significantly impact
Chinese imports in 2001. As international relationship gets tense and China
rises, international conflicts could decrease trade.We analyze impact of
historic political conflict. We measure regional change of number of people
passing imperial exam because of war. War leads to an unsuccessful reform and
shocks elites. Elites in different regions have different ideas about
modernization, and the change of number of people passing exam is quite
different in different regions after war. Regional number of people passing
exam increases 1% after war, imports from then empires decrease 2.050% in 2001,
and this shows impact of cultural barrier. Manufactured goods can be impacted
because brands can be identified easily. Risk aversion of expensive products in
conservative regions can increase imports of equipment. Value chains need deep
trust, and this decreases imports of foreign company and assembly trade.",2020
http://arxiv.org/abs/2012.12199v1,Involuntary unemployment in overlapping generations model due to instability of the economy,2020-12-13 09:23:09+00:00,['Yasuhito Tanaka'],econ.GN,"The existence of involuntary unemployment advocated by J. M. Keynes is a very
important problem of the modern economic theory. Using a three-generations
overlapping generations model, we show that the existence of involuntary
unemployment is due to the instability of the economy. Instability of the
economy is the instability of the difference equation about the equilibrium
price around the full-employment equilibrium, which means that a fall in the
nominal wage rate caused by the presence of involuntary unemployment further
reduces employment. This instability is due to the negative real balance effect
that occurs when consumers' net savings (the difference between savings and
pensions) are smaller than their debt multiplied by the marginal propensity to
consume from childhood consumption.",2020
http://arxiv.org/abs/2012.07008v1,Product Differentiation and Geographical Expansion of Exports Network at Industry level,2020-12-13 09:14:46+00:00,['Xuejian Wang'],econ.GN,"Industries can enter one country first, and then enter its neighbors'
markets. Firms in the industry can expand trade network through the export
behavior of other firms in the industry. If a firm is dependent on a few
foreign markets, the political risks of the markets will hurt the firm. The
frequent trade disputes reflect the importance of the choice of export
destinations. Although the market diversification strategy was proposed before,
most firms still focus on a few markets, and the paper shows reasons.In this
paper, we assume the entry cost of firms is not all sunk cost, and show 2 ways
that product heterogeneity impacts extensive margin of exports theoretically
and empirically. Firstly, the increase in product heterogeneity promotes the
increase in market power and profit, and more firms are able to pay the entry
cost. If more firms enter the market, the information of the market will be
known by other firms in the industry. Firms can adjust their behavior according
to other firms, so the information changes entry cost and is not sunk cost
completely. The information makes firms more likely to entry the market, and
enter the surrounding markets of existing markets of other firms in the
industry. When firms choose new markets, they tend to enter the markets with
few competitors first.Meanwhile, product heterogeneity will directly affect the
firms' network expansion, and the reduction of product heterogeneity will
increase the value of peer information. This makes firms more likely to entry
the market, and firms in the industry concentrate on the markets.",2020
http://arxiv.org/abs/2012.06742v1,Multi-market Oligopoly of Equal Capacity,2020-12-12 06:37:49+00:00,"['Ruda Zhang', 'Roger Ghanem']",math.OC,"We consider a variant of Cournot competition, where multiple firms allocate
the same amount of resource across multiple markets. We prove that the game has
a unique pure-strategy Nash equilibrium (NE), which is symmetric and is
characterized by the maximal point of a ""potential function"". The NE is
globally asymptotically stable under the gradient adjustment process, and is
not socially optimal in general. An application is in transportation, where
drivers allocate time over a street network.",2020
http://arxiv.org/abs/2112.15539v1,Fuzzy Core Equivalence in Large Economies: A Role for the Infinite-Dimensional Lyapunov Theorem,2021-12-31 16:31:58+00:00,"['M. Ali Khan', 'Nobusumi Sagara']",econ.TH,"We present the equivalence between the fuzzy core and the core under minimal
assumptions. Due to the exact version of the Lyapunov convexity theorem in
Banach spaces, we clarify that the additional structure of commodity spaces and
preferences is unnecessary whenever the measure space of agents is ""saturated"".
As a spin-off of the above equivalence, we obtain the coincidence of the core,
the fuzzy core, and the Schmeidler's restricted core under minimal assumptions.
The coincidence of the fuzzy core and the restricted core has not been
articulated anywhere.",2021
http://arxiv.org/abs/2112.15401v1,Towards the global vision of engagement of Generation Z at the workplace: Mathematical modeling,2021-12-31 12:04:44+00:00,"['Radosław A. Kycia', 'Agnieszka Niemczynowicz', 'Joanna Nieżurawska-Zając']",econ.GN,"Correlation and cluster analyses (k-Means, Gaussian Mixture Models) were
performed on Generation Z engagement surveys at the workplace. The clustering
indicates relations between various factors that describe the engagement of
employees. The most noticeable factors are a clear statement about the
responsibilities at work, and challenging work. These factors are essential in
practice. The results of this paper can be used in preparing better
motivational systems aimed at Generation Z employees.",2021
http://arxiv.org/abs/2112.15294v1,Macroeconomic and financial management in an uncertain world: What can we learn from complexity science?,2021-12-31 04:25:02+00:00,['Thitithep Sitthiyot'],econ.GN,"This paper discusses serious drawbacks of existing knowledge in
macroeconomics and finance in explaining and predicting economic and financial
phenomena. Complexity science is proposed as an alternative approach to be used
in order to better understand how economy and financial market work. This paper
argues that understanding characteristics of complex system could greatly
benefit financial analysts, financial regulators, as well as macroeconomic
policy makers.",2021
http://arxiv.org/abs/2112.15291v1,A simple method for estimating the Lorenz curve,2021-12-31 04:05:10+00:00,"['Thitithep Sitthiyot', 'Kanyarat Holasut']",econ.GN,"Given many popular functional forms for the Lorenz curve do not have a
closed-form expression for the Gini index and no study has utilized the
observed Gini index to estimate parameter(s) associated with the corresponding
parametric functional form, a simple method for estimating the Lorenz curve is
introduced. It utilizes 3 indicators, namely, the Gini index and the income
shares of the bottom and the top in order to calculate the values of parameters
associated with the specified functional form which has a closed-form
expression for the Gini index. No error minimization technique is required in
order to estimate the Lorenz curve. The data on the Gini index and the income
shares of 4 countries that have different level of income inequality, economic,
sociological, and regional backgrounds from the United Nations University-World
Income Inequality Database are used to illustrate how the simple method works.
The overall results indicate that the estimated Lorenz curves fit the actual
observations practically well. This simple method could be useful in the
situation where the availability of data on income distribution is low.
However, if more data on income distribution are available, this study shows
that the specified functional form could be used to directly estimate the
Lorenz curve. Moreover, the estimated values of the Gini index calculated based
on the specified functional form are virtually identical to their actual
observations.",2021
http://arxiv.org/abs/2112.15284v1,A simple method for measuring inequality,2021-12-31 03:53:54+00:00,"['Thitithep Sitthiyot', 'Kanyarat Holasut']",econ.GN,"To simultaneously overcome the limitation of the Gini index in that it is
less sensitive to inequality at the tails of income distribution and the
limitation of the inter-decile ratios that ignore inequality in the middle of
income distribution, an inequality index is introduced. It comprises three
indicators, namely, the Gini index, the income share held by the top 10%, and
the income share held by the bottom 10%. The data from the World Bank database
and the Organization for Economic Co-operation and Development Income
Distribution Database between 2005 and 2015 are used to demonstrate how the
inequality index works. The results show that it can distinguish income
inequality among countries that share the same Gini index but have different
income gaps between the top 10% and the bottom 10%. It could also distinguish
income inequality among countries that have the same ratio of income share held
by the top 10% to income share held by the bottom 10% but differ in the values
of the Gini index. In addition, the inequality index could capture the dynamics
where the Gini index of a country is stable over time but the ratio of income
share of the top 10% to income share of the bottom 10% is increasing.
Furthermore, the inequality index could be applied to other scientific
disciplines as a measure of statistical heterogeneity and for size
distributions of any non-negative quantities.",2021
http://arxiv.org/abs/2201.00013v1,The International Monetary Funds intervention in education systems and its impact on childrens chances of completing school,2021-12-30 22:56:49+00:00,['Adel Daoud'],econ.GN,"Enabling children to acquire an education is one of the most effective means
to reduce inequality, poverty, and ill-health globally. While in normal times a
government controls its educational policies, during times of macroeconomic
instability, that control may shift to supporting international organizations,
such as the International Monetary Fund (IMF). While much research has focused
on which sectors has been affected by IMF policies, scholars have devoted
little attention to the policy content of IMF interventions affecting the
education sector and childrens education outcomes: denoted IMF education
policies. This article evaluates the extent which IMF education policies exist
in all programs and how these policies and IMF programs affect childrens
likelihood of completing schools. While IMF education policies have a small
adverse effect yet statistically insignificant on childrens probability of
completing school, these policies moderate effect heterogeneity for IMF
programs. The effect of IMF programs (joint set of policies) adversely effect
childrens chances of completing school by six percentage points. By analyzing
how IMF-education policies but also how IMF programs affect the education
sector in low and middle-income countries, scholars will gain a deeper
understanding of how such policies will likely affect downstream outcomes.",2021
http://arxiv.org/abs/2112.15155v2,Auction Throttling and Causal Inference of Online Advertising Effects,2021-12-30 18:21:04+00:00,"['George Gui', 'Harikesh Nair', 'Fengshi Niu']",econ.EM,"Causally identifying the effect of digital advertising is challenging,
because experimentation is expensive, and observational data lacks random
variation. This paper identifies a pervasive source of naturally occurring,
quasi-experimental variation in user-level ad-exposure in digital advertising
campaigns. It shows how this variation can be utilized by ad-publishers to
identify the causal effect of advertising campaigns. The variation pertains to
auction throttling, a probabilistic method of budget pacing that is widely used
to spread an ad-campaign`s budget over its deployed duration, so that the
campaign`s budget is not exceeded or overly concentrated in any one period. The
throttling mechanism is implemented by computing a participation probability
based on the campaign`s budget spending rate and then including the campaign in
a random subset of available ad-auctions each period according to this
probability. We show that access to logged-participation probabilities enables
identifying the local average treatment effect (LATE) in the ad-campaign. We
present a new estimator that leverages this identification strategy and outline
a bootstrap procedure for quantifying its variability. We apply our method to
real-world ad-campaign data from an e-commerce advertising platform, which uses
such throttling for budget pacing. We show our estimate is statistically
different from estimates derived using other standard observational methods
such as OLS and two-stage least squares estimators. Our estimated conversion
lift is 110%, a more plausible number than 600%, the conversion lifts estimated
using naive observational methods.",2021
http://arxiv.org/abs/2112.15114v3,Estimating a Continuous Treatment Model with Spillovers: A Control Function Approach,2021-12-30 16:16:30+00:00,['Tadao Hoshino'],econ.EM,"We study a continuous treatment effect model in the presence of treatment
spillovers through social networks. We assume that one's outcome is affected
not only by his/her own treatment but also by a (weighted) average of his/her
neighbors' treatments, both of which are treated as endogenous variables. Using
a control function approach with appropriate instrumental variables, we show
that the conditional mean potential outcome can be nonparametrically
identified. We also consider a more empirically tractable semiparametric model
and develop a three-step estimation procedure for this model. As an empirical
illustration, we investigate the causal effect of the regional unemployment
rate on the crime rate.",2021
http://arxiv.org/abs/2112.15108v1,Modeling and Forecasting Intraday Market Returns: a Machine Learning Approach,2021-12-30 16:05:17+00:00,"['Iuri H. Ferreira', 'Marcelo C. Medeiros']",econ.EM,"In this paper we examine the relation between market returns and volatility
measures through machine learning methods in a high-frequency environment. We
implement a minute-by-minute rolling window intraday estimation method using
two nonlinear models: Long-Short-Term Memory (LSTM) neural networks and Random
Forests (RF). Our estimations show that the CBOE Volatility Index (VIX) is the
strongest candidate predictor for intraday market returns in our analysis,
specially when implemented through the LSTM model. This model also improves
significantly the performance of the lagged market return as predictive
variable. Finally, intraday RF estimation outputs indicate that there is no
performance improvement with this method, and it may even worsen the results in
some cases.",2021
http://arxiv.org/abs/2112.14902v2,Thirty Years of Academic Finance,2021-12-30 03:04:53+00:00,"['David Ardia', 'Keven Bluteau', 'Mohammad Abbas Meghani']",q-fin.GN,"We study how the financial literature has evolved in scale, research team
composition, and article topicality across 32 finance-focused academic journals
from 1992 to 2021. We document that the field has vastly expanded regarding
outlets and published articles. Teams have become larger, and the proportion of
women participating in research has increased significantly. Using the
Structural Topic Model, we identify 45 topics discussed in the literature. We
investigate the topic coverage of individual journals and can identify highly
specialized and generalist outlets, but our analyses reveal that most journals
have covered more topics over time, thus becoming more generalist. Finally, we
find that articles with at least one woman author focus more on topics related
to social and governance aspects of corporate finance. We also find that teams
with at least one top-tier institution scholar tend to focus more on
theoretical aspects of finance.",2021
http://arxiv.org/abs/2112.14849v1,Institutional Quality and the Wealth of Autocrats,2021-12-29 22:17:55+00:00,"['Christopher Boudreaux', 'Randall Holcombe']",econ.GN,"One frequently given explanation for why autocrats maintain corrupt and
inefficient institutions is that the autocrats benefit personally even though
the citizens of their countries are worse off. The empirical evidence does not
support this hypothesis. Autocrats in countries with low-quality institutions
do tend to be wealthy, but typically, they were wealthy before they assumed
power. A plausible explanation, consistent with the data, is that wealthy
individuals in countries with inefficient and corrupt institutions face the
threat of having their wealth appropriated by government, so have the incentive
to use some of their wealth to seek political power to protect the rest of
their wealth from confiscation. While autocrats may use government institutions
to increase their wealth, autocrats in countries with low-quality institutions
tend to be wealthy when they assume power, because wealthy individuals have the
incentive to use their wealth to acquire political power to protect themselves
from a potentially predatory government.",2021
http://arxiv.org/abs/2112.14846v1,An Analysis of an Alternative Pythagorean Expected Win Percentage Model: Applications Using Major League Baseball Team Quality Simulations,2021-12-29 22:08:24+00:00,"['Justin Ehrlich', 'Christopher Boudreaux', 'James Boudreau', 'Shane Sanders']",econ.EM,"We ask if there are alternative contest models that minimize error or
information loss from misspecification and outperform the Pythagorean model.
This article aims to use simulated data to select the optimal expected win
percentage model among the choice of relevant alternatives. The choices include
the traditional Pythagorean model and the difference-form contest success
function (CSF). Method. We simulate 1,000 iterations of the 2014 MLB season for
the purpose of estimating and analyzing alternative models of expected win
percentage (team quality). We use the open-source, Strategic Baseball Simulator
and develop an AutoHotKey script that programmatically executes the SBS
application, chooses the correct settings for the 2014 season, enters a unique
ID for the simulation data file, and iterates these steps 1,000 times. We
estimate expected win percentage using the traditional Pythagorean model, as
well as the difference-form CSF model that is used in game theory and public
choice economics. Each model is estimated while accounting for fixed (team)
effects. We find that the difference-form CSF model outperforms the traditional
Pythagorean model in terms of explanatory power and in terms of
misspecification-based information loss as estimated by the Akaike Information
Criterion. Through parametric estimation, we further confirm that the simulator
yields realistic statistical outcomes. The simulation methodology offers the
advantage of greatly improved sample size. As the season is held constant, our
simulation-based statistical inference also allows for estimation and model
comparison without the (time series) issue of non-stationarity. The results
suggest that improved win (productivity) estimation can be achieved through
alternative CSF specifications.",2021
http://arxiv.org/abs/2112.14748v2,Adaptive Transit Design: Optimizing Fixed and Demand Responsive Multi-Modal Transportation via Continuous Approximation,2021-12-29 18:53:06+00:00,"[""Giovanni Calabro'"", 'Andrea Araldo', 'Simon Oh', 'Ravi Seshadri', 'Giuseppe Inturri', 'Moshe Ben-Akiva']",econ.GN,"In most cities, transit consists solely of fixed-route transportation, whence
the inherent limited Quality of Service for travellers in suburban areas and
during off-peak periods. On the other hand, completely replacing fixed-route
(FR) with demand-responsive (DR) transit would imply a huge operational cost.
It is still unclear how to integrate DR transportation into current transit
systems to take full advantage of it. We propose a Continuous Approximation
model of a transit system that gets the best from fixed-route and DR
transportation. Our model allows deciding whether to deploy a FR or a DR
feeder, in each sub-region of an urban conurbation and each time of day, and to
redesign the line frequencies and the stop spacing of the main trunk service.
Since such a transit design can adapt to the spatial and temporal variation of
the demand, we call it Adaptive Transit. Numerical results show that, with
respect to conventional transit, Adaptive Transit significantly improves
user-related cost, by drastically reducing access time to the main trunk
service. Such benefits are particularly remarkable in the suburbs. Moreover,
the generalized cost, including agency and user cost, is also reduced. These
findings are also confirmed in scenarios with automated vehicles. Our model can
assist in planning future-generation transit systems, able to improve urban
mobility by appropriately combining fixed and DR transportation.",2021
http://arxiv.org/abs/2112.14529v3,Volatility of volatility estimation: central limit theorems for the Fourier transform estimator and empirical study of the daily time series stylized facts,2021-12-29 12:53:02+00:00,"['Giacomo Toscano', 'Giulia Livieri', 'Maria Elvira Mancino', 'Stefano Marmi']",math.ST,"We study the asymptotic normality of two feasible estimators of the
integrated volatility of volatility based on the Fourier methodology, which
does not require the pre-estimation of the spot volatility. We show that the
bias-corrected estimator reaches the optimal rate $n^{1/4}$, while the
estimator without bias-correction has a slower convergence rate and a smaller
asymptotic variance. Additionally, we provide simulation results that support
the theoretical asymptotic distribution of the rate-efficient estimator and
show the accuracy of the latter in comparison with a rate-optimal estimator
based on the pre-estimation of the spot volatility. Finally, using the
rate-optimal Fourier estimator, we reconstruct the time series of the daily
volatility of volatility of the S\&P500 and EUROSTOXX50 indices over long
samples and provide novel insight into the existence of stylized facts about
the volatility of volatility dynamics.",2021
http://arxiv.org/abs/2112.14514v15,"Technology, Institution, and Regional Growth: Evidence from Mineral Mining Industry in Industrializing Japan",2021-12-29 11:47:08+00:00,['Kota Ogasawara'],econ.GN,"Coal extraction was an influential economic activity in interwar Japan.
Initially, coal mines employed both males and females as the workforce in the
pits. However, the innovation of labor-saving technologies and the renewal of
traditional extraction methodology induced institutional change. This was
manifested by the revision of labor regulations affecting female miners in the
early 1930s. This dramatically changed the mining workplace, making skilled
males the principal miners engaged in underground work. This paper investigates
the impact of coal mining on regional growth and assesses how the institutional
changes induced by the amended labor regulations affected its processes. By
linking the mines' location information with both registration and census-based
statistics, it was found that coal mines led to remarkable population growth.
Fertility rate increased following the implementation of labor regulations that
required female miners to leave the workforce and start families. The
regulations prohibited female miners from risky underground work. This
reduction in occupational hazard also improved early-life mortality via the
mortality selection mechanism in utero.",2021
http://arxiv.org/abs/2112.15447v1,Analysis of Performance of Drivers and Usage of Overtime Hours: A Case Study of a Higher Educational Institution,2021-12-29 07:32:58+00:00,['K. C. Sanjeevani Perera'],econ.GN,"This study attempted to analyze whether there is a relationship between the
performance of drivers and the number of overtime hours worked by them. The
number of overtime hours worked by the drivers in the pool for the years 2017
and 2018 were extracted from the overtime registers and feedback received on
the performance of drivers from staff members who frequently traveled in the
University vehicles were used for this study. The overall performance of a
driver was decided by taking the aggregate of marks received by him for the
traits: skillfulness, patience, responsibility, customer service and care for
the vehicle. The type of vehicle the driver is assigned for is also taken into
account in the analysis of this study. The study revealed that there is no
significant relationship between the performance of the drivers and the number
of overtime hours worked by them but the type of vehicle and the condition of
the vehicle affects attracting long journeys to them which enable them to earn
more overtime hours.",2021
http://arxiv.org/abs/2112.14377v2,DeepHAM: A Global Solution Method for Heterogeneous Agent Models with Aggregate Shocks,2021-12-29 03:09:19+00:00,"['Jiequn Han', 'Yucheng Yang', 'Weinan E']",econ.GN,"An efficient, reliable, and interpretable global solution method, the Deep
learning-based algorithm for Heterogeneous Agent Models (DeepHAM), is proposed
for solving high dimensional heterogeneous agent models with aggregate shocks.
The state distribution is approximately represented by a set of optimal
generalized moments. Deep neural networks are used to approximate the value and
policy functions, and the objective is optimized over directly simulated paths.
In addition to being an accurate global solver, this method has three
additional features. First, it is computationally efficient in solving complex
heterogeneous agent models, and it does not suffer from the curse of
dimensionality. Second, it provides a general and interpretable representation
of the distribution over individual states, which is crucial in addressing the
classical question of whether and how heterogeneity matters in macroeconomics.
Third, it solves the constrained efficiency problem as easily as it solves the
competitive equilibrium, which opens up new possibilities for studying optimal
monetary and fiscal policies in heterogeneous agent models with aggregate
shocks.",2021
http://arxiv.org/abs/2112.14356v5,Private Private Information,2021-12-29 01:30:39+00:00,"['Kevin He', 'Fedor Sandomirskiy', 'Omer Tamuz']",econ.TH,"Private signals model noisy information about an unknown state. Although
these signals are called ""private,"" they may still carry information about each
other. Our paper introduces the concept of private private signals, which
contain information about the state but not about other signals. To achieve
privacy, signal quality may need to be sacrificed. We study the informativeness
of private private signals and characterize those that are optimal in the sense
that they cannot be made more informative without violating privacy. We discuss
implications for privacy in recommendation systems, information design, causal
inference, and mechanism design.",2021
http://arxiv.org/abs/2112.14265v5,Learning in Repeated Interactions on Networks,2021-12-28 19:04:10+00:00,"['Wanying Huang', 'Philipp Strack', 'Omer Tamuz']",econ.TH,"We study how long-lived, rational agents learn in a social network. In every
period, after observing the past actions of his neighbors, each agent receives
a private signal, and chooses an action whose payoff depends only on the state.
Since equilibrium actions depend on higher order beliefs, it is difficult to
characterize behavior. Nevertheless, we show that regardless of the size and
shape of the network, the utility function, and the patience of the agents, the
speed of learning in any equilibrium is bounded from above by a constant that
only depends on the private signal distribution.",2021
http://arxiv.org/abs/2112.14249v4,Nested Nonparametric Instrumental Variable Regression,2021-12-28 18:29:56+00:00,"['Isaac Meza', 'Rahul Singh']",stat.ML,"Several causal parameters in short panel data models are functionals of a
nested nonparametric instrumental variable regression (nested NPIV). Recent
examples include mediated, time varying, and long term treatment effects
identified using proxy variables. In econometrics, examples arise in triangular
simultaneous equations and hedonic price systems. However, it appears that
explicit mean square convergence rates for nested NPIV are unknown, preventing
inference on some of these parameters with generic machine learning. A major
challenge is compounding ill posedness due to the nested inverse problems. To
limit how ill posedness compounds, we introduce two techniques: relative well
posedness, and multiple robustness to ill posedness. With these techniques, we
provide explicit mean square rates for nested NPIV and efficient inference for
recently identified causal parameters. Our nonasymptotic analysis accommodates
neural networks, random forests, and reproducing kernel Hilbert spaces. It
extends to causal functions, e.g. heterogeneous long term treatment effects.",2021
http://arxiv.org/abs/2112.14074v3,On the Equivalence of Two Competing Affirmative Actions in School Choice,2021-12-28 10:13:10+00:00,['Yun Liu'],econ.TH,"This note analyzes the outcome equivalence conditions of two popular
affirmative action policies, majority quota and minority reserve, under the
student optimal stable mechanism. These two affirmative actions generate an
identical matching outcome, if the market either is effectively competitive or
contains a sufficiently large number of schools.",2021
http://arxiv.org/abs/2112.14054v1,Uniformly Self-Justified Equilibria,2021-12-28 09:08:57+00:00,"['Felix Kubler', 'Simon Scheidegger']",econ.TH,"We consider dynamic stochastic economies with heterogeneous agents and
introduce the concept of uniformly self-justified equilibria (USJE) --
temporary equilibria for which forecasts are best uniform approximations to a
selection of the equilibrium correspondence. In a USJE, individuals'
forecasting functions for the next period's endogenous variables are assumed to
lie in a compact, finite-dimensional set of functions, and the forecasts
constitute the best approximation within this set. We show that USJE always
exist and develop a simple algorithm to compute them. Therefore, they are more
tractable than rational expectations equilibria that do not always exist. As an
application, we discuss a stochastic overlapping generations exchange economy
and provide numerical examples to illustrate the concept of USJE and the
computational method.",2021
http://arxiv.org/abs/2202.00109v2,Measuring poverty in India with machine learning and remote sensing,2021-12-27 22:28:49+00:00,"['Adel Daoud', 'Felipe Jordan', 'Makkunda Sharma', 'Fredrik Johansson', 'Devdatt Dubhashi', 'Sourabh Paul', 'Subhashis Banerjee']",econ.GN,"In this paper, we use deep learning to estimate living conditions in India.
We use both census and surveys to train the models. Our procedure achieves
comparable results to those found in the literature, but for a wide range of
outcomes.",2021
http://arxiv.org/abs/2112.13911v2,The Economics of Interstellar Flight,2021-12-27 21:35:56+00:00,"['Philip Lubin', 'Alexander N. Cohen']",econ.GN,"Large scale directed energy offers the possibility of radical transformation
in a variety of areas, including the ability to achieve relativistic flight
that will enable the first interstellar missions, as well as rapid
interplanetary transit. In addition, the same technology will allow for
long-range beamed power for ion, ablation, and thermal engines, as well as
long-range recharging of distant spacecraft, long-range and ultra high
bandwidth laser communications, and many additional applications that include
remote composition analysis, manipulation of asteroids, and full planetary
defense. Directed energy relies on photonics which, like electronics, is an
exponentially expanding growth area driven by diverse economic interests that
allows transformational advances in space exploration and capability. We have
made enormous technological progress in the last few years to enable this
long-term vision. In addition to the technological challenges, we must face the
economic challenges to bring the vision to reality. The path ahead requires a
fundamental change in the system designs to allow for the radical cost
reductions required. To afford the full-scale realization of this vision we
will need to bring to fore integrated photonics and mass production as a path
forward. Fortunately, integrated photonics is a technology driven by vast
consumer need for high speed data delivery. We outline the fundamental physics
that drive the economics and derive an analytic cost model that allows us to
logically plan the path ahead.",2021
http://arxiv.org/abs/2112.13850v2,Using maps to predict economic activity,2021-12-27 14:13:20+00:00,"['Imryoung Jeong', 'Hyunjoo Yang']",econ.GN,"We introduce a novel machine learning approach to leverage historical and
contemporary maps and systematically predict economic statistics. Our simple
algorithm extracts meaningful features from the maps based on their color
compositions for predictions. We apply our method to grid-level population
levels in Sub-Saharan Africa in the 1950s and South Korea in 1930, 1970, and
2015. Our results show that maps can reliably predict population density in the
mid-20th century Sub-Saharan Africa using 9,886 map grids (5km by 5 km).
Similarly, contemporary South Korean maps can generate robust predictions on
income, consumption, employment, population density, and electric consumption.
In addition, our method is capable of predicting historical South Korean
population growth over a century.",2021
http://arxiv.org/abs/2112.13649v1,Random Rank-Dependent Expected Utility,2021-12-27 13:18:42+00:00,"['Nail Kashaev', 'Victor Aguiar']",econ.TH,"We present a novel characterization of random rank-dependent expected utility
for finite datasets and finite prizes. The test lends itself to statistical
testing using the tools in Kitamura and Stoye (2018).",2021
http://arxiv.org/abs/2112.13849v3,The Long-Run Impact of Electoral Violence on Health and Human Capital in Kenya,2021-12-27 12:23:04+00:00,['Roxana Gutiérrez-Romero'],econ.GN,"This paper examines the long-term effects of prenatal, childhood, and teen
exposure to electoral violence on health and human capital. Furthermore, it
investigates whether these effects are passed down to future generations. We
exploit the temporal and spatial variation of electoral violence in Kenya
between 1992 and 2013 in conjunction with a nationally representative survey to
identify people exposed to such violence. Using coarsened matching, we find
that exposure to electoral violence between prenatal and the age of sixteen
reduces adult height. Previous research has demonstrated that protracted,
large-scale armed conflicts can pass down stunting effects to descendants. In
line with these studies, we find that the low-scale but recurrent electoral
violence in Kenya has affected the height-for-age of children whose parents
were exposed to such violence during their growing years. Only boys exhibit
this intergenerational effect, possibly due to their increased susceptibility
to malnutrition and stunting in Sub-Saharan Africa. In contrast to previous
research on large-scale conflicts, childhood exposure to electoral violence has
no long-term effect on educational attainment or household consumption per
capita. Most electoral violence in Kenya has occurred during school breaks,
which may have mitigated its long-term effects on human capital and earning
capacity.",2021
http://arxiv.org/abs/2201.11214v1,Democratising Risk: In Search of a Methodology to Study Existential Risk,2021-12-27 08:21:52+00:00,"['Carla Zoe Cremer', 'Luke Kemp']",econ.GN,"Studying potential global catastrophes is vital. The high stakes of
existential risk studies (ERS) necessitate serious scrutiny and
self-reflection. We argue that existing approaches to studying existential risk
are not yet fit for purpose, and perhaps even run the risk of increasing harm.
We highlight general challenges in ERS: accommodating value pluralism, crafting
precise definitions, developing comprehensive tools for risk assessment,
dealing with uncertainty, and accounting for the dangers associated with taking
exceptional actions to mitigate or prevent catastrophes. The most influential
framework for ERS, the 'techno-utopian approach' (TUA), struggles with these
issues and has a unique set of additional problems: it unnecessarily combines
the study of longtermism and longtermist ethics with the study of extinction,
relies on a non-representative moral worldview, uses ambiguous and inadequate
definitions, fails to incorporate insights from risk assessment in relevant
fields, chooses arbitrary categorisations of risk, and advocates for dangerous
mitigation strategies. Its moral and empirical assumptions might be
particularly vulnerable to securitisation and misuse. We suggest several key
improvements: separating the study of extinction ethics (ethical implications
of extinction) and existential ethics (the ethical implications of different
societal forms), from the analysis of human extinction and global catastrophe;
drawing on the latest developments in risk assessment literature; diversifying
the field, and; democratising its policy recommendations.",2021
http://arxiv.org/abs/2112.13506v1,Estimation based on nearest neighbor matching: from density ratio to average treatment effect,2021-12-27 04:45:29+00:00,"['Zhexiao Lin', 'Peng Ding', 'Fang Han']",math.ST,"Nearest neighbor (NN) matching as a tool to align data sampled from different
groups is both conceptually natural and practically well-used. In a landmark
paper, Abadie and Imbens (2006) provided the first large-sample analysis of NN
matching under, however, a crucial assumption that the number of NNs, $M$, is
fixed. This manuscript reveals something new out of their study and shows that,
once allowing $M$ to diverge with the sample size, an intrinsic statistic in
their analysis actually constitutes a consistent estimator of the density
ratio. Furthermore, through selecting a suitable $M$, this statistic can attain
the minimax lower bound of estimation over a Lipschitz density function class.
Consequently, with a diverging $M$, the NN matching provably yields a doubly
robust estimator of the average treatment effect and is semiparametrically
efficient if the density functions are sufficiently smooth and the outcome
model is appropriately specified. It can thus be viewed as a precursor of
double machine learning estimators.",2021
http://arxiv.org/abs/2112.13495v1,Multiple Randomization Designs,2021-12-27 03:31:10+00:00,"['Patrick Bajari', 'Brian Burdick', 'Guido W. Imbens', 'Lorenzo Masoero', 'James McQueen', 'Thomas Richardson', 'Ido M. Rosen']",stat.ME,"In this study we introduce a new class of experimental designs. In a
classical randomized controlled trial (RCT), or A/B test, a randomly selected
subset of a population of units (e.g., individuals, plots of land, or
experiences) is assigned to a treatment (treatment A), and the remainder of the
population is assigned to the control treatment (treatment B). The difference
in average outcome by treatment group is an estimate of the average effect of
the treatment. However, motivating our study, the setting for modern
experiments is often different, with the outcomes and treatment assignments
indexed by multiple populations. For example, outcomes may be indexed by buyers
and sellers, by content creators and subscribers, by drivers and riders, or by
travelers and airlines and travel agents, with treatments potentially varying
across these indices. Spillovers or interference can arise from interactions
between units across populations. For example, sellers' behavior may depend on
buyers' treatment assignment, or vice versa. This can invalidate the simple
comparison of means as an estimator for the average effect of the treatment in
classical RCTs. We propose new experiment designs for settings in which
multiple populations interact. We show how these designs allow us to study
questions about interference that cannot be answered by classical randomized
experiments. Finally, we develop new statistical methods for analyzing these
Multiple Randomization Designs.",2021
http://arxiv.org/abs/2112.13398v5,Long Story Short: Omitted Variable Bias in Causal Machine Learning,2021-12-26 15:38:23+00:00,"['Victor Chernozhukov', 'Carlos Cinelli', 'Whitney Newey', 'Amit Sharma', 'Vasilis Syrgkanis']",econ.EM,"We develop a general theory of omitted variable bias for a wide range of
common causal parameters, including (but not limited to) averages of potential
outcomes, average treatment effects, average causal derivatives, and policy
effects from covariate shifts. Our theory applies to nonparametric models,
while naturally allowing for (semi-)parametric restrictions (such as partial
linearity) when such assumptions are made. We show how simple plausibility
judgments on the maximum explanatory power of omitted variables are sufficient
to bound the magnitude of the bias, thus facilitating sensitivity analysis in
otherwise complex, nonlinear models. Finally, we provide flexible and efficient
statistical inference methods for the bounds, which can leverage modern machine
learning algorithms for estimation. These results allow empirical researchers
to perform sensitivity analyses in a flexible class of machine-learned causal
models using very simple, and interpretable, tools. We demonstrate the utility
of our approach with two empirical examples.",2021
http://arxiv.org/abs/2112.14697v1,The Inflation Game,2021-12-26 13:52:25+00:00,['Wolfgang Kuhle'],econ.TH,"We study a game where households convert paper assets, such as money, into
consumption goods, to preempt inflation. The game features a unique equilibrium
with high (low) inflation, if money supply is high (low). For intermediate
levels of money supply, there exist multiple equilibria with either high or low
inflation. Equilibria with moderate inflation, however, do not exist, and can
thus not be targeted by a central bank. That is, depending on agents'
equilibrium play, money supply is always either too high or too low for
moderate inflation. We also show that inflation rates of long-lived goods, such
as houses, cars, expensive watches, furniture, or paintings, are a leading
indicator for broader, economy wide, inflation.",2021
http://arxiv.org/abs/2202.00127v1,On Market Design and Latency Arbitrage,2021-12-26 13:32:18+00:00,['Wolfgang Kuhle'],econ.GN,"We argue that contemporary stock market designs are, due to traders'
inability to fully express their preferences over the execution times of their
orders, prone to latency arbitrage. In turn, we propose a new order type which
allows traders to specify the time at which their orders are executed after
reaching the exchange. Using this order type, traders can synchronize order
executions across different exchanges, such that high-frequency traders, even
if they operate at the speed of light, can no-longer engage in latency
arbitrage.",2021
http://arxiv.org/abs/2112.13228v2,Robust Estimation of Average Treatment Effects from Panel Data,2021-12-25 12:20:35+00:00,"['Sayoni Roychowdhury', 'Indrila Ganguly', 'Abhik Ghosh']",stat.ME,"In order to evaluate the impact of a policy intervention on a group of units
over time, it is important to correctly estimate the average treatment effect
(ATE) measure. Due to lack of robustness of the existing procedures of
estimating ATE from panel data, in this paper, we introduce a robust estimator
of the ATE and the subsequent inference procedures using the popular approach
of minimum density power divergence inference. Asymptotic properties of the
proposed ATE estimator are derived and used to construct robust test statistics
for testing parametric hypotheses related to the ATE. Besides asymptotic
analyses of efficiency and powers, extensive simulation studies are conducted
to study the finite-sample performances of our proposed estimation and testing
procedures under both pure and contaminated data. The robustness of the ATE
estimator is further investigated theoretically through the influence functions
analyses. Finally our proposal is applied to study the long-term economic
effects of the 2004 Indian Ocean earthquake and tsunami on the (per-capita)
gross domestic products (GDP) of five mostly affected countries, namely
Indonesia, Sri Lanka, Thailand, India and Maldives.",2021
http://arxiv.org/abs/2112.12975v1,Identification of misreported beliefs,2021-12-24 07:32:19+00:00,['Elias Tsakas'],econ.TH,"It is well-known that subjective beliefs cannot be identified with
traditional choice data unless we impose the strong assumption that preferences
are state-independent. This is seen as one of the biggest pitfalls of
incentivized belief elicitation. The two common approaches are either to
exogenously assume that preferences are state-independent, or to use
intractable elicitation mechanisms that require an awful lot of hard-to-get
non-traditional choice data. In this paper we use a third approach, introducing
a novel methodology that retains the simplicity of standard elicitation
mechanisms without imposing the awkward state-independence assumption. The cost
is that instead of insisting on full identification of beliefs, we seek
identification of misreporting. That is, we elicit beliefs with a standard
simple elicitation mechanism, and then by means of a single additional
observation we can tell whether the reported beliefs deviate from the actual
beliefs, and if so, in which direction they do.",2021
http://arxiv.org/abs/2112.13844v1,Stability analysis of heterogeneous oligopoly games of increasing players with quadratic costs,2021-12-24 03:52:20+00:00,['Xiaoliang Li'],econ.TH,"In this discussion draft, we explore heterogeneous oligopoly games of
increasing players with quadratic costs, where the market is supposed to have
the isoelastic demand. For each of the models considered in this draft, we
analytically investigate the necessary and sufficient condition of the local
stability of its positive equilibrium. Furthermore, we rigorously prove that
the stability regions are enlarged as the number of involved firms is
increasing.",2021
http://arxiv.org/abs/2112.13842v1,Economics of Innovation and Perceptions of Renewed Education and Curriculum Design in Bangladesh,2021-12-23 23:14:00+00:00,"['Shifa Taslim Chowdhury', 'Mohammad Nur Nobi', 'Anm Moinul Islam']",econ.GN,"The creative Education system is one of the effective education systems in
many countries like Finland, Denmark, and South Korea. Bangladesh Government
has also launched the creative curriculum system in 2009 in both primary and
secondary levels, where changes have been made in educational contents and exam
question patterns. These changes in the previous curriculum aimed to avoid
memorization and less creativity and increase the students' level of
understanding and critical thinking. Though the Government has taken these
steps, the quality of the educational system in Bangladesh is still
deteriorating. Since the curriculum has been changed recently, this policy
issue got massive attention of the people because the problem of a substandard
education system has arisen. Many students have poor performances in
examinations, including entrance hall exams in universities and board
examinations. This deteriorating situation is mostly for leakage of question
paper, inadequate equipment and materials, and insufficient training. As a
result, the existing education system has failed to provide the standard level
of education. This research will discuss and find why this creative educational
system is getting impacted by these factors. It will be qualitative research. A
systematic questionnaire will interview different school teachers, parents,
experts, and students.",2021
http://arxiv.org/abs/2112.12766v2,Intra-Household Management of Joint Resources: Evidence from Malawi,2021-12-23 18:39:50+00:00,['Anna Josephson'],econ.GN,"In models of intra-household resource allocation, the earnings from joint
work between two or more household members are often omitted. I test
assumptions about complete pooling of resources within a household, by
accounting for income earned jointly by multiple household members, in addition
to income earned individually by men and women. Applied in the case of Malawi,
I find that by explicitly including intra-household collaboration, I find
evidence of partial income pooling and partial insurance within the household,
specifically for expenditures on food. Importantly, including joint income
reveals dynamics between household members, as well as opportunities and
vulnerabilities which may previously be obfuscated in simpler, binary
specifications. Contrasting with previous studies and empirical practice, my
findings suggest that understanding detailed intra-household interactions and
their outcomes on household behavior have important consequences for household
resource allocation and decision making.",2021
http://arxiv.org/abs/2112.12621v1,Should transparency be (in-)transparent? On monitoring aversion and cooperation in teams,2021-12-23 15:01:39+00:00,"['Michalis Drouvelis', 'Johannes Jarke-Neuert', 'Johannes Lohse']",econ.GN,"Many modern organisations employ methods which involve monitoring of
employees' actions in order to encourage teamwork in the workplace. While
monitoring promotes a transparent working environment, the effects of making
monitoring itself transparent may be ambiguous and have received surprisingly
little attention in the literature. Using a novel laboratory experiment, we
create a working environment in which first movers can (or cannot) observe
second mover's monitoring at the end of a round. Our framework consists of a
standard repeated sequential Prisoner's Dilemma, where the second mover can
observe the choices made by first movers either exogenously or endogenously. We
show that mutual cooperation occurs significantly more frequently when
monitoring is made transparent. Additionally, our results highlight the key
role of conditional cooperators (who are more likely to monitor) in promoting
teamwork. Overall, the observed cooperation enhancing effects are due to
monitoring actions that carry information about first movers who use it to
better screen the type of their co-player and thereby reduce the risk of being
exploited.",2021
http://arxiv.org/abs/2112.12464v1,"A meta-analysis of residential PV adoption: the important role of perceived benefits, intentions and antecedents in solar energy acceptance",2021-12-23 11:04:24+00:00,"['Emily Schulte', 'Fabian Scheller', 'Daniel Sloot', 'Thomas Bruckner']",econ.GN,"The adoption of residential photovoltaic systems (PV) is seen as an important
part of the sustainable energy transition. To facilitate this process, it is
crucial to identify the determinants of solar adoption. This paper follows a
meta-analytical structural equation modeling approach, presenting a
meta-analysis of studies on residential PV adoption intention, and assessing
four behavioral models based on the theory of planned behavior to advance
theory development. Of 653 initially identified studies, 110 remained for
full-text screening. Only eight studies were sufficiently homogeneous, provided
bivariate correlations, and could thus be integrated into the meta-analysis.The
pooled correlations across primary studies revealed medium to large
correlations between environmental concern, novelty seeking, perceived
benefits, subjective norm and intention to adopt a residential PV system,
whereas socio-demographic variables were uncorrelated with intention.
Meta-analytical structural equation modeling revealed a model (N = 1,714) in
which adoption intention was predicted by benefits and perceived behavioral
control, and benefits in turn could be explained by environmental concern,
novelty seeking, and subjective norm. Our results imply that measures should
primarily focus on enhancing the perception of benefits. Based on obstacles we
encountered within the analysis, we suggest guidelines to facilitate the future
aggregation of scientific evidence, such as the systematic inclusion of key
variables and reporting of bivariate correlations.",2021
http://arxiv.org/abs/2112.11867v1,"Product traits, decision-makers, and household low-carbon technology adoptions: moving beyond single empirical studies",2021-12-22 13:24:00+00:00,"['Emily Schulte', 'Fabian Scheller', 'Wilmer Pasut', 'Thomas Bruckner']",econ.GN,"Although single empirical studies provide important insights into who adopts
a specific LCT for what reason, fundamental questions concerning the relations
between decision subject (= who decides), decision object (= what is decided
upon) and context (= when and where it is decided) remain unanswered. In this
paper, this research gap is addressed by deriving a decision framework for
residential decision-making, suggesting that traits of decision subject and
object are determinants of financial, environmental, symbolic, normative,
effort and technical considerations preceding adoption. Thereafter, the
decision framework is initially verified by employing literature on the
adoption of photovoltaic systems, energy-efficient appliances and green
tariffs. Of the six proposed relations, two could be confirmed (financial and
environmental), one could be rejected (effort), and three could neither be
confirmed nor rejected due to lacking evidence. Future research on LCT adoption
could use the decision framework as a guidepost to establish a more coordinated
and integrated approach, ultimately allowing to address fundamental questions.",2021
http://arxiv.org/abs/2112.11822v1,"The co-evolutionary relationship between digitalization and organizational agility: Ongoing debates, theoretical developments and future research perspectives",2021-12-22 11:57:40+00:00,"['Francesco Ciampi', 'Monica Faraoni', 'Jacopo Ballerini', 'Francesco Meli']",econ.GN,"This study is the first to provide a systematic review of the literature
focused on the relationship between digitalization and organizational agility
(OA). It applies the bibliographic coupling method to 171 peer-reviewed
contributions published by 30 June 2021. It uses the digitalization perspective
to investigate the enablers, barriers and benefits of processes aimed at
providing firms with the agility required to effectively face increasingly
turbulent environments. Three different, though interconnected, thematic
clusters are discovered and analysed, respectively focusing on big-data
analytic capabilities as crucial drivers of OA, the relationship between
digitalization and agility at a supply chain level, and the role of information
technology capabilities in improving OA. By adopting a dynamic capabilities
perspective, this study overcomes the traditional view, which mainly considers
digital capabilities enablers of OA, rather than as possible outcomes. Our
findings reveal that, in addition to being complex, the relationship between
digitalization and OA has a bidirectional character. This study also identifies
extant research gaps and develops 13 original research propositions on possible
future research pathways and new managerial solutions.",2021
http://arxiv.org/abs/2112.15431v1,"Forecasting pandemic tax revenues in a small, open economy",2021-12-22 10:39:59+00:00,['Fabio Ashtar Telarico'],econ.GN,"Tax analysis and forecasting of revenues are of paramount importance to
ensure fiscal policy's viability and sustainability. However, the measures
taken to contain the spread of the recent pandemic pose an unprecedented
challenge to established models and approaches. This paper proposes a model to
forecast tax revenues in Bulgaria for the fiscal years 2020-2022 built in
accordance with the International Monetary Fund's recommendations on a dataset
covering the period between 1995 and 2019. The study further discusses the
actual trustworthiness of official Bulgarian forecasts, contrasting those
figures with the model previously estimated. This study's quantitative results
both confirm the pandemic's assumed negative impact on tax revenues and prove
that econometrics can be tweaked to produce consistent revenue forecasts even
in the relatively-unexplored case of Bulgaria offering new insights to
policymakers and advocates.",2021
http://arxiv.org/abs/2112.11751v1,Bayesian Approaches to Shrinkage and Sparse Estimation,2021-12-22 09:35:27+00:00,"['Dimitris Korobilis', 'Kenichi Shimizu']",econ.EM,"In all areas of human knowledge, datasets are increasing in both size and
complexity, creating the need for richer statistical models. This trend is also
true for economic data, where high-dimensional and nonlinear/nonparametric
inference is the norm in several fields of applied econometric work. The
purpose of this paper is to introduce the reader to the world of Bayesian model
determination, by surveying modern shrinkage and variable selection algorithms
and methodologies. Bayesian inference is a natural probabilistic framework for
quantifying uncertainty and learning about model parameters, and this feature
is particularly important for inference in modern models of high dimensions and
increased complexity.
  We begin with a linear regression setting in order to introduce various
classes of priors that lead to shrinkage/sparse estimators of comparable value
to popular penalized likelihood estimators (e.g.\ ridge, lasso). We explore
various methods of exact and approximate inference, and discuss their pros and
cons. Finally, we explore how priors developed for the simple regression
setting can be extended in a straightforward way to various classes of
interesting econometric models. In particular, the following case-studies are
considered, that demonstrate application of Bayesian shrinkage and variable
selection strategies to popular econometric contexts: i) vector autoregressive
models; ii) factor models; iii) time-varying parameter regressions; iv)
confounder selection in treatment effects models; and v) quantile regression
models. A MATLAB package and an accompanying technical manual allow the reader
to replicate many of the algorithms described in this review.",2021
http://arxiv.org/abs/2112.14713v1,"Perspectives in Public and University Sector Co-operation in the Change of Higher Education Model in Hungary, in Light of China's Experience",2021-12-21 19:45:18+00:00,"['Attila Lajos Makai', 'Szabolcs Ramhap']",econ.GN,"The model shift of higher education in Hungary brought not only the deepening
of university-industry relations and technology transfer processes, but also
contribute the emerging role of universities in shaping regional innovation
policy. This process provides a new framework for cooperation between actors in
regional innovation ecosystems and raises the relationship between
economic-governmental-academic systems to a new level. Active involvement of
government, the predominance of state resources, and the strong
innovation-organizing power of higher education institutions are similarities
that characterize both the Hungarian and Chinese innovation systems. This paper
attempts to gather Chinese good practices whose adaptation can contribute to
successful public-university collaboration. In the light of the examined
practices, the processes related to the university model shift implemented so
far can be placed in a new context, which are presented through the example of
the Sz\'echenyi Istv\'an University of Gy\H{o}r.",2021
http://arxiv.org/abs/2112.11499v1,The Changing Role of Entrepreneurial Universities in the Altering Innovation Policy: Opportunities Arising from the Paradigm Change in Light of the Experience of Széchenyi István University,2021-12-21 19:38:13+00:00,"['Attila Lajos Makai', 'Szabolcs Rámháp']",econ.GN,"The progress made by the entrepreneurial university, which is a newly
emerging category in Hungarian higher education after its change of model, has
not only deepened relations between universities and the industry and
intensified the technology and knowledge transfer processes, but also increased
the role of universities in shaping regional innovation policy. This
transformation places co-operation between the actors of the regional
innovation ecosystem and the relationships between the economic, governmental
and academic systems into a new framework. The purpose of this paper is to
describe the process of the change in the model through a specific example, and
to outline the future possibilities of university involvement in the currently
changing Hungarian innovation policy system.",2021
http://arxiv.org/abs/2112.11449v2,Doubly-Valid/Doubly-Sharp Sensitivity Analysis for Causal Inference with Unmeasured Confounding,2021-12-21 18:55:12+00:00,"['Jacob Dorn', 'Kevin Guo', 'Nathan Kallus']",stat.ME,"We consider the problem of constructing bounds on the average treatment
effect (ATE) when unmeasured confounders exist but have bounded influence.
Specifically, we assume that omitted confounders could not change the odds of
treatment for any unit by more than a fixed factor. We derive the sharp partial
identification bounds implied by this assumption by leveraging distributionally
robust optimization, and we propose estimators of these bounds with several
novel robustness properties. The first is double sharpness: our estimators
consistently estimate the sharp ATE bounds when one of two nuisance parameters
is misspecified and achieve semiparametric efficiency when all nuisance
parameters are suitably consistent. The second is double validity: even when
most nuisance parameters are misspecified, our estimators still provide valid
but possibly conservative bounds for the ATE and our Wald confidence intervals
remain valid even when our estimators are not asymptotically normal. As a
result, our estimators provide a highly credible method for sensitivity
analysis of causal inferences.",2021
http://arxiv.org/abs/2112.11931v1,Startup Ecosystem Rankings,2021-12-21 17:22:19+00:00,['Attila Lajos Makai'],cs.DL,"The number, importance, and popularity of rankings measuring innovation
performance and the strength and resources of ecosystems that provide its
spatial framework are on an increasing trend globally. In addition to
influencing the specific decisions taken by economic actors, these rankings
significantly impact the development of innovation-related policies at
regional, national, and international levels. The importance of startup
ecosystems is proven by the growing scientific interest, which is demonstrated
by the increasing number of related scientific articles. The concept of the
startup ecosystem is a relatively new category, the application of which in
everyday and scientific life has been gaining ground since the end of the
2000s. In parallel, of course, the demand for measurability and comparability
has emerged among decision-makers and scholars. This demand is met by startup
ecosystem rankings, which now measure and rank the performance of individual
ecosystems on a continental and global scale. However, while the number of
scientific publications examining rankings related to higher education,
economic performance, or even innovation, can be measured in the order of
thousands, scientific research has so far rarely or tangentially addressed the
rankings of startup ecosystems. This study and the related research intend to
fill this gap by presenting and analysing the characteristics of global
rankings and identifying possible future research directions.",2021
http://arxiv.org/abs/2112.11320v2,Bidding in Multi-Unit Auctions under Limited Information,2021-12-21 16:13:50+00:00,"['Bernhard Kasberger', 'Kyle Woodward']",econ.TH,"We study multi-unit auctions in which bidders have limited knowledge of
opponent strategies and values. We characterize optimal prior-free bids; these
bids minimize the maximal loss in expected utility resulting from uncertainty
surrounding opponent behavior. Optimal bids are readily computable despite
bidders having multi-dimensional private information, and in certain cases
admit closed-form solutions. In the pay-as-bid auction the minimax-loss bid is
unique; in the uniform-price auction the minimax-loss bid is unique if the
bidder is allowed to determine the quantities for which they bid, as in many
practical applications. We compare minimax-loss bids and auction outcomes
across auction formats, and derive testable predictions.",2021
http://arxiv.org/abs/2112.11315v1,Efficient Estimation of State-Space Mixed-Frequency VARs: A Precision-Based Approach,2021-12-21 16:06:04+00:00,"['Joshua C. C. Chan', 'Aubrey Poon', 'Dan Zhu']",econ.EM,"State-space mixed-frequency vector autoregressions are now widely used for
nowcasting. Despite their popularity, estimating such models can be
computationally intensive, especially for large systems with stochastic
volatility. To tackle the computational challenges, we propose two novel
precision-based samplers to draw the missing observations of the low-frequency
variables in these models, building on recent advances in the band and sparse
matrix algorithms for state-space models. We show via a simulation study that
the proposed methods are more numerically accurate and computationally
efficient compared to standard Kalman-filter based methods. We demonstrate how
the proposed method can be applied in two empirical macroeconomic applications:
estimating the monthly output gap and studying the response of GDP to a
monetary policy shock at the monthly frequency. Results from these two
empirical applications highlight the importance of incorporating high-frequency
indicators in macroeconomic models.",2021
http://arxiv.org/abs/2112.11263v1,Estimating economic severity of Air Traffic Flow Management regulations,2021-12-21 14:43:34+00:00,"['Luis Delgado', 'Gérald Gurtner', 'Tatjana Bolić', 'Lorenzo Castelli']",econ.GN,"The development of trajectory-based operations and the rolling network
operations plan in European air traffic management network implies a move
towards more collaborative, strategic flight planning. This opens up the
possibility for inclusion of additional information in the collaborative
decision-making process. With that in mind, we define the indicator for the
economic risk of network elements (e.g., sectors or airports) as the expected
costs that the elements impose on airspace users due to Air Traffic Flow
Management (ATFM) regulations. The definition of the indicator is based on the
analysis of historical ATFM regulations data, that provides an indication of
the risk of accruing delay. This risk of delay is translated into a monetary
risk for the airspace users, creating the new metric of the economic risk of a
given airspace element. We then use some machine learning techniques to find
the parameters leading to this economic risk. The metric is accompanied by an
indication of the accuracy of the delay cost prediction model. Lastly, the
economic risk is transformed into a qualitative economic severity
classification. The economic risks and consequently economic severity can be
estimated for different temporal horizons and time periods providing an
indicator which can be used by Air Navigation Service Providers to identify
areas which might need the implementation of strategic measures (e.g.,
resectorisation or capacity provision change), and by Airspace Users to
consider operation of routes which use specific airspace regions.",2021
http://arxiv.org/abs/2112.11064v1,Ranking and Selection from Pairwise Comparisons: Empirical Bayes Methods for Citation Analysis,2021-12-21 09:46:29+00:00,"['Jiaying Gu', 'Roger Koenker']",econ.EM,"We study the Stigler model of citation flows among journals adapting the
pairwise comparison model of Bradley and Terry to do ranking and selection of
journal influence based on nonparametric empirical Bayes procedures.
Comparisons with several other rankings are made.",2021
http://arxiv.org/abs/2112.10993v3,Learning in Random Utility Models Via Online Decision Problems,2021-12-21 05:32:02+00:00,['Emerson Melo'],econ.TH,"This paper studies the Random Utility Model (RUM) in a repeated stochastic
choice situation, in which the decision maker is imperfectly informed about the
payoffs of each available alternative. We develop a gradient-based learning
algorithm by embedding the RUM into an online decision problem. We show that a
large class of RUMs are Hannan consistent (\citet{Hahn1957}); that is, the
average difference between the expected payoffs generated by a RUM and that of
the best-fixed policy in hindsight goes to zero as the number of periods
increase. In addition, we show that our gradient-based algorithm is equivalent
to the Follow the Regularized Leader (FTRL) algorithm, which is widely used in
the machine learning literature to model learning in repeated stochastic choice
problems. Thus, we provide an economically grounded optimization framework to
the FTRL algorithm. Finally, we apply our framework to study recency bias,
no-regret learning in normal form games, and prediction markets.",2021
http://arxiv.org/abs/2112.10812v7,Contextually Private Mechanisms,2021-12-20 19:16:32+00:00,"['Andreas Haupt', 'Zoë Hitzig']",econ.TH,"We introduce a framework for comparing the privacy of different mechanisms. A
mechanism designer employs a dynamic protocol to elicit agents' private
information. Protocols produce a set of contextual privacy violations --
information learned about agents that may be superfluous given the context. A
protocol is maximally contextually private if there is no protocol that
produces a subset of the violations it produces, while still implementing the
choice rule. We show that selecting a maximally contextually private protocol
involves a deliberate decision about whose privacy is most important to
protect, and these protocols delay questions to those they aim to protect.
Taking the second-price auction rule as an instructive example, we derive two
novel designs that are maximally contextually private: the ascending-join and
overdescending-join protocols.",2021
http://arxiv.org/abs/2112.11562v1,Can large-scale R&I funding stimulate post-crisis recovery growth? Evidence for Finland during COVID-19,2021-12-20 16:33:10+00:00,"['Timo Mitze', 'Teemu Makkonen']",econ.GN,"The COVID-19 pandemic and subsequent public health restrictions led to a
significant slump in economic activities around the globe. This slump has met
by various policy actions to cushion the detrimental socio-economic
consequences of the COVID-19 crisis and eventually bring the economy back on
track. We provide an ex-ante evaluation of the effectiveness of a massive
increase in research and innovation (R&I) funding in Finland to stimulate
post-crisis recovery growth through an increase in R&I activities of Finnish
firms. We make use of the fact that novel R&I grants for firms in disruptive
circumstances granted in 2020 were allocated through established R&I policy
channels. This allows us to estimate the structural link between R&I funding
and economic growth for Finnish NUTS-3 regions using pre-COVID-19 data.
Estimates are then used to forecast regional recovery growth out of sample and
to quantify the growth contribution of R&I funding. Depending on the chosen
scenario, our forecasts point to a mean recovery growth rate of GDP between
2-4% in 2021 after a decline of up to -2.5% in 2020. R&I funding constitutes a
significant pillar of the recovery process with mean contributions in terms of
GDP growth of between 0.4% and 1%.",2021
http://arxiv.org/abs/2112.10584v1,A dynamic theory of spatial externalities,2021-12-20 15:02:33+00:00,"['Raouf Boucekkine', 'Giorgio Fabbri', 'Salvatore Federico', 'Fausto Gozzi']",econ.TH,"We characterize the shape of spatial externalities in a continuous time and
space differential game with transboundary pollution. We posit a realistic
spatiotemporal law of motion for pollution (diffusion and advection), and
tackle spatiotemporal non-cooperative (and cooperative) differential games.
Precisely, we consider a circle partitioned into several states where a local
authority decides autonomously about its investment, production and depollution
strategies over time knowing that investment/production generates pollution,
and pollution is transboundary. The time horizon is infinite. We allow for a
rich set of geographic heterogeneities across states. We solve analytically the
induced non-cooperative differential game and characterize its long-term
spatial distributions. In particular, we prove that there exist a Perfect
Markov Equilibrium, unique among the class of the affine feedbacks. We further
provide with a full exploration of the free riding problem and the associated
border effect.",2021
http://arxiv.org/abs/2112.11564v1,Associational and plausible causal effects of COVID-19 public health policies on economic and mental distress,2021-12-20 14:29:29+00:00,"['Reka Sundaram-Stukel', 'Richard J Davidson']",econ.GN,"Background The COVID-19 pandemic has increased mental distress globally. The
proportion of people reporting anxiety is 26%, and depression is 34% points.
Disentangling associational and causal contributions of behavior, COVID-19
cases, and economic distress on mental distress will dictate different
mitigation strategies to reduce long-term pandemic-related mental distress.
Methods We use the Household Pulse Survey (HPS) April 2020 to February 2021
data to examine mental distress among U.S. citizens attributable to COVID-19.
We combined HPS survey data with publicly available state-level weekly:
COVID-19 case and death data from the Centers for Disease Control, public
policies, and Apple and Google mobility data. Finally, we constructed economic
and mental distress measures to estimate structural models with lag dependent
variables to tease out public health policies' associational and causal path
coefficients on economic and mental distress. Findings From April 2020 to
February 2021, we found that anxiety and depression had steadily climbed in the
U.S. By design, mobility restrictions primarily affected public health policies
where businesses and restaurants absorbed the biggest hit. Period t-1 COVID-19
cases increased job loss by 4.1% and economic distress by 6.3% points in the
same period. Job-loss and housing insecurity in t-1 increased period t mental
distress by 29.1% and 32.7%, respectively. However, t-1 food insecurity
decreased mental distress by 4.9% in time t. The pandemic-related potential
causal path coefficient of period t-1 economic distress on period t depression
is 57.8%, and anxiety is 55.9%. Thus, we show that period t-1 COVID-19 case
information, behavior, and economic distress may be causally associated with
pandemic related period t mental distress.",2021
http://arxiv.org/abs/2112.10542v2,Heckman-Selection or Two-Part models for alcohol studies? Depends,2021-12-20 14:08:35+00:00,['Reka Sundaram-Stukel'],econ.EM,"Aims: To re-introduce the Heckman model as a valid empirical technique in
alcohol studies. Design: To estimate the determinants of problem drinking using
a Heckman and a two-part estimation model. Psychological and neuro-scientific
studies justify my underlying estimation assumptions and covariate exclusion
restrictions. Higher order tests checking for multicollinearity validate the
use of Heckman over the use of two-part estimation models. I discuss the
generalizability of the two models in applied research. Settings and
Participants: Two pooled national population surveys from 2016 and 2017 were
used: the Behavioral Risk Factor Surveillance Survey (BRFS), and the National
Survey of Drug Use and Health (NSDUH). Measurements: Participation in problem
drinking and meeting the criteria for problem drinking. Findings: Both U.S.
national surveys perform well with the Heckman model and pass all higher order
tests. The Heckman model corrects for selection bias and reveals the direction
of bias, where the two-part model does not. For example, the coefficients on
age are upward biased and unemployment is downward biased in the two-part where
the Heckman model does not have a selection bias. Covariate exclusion
restrictions are sensitive to survey conditions and are contextually
generalizable. Conclusions: The Heckman model can be used for alcohol (smoking
studies as well) if the underlying estimation specification passes higher order
tests for multicollinearity and the exclusion restrictions are justified with
integrity for the data used. Its use is merit-worthy because it corrects for
and reveals the direction and the magnitude of selection bias where the
two-part does not.",2021
http://arxiv.org/abs/2112.11565v1,Double Standards: The Implications of Near Certainty Drone Strikes in Pakistan,2021-12-20 04:53:48+00:00,"['Shyam Raman', 'Paul Lushenko', 'Sarah Kreps']",econ.GN,"In 2013, U.S. President Barack Obama announced a policy to minimize civilian
casualties following drone strikes in undeclared theaters of war. The policy
calibrated Obamas approval of strikes against the near certainty of no civilian
casualties. Scholars do not empirically study the merits of Obamas policy.
Rather, they rely on descriptive trends for civilian casualties in Pakistan to
justify competing claims for the policys impact. We provide a novel estimate
for the impact of Obamas policy for civilian casualties in Pakistan following
U.S. drone strikes. We employ a regression discontinuity design to estimate the
effect of Obamas policy for civilian casualties, strike precision, and adverted
civilian casualties. We find a discontinuity in civilian casualties
approximately two years before Obamas policy announcement, corroborating our
primary research including interviews with senior officials responsible for
implementing the near certainty standard. After confirming the sharp cutoff, we
estimate the policy resulted in a reduction of 12 civilian deaths per month or
2 casualties per strike. The policy also enhanced the precision of U.S. drone
strikes to the point that they only killed the intended targets. Finally, we
use a Monte Carlo simulation to estimate that the policy adverted 320 civilian
casualties. We then conduct a Value of Statistical Life calculation to show
that the adverted civilian casualties represent a gain of 80 to 260 million
U.S. dollars. In addition to conditioning social and political outcomes, then,
the near certainty standard also imposed economic implications that are much
less studied.",2021
http://arxiv.org/abs/2112.09975v5,Algorithm Design: A Fairness-Accuracy Frontier,2021-12-18 17:43:41+00:00,"['Annie Liang', 'Jay Lu', 'Xiaosheng Mu', 'Kyohei Okumura']",econ.TH,"Algorithm designers increasingly optimize not only for accuracy, but also for
the fairness of the algorithm across pre-defined groups. We study the tradeoff
between fairness and accuracy for any given set of inputs to the algorithm. We
propose and characterize a fairness-accuracy frontier, which consists of the
optimal points across a broad range of preferences over fairness and accuracy.
Our results identify a simple property of the inputs, group-balance, which
qualitatively determines the shape of the frontier. We further study an
information-design problem where the designer flexibly regulates the inputs
(e.g., by coarsening an input or banning its use) but the algorithm is chosen
by another agent. Whether it is optimal to ban an input generally depends on
the designer's preferences. But when inputs are group-balanced, then excluding
group identity is strictly suboptimal for all designers, and when the designer
has access to group identity, then it is strictly suboptimal to exclude any
informative input.",2021
http://arxiv.org/abs/2112.09850v1,"Paternalism, Autonomy, or Both? Experimental Evidence from Energy Saving Programs",2021-12-18 05:59:17+00:00,"['Takanori Ida', 'Takunori Ishihara', 'Koichiro Ito', 'Daido Kido', 'Toru Kitagawa', 'Shosei Sakaguchi', 'Shusaku Sasaki']",econ.GN,"Identifying who should be treated is a central question in economics. There
are two competing approaches to targeting - paternalistic and autonomous. In
the paternalistic approach, policymakers optimally target the policy given
observable individual characteristics. In contrast, the autonomous approach
acknowledges that individuals may possess key unobservable information on
heterogeneous policy impacts, and allows them to self-select into treatment. In
this paper, we propose a new approach that mixes paternalistic assignment and
autonomous choice. Our approach uses individual characteristics and empirical
welfare maximization to identify who should be treated, untreated, and decide
whether to be treated themselves. We apply this method to design a targeting
policy for an energy saving programs using data collected in a randomized field
experiment. We show that optimally mixing paternalistic assignments and
autonomous choice significantly improves the social welfare gain of the policy.
Exploiting random variation generated by the field experiment, we develop a
method to estimate average treatment effects for each subgroup of individuals
who would make the same autonomous treatment choice. Our estimates confirm that
the estimated assignment policy optimally allocates individuals to be treated,
untreated, or choose themselves based on the relative merits of paternalistic
assignments and autonomous choice for individuals types.",2021
http://arxiv.org/abs/2112.09783v1,More Reviews May Not Help: Evidence from Incentivized First Reviews on Airbnb,2021-12-17 21:56:34+00:00,"['Andrey Fradkin', 'David Holtz']",econ.GN,"Online reviews are typically written by volunteers and, as a consequence,
information about seller quality may be under-provided in digital marketplaces.
We study the extent of this under-provision in a large-scale randomized
experiment conducted by Airbnb. In this experiment, buyers are offered a coupon
to review listings that have no prior reviews. The treatment induces additional
reviews and these reviews tend to be more negative than reviews in the control
group, consistent with selection bias in reviewing. Reviews induced by the
treatment result in a temporary increase in transactions but these transactions
are for fewer nights, on average. The effects on transactions and nights per
transaction cancel out so that there is no detectable effect on total nights
sold and revenue. Measures of transaction quality in the treatment group fall,
suggesting that incentivized reviews do not improve matching. We show how
market conditions and the design of the reputation system can explain our
findings.",2021
http://arxiv.org/abs/2112.09478v1,Free-Riding for Future: Field Experimental Evidence of Strategic Substitutability in Climate Protest,2021-12-17 12:40:51+00:00,"['Johannes Jarke-Neuert', 'Grischa Perino', 'Henrike Schwickert']",econ.GN,"We test the hypothesis that protest participation decisions in an adult
population of potential climate protesters are interdependent. Subjects
(n=1,510) from the four largest German cities were recruited two weeks before
protest date. We measured participation (ex post) and beliefs about the other
subjects' participation (ex ante) in an online survey, used a randomized
informational intervention to induce exogenous variance in beliefs, and
estimated the causal effect of a change in belief on the probability of
participation using a control function approach. Participation decisions are
found to be strategic substitutes: a one percentage-point increase of belief
causes a .67 percentage-point decrease in the probability of participation in
the average subject.",2021
http://arxiv.org/abs/2112.09443v2,Distance Functions and Generalized Means: Duality and Taxonomy,2021-12-17 11:24:05+00:00,['Walter Briec'],econ.TH,"This paper introduces in production theory a large class of efficiency
measures that can be derived from the notion of utility function. This article
also establishes a relation between these distance functions and Stone-Geary
utility functions. More specifically, the paper focusses on new distance
function that generalizes several existing efficiency measures. The new
distance function is inspired from the Atkinson inequality index and maximizes
the sum of the netput expansions required to reach an efficient point. A
generalized duality theorem is proved and a duality result linking the new
distance functions and the profit function is obtained. For all feasible
production vectors, it includes as special cases most of the dual
correspondences previously established in the literature. Finally, we identify
a large class of measures for which these duality results can be obtained
without convexity.",2021
http://arxiv.org/abs/2112.09259v2,"Robustness, Heterogeneous Treatment Effects and Covariate Shifts",2021-12-16 23:53:42+00:00,['Pietro Emilio Spini'],econ.EM,"This paper studies the robustness of estimated policy effects to changes in
the distribution of covariates. Robustness to covariate shifts is important,
for example, when evaluating the external validity of quasi-experimental
results, which are often used as a benchmark for evidence-based policy-making.
I propose a novel scalar robustness metric. This metric measures the magnitude
of the smallest covariate shift needed to invalidate a claim on the policy
effect (for example, $ATE \geq 0$) supported by the quasi-experimental
evidence. My metric links the heterogeneity of policy effects and robustness in
a flexible, nonparametric way and does not require functional form assumptions.
I cast the estimation of the robustness metric as a de-biased GMM problem. This
approach guarantees a parametric convergence rate for the robustness metric
while allowing for machine learning-based estimators of policy effect
heterogeneity (for example, lasso, random forest, boosting, neural nets). I
apply my procedure to the Oregon Health Insurance experiment. I study the
robustness of policy effects estimates of health-care utilization and financial
strain outcomes, relative to a shift in the distribution of context-specific
covariates. Such covariates are likely to differ across US states, making
quantification of robustness an important exercise for adoption of the
insurance policy in states other than Oregon. I find that the effect on
outpatient visits is the most robust among the metrics of health-care
utilization considered.",2021
http://arxiv.org/abs/2112.09170v5,Reinforcing RCTs with Multiple Priors while Learning about External Validity,2021-12-16 19:38:09+00:00,"['Frederico Finan', 'Demian Pouzo']",econ.EM,"This paper introduces a framework for incorporating prior information into
the design of sequential experiments. These sources may include past
experiments, expert opinions, or the experimenter's intuition. We model the
problem using a multi-prior Bayesian approach, mapping each source to a
Bayesian model and aggregating them based on posterior probabilities. Policies
are evaluated on three criteria: learning the parameters of payoff
distributions, the probability of choosing the wrong treatment, and average
rewards. Our framework demonstrates several desirable properties, including
robustness to sources lacking external validity, while maintaining strong
finite sample performance.",2021
http://arxiv.org/abs/2112.09065v2,Macroscopic properties of buyer-seller networks in online marketplaces,2021-12-16 18:00:47+00:00,"['Alberto Bracci', 'Jörn Boehnke', 'Abeer ElBahrawy', 'Nicola Perra', 'Alexander Teytelboym', 'Andrea Baronchelli']",physics.soc-ph,"Online marketplaces are the main engines of legal and illegal e-commerce, yet
their empirical properties are poorly understood due to the absence of
large-scale data. We analyze two comprehensive datasets containing 245M
transactions (16B USD) that took place on online marketplaces between 2010 and
2021, covering 28 dark web marketplaces, i.e., unregulated markets whose main
currency is Bitcoin, and 144 product markets of one popular regulated
e-commerce platform. We show that transactions in online marketplaces exhibit
strikingly similar patterns despite significant differences in language,
lifetimes, products, regulation, and technology. Specifically, we find
remarkable regularities in the distributions of transaction amounts, number of
transactions, inter-event times and time between first and last transactions.
We show that buyer behavior is affected by the memory of past interactions and
use this insight to propose a model of network formation reproducing our main
empirical observations. Our findings have implications for understanding market
power on online marketplaces as well as inter-marketplace competition, and
provide empirical foundation for theoretical economic models of online
marketplaces.",2021
http://arxiv.org/abs/2112.08934v4,Lassoed Boosting and Linear Prediction in the Equities Market,2021-12-16 15:00:37+00:00,['Xiao Huang'],econ.EM,"We consider a two-stage estimation method for linear regression. First, it
uses the lasso in Tibshirani (1996) to screen variables and, second,
re-estimates the coefficients using the least-squares boosting method in
Friedman (2001) on every set of selected variables. Based on the large-scale
simulation experiment in Hastie et al. (2020), lassoed boosting performs as
well as the relaxed lasso in Meinshausen (2007) and, under certain scenarios,
can yield a sparser model. Applied to predicting equity returns, lassoed
boosting gives the smallest mean-squared prediction error compared to several
other methods.",2021
http://arxiv.org/abs/2112.08546v2,Uniform Convergence Results for the Local Linear Regression Estimation of the Conditional Distribution,2021-12-16 01:04:23+00:00,['Haitian Xie'],econ.EM,"This paper examines the local linear regression (LLR) estimate of the
conditional distribution function $F(y|x)$. We derive three uniform convergence
results: the uniform bias expansion, the uniform convergence rate, and the
uniform asymptotic linear representation. The uniformity in the above results
is with respect to both $x$ and $y$ and therefore has not previously been
addressed in the literature on local polynomial regression. Such uniform
convergence results are especially useful when the conditional distribution
estimator is the first stage of a semiparametric estimator. We demonstrate the
usefulness of these uniform results with two examples: the stochastic
equicontinuity condition in $y$, and the estimation of the integrated
conditional distribution function.",2021
http://arxiv.org/abs/2112.11563v2,Cultural Diversity and Its Impact on Governance,2021-12-15 23:36:56+00:00,"['Tomáš Evan', 'Vladimír Holý']",econ.GN,"Hofstede's six cultural dimensions make it possible to measure the culture of
countries but are criticized for assuming the homogeneity of each country. In
this paper, we propose two measures based on Hofstede's cultural dimensions
which take into account the heterogeneous structure of citizens with respect to
their countries of origin. Using these improved measures, we study the
influence of heterogeneous culture and cultural diversity on the quality of
institutions measured by the six worldwide governance indicators. We use a
linear regression model allowing for dependence in spatial and temporal
dimensions as well as high correlation between the governance indicators. Our
results show that the effect of cultural diversity improves some of the
governance indicators while worsening others depending on the individual
Hofstede cultural dimension.",2021
http://arxiv.org/abs/2112.08153v2,Taxes and Market Power: A Principal Components Approach,2021-12-15 14:21:27+00:00,"['Andrea Galeotti', 'Benjamin Golub', 'Sanjeev Goyal', 'Eduard Talamàs', 'Omer Tamuz']",econ.TH,"Suppliers of differentiated goods make simultaneous pricing decisions, which
are strategically linked. Because of market power, the equilibrium is
inefficient. We study how a policymaker should target a budget-balanced
tax-and-subsidy policy to increase welfare. A key tool is a certain basis for
the goods space, determined by the network of interactions among suppliers. It
consists of eigenbundles -- orthogonal in the sense that a tax on any
eigenbundle passes through only to its own price -- with pass-through
coefficients determined by associated eigenvalues. Our basis permits a simple
characterization of optimal interventions. A planner maximizing consumer
surplus should tax eigenbundles with low pass-through and subsidize ones with
high pass-through. The Pigouvian leverage of the system -- the gain in consumer
surplus achievable by an optimal tax scheme -- depends only on the dispersion
of the eigenvalues of the matrix of strategic interactions. We interpret these
results in terms of the network structure of the market.",2021
http://arxiv.org/abs/2112.08092v2,Testing Instrument Validity with Covariates,2021-12-15 13:06:22+00:00,"['Thomas Carr', 'Toru Kitagawa']",econ.EM,"We develop a novel test of the instrumental variable identifying assumptions
for heterogeneous treatment effect models with conditioning covariates. We
assume semiparametric dependence between potential outcomes and conditioning
covariates. This allows us to obtain testable equality and inequality
restrictions among the subdensities of estimable partial residuals. We propose
jointly testing these restrictions. To improve power, we introduce
distillation, where a trimmed sample is used to test the inequality
restrictions. In Monte Carlo exercises we find gains in finite sample power
from testing restrictions jointly and distillation. We apply our test procedure
to three instruments and reject the null for one.",2021
http://arxiv.org/abs/2112.08071v2,Stock prices and Macroeconomic indicators: Investigating a correlation in Indian context,2021-12-15 12:23:11+00:00,"['Dhruv Rawat', 'Sujay Patni', 'Ram Mehta']",econ.GN,"The objective of this paper is to find the existence of a relationship
between stock market prices and the fundamental macroeconomic indicators. We
build a Vector Auto Regression (VAR) model comprising of nine major
macroeconomic indicators (interest rate, inflation, exchange rate, money
supply, gdp, fdi, trade-gdp ratio, oil prices, gold prices) and then try to
forecast them for next 5 years. Finally we calculate cross-correlation of these
forecasted values with the BSE Sensex closing price for each of those years. We
find very high correlation of the closing price with exchange rate and money
supply in the Indian economy.",2021
http://arxiv.org/abs/2112.07985v1,Solving the Data Sparsity Problem in Predicting the Success of the Startups with Machine Learning Methods,2021-12-15 09:21:32+00:00,"['Dafei Yin', 'Jing Li', 'Gaosheng Wu']",cs.LG,"Predicting the success of startup companies is of great importance for both
startup companies and investors. It is difficult due to the lack of available
data and appropriate general methods. With data platforms like Crunchbase
aggregating the information of startup companies, it is possible to predict
with machine learning algorithms. Existing research suffers from the data
sparsity problem as most early-stage startup companies do not have much data
available to the public. We try to leverage the recent algorithms to solve this
problem. We investigate several machine learning algorithms with a large
dataset from Crunchbase. The results suggest that LightGBM and XGBoost perform
best and achieve 53.03% and 52.96% F1 scores. We interpret the predictions from
the perspective of feature contribution. We construct portfolios based on the
models and achieve high success rates. These findings have substantial
implications on how machine learning methods can help startup companies and
investors.",2021
http://arxiv.org/abs/2112.07314v1,The road to safety- Examining the nexus between road infrastructure and crime in rural India,2021-12-14 11:48:34+00:00,"['Ritika Jain', 'Shreya Biswas']",econ.GN,"This study examines the relationship between road infrastructure and crime
rate in rural India using a nationally representative survey. On the one hand,
building roads in villages may increase connectivity, boost employment, and
lead to better living standards, reducing criminal activities. On the other
hand, if the benefits of roads are non-uniformly distributed among villagers,
it may lead to higher inequality and possibly higher crime. We empirically test
the relationship using the two waves of the Indian Human Development Survey. We
use an instrumental variable estimation strategy and observe that building
roads in rural parts of India has reduced crime. The findings are robust to
relaxing the strict instrument exogeneity condition and using alternate
measures. On exploring the pathways, we find that improved street lighting,
better public bus services and higher employment are a few of the direct
potential channels through which road infrastructure impedes crime. We also
find a negative association between villages with roads and various types of
inequality measures confirming the broad economic benefits of roads. Our study
also highlights that the negative impact of roads on crime is more pronounced
in states with weaker institutions and higher income inequality.",2021
http://arxiv.org/abs/2112.07277v2,Modal equilibrium of a tradable credit scheme with a trip-based MFD and logit-based decision-making,2021-12-14 10:30:40+00:00,"['Louis Balzer', 'Ludovic Leclercq']",econ.GN,"The literature about tradable credit schemes (TCS) as a demand management
system alleviating congestion flourished in the past decade. Most proposed
formulations are based on static models and thus do not account for the
congestion dynamics. This paper considers elastic demand and implements a TCS
to foster modal shift by restricting the number of cars allowed in the network
over the day. A trip-based Macroscopic Fundamental Diagram (MFD) model
represents the traffic dynamics at the whole urban scale. We assume the users
have different OD pairs and choose between driving their car or riding the
transit following a logit model. We aim to compute the modal shares and credit
price at equilibrium under TCS. The travel times are linearized with respect to
the modal shares to improve the convergence. We then present a method to find
the credit charge minimizing the total travel time alone or combined with the
carbon emission. The proposed methodology is illustrated with a typical demand
profile from 7:00 to 10:00 for Lyon Metropolis. We show that traffic dynamics
and trip heterogeneity matter when deriving the modal equilibrium under a TCS.
A method is described to compute the linearization of the travel times and
compared against a classical descend method (MSA). The proposed linearization
is a promising tool to circumvent the complexity of the implicit formulation of
the trip-based MFD. Under an optimized TCS, the total travel time decreases by
17% and the carbon emission by 45% by increasing the PT share by 24 points.",2021
http://arxiv.org/abs/2112.07273v1,Urban Housing Prices and Migration's Fertility Intentions: Based on the 2018 China Migrants' Dynamic Survey,2021-12-14 10:09:08+00:00,"['Jingwen Tan', 'Shixi Kang']",econ.GN,"While the size of China's mobile population continues to expand, the
fertility rate is significantly lower than the stable generation replacement
level of the population, and the structural imbalance of human resource supply
has attracted widespread attention. This paper uses LPM and Probit models to
estimate the impact of house prices on the fertility intentions of the mobile
population based on data from the 2018 National Mobile Population Dynamics
Monitoring Survey. The lagged land sales price is used as an instrumental
variable of house price to mitigate the potential endogeneity problem. The
results show that for every 100\% increase in the ratio of house price to
household income of mobile population, the fertility intention of the female
mobile population of working age at the inflow location will decrease by
4.42\%, and the marginal effect of relative house price on labor force
fertility intention is EXP(-0.222); the sensitivity of mobile population
fertility intention to house price is affected by the moderating effect of
infrastructure construction at the inflow location. The willingness to have
children in the inflow area is higher for female migrants of working age with
lower age, smaller family size and higher education. Based on the above
findings, the study attempts to provide a new practical perspective for the
mainline institutional change and balanced economic development in China's
economic transition phase.",2021
http://arxiv.org/abs/2112.07268v2,Finding the Instrumental Variables of Household Registration: A discussion of the impact of China's household registration system on the citizenship of the migrant population,2021-12-14 09:59:40+00:00,"['Jingwen Tan', 'Shixi Kang']",econ.GN,"Due to the specificity of China's dualistic household registration system and
the differences in the rights and interests attached to it, household
registration is prevalent as a control variable in the empirical evidence. In
the context of family planning policies, this paper proposes to use family size
and number of children as instrumental variables for household registration,
and discusses qualitatively and statistically verifies their relevance and
exogeneity, while empirically analyzing the impact of the household
registration system on citizenship of the mobile population. After controlling
for city, individual control variables and fixed effects, the following
conclusions are drawn: family size and number of children pass the
over-identification test when used as instrumental variables for household
registration; non-agricultural households have about 20.2% lower settlement
intentions and 7.28% lower employment levels in inflow cities than agricultural
households; the mechanism of the effect of the nature of household registration
on employment still holds for the non-mobile population group.",2021
http://arxiv.org/abs/2112.07247v3,30.000 ways to reach 55% decarbonization of the European electricity sector,2021-12-14 09:20:34+00:00,"['Tim T. Pedersen', 'Mikael Skou Andersen', 'Marta Victoria', 'Gorm B. Andresen']",econ.GN,"Climate change mitigation is a global challenge that, however, needs to be
resolved by national-level authorities, resembling a tragedy of the commons.
This paradox is reflected at European scale, as climate commitments are made by
the EU collectively, but implementation is the responsibility of individual
Member States. Here, we investigate 30.000 near-optimal effort-sharing
scenarios where the European electricity sector is decarbonized by at least 55%
relative to 1990, in line with 2030 ambitions. Using a highly detailed
brownfield electricity system optimization model, the optimal electricity
system is simulated for a suite of effort-sharing scenarios. Results reveal
large inequalities in the efforts required to decarbonize national electricity
sectors, with some countries facing cost-optimal pathways to reach 55% emission
reductions, while others are confronted with relatively high abatement costs.
Specifically, we find that several countries with modest or low levels of GDP
per capita will experience high abatement costs, and when passed over into
electricity prices this may lead to increased energy poverty in certain parts
of Europe",2021
http://arxiv.org/abs/2112.07218v2,Regulating Transportation Network Companies with a Mixture of Autonomous Vehicles and For-Hire Human Drivers,2021-12-14 08:10:33+00:00,"['Di Ao', 'Jing Gao', 'Zhijie Lai', 'Sen Li']",math.OC,"This paper investigates the equity impacts of autonomous vehicles (AV) on
for-hire human drivers and passengers in a ride-hailing market, and examines
regulation policies that protect human drivers and improve transport equity for
ride-hailing passengers. We consider a transportation network companies (TNC)
that employs a mixture of AVs and human drivers to provide ride-hailing
services. The TNC platform determines the spatial prices, fleet size, human
driver payments, and vehicle relocation strategies to maximize its profit,
while individual passengers choose between different transport modes to
minimize their travel costs. A market equilibrium model is proposed to capture
the interactions among passengers, human drivers, AVs, and TNC over the
transportation network. The overall problem is formulated as a non-concave
program, and an algorithm is developed to derive its approximate solution with
a theoretical performance guarantee. Our study shows that TNC prioritizes AV
deployment in higher-demand areas to make a higher profit. As AVs flood into
these higher-demand areas, they compete with human drivers in the urban core
and push them to relocate to suburbs. This leads to reduced earning
opportunities for human drivers and increased spatial inequity for passengers.
To mitigate these concerns, we consider: (a) a minimum wage for human drivers;
and (b) a restrictive pickup policy that prohibits AVs from picking up
passengers in higher-demand areas. In the former case, we show that a minimum
wage for human drivers will protect them from the negative impact of AVs with
negligible impacts on passengers. However, there exists a threshold beyond
which the minimum wage will trigger the platform to replace the majority of
human drivers with AVs.",2021
http://arxiv.org/abs/2112.07155v2,Behavioral Foundations of Nested Stochastic Choice and Nested Logit,2021-12-14 04:30:14+00:00,"['Matthew Kovach', 'Gerelt Tserenjigmid']",econ.TH,"We provide the first behavioral characterization of nested logit, a
foundational and widely applied discrete choice model, through the introduction
of a non-parametric version of nested logit that we call Nested Stochastic
Choice (NSC). NSC is characterized by a single axiom that weakens Independence
of Irrelevant Alternatives based on revealed similarity to allow for the
similarity effect. Nested logit is characterized by an additional
menu-independence axiom. Our axiomatic characterization leads to a practical,
data-driven algorithm that identifies the true nest structure from choice data.
We also discuss limitations of generalizing nested logit by studying the
testable implications of cross-nested logit.",2021
http://arxiv.org/abs/2112.07149v2,Factor Models with Sparse VAR Idiosyncratic Components,2021-12-14 04:10:59+00:00,"['Jonas Krampe', 'Luca Margaritella']",stat.ME,"We reconcile the two worlds of dense and sparse modeling by exploiting the
positive aspects of both. We employ a factor model and assume {the dynamic of
the factors is non-pervasive while} the idiosyncratic term follows a sparse
vector autoregressive model (VAR) {which allows} for cross-sectional and time
dependence. The estimation is articulated in two steps: first, the factors and
their loadings are estimated via principal component analysis and second, the
sparse VAR is estimated by regularized regression on the estimated
idiosyncratic components. We prove the consistency of the proposed estimation
approach as the time and cross-sectional dimension diverge. In the second step,
the estimation error of the first step needs to be accounted for. Here, we do
not follow the naive approach of simply plugging in the standard rates derived
for the factor estimation. Instead, we derive a more refined expression of the
error. This enables us to derive tighter rates. We discuss the implications of
our model for forecasting, factor augmented regression, bootstrap of factor
models, and time series dependence networks via semi-parametric estimation of
the inverse of the spectral density matrix.",2021
http://arxiv.org/abs/2112.07121v5,Semiparametric Conditional Factor Models in Asset Pricing,2021-12-14 02:46:21+00:00,"['Qihui Chen', 'Nikolai Roussanov', 'Xiaoliang Wang']",econ.EM,"We introduce a simple and tractable methodology for estimating semiparametric
conditional latent factor models. Our approach disentangles the roles of
characteristics in capturing factor betas of asset returns from ``alpha.'' We
construct factors by extracting principal components from Fama-MacBeth managed
portfolios. Applying this methodology to the cross-section of U.S. individual
stock returns, we find compelling evidence of substantial nonzero pricing
errors, even though our factors demonstrate superior performance in standard
asset pricing tests. Unexplained ``arbitrage'' portfolios earn high Sharpe
ratios, which decline over time. Combining factors with these orthogonal
portfolios produces out-of-sample Sharpe ratios exceeding 4.",2021
http://arxiv.org/abs/2112.07014v1,Identifying Marginal Treatment Effects in the Presence of Sample Selection,2021-12-13 21:08:49+00:00,"['Otávio Bartalotti', 'Désiré Kédagni', 'Vitor Possebom']",econ.EM,"This article presents identification results for the marginal treatment
effect (MTE) when there is sample selection. We show that the MTE is partially
identified for individuals who are always observed regardless of treatment, and
derive uniformly sharp bounds on this parameter under three increasingly
restrictive sets of assumptions. The first result imposes standard MTE
assumptions with an unrestricted sample selection mechanism. The second set of
conditions imposes monotonicity of the sample selection variable with respect
to treatment, considerably shrinking the identified set. Finally, we
incorporate a stochastic dominance assumption which tightens the lower bound
for the MTE. Our analysis extends to discrete instruments. The results rely on
a mixture reformulation of the problem where the mixture weights are
identified, extending Lee's (2009) trimming procedure to the MTE context. We
propose estimators for the bounds derived and use data made available by Deb,
Munking and Trivedi (2006) to empirically illustrate the usefulness of our
approach.",2021
http://arxiv.org/abs/2112.06822v1,Quantile Regression under Limited Dependent Variable,2021-12-13 17:33:54+00:00,"['Javier Alejo', 'Gabriel Montes-Rojas']",econ.EM,"A new Stata command, ldvqreg, is developed to estimate quantile regression
models for the cases of censored (with lower and/or upper censoring) and binary
dependent variables. The estimators are implemented using a smoothed version of
the quantile regression objective function. Simulation exercises show that it
correctly estimates the parameters and it should be implemented instead of the
available quantile regression methods when censoring is present. An empirical
application to women's labor supply in Uruguay is considered.",2021
http://arxiv.org/abs/2112.06817v1,Insurance design and arson-type risks,2021-12-13 17:24:39+00:00,['Jean-Gabriel Lauzier'],econ.TH,"We design the insurance contract when the insurer faces arson-type risks. The
optimal contract must be manipulation-proof. It is therefore continuous, it has
a bounded slope, and it satisfies the no-sabotage condition when arson-type
actions are free. Any contract that mixes a deductible, coinsurance and an
upper limit is manipulation-proof. We also show that the ability to perform
arson-type actions reduces the insured's welfare as less coverage is offered in
equilibrium.",2021
http://arxiv.org/abs/2112.06815v1,Envelope theorem and discontinuous optimisation: the case of positioning choice problems,2021-12-13 17:20:55+00:00,['Jean-Gabriel Lauzier'],econ.TH,"This article examines differentiability properties of the value function of
positioning choice problems, a class of optimisation problems in
finite-dimensional Euclidean spaces. We show that positioning choice problems'
value function is always almost everywhere differentiable even when the
objective function is discontinuous. To obtain this result we first show that
the Dini superdifferential is always well-defined for the maxima of positioning
choice problems. This last property allows to state first-order necessary
conditions in terms of Dini supergradients. We then prove our main result,
which is an ad-hoc envelope theorem for positioning choice problems. Lastly,
after discussing necessity of some key assumptions, we conjecture that similar
theorems might hold in other spaces as well.",2021
http://arxiv.org/abs/2112.06811v1,Ex-post moral hazard and manipulation-proof contracts,2021-12-13 17:13:14+00:00,['Jean-Gabriel Lauzier'],econ.TH,"We examine the trade-off between the provision of incentives to exert costly
effort (ex-ante moral hazard) and the incentives needed to prevent the agent
from manipulating the profit observed by the principal (ex-post moral hazard).
Formally, we build a model of two-stage hidden actions where the agent can both
influence the expected revenue of a business and manipulate its observed
profit. We show that manipulation-proofness is sensitive to the interaction
between the manipulation technology and the probability distribution of the
stochastic output. The optimal contract is manipulation-proof whenever the
manipulation technology is linear. However, a convex manipulation technology
sometimes leads to contracts with manipulations in equilibrium. Whenever the
distribution satisfies the monotone likelihood ratio property, we can always
find a manipulation technology for which the optimal contract is not
manipulation-proof.",2021
http://arxiv.org/abs/2112.06605v5,Will enterprise digital transformation affect diversification strategy?,2021-12-13 12:40:40+00:00,"['Ge-zhi Wu', 'Da-ming You']",econ.GN,"This paper empirically examines the impact of enterprise digital
transformation on the level of enterprise diversification. It is found that the
digital transformation of enterprises has significantly improved the level of
enterprise diversification, and the conclusion has passed a series of
robustness tests and endogenous tests. Through mechanism analysis, we find that
the promotion effect of enterprise digital transformation on enterprise
diversification is mainly realized through market power channel and firm risk
channel, the pursuit of establishing market power, monopoly profits and
challenge the monopolistic position of market occupiers based on digital
transformation and the decentralization strategy to deal with the risks
associated with digital transformation are important reasons for enterprises to
adopt diversification strategy under the background of digital transformation.
Although the organization costs channel, transaction costs channel, block
holder control channel, industry type and information asymmetry channel have
some influence on the main effect of this paper, they are not the main channel
because they have not passed the inter group regression coefficient difference
test statistically.",2021
http://arxiv.org/abs/2112.06363v16,Risk and optimal policies in bandit experiments,2021-12-13 00:41:19+00:00,['Karun Adusumilli'],econ.EM,"We provide a decision theoretic analysis of bandit experiments under local
asymptotics. Working within the framework of diffusion processes, we define
suitable notions of asymptotic Bayes and minimax risk for these experiments.
For normally distributed rewards, the minimal Bayes risk can be characterized
as the solution to a second-order partial differential equation (PDE). Using a
limit of experiments approach, we show that this PDE characterization also
holds asymptotically under both parametric and non-parametric distributions of
the rewards. The approach further describes the state variables it is
asymptotically sufficient to restrict attention to, and thereby suggests a
practical strategy for dimension reduction. The PDEs characterizing minimal
Bayes risk can be solved efficiently using sparse matrix routines or
Monte-Carlo methods. We derive the optimal Bayes and minimax policies from
their numerical solutions. These optimal policies substantially dominate
existing methods such as Thompson sampling; the risk of the latter is often
twice as high.",2021
http://arxiv.org/abs/2112.06357v2,An installation-level model of China's coal sector shows how its decarbonization and energy security plans will reduce overseas coal imports,2021-12-13 00:01:31+00:00,"['Jorrit Gosens', 'Alex Turnbull', 'Frank Jotzo']",econ.GN,"China aims for net-zero carbon emissions by 2060, and an emissions peak
before 2030. This will reduce its consumption of coal for power generation and
steel making. Simultaneously, China aims for improved energy security,
primarily with expanded domestic coal production and transport infrastructure.
Here, we analyze effects of both these pressures on seaborne coal imports, with
a purpose-built model of China's coal production, transport, and consumption
system with installation-level geospatial and technical detail. This represents
a 1000-fold increase in granularity versus earlier models, allowing
representation of aspects that have previously been obscured. We find that
reduced Chinese coal consumption affects seaborne imports much more strongly
than domestic supply. Recent expansions of rail and port capacity, which reduce
costs of getting domestic coal to Southern coastal provinces, will further
reduce demand for seaborne thermal coal and amplify the effect of
decarbonisation on coal imports. Seaborne coking coal imports are also likely
to fall, because of expanded supply of cheap and high quality coking coal from
neighbouring Mongolia.",2021
http://arxiv.org/abs/2112.06290v1,A q-spin Potts model of markets: Gain-loss asymmetry in stock indices as an emergent phenomenon,2021-12-12 18:15:33+00:00,['Stefan Bornholdt'],physics.soc-ph,"Spin models of markets inspired by physics models of magnetism, as the Ising
model, allow for the study of the collective dynamics of interacting agents in
a market. The number of possible states has been mostly limited to two (buy or
sell) or three options. However, herding effects of competing stocks and the
collective dynamics of a whole market may escape our reach in the simplest
models. Here I study a q-spin Potts model version of a simple Ising market
model to represent the dynamics of a stock market index in a spin model. As a
result, a self-organized gain-loss asymmetry in the time series of an index
variable composed of stocks in this market is observed.",2021
http://arxiv.org/abs/2112.06192v1,Housing Price Prediction Model Selection Based on Lorenz and Concentration Curves: Empirical Evidence from Tehran Housing Market,2021-12-12 09:44:28+00:00,['Mohammad Mirbagherijam'],econ.EM,"This study contributes a house price prediction model selection in Tehran
City based on the area between Lorenz curve (LC) and concentration curve (CC)
of the predicted price by using 206,556 observed transaction data over the
period from March 21, 2018, to February 19, 2021. Several different methods
such as generalized linear models (GLM) and recursive partitioning and
regression trees (RPART), random forests (RF) regression models, and neural
network (NN) models were examined house price prediction. We used 90% of all
data samples which were chosen randomly to estimate the parameters of pricing
models and 10% of remaining datasets to test the accuracy of prediction.
Results showed that the area between the LC and CC curves (which are known as
ABC criterion) of real and predicted prices in the test data sample of the
random forest regression model was less than by other models under study. The
comparison of the calculated ABC criteria leads us to conclude that the
nonlinear regression models such as RF regression models give an accurate
prediction of house prices in Tehran City.",2021
http://arxiv.org/abs/2112.06032v1,Robust Implementation with Costly Information,2021-12-11 17:23:28+00:00,"['Harry Pei', 'Bruno Strulovici']",econ.TH,"We study whether a planner can robustly implement a state-contingent social
choice function when (i) agents must incur a cost to learn the state and (ii)
the planner faces uncertainty regarding agents' preferences over outcomes,
information costs, and beliefs and higher-order beliefs about one another's
payoffs. We propose mechanisms that can approximately implement any desired
social choice function when the perturbations concerning agents' payoffs have
small ex ante probability. The mechanism is also robust to trembles in agents'
strategies and when agents receive noisy information about the state.",2021
http://arxiv.org/abs/2112.05950v1,Analysis of stability and bifurcation for two heterogeneous triopoly games with the isoelastic demand,2021-12-11 11:01:20+00:00,['Xiaoliang Li'],math.DS,"In this paper, we investigate two heterogeneous triopoly games where the
demand function of the market is isoelastic. The local stability and the
bifurcation of these games are systematically analyzed using the symbolic
approach proposed by the author. The novelty of the present work is twofold. On
one hand, the results of this paper are analytical, which are different from
the existing results in the literature based on observations through numerical
simulations. In particular, we rigorously prove the existence of double routes
to chaos through the period-doubling bifurcation and through the Neimark-Sacker
bifurcation. On the other hand, for the special case of the involved firms
having identical marginal costs, we acquire the necessary and sufficient
conditions of the local stability for both models. By further analyzing these
conditions, it seems that that the presence of the local monopolistic
approximation (LMA) mechanism might have a stabilizing effect for heterogeneous
triopoly games with the isoelastic demand.",2021
http://arxiv.org/abs/2112.05948v2,Stability of Cournot duopoly games with isoelastic demands and quadratic costs,2021-12-11 10:52:07+00:00,"['Xiaoliang Li', 'Li Su']",cs.SC,"In this discussion draft, we explore different duopoly games of players with
quadratic costs, where the market is supposed to have the isoelastic demand.
Different from the usual approaches based on numerical computations, the
methods used in the present work are built on symbolic computations, which can
produce analytical and rigorous results. Our investigations show that the
stability regions are enlarged for the games considered in this work compared
to their counterparts with linear costs, which generalizes the classical
results of ""F. M. Fisher. The stability of the Cournot oligopoly solution: The
effects of speeds of adjustment and increasing marginal costs. The Review of
Economic Studies, 28(2):125--135, 1961."".",2021
http://arxiv.org/abs/2112.05876v1,The Past as a Stochastic Process,2021-12-11 00:15:59+00:00,"['David H. Wolpert', 'Michael H. Price', 'Stefani A. Crabtree', 'Timothy A. Kohler', 'Jurgen Jost', 'James Evans', 'Peter F. Stadler', 'Hajime Shimao', 'Manfred D. Laubichler']",stat.AP,"Historical processes manifest remarkable diversity. Nevertheless, scholars
have long attempted to identify patterns and categorize historical actors and
influences with some success. A stochastic process framework provides a
structured approach for the analysis of large historical datasets that allows
for detection of sometimes surprising patterns, identification of relevant
causal actors both endogenous and exogenous to the process, and comparison
between different historical cases. The combination of data, analytical tools
and the organizing theoretical framework of stochastic processes complements
traditional narrative approaches in history and archaeology.",2021
http://arxiv.org/abs/2112.05822v1,"U.S. Long-Term Earnings Outcomes by Sex, Race, Ethnicity, and Place of Birth",2021-12-10 20:41:26+00:00,"['Kevin L. McKinney', 'John M. Abowd', 'Hubert P. Janicki']",econ.GN,"This paper is part of the Global Income Dynamics Project cross-country
comparison of earnings inequality, volatility, and mobility. Using data from
the U.S. Census Bureau's Longitudinal Employer-Household Dynamics (LEHD)
infrastructure files we produce a uniform set of earnings statistics for the
U.S. From 1998 to 2019, we find U.S. earnings inequality has increased and
volatility has decreased. The combination of increased inequality and reduced
volatility suggest earnings growth differs substantially across different
demographic groups. We explore this further by estimating 12-year average
earnings for a single cohort of age 25-54 eligible workers. Differences in
labor supply (hours paid and quarters worked) are found to explain almost 90%
of the variation in worker earnings, although even after controlling for labor
supply substantial earnings differences across demographic groups remain
unexplained. Using a quantile regression approach, we estimate counterfactual
earnings distributions for each demographic group. We find that at the bottom
of the earnings distribution differences in characteristics such as hours paid,
geographic division, industry, and education explain almost all the earnings
gap, however above the median the contribution of the differences in the
returns to characteristics becomes the dominant component.",2021
http://arxiv.org/abs/2112.05811v1,"On the Stability, Economic Efficiency and Incentive Compatibility of Electricity Market Dynamics",2021-12-10 20:04:44+00:00,"['Pengcheng You', 'Yan Jiang', 'Enoch Yeung', 'Dennice F. Gayme', 'Enrique Mallada']",math.OC,"This paper focuses on the operation of an electricity market that accounts
for participants that bid at a sub-minute timescale. To that end, we model the
market-clearing process as a dynamical system, called market dynamics, which is
temporally coupled with the grid frequency dynamics and is thus required to
guarantee system-wide stability while meeting the system operational
constraints. We characterize participants as price-takers who rationally update
their bids to maximize their utility in response to real-time schedules of
prices and dispatch. For two common bidding mechanisms, based on quantity and
price, we identify a notion of alignment between participants' behavior and
planners' goals that leads to a saddle-based design of the market that
guarantees convergence to a point meeting all operational constraints. We
further explore cases where this alignment property does not hold and observe
that misaligned participants' bidding can destabilize the closed-loop system.
We thus design a regularized version of the market dynamics that recovers all
the desirable stability and steady-state performance guarantees. Numerical
tests validate our results on the IEEE 39-bus system.",2021
http://arxiv.org/abs/2112.05671v2,On the Assumptions of Synthetic Control Methods,2021-12-10 17:07:14+00:00,"['Claudia Shi', 'Dhanya Sridhar', 'Vishal Misra', 'David M. Blei']",stat.ME,"Synthetic control (SC) methods have been widely applied to estimate the
causal effect of large-scale interventions, e.g., the state-wide effect of a
change in policy. The idea of synthetic controls is to approximate one unit's
counterfactual outcomes using a weighted combination of some other units'
observed outcomes. The motivating question of this paper is: how does the SC
strategy lead to valid causal inferences? We address this question by
re-formulating the causal inference problem targeted by SC with a more
fine-grained model, where we change the unit of the analysis from ""large units""
(e.g., states) to ""small units"" (e.g., individuals in states). Under this
re-formulation, we derive sufficient conditions for the non-parametric causal
identification of the causal effect. We highlight two implications of the
reformulation: (1) it clarifies where ""linearity"" comes from, and how it falls
naturally out of the more fine-grained and flexible model, and (2) it suggests
new ways of using available data with SC methods for valid causal inference, in
particular, new ways of selecting observations from which to estimate the
counterfactual.",2021
http://arxiv.org/abs/2301.00292v6,Inference for Large Panel Data with Many Covariates,2022-12-31 21:07:24+00:00,"['Markus Pelger', 'Jiacheng Zou']",econ.EM,"This paper proposes a novel testing procedure for selecting a sparse set of
covariates that explains a large dimensional panel. Our selection method
provides correct false detection control while having higher power than
existing approaches. We develop the inferential theory for large panels with
many covariates by combining post-selection inference with a novel multiple
testing adjustment. Our data-driven hypotheses are conditional on the sparse
covariate selection. We control for family-wise error rates for covariate
discovery for large cross-sections. As an easy-to-use and practically relevant
procedure, we propose Panel-PoSI, which combines the data-driven adjustment for
panel multiple testing with valid post-selection p-values of a generalized
LASSO, that allows us to incorporate priors. In an empirical study, we select a
small number of asset pricing factors that explain a large cross-section of
investment strategies. Our method dominates the benchmarks out-of-sample due to
its better size and power.",2022
http://arxiv.org/abs/2301.00277v2,Higher-order Refinements of Small Bandwidth Asymptotics for Density-Weighted Average Derivative Estimators,2022-12-31 19:50:50+00:00,"['Matias D. Cattaneo', 'Max H. Farrell', 'Michael Jansson', 'Ricardo Masini']",econ.EM,"The density weighted average derivative (DWAD) of a regression function is a
canonical parameter of interest in economics. Classical first-order large
sample distribution theory for kernel-based DWAD estimators relies on tuning
parameter restrictions and model assumptions that imply an asymptotic linear
representation of the point estimator. These conditions can be restrictive, and
the resulting distributional approximation may not be representative of the
actual sampling distribution of the statistic of interest. In particular, the
approximation is not robust to bandwidth choice. Small bandwidth asymptotics
offers an alternative, more general distributional approximation for
kernel-based DWAD estimators that allows for, but does not require, asymptotic
linearity. The resulting inference procedures based on small bandwidth
asymptotics were found to exhibit superior finite sample performance in
simulations, but no formal theory justifying that empirical success is
available in the literature. Employing Edgeworth expansions, this paper shows
that small bandwidth asymptotic approximations lead to inference procedures
with higher-order distributional properties that are demonstrably superior to
those of procedures based on asymptotic linear approximations.",2022
http://arxiv.org/abs/2301.00251v3,Feature Selection for Personalized Policy Analysis,2022-12-31 16:52:46+00:00,"['Maria Nareklishvili', 'Nicholas Polson', 'Vadim Sokolov']",econ.EM,"In this paper, we propose Forest-PLS, a feature selection method for
analyzing policy effect heterogeneity in a more flexible and comprehensive
manner than is typically available with conventional methods. In particular,
our method is able to capture policy effect heterogeneity both within and
across subgroups of the population defined by observable characteristics. To
achieve this, we employ partial least squares to identify target components of
the population and causal forests to estimate personalized policy effects
across these components. We show that the method is consistent and leads to
asymptotically normally distributed policy effects. To demonstrate the efficacy
of our approach, we apply it to the data from the Pennsylvania Reemployment
Bonus Experiments, which were conducted in 1988-1989. The analysis reveals that
financial incentives can motivate some young non-white individuals to enter the
labor market. However, these incentives may also provide a temporary financial
cushion for others, dissuading them from actively seeking employment. Our
findings highlight the need for targeted, personalized measures for young
non-white male participants.",2022
http://arxiv.org/abs/2301.00237v2,Market Design with Distributional Objectives,2022-12-31 16:05:19+00:00,"['Isa E. Hafalir', 'Fuhito Kojima', 'M. Bumin Yenmez', 'Koji Yokote']",econ.TH,"We provide optimal solutions to an institution that has distributional
objectives when choosing from a set of applications based on merit (or
priority). For example, in college admissions, administrators may want to admit
a diverse class in addition to choosing students with the highest
qualifications. We provide a family of choice rules that maximize merit subject
to attaining a level of the distributional objective. We study the desirable
properties of choice rules in this family and use them to find all subsets of
applications on the Pareto frontier of the distributional objective and merit.
In addition, we provide two novel characterizations of matroids.",2022
http://arxiv.org/abs/2301.00232v2,Efficient Market Design with Distributional Objectives,2022-12-31 15:52:17+00:00,"['Isa E. Hafalir', 'Fuhito Kojima', 'M. Bumin Yenmez']",econ.TH,"Given an initial matching and a policy objective on the distribution of agent
types to institutions, we study the existence of a mechanism that weakly
improves the distributional objective and satisfies constrained efficiency,
individual rationality, and strategy-proofness. We show that such a mechanism
need not exist in general. We introduce a new notion of discrete concavity,
which we call pseudo M$^{\natural}$-concavity, and construct a mechanism with
the desirable properties when the distributional objective satisfies this
notion. We provide several practically relevant distributional objectives that
are pseudo M$^{\natural}$-concave.",2022
http://arxiv.org/abs/2301.00092v2,Inference on Time Series Nonparametric Conditional Moment Restrictions Using General Sieves,2022-12-31 01:44:17+00:00,"['Xiaohong Chen', 'Yuan Liao', 'Weichen Wang']",stat.ML,"General nonlinear sieve learnings are classes of nonlinear sieves that can
approximate nonlinear functions of high dimensional variables much more
flexibly than various linear sieves (or series). This paper considers general
nonlinear sieve quasi-likelihood ratio (GN-QLR) based inference on expectation
functionals of time series data, where the functionals of interest are based on
some nonparametric function that satisfy conditional moment restrictions and
are learned using multilayer neural networks. While the asymptotic normality of
the estimated functionals depends on some unknown Riesz representer of the
functional space, we show that the optimally weighted GN-QLR statistic is
asymptotically Chi-square distributed, regardless whether the expectation
functional is regular (root-$n$ estimable) or not. This holds when the data are
weakly dependent beta-mixing condition. We apply our method to the off-policy
evaluation in reinforcement learning, by formulating the Bellman equation into
the conditional moment restriction framework, so that we can make inference
about the state-specific value functional using the proposed GN-QLR method with
time series data. In addition, estimating the averaged partial means and
averaged partial derivatives of nonparametric instrumental variables and
quantile IV models are also presented as leading examples. Finally, a Monte
Carlo study shows the finite sample performance of the procedure",2022
http://arxiv.org/abs/2301.00091v1,Wealth Redistribution and Mutual Aid: Comparison using Equivalent/Nonequivalent Exchange Models of Econophysics,2022-12-31 01:37:26+00:00,['Takeshi Kato'],econ.TH,"Given the wealth inequality worldwide, there is an urgent need to identify
the mode of wealth exchange through which it arises. To address the research
gap regarding models that combine equivalent exchange and redistribution, this
study compares an equivalent market exchange with redistribution based on power
centers and a nonequivalent exchange with mutual aid using the Polanyi,
Graeber, and Karatani modes of exchange. Two new exchange models based on
multi-agent interactions are reconstructed following an econophysics approach
for evaluating the Gini index (inequality) and total exchange (economic flow).
Exchange simulations indicate that the evaluation parameter of the total
exchange divided by the Gini index can be expressed by the same saturated
curvilinear approximate equation using the wealth transfer rate and time period
of redistribution and the surplus contribution rate of the wealthy and the
saving rate. However, considering the coercion of taxes and its associated
costs and independence based on the morality of mutual aid, a nonequivalent
exchange without return obligation is preferred. This is oriented toward
Graeber's baseline communism and Karatani's mode of exchange D, with
implications for alternatives to the capitalist economy.",2022
http://arxiv.org/abs/2212.14622v4,Identifying causal effects with subjective ordinal outcomes,2022-12-30 10:17:09+00:00,['Leonard Goff'],econ.EM,"Survey questions often ask respondents to select from ordered scales where
the meanings of the categories are subjective, leaving each individual free to
apply their own definitions in answering. This paper studies the use of these
responses as an outcome variable in causal inference, accounting for variation
in interpretation of the categories across individuals. I find that when a
continuous treatment variable is statistically independent of both i) potential
outcomes; and ii) heterogeneity in reporting styles, a nonparametric regression
of response category number on that treatment variable recovers a quantity
proportional to an average causal effect among individuals who are on the
margin between successive response categories. The magnitude of a given
regression coefficient is not meaningful on its own, but the ratio of local
regression derivatives with respect to two such treatment variables identifies
the relative magnitudes of convex averages of their effects. These results can
be seen as limiting cases of analogous results for binary treatment variables,
though comparisons of magnitude involving discrete treatments are not as
readily interpretable outside of the limit. I obtain a partial identification
result for comparisons involving discrete treatments under further assumptions.
An empirical application illustrates the results by revisiting the effects of
income comparisons on subjective well-being, without assuming cardinality or
interpersonal comparability of responses.",2022
http://arxiv.org/abs/2301.00666v2,E-commerce users' preferences for delivery options,2022-12-30 00:31:58+00:00,"['Yuki Oyama', 'Daisuke Fukuda', 'Naoto Imura', 'Katsuhiro Nishinari']",econ.GN,"Many e-commerce marketplaces offer their users fast delivery options for free
to meet the increasing needs of users, imposing an excessive burden on city
logistics. Therefore, understanding e-commerce users' preference for delivery
options is a key to designing logistics policies. To this end, this study
designs a stated choice survey in which respondents are faced with choice tasks
among different delivery options and time slots, which was completed by 4,062
users from the three major metropolitan areas in Japan. To analyze the data,
mixed logit models capturing taste heterogeneity as well as flexible
substitution patterns have been estimated. The model estimation results
indicate that delivery attributes including fee, time, and time slot size are
significant determinants of the delivery option choices. Associations between
users' preferences and socio-demographic characteristics, such as age, gender,
teleworking frequency and the presence of a delivery box, were also suggested.
Moreover, we analyzed two willingness-to-pay measures for delivery, namely, the
value of delivery time savings (VODT) and the value of time slot shortening
(VOTS), and applied a non-semiparametric approach to estimate their
distributions in a data-oriented manner. Although VODT has a large
heterogeneity among respondents, the estimated median VODT is 25.6 JPY/day,
implying that more than half of the respondents would wait an additional day if
the delivery fee were increased by only 26 JPY, that is, they do not
necessarily need a fast delivery option but often request it when cheap or
almost free. Moreover, VOTS was found to be low, distributed with the median of
5.0 JPY/hour; that is, users do not highly value the reduction in time slot
size in monetary terms. These findings on e-commerce users' preferences can
help in designing levels of service for last-mile delivery to significantly
improve its efficiency.",2022
http://arxiv.org/abs/2212.14475v6,Innovation through intra and inter-regional interaction in economic geography,2022-12-29 22:23:50+00:00,"['José M. Gaspar', 'Minoru Osawa']",econ.TH,"We develop a two-region economic geography model with vertical innovations
that improve the quality of manufactured varieties produced in each region. The
chance of innovation depends on the \emph{related variety}, i.e. the importance
of interaction between researchers within the same region rather than across
different regions. As economic integration increases from a low level, a higher
related variety is associated with more agglomerated spatial configurations.
However, if the interaction with foreign scientists is relatively more
important for innovation, economic activities may (completely) re-disperse
after an initial phase of agglomeration due to the increase in the relative
importance of a higher chance of innovation in the less industrialized region.
This non-monotonic relationship between economic integration and spatial
imbalances may exhibit very diverse qualitative properties, not yet described
in the literature.",2022
http://arxiv.org/abs/2212.14444v5,Empirical Bayes When Estimation Precision Predicts Parameters,2022-12-29 19:44:59+00:00,['Jiafeng Chen'],econ.EM,"Gaussian empirical Bayes methods usually maintain a precision independence
assumption: The unknown parameters of interest are independent from the known
standard errors of the estimates. This assumption is often theoretically
questionable and empirically rejected. This paper proposes to model the
conditional distribution of the parameter given the standard errors as a
flexibly parametrized location-scale family of distributions, leading to a
family of methods that we call CLOSE. The CLOSE framework unifies and
generalizes several proposals under precision dependence. We argue that the
most flexible member of the CLOSE family is a minimalist and computationally
efficient default for accounting for precision dependence. We analyze this
method and show that it is competitive in terms of the regret of subsequent
decisions rules. Empirically, using CLOSE leads to sizable gains for selecting
high-mobility Census tracts.",2022
http://arxiv.org/abs/2212.14411v5,Near-Optimal Non-Parametric Sequential Tests and Confidence Sequences with Possibly Dependent Observations,2022-12-29 18:37:08+00:00,"['Aurelien Bibaut', 'Nathan Kallus', 'Michael Lindon']",stat.ME,"Sequential tests and their implied confidence sequences, which are valid at
arbitrary stopping times, promise flexible statistical inference and on-the-fly
decision making. However, strong guarantees are limited to parametric
sequential tests that under-cover in practice or concentration-bound-based
sequences that over-cover and have suboptimal rejection times. In this work, we
consider classic delayed-start normal-mixture sequential probability ratio
tests, and we provide the first asymptotic type-I-error and
expected-rejection-time guarantees under general non-parametric data generating
processes, where the asymptotics are indexed by the test's burn-in time. The
type-I-error results primarily leverage a martingale strong invariance
principle and establish that these tests (and their implied confidence
sequences) have type-I error rates asymptotically equivalent to the desired
(possibly varying) $\alpha$-level. The expected-rejection-time results
primarily leverage an identity inspired by It\^o's lemma and imply that, in
certain asymptotic regimes, the expected rejection time is asymptotically
equivalent to the minimum possible among $\alpha$-level tests. We show how to
apply our results to sequential inference on parameters defined by estimating
equations, such as average treatment effects. Together, our results establish
these (ostensibly parametric) tests as general-purpose, non-parametric, and
near-optimal. We illustrate this via numerical simulations and a real-data
application to A/B testing at Netflix.",2022
http://arxiv.org/abs/2212.14367v1,Optimal Robust Mechanism in Bilateral Trading,2022-12-29 16:44:12+00:00,['Komal Malik'],econ.TH,"We consider a model of bilateral trade with private values. The value of the
buyer and the cost of the seller are jointly distributed. The true joint
distribution is unknown to the designer, however, the marginal distributions of
the value and the cost are known to the designer. The designer wants to find a
trading mechanism that is robustly Bayesian incentive compatible, robustly
individually rational, budget-balanced and maximizes the expected gains from
trade over all such mechanisms. We refer to such a mechanism as an optimal
robust mechanism. We establish equivalence between Bayesian incentive
compatible mechanisms (BIC) and dominant strategy mechanisms (DSIC). We
characterise the worst distribution for a given mechanism and use this
characterisation to find an optimal robust mechanism. We show that there is an
optimal robust mechanism that is deterministic (posted-price), dominant
strategy incentive compatible, and ex-post individually rational. We also
derive an explicit expression of the posted-price of such an optimal robust
mechanism. We also show the equivalence between the efficiency gains from the
optimal robust mechanism (max-min problem) and guaranteed efficiency gains if
the designer could choose the mechanism after observing the true joint
distribution (min-max problem).",2022
http://arxiv.org/abs/2212.14185v1,What Estimators Are Unbiased For Linear Models?,2022-12-29 06:19:44+00:00,"['Lihua Lei', 'Jeffrey Wooldridge']",econ.EM,"The recent thought-provoking paper by Hansen [2022, Econometrica] proved that
the Gauss-Markov theorem continues to hold without the requirement that
competing estimators are linear in the vector of outcomes. Despite the elegant
proof, it was shown by the authors and other researchers that the main result
in the earlier version of Hansen's paper does not extend the classic
Gauss-Markov theorem because no nonlinear unbiased estimator exists under his
conditions. To address the issue, Hansen [2022] added statements in the latest
version with new conditions under which nonlinear unbiased estimators exist.
  Motivated by the lively discussion, we study a fundamental problem: what
estimators are unbiased for a given class of linear models? We first review a
line of highly relevant work dating back to the 1960s, which, unfortunately,
have not drawn enough attention. Then, we introduce notation that allows us to
restate and unify results from earlier work and Hansen [2022]. The new
framework also allows us to highlight differences among previous conclusions.
Lastly, we establish new representation theorems for unbiased estimators under
different restrictions on the linear model, allowing the coefficients and
covariance matrix to take only a finite number of values, the higher moments of
the estimator and the dependent variable to exist, and the error distribution
to be discrete, absolutely continuous, or dominated by another probability
measure. Our results substantially generalize the claims of parallel
commentaries on Hansen [2022] and a remarkable result by Koopmann [1982].",2022
http://arxiv.org/abs/2212.14159v1,Innovation in times of Covid-19,2022-12-29 02:44:01+00:00,"['Torsten Heinrich', 'Jangho Yang']",econ.GN,"Did the Covid-19 pandemic have an impact on innovation? Past economic
disruptions, anecdotal evidence, and the previous literature suggest a decline
with substantial differences between industries. We leverage USPTO patent
application data to investigate and quantify the disturbance. We assess
differences by field of technology (at the CPC subclass level) as well as the
impact of direct and indirect relevance for the management of the pandemic.
Direct Covid-19 relevance is identified from a keyword search of the patent
application fulltexts; indirect Covid-19 relevance is derived from past CPC
subclass to subclass citation patterns. We find that direct Covid-19 relevance
is associated with a strong boost to the growth of the number of patent
applications in the first year of the pandemic at the same order of magnitude
(in percentage points) as the percentage of patents referencing Covid-19. We
find no effect for indirect Covid-19 relevance, indicating a focus on applied
research at the expense of more basic research. Fields of technology (CPC
mainsections) have an additional significant impact, with, e.g., mainsections A
(human necessities) and C (chemistry, metallurgy) having a strong performance.",2022
http://arxiv.org/abs/2212.14105v3,Supercompliers,2022-12-28 21:53:57+00:00,"['Matthew L. Comey', 'Amanda R. Eng', 'Pauline Leung', 'Zhuan Pei']",econ.EM,"In a binary-treatment instrumental variable framework, we define
supercompliers as the subpopulation whose treatment take-up positively responds
to eligibility and whose outcome positively responds to take-up. Supercompliers
are the only subpopulation to benefit from treatment eligibility and, hence,
are important for policy. We provide tools to characterize supercompliers under
a set of jointly testable assumptions. Specifically, we require standard
assumptions from the local average treatment effect literature plus an outcome
monotonicity assumption. Estimation and inference can be conducted with
instrumental variable regression. In two job-training experiments, we
demonstrate our machinery's utility, particularly in incorporating social
welfare weights into marginal-value-of-public-funds analysis.",2022
http://arxiv.org/abs/2212.14075v2,Forward Orthogonal Deviations GMM and the Absence of Large Sample Bias,2022-12-28 19:38:03+00:00,['Robert F. Phillips'],econ.EM,"It is well known that generalized method of moments (GMM) estimators of
dynamic panel data regressions can have significant bias when the number of
time periods ($T$) is not small compared to the number of cross-sectional units
($n$). The bias is attributed to the use of many instrumental variables. This
paper shows that if the maximum number of instrumental variables used in a
period increases with $T$ at a rate slower than $T^{1/2}$, then GMM estimators
that exploit the forward orthogonal deviations (FOD) transformation do not have
asymptotic bias, regardless of how fast $T$ increases relative to $n$. This
conclusion is specific to using the FOD transformation. A similar conclusion
does not necessarily apply when other transformations are used to remove fixed
effects. Monte Carlo evidence illustrating the analytical results is provided.",2022
http://arxiv.org/abs/2212.13996v1,Robustifying Markowitz,2022-12-28 18:09:14+00:00,"['Wolfgang Karl Härdle', 'Yegor Klochkov', 'Alla Petukhina', 'Nikita Zhivotovskiy']",econ.EM,"Markowitz mean-variance portfolios with sample mean and covariance as input
parameters feature numerous issues in practice. They perform poorly out of
sample due to estimation error, they experience extreme weights together with
high sensitivity to change in input parameters. The heavy-tail characteristics
of financial time series are in fact the cause for these erratic fluctuations
of weights that consequently create substantial transaction costs. In
robustifying the weights we present a toolbox for stabilizing costs and weights
for global minimum Markowitz portfolios. Utilizing a projected gradient descent
(PGD) technique, we avoid the estimation and inversion of the covariance
operator as a whole and concentrate on robust estimation of the gradient
descent increment. Using modern tools of robust statistics we construct a
computationally efficient estimator with almost Gaussian properties based on
median-of-means uniformly over weights. This robustified Markowitz approach is
confirmed by empirical studies on equity markets. We demonstrate that
robustified portfolios reach the lowest turnover compared to shrinkage-based
and constrained portfolios while preserving or slightly improving out-of-sample
performance.",2022
http://arxiv.org/abs/2212.13966v1,What is expected for China's SARS-CoV-2 epidemic?,2022-12-28 17:19:24+00:00,"['Carlos Hernandez-Suarez', 'Efren Murillo-Zamora']",econ.GN,"Recently, China announced that its ""zero-covid"" policy would end, which will
bring serious challenges to the country's health system. In here we provide
simple calculations that allows us to provide an estimate of what is expected
as an outcome in terms of fatalities, using the fact that it is a highly
contagious disease that will expose most of a highly vaccinated population to
the virus. We use recent findings regarding the amount of reduction in the risk
of severe outcome achieved by vaccination and arrive to an estimate of 1.1 m
deaths, 60% of these are males. In our model, 84% percent of deaths occur in
individuals with age 55 years or older. In a scenario in which this protection
is completely lost due to waning and the infection fatality rate of the
prevalent strain reaches similar levels to the observed in the beginning of the
epidemic, the death toll could reach 2.4 m, 93% in 55 years or older.",2022
http://arxiv.org/abs/2301.00680v1,Large-Scale 3D Printing -- Market Analysis,2022-12-28 17:01:20+00:00,['Razan Abdelazim Idris Alzain'],econ.GN,"The aim of this research is to get a better understanding of the future of
large-scale 3D printing. By developing the market analysis, it will be clear
whether large-scale 3D printing is becoming more of a preferred way of printing
custom-made parts for production companies. Companies can then choose whether
to change their ways, for a more profitable less costly method, or stay on the
route they are on. By getting deep into this topic, a new world of technology
is then being discovered and familiarized. With a mix of theoretical and
practical relevance, a complete coverage could be made on large-scale 3D
printing. This paper could then cover all aspects of this topic, and the reader
could then make their own judgment if large-scale 3D printing would be the best
option.",2022
http://arxiv.org/abs/2301.00681v2,Historical Patterns and Recent Impacts of Chinese Investors in United States Real Estate,2022-12-28 07:57:38+00:00,['Kevin Sun'],econ.GN,"Since supplanting Canada in 2014, Chinese investors have been the lead
foreign buyers of U.S. real estate, concentrating their purchases in urban
areas with higher Chinese populations like California. The reasons for
investment include prestige, freedom from capital confiscation, and safe,
diversified opportunities from abroad simply being more lucrative and available
than in their home country, where the market is eroding. Interestingly, since
2019, Chinese investors have sold a net 23.6 billion dollars of U.S. commercial
real estate, a stark contrast to past acquisitions between 2013 to 2018 where
they were net buyers of almost 52 billion dollars worth of properties. A
similar trend appears in the residential real estate segment too. In both 2017
and 2018, Chinese buyers purchased over 40,000 U.S. residential properties
which were halved in 2019 and steadily declined to only 6,700 in the past year.
This turnaround in Chinese investment can be attributed to a deteriorating
relationship between the U.S. and China during the Trump Presidency, financial
distress in China, and new Chinese government regulations prohibiting outbound
investments. Additionally, while Chinese investment is a small share of U.S.
real estate (~1.5% at its peak), it has outsized impacts on market valuations
of home prices in U.S. zip codes with higher populations of foreign-born
Chinese, increasing property prices and exacerbating the issue of housing
affordability in these areas. This paper investigates the rapid growth and
decline of Chinese investment in U.S. real estate and its effect on U.S. home
prices in certain demographics.",2022
http://arxiv.org/abs/2212.13638v6,Battling the Coronavirus Infodemic Among Social Media Users in Kenya and Nigeria,2022-12-27 23:09:44+00:00,"['Molly Offer-Westort', 'Leah R. Rosenzweig', 'Susan Athey']",cs.SI,"How can we induce social media users to be discerning when sharing
information during a pandemic? An experiment on Facebook Messenger with users
from Kenya (n = 7,498) and Nigeria (n = 7,794) tested interventions designed to
decrease intentions to share COVID-19 misinformation without decreasing
intentions to share factual posts. The initial stage of the study incorporated:
(i) a factorial design with 40 intervention combinations; and (ii) a contextual
adaptive design, increasing the probability of assignment to treatments that
worked better for previous subjects with similar characteristics. The second
stage evaluated the best-performing treatments and a targeted treatment
assignment policy estimated from the data. We precisely estimate null effects
from warning flags and related article suggestions, tactics used by social
media platforms. However, nudges to consider information's accuracy reduced
misinformation sharing relative to control by 4.9% (estimate = -2.3 pp, s.e. =
1.0 , Z = -2.31, p = 0.021, 95% CI = [-4.2 , -0.35]). Such low-cost scalable
interventions may improve the quality of information circulating online.",2022
http://arxiv.org/abs/2212.13622v1,A Statistical Inquiry into Gender-Based Income Inequality in Canada,2022-12-27 21:24:28+00:00,['Ali R. Kaazempur-Mofrad'],stat.AP,"Income inequality distribution between social groups has been a global
challenge. The focus of this study is to investigate the potential impact of
female income on family size and purchasing power. Using statistical methods
such as simple linear regression, maximum likelihood analysis, and hypothesis
testing, I evaluated and investigated the variability of female pre-tax income
with respect to family size. The results obtained from this study illustrate
that for each additional household member, the average purchasing power
decreases. Additionally, the Bayesian analysis indicates that the probability
for an individual with a pre-tax income of at least one and two standard
deviations above the population mean is female is approximately 1/3 and 1/4,
respectively, further highlighting the gender-based income inequality in
Canada. This analysis concludes that although female pre-tax income has no
statistically significant impact on family size, the female pre-tax income per
person has a statistically significant impact on family size.",2022
http://arxiv.org/abs/2212.13611v1,Hidden costs of La Mancha's production model and drivers of change,2022-12-27 20:53:48+00:00,"['Máximo Florín', 'Rafael U. Gosálvez']",econ.GN,"The territory of La Mancha, its rural areas, and its landscapes suffer a kind
of atherosclerosis (""the silent killer"") because of the increase in artificial
surfaces, the fragmentation of the countryside by various infrastructures, the
abandonment of small and medium-sized farms and the loss of agricultural,
material, and intangible heritage. At the same time, agricultural
industrialization hides, behind a supposed productive efficiency, the
deterioration of the quantitative and qualitative ecological status of surface
and groundwater bodies, and causes air pollution, greenhouse gas emissions,
loss of soil fertility, drainage and plowing of wetlands, forgetfulness of the
ancestral environmental heritage, of the emergence of uses and customs of
collective self-government and reduction of the adaptive capacity of
traditional agroecosystems. This work aims, firstly, to shed light on the true
costs of the main causes of environmental degradation in the territory of La
Mancha, while deteriorating relations between rural and urban areas and
determining the loss of territorial identity of La Mancha. the population. In
addition, drivers of change toward a more sustainable social, economic,
hydrological, environmental, and cultural production model are identified.",2022
http://arxiv.org/abs/2212.13449v13,Self-progressive choice models,2022-12-27 11:12:18+00:00,['Kemal Yildiz'],econ.TH,"Consider a population of heterogenous agents whose choice behaviors are
partially \textit{comparable} according to a given \textit{primitive
ordering}.The set of choice functions admissible in the population specifies a
\textit{choice model}. As a criterion to guide the model selection process, we
propose \textit{self-progressiveness}, ensuring that each aggregate choice
behavior explained by the model has a unique orderly representation within the
model itself. We establish an equivalence between self-progressive choice
models and well-known algebraic structures called \textit{lattices}.
  This equivalence provides for a precise recipe to restrict or extend any
choice model for unique orderly representation. Following this recipe, we
identify the set of choice functions that are essential for the unique orderly
representation of random utility functions. This extended model offers an
intuitive explanation for the \textit{choice overload} phenomena. We provide
the necessary and sufficient conditions for identifying the underlying
primitive ordering.",2022
http://arxiv.org/abs/2212.13371v1,Measuring an artificial intelligence agent's trust in humans using machine incentives,2022-12-27 06:05:49+00:00,"['Tim Johnson', 'Nick Obradovich']",cs.AI,"Scientists and philosophers have debated whether humans can trust advanced
artificial intelligence (AI) agents to respect humanity's best interests. Yet
what about the reverse? Will advanced AI agents trust humans? Gauging an AI
agent's trust in humans is challenging because--absent costs for
dishonesty--such agents might respond falsely about their trust in humans. Here
we present a method for incentivizing machine decisions without altering an AI
agent's underlying algorithms or goal orientation. In two separate experiments,
we then employ this method in hundreds of trust games between an AI agent (a
Large Language Model (LLM) from OpenAI) and a human experimenter (author TJ).
In our first experiment, we find that the AI agent decides to trust humans at
higher rates when facing actual incentives than when making hypothetical
decisions. Our second experiment replicates and extends these findings by
automating game play and by homogenizing question wording. We again observe
higher rates of trust when the AI agent faces real incentives. Across both
experiments, the AI agent's trust decisions appear unrelated to the magnitude
of stakes. Furthermore, to address the possibility that the AI agent's trust
decisions reflect a preference for uncertainty, the experiments include two
conditions that present the AI agent with a non-social decision task that
provides the opportunity to choose a certain or uncertain option; in those
conditions, the AI agent consistently chooses the certain option. Our
experiments suggest that one of the most advanced AI language models to date
alters its social behavior in response to incentives and displays behavior
consistent with trust toward a human interlocutor when incentivized.",2022
http://arxiv.org/abs/2212.13324v2,Spectral and post-spectral estimators for grouped panel data models,2022-12-26 23:30:37+00:00,"['Denis Chetverikov', 'Elena Manresa']",econ.EM,"In this paper, we develop spectral and post-spectral estimators for grouped
panel data models. Both estimators are consistent in the asymptotics where the
number of observations $N$ and the number of time periods $T$ simultaneously
grow large. In addition, the post-spectral estimator is $\sqrt{NT}$-consistent
and asymptotically normal with mean zero under the assumption of well-separated
groups even if $T$ is growing much slower than $N$. The post-spectral estimator
has, therefore, theoretical properties that are comparable to those of the
grouped fixed-effect estimator developed by Bonhomme and Manresa (2015). In
contrast to the grouped fixed-effect estimator, however, our post-spectral
estimator is computationally straightforward.",2022
http://arxiv.org/abs/2212.13226v3,An Effective Treatment Approach to Difference-in-Differences with General Treatment Patterns,2022-12-26 17:20:59+00:00,['Takahide Yanagi'],econ.EM,"We consider a general difference-in-differences model in which the treatment
variable of interest may be non-binary and its value may change in each period.
It is generally difficult to estimate treatment parameters defined with the
potential outcome given the entire path of treatment adoption, because each
treatment path may be experienced by only a small number of observations. We
propose an alternative approach using the concept of effective treatment, which
summarizes the treatment path into an empirically tractable low-dimensional
variable, and develop doubly robust identification, estimation, and inference
methods. We also provide a companion R software package.",2022
http://arxiv.org/abs/2212.13176v1,The fate of the American dream: A first passage under resetting approach to income dynamics,2022-12-26 14:33:45+00:00,"['Petar Jolakoski', 'Arnab Pal', 'Trifce Sandev', 'Ljupco Kocarev', 'Ralf Metzler', 'Viktor Stojkoski']",econ.GN,"Detailed knowledge of individual income dynamics is crucial for investigating
the existence of the American dream: Are we able to improve our income status
during our working life? This key question simply boils down to observing
individual status and how it moves between two thresholds: the current income
and the desired income. Yet, our knowledge of these temporal properties of
income remains limited since we rely on estimates coming from transition
matrices which simplify income dynamics by aggregating the individual changes
into quantiles and thus overlooking significant microscopic variations. Here,
we bridge this gap by employing First Passage Time concepts in a baseline
stochastic process with resetting used for modeling income dynamics and
developing a framework that is able to crucially disaggregate the temporal
properties of income to the level of an individual worker. We find analytically
and illustrate numerically that our framework is orthogonal to the transition
matrix approach and leads to improved and more granular estimates. Moreover, to
facilitate empirical applications of the framework, we introduce a publicly
available statistical methodology, and showcase the application using the USA
income dynamics data. These results help to improve our understanding on the
temporal properties of income in real economies and provide a set of tools for
designing policy interventions.",2022
http://arxiv.org/abs/2212.13145v1,Orthogonal Series Estimation for the Ratio of Conditional Expectation Functions,2022-12-26 13:01:17+00:00,"['Kazuhiko Shinoda', 'Takahiro Hoshino']",econ.EM,"In various fields of data science, researchers are often interested in
estimating the ratio of conditional expectation functions (CEFR). Specifically
in causal inference problems, it is sometimes natural to consider ratio-based
treatment effects, such as odds ratios and hazard ratios, and even
difference-based treatment effects are identified as CEFR in some empirically
relevant settings. This chapter develops the general framework for estimation
and inference on CEFR, which allows the use of flexible machine learning for
infinite-dimensional nuisance parameters. In the first stage of the framework,
the orthogonal signals are constructed using debiased machine learning
techniques to mitigate the negative impacts of the regularization bias in the
nuisance estimates on the target estimates. The signals are then combined with
a novel series estimator tailored for CEFR. We derive the pointwise and uniform
asymptotic results for estimation and inference on CEFR, including the validity
of the Gaussian bootstrap, and provide low-level sufficient conditions to apply
the proposed framework to some specific examples. We demonstrate the
finite-sample performance of the series estimator constructed under the
proposed framework by numerical simulations. Finally, we apply the proposed
method to estimate the causal effect of the 401(k) program on household assets.",2022
http://arxiv.org/abs/2301.04607v1,The Impact of National Culture on Innovation A Comparative Analysis between Developed and Developing Nations during the Pre and Post Crisis Period 2007_2021,2022-12-26 10:20:29+00:00,"['Han-Sol Lee', 'Sergey U. Chernikov', 'Szabolcs Nagy', 'Ekaterina A. Degtereva']",econ.GN,"This empirical study investigates the impact of the Hofstede cultural
dimensions (HCD) on the Global Innovation Index (GII) scores in four different
years (2007, 2009, 2019 and 2021) to compare the impacts during the pre- and
post-crisis (financial and COVID-19) period by employing ordinary least square
(OLS) and robust least square (Robust) analyses. The purpose of this study is
to identify the impact of cultural factors on the innovation development for
different income groups during the pre- and post-crisis period. We found that,
in general, the same cultural properties were required for countries to enhance
innovation inputs and outputs regardless of pre- and post-crisis periods and
time variances. The significant cultural factors (driving forces) of the
innovation performance do not change over time. However, our empirical results
revealed that not the crisis itself but the income group (either developed or
developing) is the factor that influences the relationship between cultural
properties and innovation. It is also worth noting that cultural properties
have lost much of their impact on innovation, particularly in developing
countries, during recent periods. It is highly likely that in terms of
innovation, no cultural development or change can significantly impact the
innovation output of developing countries without the construction of the
appropriate systems.",2022
http://arxiv.org/abs/2301.04609v1,The Effects of Hofstede's Cultural Dimensions on Pro-Environmental Behaviour: How Culture Influences Environmentally Conscious Behaviour,2022-12-26 09:53:29+00:00,"['Szabolcs Nagy', 'Csilla Konyha Molnarne']",econ.GN,"The need for a more sustainable lifestyle is a key focus for several
countries. Using a questionnaire survey conducted in Hungary, this paper
examines how culture influences environmentally conscious behaviour. Having
investigated the direct impact of Hofstedes cultural dimensions on
pro-environmental behaviour, we found that the culture of a country hardly
affects actual environmentally conscious behaviour. The findings indicate that
only individualism and power distance have a significant but weak negative
impact on pro-environmental behaviour. Based on the findings, we can state that
a positive change in culture is a necessary but not sufficient condition for
making a country greener.",2022
http://arxiv.org/abs/2301.01279v1,The relationship between content marketing and the traditional marketing communication tools,2022-12-26 09:38:13+00:00,"['Szabolcs Nagy', 'Gergo Hajdu']",econ.GN,"Digitalization is making a significant impact on marketing. New marketing
approaches and tools are emerging which are not always clearly categorised.
This article seeks to investigate the relationship between one of the novel
marketing tools, content marketing, and the five elements of the traditional
marketing communication mix. Based on an extensive literature review, this
paper analyses the main differences and similarities between them. This article
aims to generate a debate on the status of content marketing. According to the
authors' opinion, content marketing can be considered as the sixth marketing
communication mix element. However, further research is needed to fill in the
existing knowledge gap.",2022
http://arxiv.org/abs/2301.01278v1,Students Perceptions of Sustainable Universities in Hungary. An Importance-Performance Analysis,2022-12-26 09:14:01+00:00,"['Szabolcs Nagy', 'Mariann Veresne Somosi']",econ.GN,"In order to succeed, universities are forced to respond to the new challenges
in the rapidly changing world. The recently emerging fourth-generation
universities should meet sustainability objectives to better serve their
students and their communities. It is essential for universities to measure
their sustainability performance to capitalise on their core strengths and to
overcome their weaknesses. In line with the stakeholder theory, the objective
of this study was to investigate students perceptions of university
sustainability including their expectations about and satisfaction with the
efforts that universities make towards sustainability. This paper proposes a
new approach that combines the sustainable university scale, developed by the
authors, with the importance-performance analysis to identify key areas of
university sustainability. To collect data, an online survey was conducted in
Hungary in 2019. The sustainable university scale was found to be a reliable
construct to measure different aspects of university sustainability. Results of
the importance-performance analysis suggest that students consider Hungarian
universities unsustainable. Research findings indicate that Hungarian
universities perform poorly in sustainable purchasing and renewable energy use,
but their location and their efforts towards separate waste collection are
their major competitive advantages. The main domains of university
sustainability were also discussed. This study provides university
decision-makers and researchers with insightful results supporting the
transformation of traditional universities into sustainable, fourth-generation
higher education institutions.",2022
http://arxiv.org/abs/2301.01277v1,Consumer acceptance of the use of artificial intelligence in online shopping: evidence from Hungary,2022-12-26 09:03:28+00:00,"['Szabolcs Nagy', 'Noemi Hajdu']",econ.GN,"The rapid development of technology has drastically changed the way consumers
do their shopping. The volume of global online commerce has significantly been
increasing partly due to the recent COVID-19 crisis that has accelerated the
expansion of e-commerce. A growing number of webshops integrate Artificial
Intelligence (AI), state-of-the-art technology into their stores to improve
customer experience, satisfaction and loyalty. However, little research has
been done to verify the process of how consumers adopt and use AI-powered
webshops. Using the technology acceptance model (TAM) as a theoretical
background, this study addresses the question of trust and consumer acceptance
of Artificial Intelligence in online retail. An online survey in Hungary was
conducted to build a database of 439 respondents for this study. To analyse
data, structural equation modelling (SEM) was used. After the respecification
of the initial theoretical model, a nested model, which was also based on TAM,
was developed and tested. The widely used TAM was found to be a suitable
theoretical model for investigating consumer acceptance of the use of
Artificial Intelligence in online shopping. Trust was found to be one of the
key factors influencing consumer attitudes towards Artificial Intelligence.
Perceived usefulness as the other key factor in attitudes and behavioural
intention was found to be more important than the perceived ease of use. These
findings offer valuable implications for webshop owners to increase customer
acceptance",2022
http://arxiv.org/abs/2212.13841v1,"Motivations and locational factors of FDI in CIS countries: Empirical evidence from South Korean FDI in Kazakhstan, Russia, and Uzbekistan",2022-12-26 08:53:26+00:00,"['Han-Sol Lee', 'Sergey U. Chernikov', 'Szabolcs Nagy']",econ.GN,"Considering the growing significance of Eurasian economic ties because of
South Korea s New Northern Policy and Russia s New Eastern Policy, this study
investigates the motivations and locational factors of South Korean foreign
direct investment (FDI) in three countries in the Commonwealth of Independent
States (CIS: Kazakhstan, Russia, and Uzbekistan) by employing panel analysis
(pooled ordinary least squares (OLS), fixed effects, random effects) using data
from 1993 to 2017. The results show the positive and significant coefficients
of GDP, resource endowments, and inflation. Unlike conventional South Korean
outward FDI, labour-seeking is not defined as a primary purpose. Exchange
rates, political rights, and civil liberties are identified as insignificant.
The authors conclude that South Korean FDI in Kazakhstan, Russia, and
Uzbekistan is associated with market-seeking (particularly in Kazakhstan and
Russia) and natural resource-seeking, especially the former. From a policy
perspective, our empirical evidence suggests that these countries host
governments could implement mechanisms to facilitate the movement of goods
across regions and countries to increase the attractiveness of small local
markets. The South Korean government could develop financial support and risk
sharing programmes to enhance natural resource-seeking investments and mutual
exchange programmes to overcome the red syndrome complex in South Korean
society.",2022
http://arxiv.org/abs/2212.13840v1,The relationship between social innovation and digital economy and society,2022-12-26 08:43:04+00:00,"['Szabolcs Nagy', 'Mariann Veresne Somosi']",econ.GN,"The information age is also an era of escalating social problems. The digital
transformation of society and the economy is already underway in all countries,
although the progress in this transformation can vary widely. There are more
social innovation projects addressing global and local social problems in some
countries than in others. This suggests that different levels of digital
transformation might influence the social innovation potential. Using the
International Digital Economy and Society Index and the Social Innovation
Index, this study investigates how digital transformation of the economy and
society affects the capacity for social innovation. A dataset of 29 countries
was analysed using both simple and multiple linear regressions and Pearsons
correlation. Based on the research findings, it can be concluded that the
digital transformation of the economy and society has a significant positive
impact on the capacity for social innovation. It was also found that the
integration of digital technology plays a critical role in digital
transformation. Therefore, the progress in digital transformation is beneficial
to social innovation capacity. In line with the research findings, this study
outlines the implications and possible directions for policy.",2022
http://arxiv.org/abs/2212.13839v1,The Impact of the Pharmaceutical Industry on the Innovation Performance of European Countries,2022-12-26 06:45:11+00:00,"['Szabolcs Nagy', 'Sergey U. Chernikov', 'Ekaterina Degtereva']",econ.GN,"There are significant differences in innovation performance between
countries. Additionally, the pharmaceutical sector is stronger in some
countries than in others. This suggests that the development of the
pharmaceutical industry can influence a country's innovation performance. Using
the Global Innovation Index and selected performance measures of the
pharmaceutical sector, this study examines how the pharmaceutical sector
influences the innovation performance of countries from the European context.
The dataset of 27 European countries was analysed using simple, and multiple
linear regressions and Pearson correlation. Our findings show that only three
indicators of the pharmaceutical industry, more precisely pharmaceutical
Research and Development, pharmaceutical exports, and pharmaceutical employment
explain the innovation performance of a country largely. Pharmaceutical
Research and Development and exports have a significant positive impact on a
country's innovation performance, whereas employment in the pharmaceutical
industry has a slightly negative impact. Additionally, global innovation
performance has been found to positively influence life expectancy. We further
outline the implications and possible policy directions based on these
findings.",2022
http://arxiv.org/abs/2212.12981v3,Tensor PCA for Factor Models,2022-12-26 01:47:06+00:00,"['Andrii Babii', 'Eric Ghysels', 'Junsu Pan']",econ.EM,"Modern empirical analysis often relies on high-dimensional panel datasets
with non-negligible cross-sectional and time-series correlations. Factor models
are natural for capturing such dependencies. A tensor factor model describes
the $d$-dimensional panel as a sum of a reduced rank component and an
idiosyncratic noise, generalizing traditional factor models for two-dimensional
panels. We consider a tensor factor model corresponding to the notion of a
reduced multilinear rank of a tensor. We show that for a strong factor model, a
simple tensor principal component analysis algorithm is optimal for estimating
factors and loadings. When the factors are weak, the convergence rate of simple
TPCA can be improved with alternating least-squares iterations. We also provide
inferential results for factors and loadings and propose the first test to
select the number of factors. The new tools are applied to the problem of
imputing missing values in a multidimensional panel of firm characteristics.",2022
http://arxiv.org/abs/2212.12891v1,Analysis of the Driving Factors of Implementing Green Supply Chain Management in SME in the City of Semarang,2022-12-25 11:17:17+00:00,"['Nanang Adie Setyawan', 'Hadiahti Utami', 'Bayu Setyo Nugroho', 'Mellasanti Ayuwardani', 'Suharmanto']",econ.GN,"This study set out to determine what motivated SMEs in Semarang City to
undertake green supply chain management during the COVID-19 and New Normal
pandemics. The purposive sampling approach was used as the sampling methodology
in this investigation. There are 100 respondents in the research samples. The
AMOS 24.0 program's structural equation modelling (SEM) is used in this
research method. According to the study's findings, the Strategic Orientation
variable significantly and favourably affects the Green Supply Chain Management
variable expected to have a value of 0.945, and the Government Regulation
variable has a positive and strong influence on the variable Green Supply Chain
Management with an estimated value of 0.070, the Green Supply Chain Management
variable with an estimated value of has a positive and significant impact on
the environmental performance variable. 0.504, the Strategic Orientation
variable with an estimated value of has a positive and significant impact on
the environmental performance variable. 0.442, The Environmental Performance
variable is directly impacted positively and significantly by the Government
Regulation variable, with an estimated value of 0.041. This significant
positive influence is because SMEs in Semarang City have government
regulations, along with government support for facilities regarding efforts to
implement the concept of environmental concern, causing high environmental
performance caused by the optimal implementation of Green supply chain
management is built on a collaboration between the government and the supply
chain's participants.",2022
http://arxiv.org/abs/2212.12797v1,New trends in South-South migration: The economic impact of COVID-19 and immigration enforcement,2022-12-24 18:27:30+00:00,"['Roxana Gutiérrez-Romero', 'Nayeli Salgado']",econ.GN,"This paper evaluates the impact of the pandemic and enforcement at the US and
Mexican borders on the emigration of Guatemalans during 2017-2020. During this
period, the number of crossings from Guatemala fell by 10%, according to the
Survey of Migration to the Southern Border of Mexico. Yet, there was a rise of
nearly 30% in the number of emigration crossings of male adults travelling with
their children. This new trend was partly driven by the recent reduction in the
number of children deported from the US. For a one-point reduction in the
number of children deported from the US to Guatemalan municipalities, there was
an increase of nearly 14 in the number of crossings made by adult males leaving
from Guatemala for Mexico; and nearly 0.5 additional crossings made by male
adults travelling with their children. However, the surge of emigrants
travelling with their children was also driven by the acute economic shock that
Guatemala experienced during the pandemic. During this period, air pollution in
the analysed Guatemalan municipalities fell by 4%, night light per capita fell
by 15%, and homicide rates fell by 40%. Unlike in previous years, emigrants are
fleeing poverty rather than violence. Our findings suggest that a reduction in
violence alone will not be sufficient to reduce emigration flows from Central
America, but that economic recovery is needed.",2022
http://arxiv.org/abs/2212.12796v1,Violence in Guatemala pushes adults and children to seek work in Mexico,2022-12-24 18:21:33+00:00,['Roxana Gutiérrez-Romero'],econ.GN,"This article estimates the impact of violence on emigration crossings from
Guatemala to Mexico as final destination during 2009-2017. To identify causal
effects, we use as instruments the variation in deforestation in Guatemala, and
the seizing of cocaine in Colombia. We argue that criminal organizations
deforest land in Guatemala, fueling violence and leading to emigration,
particularly during exogenous supply shocks to cocaine. A one-point increase in
the homicide rate differential between Guatemalan municipalities and Mexico,
leads to 211 additional emigration crossings made by male adults. This rise in
violence, also leads to 20 extra emigration crossings made by children.",2022
http://arxiv.org/abs/2212.12774v1,Knowledge Management in Management of Social and Economic Development of Municipalities: Highlights,2022-12-24 16:47:25+00:00,"['Maria A. Shishanina', 'Anatoly A. Sidorov']",econ.GN,"The paper discusses the process of social and economic development of
municipalities. A conclusion is made that developing an adequate model of
social and economic development using conventional approaches presents a
considerable challenge. It is proposed to use semantic modeling to represent
the social and economic development of municipalities, and cognitive mapping to
identify the set of connections that occur among indicators and that have a
direct impact on social and economic development.",2022
http://arxiv.org/abs/2212.12742v2,Global LCOEs of decentralized off-grid renewable energy systems,2022-12-24 14:50:59+00:00,"['Jann Michael Weinand', 'Maximilian Hoffmann', 'Jan Göpfert', 'Tom Terlouw', 'Julian Schönau', 'Patrick Kuckertz', 'Russell McKenna', 'Leander Kotzur', 'Jochen Linßen', 'Detlef Stolten']",econ.GN,"Recent global events emphasize the importance of a reliable energy supply.
One way to increase energy supply security is through decentralized off-grid
renewable energy systems, for which a growing number of case studies are
researched. This review gives a global overview of the levelized cost of
electricity (LCOE) for these autonomous energy systems, which range from 0.03
\$_{2021}/kWh to over 1.00 \$_{2021}/kWh worldwide. The average LCOEs for 100%
renewable energy systems have decreased by 9% annually between 2016 and 2021
from 0.54 \$_{2021}/kWh to 0.29 \$_{2021}/kWh, presumably due to cost
reductions in renewable energy and storage technologies. Furthermore, we
identify and discuss seven key reasons why LCOEs are frequently overestimated
or underestimated in literature, and how this can be prevented in the future.
Our overview can be employed to verify findings on off-grid systems, to assess
where these systems might be deployed and how costs evolve.",2022
http://arxiv.org/abs/2212.12623v2,The Simple Economics of Optimal Bundling,2022-12-24 01:00:15+00:00,['Frank Yang'],econ.TH,"We study optimal bundling when consumers differ in one dimension. We
introduce a partial order on the set of bundles defined by (i) set inclusion
and (ii) sales volumes (if sold alone and priced optimally). We show that if
the undominated bundles with respect to this partial order are nested, then
nested bundling (tiered pricing) is optimal. We characterize which nested menu
is optimal: Selling a given menu of nested bundles is optimal if a smaller
bundle in (out of) the menu sells more (less) than a bigger bundle in the menu.
We present three applications of these insights: the first two connect optimal
bundling and quality design to price elasticities and cost structures; the last
one establishes a necessary and sufficient condition for costly screening to be
optimal when a principal can use both price and nonprice screening instruments.",2022
http://arxiv.org/abs/2212.12398v1,Designing Autonomous Markets for Stablecoin Monetary Policy,2022-12-23 15:35:08+00:00,"['Ariah Klages-Mundt', 'Steffen Schuldenzucker']",cs.CR,"We develop a new type of automated market maker (AMM) that helps to maintain
stability and long-term viability in a stablecoin. This primary market AMM
(P-AMM) is an autonomous mechanism for pricing minting and redemption of
stablecoins in all possible states and is designed to achieve several desirable
properties. We first cover several case studies of current ad hoc stablecoin
issuance and redemption mechanisms, several of which have contributed to recent
stablecoin de-peggings, and formulate desirable properties of a P-AMM that
support stability and usability. We then design a P-AMM redemption curve and
show that it satisfies these properties, including bounded loss for both the
protocol and stablecoin holders. We further show that this redemption curve is
path independent and has properties of path deficiency in extended settings
involving trading fees and a separate minting curve. This means that system
health weakly improves relative to the path independent setting along any
trading curve and that there is no incentive to strategically subdivide
redemptions. Finally, we show how to implement the P-AMM efficiently on-chain.",2022
http://arxiv.org/abs/2212.12356v2,Equivalence between the Fitness-Complexity and the Sinkhorn-Knopp algorithms,2022-12-23 14:17:30+00:00,"['Dario Mazzilli', 'Manuel Sebastian Mariani', 'Flaviano Morone', 'Aurelio Patelli']",econ.GN,"We uncover the connection between the Fitness-Complexity algorithm, developed
in the economic complexity field, and the Sinkhorn-Knopp algorithm, widely used
in diverse domains ranging from computer science and mathematics to economics.
Despite minor formal differences between the two methods, both converge to the
same fixed-point solution up to normalization. The discovered connection allows
us to derive a rigorous interpretation of the Fitness and the Complexity
metrics as the potentials of a suitable energy function. Under this
interpretation, high-energy products are unfeasible for low-fitness countries,
which explains why the algorithm is effective at displaying nested patterns in
bipartite networks. We also show that the proposed interpretation reveals the
scale invariance of the Fitness-Complexity algorithm, which has practical
implications for the algorithm's implementation in different datasets. Further,
analysis of empirical trade data under the new perspective reveals three
categories of countries that might benefit from different development
strategies.",2022
http://arxiv.org/abs/2212.12285v1,The Effects of Just-in-time Delivery on Social Engagement: A Cluster Analysis,2022-12-23 12:27:02+00:00,"['Moisés Ramírez', 'Raziel Ruíz', 'Nathan Klarer']",econ.GN,"Fooji Inc. is a social media engagement platform that has created a
proprietary ""Just-in-time"" delivery network to provide prizes to social media
marketing campaign participants in real-time. In this paper, we prove the
efficacy of the ""Just-in-time"" delivery network through a cluster analysis that
extracts and presents the underlying drivers of campaign engagement.
  We utilize a machine learning methodology with a principal component analysis
to organize Fooji campaigns across these principal components. The arrangement
of data across the principal component space allows us to expose underlying
trends using a $K$-means clustering technique. The most important of these
trends is the demonstration of how the ""Just-in-time"" delivery network improves
social media engagement.",2022
http://arxiv.org/abs/2212.12251v1,The connection between Arrow theorem and Sperner lemma,2022-12-23 10:54:40+00:00,['Nikita Miku'],math.CO,"It is well known that Sperner lemma is equivalent to Brouwer fixed-point
theorem. Tanaka [12] proved that Brouwer theorem is equivalent to Arrow
theorem, hence Arrow theorem is equivalent to Sperner lemma. In this paper we
will prove this result directly. Moreover, we describe a number of other
statements equivalent to Arrow theorem.",2022
http://arxiv.org/abs/2212.12060v1,emIAM v1.0: an emulator for Integrated Assessment Models using marginal abatement cost curves,2022-12-22 22:20:11+00:00,"['Weiwei Xiong', 'Katsumasa Tanaka', 'Philippe Ciais', 'Daniel J. A. Johansson', 'Mariliis Lehtveer']",physics.ao-ph,"We developed an emulator for Integrated Assessment Models (emIAM) based on a
marginal abatement cost (MAC) curve approach. Using the output of IAMs in the
ENGAGE Scenario Explorer and the GET model, we derived a large set of MAC
curves: ten IAMs; global and eleven regions; three gases CO2, CH4, and N2O;
eight portfolios of available mitigation technologies; and two emission
sources. We tested the performance of emIAM by coupling it with a simple
climate model ACC2. We found that the optimizing climate-economy model
emIAM-ACC2 adequately reproduced a majority of original IAM emission outcomes
under similar conditions, allowing systematic explorations of IAMs with small
computational resources. emIAM can expand the capability of simple climate
models as a tool to calculate cost-effective pathways linked directly to a
temperature target.",2022
http://arxiv.org/abs/2212.12009v2,Single-Crossing Differences in Convex Environments,2022-12-22 19:35:51+00:00,"['Navin Kartik', 'SangMok Lee', 'Daniel Rappoport']",econ.TH,"An agent's preferences depend on an ordered parameter or type. We
characterize the set of utility functions with single-crossing differences
(SCD) in convex environments. These include preferences over lotteries, both in
expected utility and rank-dependent utility frameworks, and preferences over
bundles of goods and over consumption streams. Our notion of SCD does not
presume an order on the choice space. This unordered SCD is necessary and
sufficient for ''interval choice'' comparative statics. We present applications
to cheap talk, observational learning, and collective choice, showing how
convex environments arise in these problems and how SCD/interval choice are
useful. Methodologically, our main characterization stems from a result on
linear aggregations of single-crossing functions.",2022
http://arxiv.org/abs/2212.11833v4,Efficient Sampling for Realized Variance Estimation in Time-Changed Diffusion Models,2022-12-22 16:12:54+00:00,"['Timo Dimitriadis', 'Roxana Halbleib', 'Jeannine Polivka', 'Jasper Rennspies', 'Sina Streicher', 'Axel Friedrich Wolter']",econ.EM,"This paper analyzes the benefits of sampling intraday returns in intrinsic
time for the realized variance (RV) estimator. We theoretically show in finite
samples that depending on the permitted sampling information, the RV estimator
is most efficient under either hitting time sampling that samples whenever the
price changes by a pre-determined threshold, or under the new concept of
realized business time that samples according to a combination of observed
trades and estimated tick variance. The analysis builds on the assumption that
asset prices follow a diffusion that is time-changed with a jump process that
separately models the transaction times. This provides a flexible model that
allows for leverage specifications and Hawkes-type jump processes and
separately captures the empirically varying trading intensity and tick variance
processes, which are particularly relevant for disentangling the driving forces
of the sampling schemes. Extensive simulations confirm our theoretical results
and show that for low levels of noise, hitting time sampling remains superior
while for increasing noise levels, realized business time becomes the
empirically most efficient sampling scheme. An application to stock data
provides empirical evidence for the benefits of using these intrinsic sampling
schemes to construct more efficient RV estimators as well as for an improved
forecast performance.",2022
http://arxiv.org/abs/2212.11585v1,Strategic energy flows in input-output relations: a temporal multilayer approach,2022-12-22 10:23:02+00:00,"['Gian Paolo Clemente', 'Alessandra Cornaro', 'Rosanna Grassi', 'Giorgio Rizzini']",econ.GN,"The energy consumption, the transfer of resources through the international
trade, the transition towards renewable energies and the environmental
sustainability appear as key drivers in order to evaluate the resilience of the
energy systems. Concerning the consumptions, in the literature a great
attention has been paid to direct energy, but the production of goods and
services also involves indirect energy. Hence, in this work we consider
different types of embodied energy sources and the time evolution of the
sectors' and countries' interactions. Flows are indeed used to construct a
directed and weighted temporal multilayer network based respectively on
renewable and non-renewable sources, where sectors are nodes and layers are
countries. We provide a methodological approach for analysing the network
reliability and resilience and for identifying critical sectors and economies
in the system by applying the Multi-Dimensional HITS algorithm. Then, we
evaluate central arcs in the network at each time period by proposing a novel
topological indicator based on the maximum flow problem. In this way, we
provide a full view of economies, sectors and connections that play a relevant
role over time in the network and whose removal could heavily affect the
stability of the system. We provide a numerical analysis based on the embodied
energy flows among countries and sectors in the period from 1990 to 2016.
Results prove that the methods are effective in catching the different patterns
between renewable and non-renewable energy sources.",2022
http://arxiv.org/abs/2212.11112v2,A Bootstrap Specification Test for Semiparametric Models with Generated Regressors,2022-12-21 15:51:24+00:00,['Elia Lapenta'],econ.EM,"This paper provides a specification test for semiparametric models with
nonparametrically generated regressors. Such variables are not observed by the
researcher but are nonparametrically identified and estimable. Applications of
the test include models with endogenous regressors identified by control
functions, semiparametric sample selection models, or binary games with
incomplete information. The statistic is built from the residuals of the
semiparametric model. A novel wild bootstrap procedure is shown to provide
valid critical values. We consider nonparametric estimators with an automatic
bias correction that makes the test implementable without undersmoothing. In
simulations the test exhibits good small sample performances, and an
application to women's labor force participation decisions shows its
implementation in a real data context.",2022
http://arxiv.org/abs/2212.11108v1,Enhancing Resilience: Model-based Simulations,2022-12-21 15:47:45+00:00,"[""d'Artis Kancs""]",econ.GN,"Since several years, the fragility of global supply chains (GSCs) is at
historically high levels. In the same time, the landscape of hybrid threats is
expanding; new forms of hybrid threats create different types of uncertainties.
This paper aims to understand the potential consequences of uncertain events -
like natural disasters, pandemics, hybrid and/or military aggression - on GSC
resilience and robustness. Leveraging a parsimonious supply chain model, we
analyse how the organisational structure of GSCs interacts with uncertainty,
and how risk-aversion vs. ambiguity-aversion, vertical integration vs. upstream
outsourcing, resilience vs. efficiency trade-offs drive a wedge between
decentralised and centralised optimal GSC diversification strategies in
presence of externalities. Parameterising the scalable data model with
World-Input Output Tables, we simulate the survival probability of a GSC and
implications for supply chain robustness and resilience. The presented
model-based simulations provide an interoperable and directly comparable
conceptualisation of positive and normative effects of counterfactual
resilience and robustness policy choices under individually optimal
(decentralised) and socially optimal (centralised) GSC organisation structures.",2022
http://arxiv.org/abs/2212.11012v2,Partly Linear Instrumental Variables Regressions without Smoothing on the Instruments,2022-12-21 13:37:17+00:00,"['Jean-Pierre Florens', 'Elia Lapenta']",econ.EM,"We consider a semiparametric partly linear model identified by instrumental
variables. We propose an estimation method that does not smooth on the
instruments and we extend the Landweber-Fridman regularization scheme to the
estimation of this semiparametric model. We then show the asymptotic normality
of the parametric estimator and obtain the convergence rate for the
nonparametric estimator. Our estimator that does not smooth on the instruments
coincides with a typical estimator that does smooth on the instruments but
keeps the respective bandwidth fixed as the sample size increases. We propose a
data driven method for the selection of the regularization parameter, and in a
simulation study we show the attractive performance of our estimators.",2022
http://arxiv.org/abs/2212.10790v1,Inference for Model Misspecification in Interest Rate Term Structure using Functional Principal Component Analysis,2022-12-21 06:19:58+00:00,['Kaiwen Hou'],econ.EM,"Level, slope, and curvature are three commonly-believed principal components
in interest rate term structure and are thus widely used in modeling. This
paper characterizes the heterogeneity of how misspecified such models are
through time. Presenting the orthonormal basis in the Nelson-Siegel model
interpretable as the three factors, we design two nonparametric tests for
whether the basis is equivalent to the data-driven functional principal
component basis underlying the yield curve dynamics, considering the ordering
of eigenfunctions or not, respectively. Eventually, we discover high dispersion
between the two bases when rare events occur, suggesting occasional
misspecification even if the model is overall expressive.",2022
http://arxiv.org/abs/2212.10337v2,Efficient Rollup Batch Posting Strategy on Base Layer,2022-12-20 15:24:16+00:00,"['Akaki Mamageishvili', 'Edward W. Felten']",cs.CR,"We design efficient and robust algorithms for the batch posting of rollup
chain calldata on the base layer chain, using tools from operations research.
We relate the costs of posting and delaying, by converting them to the same
units and adding them up. The algorithm that keeps the average and maximum
queued number of batches tolerable enough improves the posting costs of the
trivial algorithm, which posts batches immediately when they are created, by
8%. On the other hand, the algorithm that only cares moderately about the batch
queue length can improve the trivial algorithm posting costs by 29%. Our
findings can be used by layer two projects that post data to the base layer at
some regular rate.",2022
http://arxiv.org/abs/2212.10308v1,DeFi Risk Transfer: Towards A Fully Decentralized Insurance Protocol,2022-12-20 14:55:51+00:00,"['Matthias Nadler', 'Felix Bekemeier', 'Fabian Schär']",q-fin.GN,"In this paper, we propose a fully decentralized and smart contract-based
insurance protocol. We identify various issues in the Decentralized Finance
(DeFi) insurance context and propose a solution to overcome these shortcomings.
We introduce an economic model that allows for risk transfer without any
external dependencies or centralized intermediaries. In particular, our
proposal does not need any sort of subjective claim assessment, community
voting or external data providers (oracles). Moreover, it solves the problem of
over-insurance and proposes various ways to mitigate the capital inefficiencies
usually seen with DeFi collateral. The work takes inspiration from peer-to-peer
(P2P) insurance and collateralized debt obligations (CDO). We formally describe
the protocol, assess its efficiency and key properties and present a reference
implementation. Finally, we address limitations, extensions and ideas for
further research.",2022
http://arxiv.org/abs/2212.10301v3,Probabilistic Quantile Factor Analysis,2022-12-20 14:49:27+00:00,"['Dimitris Korobilis', 'Maximilian Schröder']",econ.EM,"This paper extends quantile factor analysis to a probabilistic variant that
incorporates regularization and computationally efficient variational
approximations. We establish through synthetic and real data experiments that
the proposed estimator can, in many cases, achieve better accuracy than a
recently proposed loss-based estimator. We contribute to the factor analysis
literature by extracting new indexes of \emph{low}, \emph{medium}, and
\emph{high} economic policy uncertainty, as well as \emph{loose},
\emph{median}, and \emph{tight} financial conditions. We show that the high
uncertainty and tight financial conditions indexes have superior predictive
ability for various measures of economic activity. In a high-dimensional
exercise involving about 1000 daily financial series, we find that quantile
factors also provide superior out-of-sample information compared to mean or
median factors.",2022
http://arxiv.org/abs/2212.10234v3,Auction designs to increase incentive compatibility and reduce self-scheduling in electricity markets,2022-12-20 13:21:59+00:00,"['Conleigh Byers', 'Brent Eldridge']",econ.GN,"The system operator's scheduling problem in electricity markets, called unit
commitment, is a non-convex mixed-integer program. The optimal value function
is non-convex, preventing the application of traditional marginal pricing
theory to find prices that clear the market and incentivize market participants
to follow the dispatch schedule. Units that perceive the opportunity to make a
profit may be incentivized to self-commit (submitting an offer with zero fixed
operating costs) or self-schedule their production (submitting an offer with
zero total cost). We simulate bidder behavior to show that market power can be
exercised by self-committing/scheduling. Agents can learn to increase their
profits via a reinforcement learning algorithm without explicit knowledge of
the costs or strategies of other agents. We investigate different non-convex
pricing models over a multi-period commitment window simulating the day-ahead
market and show that convex hull pricing can reduce producer incentives to
deviate from the central dispatch decision. In a realistic test system with
approximately 1000 generators, we find strategic bidding under the restricted
convex model can increase total producer profits by 4.4\% and decrease lost
opportunity costs by 2/3. While the cost to consumers with convex hull pricing
is higher at the competitive solution, the cost to consumers is higher with the
restricted convex model after strategic bidding.",2022
http://arxiv.org/abs/2212.10053v1,Dynamic spending and portfolio decisions with a soft social norm,2022-12-20 08:00:11+00:00,"['Knut Anton Mork', 'Fabian Andsem Harang', 'Haakon Andreas Trønnes', 'Vegard Skonseng Bjerketvedt']",econ.TH,"We explore the implications of a preference ordering for an investor-consumer
with a strong preference for keeping consumption above an exogenous social
norm, but who is willing to tolerate occasional dips below it. We do this by
splicing two CRRA preference orderings, one with high curvature below the norm
and the other with low curvature at or above it. We find this formulation
appealing for many endowment funds and sovereign wealth funds, including the
Norwegian Government Pension Fund Global, which inspired our research. We solve
this model analytically as well as numerically and find that annual spending
should not only be significantly lower than the expected financial return, but
mostly also procyclical. In particular, financial losses should, as a rule, be
followed by larger than proportional spending cuts, except when some smoothing
is needed to keep spending from falling too far below the social norm. Yet, at
very low wealth levels, spending should be kept particularly low in order to
build sufficient wealth to raise consumption above the social norm. Financial
risk taking should also be modest and procyclical, so that the investor
sometimes may want to ""buy at the top"" and ""sell at the bottom"". Many of these
features are shared by habitformation models and other models with some lower
bound for consumption. However, our specification is more flexible and thus
more easily adaptable to actual fund management. The nonlinearity of the policy
functions may present challenges regarding delegation to professional managers.
However, simpler rules of thumb with constant or slowly moving equity share and
consumption-wealth ratio can reach almost the same expected discounted utility.
However, the constant levels will then look very different from the
implications of expected CRRA utility or Epstein-Zin preferences in that
consumption is much lower.",2022
http://arxiv.org/abs/2212.09904v1,Sanctions and Imports of Essential Goods: A Closer Look at the Equipo Anova (2021) Results,2022-12-19 22:54:10+00:00,['Francisco Rodríguez'],econ.GN,"We revisit the results of a recent paper by Equipo Anova, who claim to find
evidence of an improvement in Venezuelan imports of food and medicines
associated with the adoption of U.S. financial sanctions towards Venezuela in
2017. We show that their results are consequence of data coding errors and
questionable methodological choices, including the use an unreasonable
functional form that implies a counterfactual of negative imports in the
absence of sanctions, the omission of data accounting for four-fifths of the
country's food imports at the time of sanctions and incorrect application of
regression discontinuity methods. Once these errors are corrected, the evidence
of a significant improvement in the level and rate of change in imports of
essentials disappears.",2022
http://arxiv.org/abs/2212.09889v2,Strategic Observational Learning,2022-12-19 22:19:26+00:00,['Dimitri Migrow'],econ.TH,"We study learning by privately informed forward-looking agents in a simple
repeated-action setting of social learning. Under a symmetric signal structure,
forward-looking agents behave myopically for any degrees of patience. Myopic
equilibrium is unique in the class of symmetric threshold strategies, and the
simplest symmetric non-monotonic strategies. If the signal structure is
asymmetric and the game is infinite, there is no equilibrium in myopic
strategies, for any positive degree of patience.",2022
http://arxiv.org/abs/2212.09868v1,Quantifying fairness and discrimination in predictive models,2022-12-19 21:38:13+00:00,['Arthur Charpentier'],econ.EM,"The analysis of discrimination has long interested economists and lawyers. In
recent years, the literature in computer science and machine learning has
become interested in the subject, offering an interesting re-reading of the
topic. These questions are the consequences of numerous criticisms of
algorithms used to translate texts or to identify people in images. With the
arrival of massive data, and the use of increasingly opaque algorithms, it is
not surprising to have discriminatory algorithms, because it has become easy to
have a proxy of a sensitive variable, by enriching the data indefinitely.
According to Kranzberg (1986), ""technology is neither good nor bad, nor is it
neutral"", and therefore, ""machine learning won't give you anything like gender
neutrality `for free' that you didn't explicitely ask for"", as claimed by
Kearns et a. (2019). In this article, we will come back to the general context,
for predictive models in classification. We will present the main concepts of
fairness, called group fairness, based on independence between the sensitive
variable and the prediction, possibly conditioned on this or that information.
We will finish by going further, by presenting the concepts of individual
fairness. Finally, we will see how to correct a potential discrimination, in
order to guarantee that a model is more ethical",2022
http://arxiv.org/abs/2212.09844v5,Robust Design and Evaluation of Predictive Algorithms under Unobserved Confounding,2022-12-19 20:41:44+00:00,"['Ashesh Rambachan', 'Amanda Coston', 'Edward Kennedy']",econ.EM,"Predictive algorithms inform consequential decisions in settings where the
outcome is selectively observed given choices made by human decision makers. We
propose a unified framework for the robust design and evaluation of predictive
algorithms in selectively observed data. We impose general assumptions on how
much the outcome may vary on average between unselected and selected units
conditional on observed covariates and identified nuisance parameters,
formalizing popular empirical strategies for imputing missing data such as
proxy outcomes and instrumental variables. We develop debiased machine learning
estimators for the bounds on a large class of predictive performance estimands,
such as the conditional likelihood of the outcome, a predictive algorithm's
mean square error, true/false positive rate, and many others, under these
assumptions. In an administrative dataset from a large Australian financial
institution, we illustrate how varying assumptions on unobserved confounding
leads to meaningful changes in default risk predictions and evaluations of
credit scores across sensitive groups.",2022
http://arxiv.org/abs/2212.09715v2,Liquid Democracy. Two Experiments on Delegation in Voting,2022-12-19 18:44:13+00:00,"['Victoria Mooers', 'Joseph Campbell', 'Alessandra Casella', 'Lucas de Lara', 'Dilip Ravindran']",econ.GN,"Proponents of participatory democracy praise Liquid Democracy: decisions are
taken by referendum, but voters delegate their votes freely. When better
informed voters are present, delegation can increase the probability of a
correct decision. However, delegation must be used sparely because it reduces
the information aggregated through voting. In two different experiments, we
find that delegation underperforms both universal majority voting and the
simpler option of abstention. In a tightly controlled lab experiment where the
subjects' precision of information is conveyed in precise mathematical terms
and very salient, the result is due to overdelegation. In a perceptual task run
online where the precision of information is not known precisely, delegation
remains very high and again underperforms both majority voting and abstention.
In addition, subjects substantially overestimate the precision of the better
informed voters, underlining that Liquid Democracy is fragile to multiple
sources of noise. The paper makes an innovative methodological contribution by
combining two very different experimental procedures: the study of voting rules
would benefit from complementing controlled experiments with known precision of
information with tests under ambiguity, a realistic assumption in many voting
situations.",2022
http://arxiv.org/abs/2212.10359v2,Simultaneous Inference of a Partially Linear Model in Time Series,2022-12-19 17:16:47+00:00,"['Jiaqi Li', 'Likai Chen', 'Kun Ho Kim', 'Tianwei Zhou']",stat.ME,"We introduce a new methodology to conduct simultaneous inference of the
nonparametric component in partially linear time series regression models where
the nonparametric part is a multivariate unknown function. In particular, we
construct a simultaneous confidence region (SCR) for the multivariate function
by extending the high-dimensional Gaussian approximation to dependent processes
with continuous index sets. Our results allow for a more general dependence
structure compared to previous works and are widely applicable to a variety of
linear and nonlinear autoregressive processes. We demonstrate the validity of
our proposed methodology by examining the finite-sample performance in the
simulation study. Finally, an application in time series, the forward premium
regression, is presented, where we construct the SCR for the foreign exchange
risk premium from the exchange rate and macroeconomic data.",2022
http://arxiv.org/abs/2212.09617v1,Expected Growth Criterion: An Axiomatization,2022-12-19 16:56:34+00:00,['Joshua Lawson'],econ.TH,"I provide necessary and sufficient conditions for an agent's preferences to
be represented by a unique ergodic transformation. Put differently, if an agent
seeks to maximize the time average growth of their wealth, what axioms must
their preferences obey? By answering this, I provide economic theorists a clear
view of where ""Ergodicity Economics"" deviates from established models.",2022
http://arxiv.org/abs/2212.09380v1,Understanding the food component of inflation,2022-12-19 11:38:43+00:00,['Emanuel Kohlscheen'],econ.GN,"This article presents evidence based on a panel of 35 countries over the past
30 years that the Phillips curve relation holds for food inflation. That is,
broader economic overheating does push up the food component of the CPI in a
systematic way. Further, general inflation expectations from professional
forecasters clearly impact food price inflation. The analysis also quantifies
the extent to which higher food production and imports, or lower food exports,
reduce food inflation. Importantly, the link between domestic and global food
prices is typically weak, with passthroughs within a year ranging from 0.07 to
0.16, after exchange rate variations are taken into account.",2022
http://arxiv.org/abs/2212.09299v1,Optimal pricing for carbon dioxide removal under inter-regional leakage,2022-12-19 08:33:43+00:00,"['Max Franks', 'Matthias Kalkuhl', 'Kai Lessmann']",econ.GN,"Carbon dioxide removal (CDR) moves atmospheric carbon to geological or
land-based sinks. In a first-best setting, the optimal use of CDR is achieved
by a removal subsidy that equals the optimal carbon tax and marginal damages.
We derive second-best policy rules for CDR subsidies and carbon taxes when no
global carbon price exists but a national government implements a unilateral
climate policy. We find that the optimal carbon tax differs from an optimal CDR
subsidy because of carbon leakage and a balance of resource trade effect.
First, the optimal removal subsidy tends to be larger than the carbon tax
because of lower supply-side leakage on fossil resource markets. Second, net
carbon exporters exacerbate this wedge to increase producer surplus of their
carbon resource producers, implying even larger removal subsidies. Third, net
carbon importers may set their removal subsidy even below their carbon tax when
marginal environmental damages are small, to appropriate producer surplus from
carbon exporters.",2022
http://arxiv.org/abs/2212.09193v2,Identification of time-varying counterfactual parameters in nonlinear panel models,2022-12-18 23:36:59+00:00,"['Irene Botosaru', 'Chris Muris']",econ.EM,"We develop a general framework for the identification of counterfactual
parameters in a class of nonlinear semiparametric panel models with fixed
effects and time effects. Our method applies to models for discrete outcomes
(e.g., two-way fixed effects binary choice) or continuous outcomes (e.g.,
censored regression), with discrete or continuous regressors. Our results do
not require parametric assumptions on the error terms or time-homogeneity on
the outcome equation. Our main results focus on static models, with a set of
results applying to models without any exogeneity conditions. We show that the
survival distribution of counterfactual outcomes is identified (point or
partial) in this class of models. This parameter is a building block for most
partial and marginal effects of interest in applied practice that are based on
the average structural function as defined by Blundell and Powell (2003, 2004).
To the best of our knowledge, ours are the first results on average partial and
marginal effects for binary choice and ordered choice models with two-way fixed
effects and non-logistic errors.",2022
http://arxiv.org/abs/2212.09007v2,PAC-Bayesian Treatment Allocation Under Budget Constraints,2022-12-18 04:22:16+00:00,['Daniel F. Pellatt'],econ.EM,"This paper considers the estimation of treatment assignment rules when the
policy maker faces a general budget or resource constraint. Utilizing the
PAC-Bayesian framework, we propose new treatment assignment rules that allow
for flexible notions of treatment outcome, treatment cost, and a budget
constraint. For example, the constraint setting allows for cost-savings, when
the costs of non-treatment exceed those of treatment for a subpopulation, to be
factored into the budget. It also accommodates simpler settings, such as
quantity constraints, and doesn't require outcome responses and costs to have
the same unit of measurement. Importantly, the approach accounts for settings
where budget or resource limitations may preclude treating all that can
benefit, where costs may vary with individual characteristics, and where there
may be uncertainty regarding the cost of treatment rules of interest. Despite
the nomenclature, our theoretical analysis examines frequentist properties of
the proposed rules. For stochastic rules that typically approach
budget-penalized empirical welfare maximizing policies in larger samples, we
derive non-asymptotic generalization bounds for the target population costs and
sharp oracle-type inequalities that compare the rules' welfare regret to that
of optimal policies in relevant budget categories. A closely related,
non-stochastic, model aggregation treatment assignment rule is shown to inherit
desirable attributes.",2022
http://arxiv.org/abs/2212.08709v3,Structural Complexities of Matching Mechanisms,2022-12-16 20:53:30+00:00,"['Yannai A. Gonczarowski', 'Clayton Thomas']",cs.GT,"We study various novel complexity measures for two-sided matching mechanisms,
applied to the two canonical strategyproof matching mechanisms, Deferred
Acceptance (DA) and Top Trading Cycles (TTC). Our metrics are designed to
capture the complexity of various structural (rather than computational)
concerns, in particular ones of recent interest within economics. We consider a
unified, flexible approach to formalizing our questions: Define a protocol or
data structure performing some task, and bound the number of bits that it
requires. Our main results apply this approach to four questions of general
interest; for mechanisms matching applicants to institutions, our questions
are:
  (1) How can one applicant affect the outcome matching?
  (2) How can one applicant affect another applicant's set of options?
  (3) How can the outcome matching be represented / communicated?
  (4) How can the outcome matching be verified?
  Holistically, our results show that TTC is more complex than DA, formalizing
previous intuitions that DA has a simpler structure than TTC. For question (2),
our result gives a new combinatorial characterization of which institutions are
removed from each applicant's set of options when a new applicant is added in
DA; this characterization may be of independent interest. For question (3), our
result gives new tight lower bounds proving that the relationship between the
matching and the priorities is more complex in TTC than in DA. We nonetheless
showcase that this higher complexity of TTC is nuanced: By constructing new
tight lower-bound instances and new verification protocols, we prove that DA
and TTC are comparable in complexity under questions (1) and (4). This more
precisely delineates the ways in which TTC is more complex than DA, and
emphasizes that diverse considerations must factor into gauging the complexity
of matching mechanisms.",2022
http://arxiv.org/abs/2212.08615v1,A smooth transition autoregressive model for matrix-variate time series,2022-12-16 17:46:46+00:00,['Andrea Bucci'],stat.ME,"In many applications, data are observed as matrices with temporal dependence.
Matrix-variate time series modeling is a new branch of econometrics. Although
stylized facts in several fields, the existing models do not account for regime
switches in the dynamics of matrices that are not abrupt. In this paper, we
extend linear matrix-variate autoregressive models by introducing a
regime-switching model capable of accounting for smooth changes, the matrix
smooth transition autoregressive model. We present the estimation processes
with the asymptotic properties demonstrated with simulated and real data.",2022
http://arxiv.org/abs/2212.08509v1,Moate Simulation of Stochastic Processes,2022-12-16 14:42:45+00:00,['Michael E. Mura'],q-fin.CP,"A novel approach called Moate Simulation is presented to provide an accurate
numerical evolution of probability distribution functions represented on grids
arising from stochastic differential processes where initial conditions are
specified. Where the variables of stochastic differential equations may be
transformed via It\^o-Doeblin calculus into stochastic differentials with a
constant diffusion term, the probability distribution function for these
variables can be simulated in discrete time steps. The drift is applied
directly to a volume element of the distribution while the stochastic diffusion
term is applied through the use of convolution techniques such as Fast or
Discrete Fourier Transforms. This allows for highly accurate distributions to
be efficiently simulated to a given time horizon and may be employed in one,
two or higher dimensional expectation integrals, e.g. for pricing of financial
derivatives. The Moate Simulation approach forms a more accurate and
considerably faster alternative to Monte Carlo Simulation for many applications
while retaining the opportunity to alter the distribution in mid-simulation.",2022
http://arxiv.org/abs/2212.08220v2,The Long-Term Effects of Teachers' Gender Stereotypes,2022-12-16 01:17:53+00:00,['Joan Martinez'],econ.GN,"This paper studies the effects of teachers' stereotypical assessments of boys
and girls on students' long-term outcomes, including high school graduation,
college attendance, and formal sector employment. I measure teachers' gender
stereotypical grading based on preconceptions about boys' aptitude in math and
science and girls' aptitude in communicating and verbal skills by analyzing
novel data on teachers' responses to the Implicit Association Test (IAT) and
differences in gender gaps between teacher-assigned and blindly graded tests.
To collect IAT scores on a national scale, I developed a large-scale
educational program accessible to teachers and students in Peruvian public
schools. This analysis provides evidence that teachers' gender stereotypes are
reflected in student evaluations: math teachers with stronger stereotypes
associating boys with scientific disciplines award male students with higher
test scores compared to blindly-graded test scores. In contrast, language arts
teachers who stereotypically associate females with humanities-based
disciplines give female students higher grades. Using graduation, college
enrollment, and matched employer-employee data on 1.7 million public high
school students expected to graduate between 2015 and 2019, I find that female
students who are placed with teachers who use more stereotypical grading
practices are less likely to graduate from high school and apply to college
than male students. In comparison to their male counterparts, female high
school graduates whose teachers employ more stereotypical grading practices are
less likely to be employed in the formal sector and have fewer paid working
hours. Furthermore, exposure to teachers with more stereotypical grading
practices reduces women's monthly earnings, thereby widening the gender pay
gap.",2022
http://arxiv.org/abs/2212.08219v4,Rationally Inattentive Statistical Discrimination: Arrow Meets Phelps,2022-12-16 01:14:37+00:00,"['Federico Echenique', 'Anqi Li']",econ.TH,"When information acquisition is costly but flexible, a principal may
rationally acquire information that favors one group over another. The former
group faces incentives to invest in becoming productive, while the latter is
discouraged from such investments. The principal, in turn, ignores the
productivity difference between groups unless the underinvested group surprises
him with a genuinely outstanding outcome. We give conditions under which the
discriminatory equilibrium is most preferred by the principal, despite all
groups being ex-ante identical. Our results inform the discussion of
affirmative action, implicit bias, and occupational segregation and
stereotypes.",2022
http://arxiv.org/abs/2212.08198v2,Economic impacts of AI-augmented R&D,2022-12-15 23:48:18+00:00,"['Tamay Besiroglu', 'Nicholas Emery-Xu', 'Neil Thompson']",econ.GN,"Since its emergence around 2010, deep learning has rapidly become the most
important technique in Artificial Intelligence (AI), producing an array of
scientific firsts in areas as diverse as protein folding, drug discovery,
integrated chip design, and weather prediction. As more scientists and
engineers adopt deep learning, it is important to consider what effect
widespread deployment would have on scientific progress and, ultimately,
economic growth. We assess this impact by estimating the idea production
function for AI in two computer vision tasks that are considered key test-beds
for deep learning and show that AI idea production is notably more
capital-intensive than traditional R&D. Because increasing the
capital-intensity of R&D accelerates the investments that make scientists and
engineers more productive, our work suggests that AI-augmented R&D has the
potential to speed up technological change and economic growth.",2022
http://arxiv.org/abs/2301.01271v1,On the notion of measurable utility on a connected and separable topological space: an order isomorphism theorem,2022-12-15 12:48:35+00:00,['Gianmarco Caldini'],econ.GN,"The aim of this article is to define a notion of cardinal utility function
called measurable utility and to define it on a connected and separable subset
of a weakly ordered topological space. The definition is equivalent to the ones
given by Frisch in 1926 and by Shapley in 1975 and postulates axioms on a set
of alternatives that allow both to ordinally rank alternatives and to compare
their utility differences. After a brief review of the philosophy of
utilitarianism and the history of utility theory, the paper introduces the
mathematical framework to represent intensity comparisons of utility and proves
a list of topological lemmas that will be used in the main result. Finally, the
article states and proves a representation theorem for a measurable utility
function defined on a connected and separable subset of a weakly ordered
topological space equipped with another weak order on its cartesian product.
Under some assumptions on the order relations, the main theorem proves
existence and uniqueness, up to positive affine transformations, of an order
isomorphism with the real line.",2022
http://arxiv.org/abs/2212.07725v2,Sequential Sampling Equilibrium,2022-12-15 11:07:44+00:00,['Duarte Gonçalves'],econ.TH,"This paper introduces an equilibrium framework based on sequential sampling
in which players face strategic uncertainty over their opponents' behavior and
acquire informative signals to resolve it. Sequential sampling equilibrium
delivers a disciplined model featuring an endogenous distribution of choices,
beliefs, and decision times, that not only rationalizes well-known deviations
from Nash equilibrium, but also makes novel predictions supported by existing
data. It grounds a relationship between empirical learning and strategic
sophistication, and generates stochastic choice through randomness inherent to
sampling, without relying on indifference or choice mistakes. Further, it
provides a rationale for Nash equilibrium when sampling costs vanish.",2022
http://arxiv.org/abs/2212.07521v2,Information and Learning in Economic Theory,2022-12-14 21:55:10+00:00,['Annie Liang'],econ.TH,"These lecture notes accompany a one-semester graduate course on information
and learning in economic theory. Topics include common knowledge, Bayesian
updating, monotone-likelihood ratio properties, affiliation, the Blackwell
order, cost of information, learning and merging of beliefs, model uncertainty,
model misspecification, and information design.",2022
http://arxiv.org/abs/2212.07384v4,Valuing Pharmaceutical Drug Innovations,2022-12-14 18:09:32+00:00,"['Gaurab Aryal', 'Federico Ciliberto', 'Leland E. Farmer', 'Ekaterina Khmelnitskaya']",econ.GN,"We propose a methodology to estimate the market value of pharmaceutical
drugs. Our approach combines an event study with a model of discounted cash
flows and uses stock market responses to drug development announcements to
infer the values. We estimate that, on average, a successful drug is valued at
\$1.62 billion, and its value at the discovery stage is \$64.3 million, with
substantial heterogeneity across major diseases. Leveraging these estimates, we
also determine the average drug development costs at various stages.
Furthermore, we explore applying our estimates to design policies that support
drug development through drug buyouts and cost-sharing agreements.",2022
http://arxiv.org/abs/2212.07379v1,The finite sample performance of instrumental variable-based estimators of the Local Average Treatment Effect when controlling for covariates,2022-12-14 18:02:46+00:00,"['Hugo Bodory', 'Martin Huber', 'Michael Lechner']",econ.EM,"This paper investigates the finite sample performance of a range of
parametric, semi-parametric, and non-parametric instrumental variable
estimators when controlling for a fixed set of covariates to evaluate the local
average treatment effect. Our simulation designs are based on empirical labor
market data from the US and vary in several dimensions, including effect
heterogeneity, instrument selectivity, instrument strength, outcome
distribution, and sample size. Among the estimators and simulations considered,
non-parametric estimation based on the random forest (a machine learner
controlling for covariates in a data-driven way) performs competitive in terms
of the average coverage rates of the (bootstrap-based) 95% confidence
intervals, while also being relatively precise. Non-parametric kernel
regression as well as certain versions of semi-parametric radius matching on
the propensity score, pair matching on the covariates, and inverse probability
weighting also have a decent coverage, but are less precise than the random
forest-based method. In terms of the average root mean squared error of LATE
estimation, kernel regression performs best, closely followed by the random
forest method, which has the lowest average absolute bias.",2022
http://arxiv.org/abs/2212.07306v2,Toxic Liquidation Spirals,2022-12-14 16:10:26+00:00,"['Jakub Warmuz', 'Amit Chaudhary', 'Daniele Pinna']",econ.GN,"On November 22nd 2022, the lending platform AAVE v2 (on Ethereum) incurred
bad debt resulting from a major liquidation event involving a single user who
had borrowed close to \$40M of CRV tokens using USDC as collateral. This
incident has prompted the Aave community to consider changes to its liquidation
threshold, and limitations on the number of illiquid coins that can be borrowed
on the platform. In this paper, we argue that the bad debt incurred by AAVE was
not due to excess volatility in CRV/USDC price activity on that day, but rather
a fundamental flaw in the liquidation logic which triggered a toxic liquidation
spiral on the platform. We note that this flaw, which is shared by a number of
major DeFi lending markets, can be easily overcome with simple changes to the
incentives driving liquidations. We claim that halting all liquidations once a
user's loan-to-value (LTV) ratio surpasses a certain threshold value can
prevent future toxic liquidation spirals and offer substantial improvement in
the bad debt that a lending market can expect to incur. Furthermore, we
strongly argue that protocols should enact dynamic liquidation incentives and
closing factor policies moving forward for optimal management of protocol risk.",2022
http://arxiv.org/abs/2212.07288v1,Smoothing volatility targeting,2022-12-14 15:43:22+00:00,"['Mauro Bernardi', 'Daniele Bianchi', 'Nicolas Bianco']",econ.EM,"We propose an alternative approach towards cost mitigation in
volatility-managed portfolios based on smoothing the predictive density of an
otherwise standard stochastic volatility model. Specifically, we develop a
novel variational Bayes estimation method that flexibly encompasses different
smoothness assumptions irrespective of the persistence of the underlying latent
state. Using a large set of equity trading strategies, we show that smoothing
volatility targeting helps to regularise the extreme leverage/turnover that
results from commonly used realised variance estimates. This has important
implications for both the risk-adjusted returns and the mean-variance
efficiency of volatility-managed portfolios, once transaction costs are
factored in. An extensive simulation study shows that our variational inference
scheme compares favourably against existing state-of-the-art Bayesian
estimation methods for stochastic volatility models.",2022
http://arxiv.org/abs/2212.07280v3,Amenity complexity and urban locations of socio-economic mixing,2022-12-14 15:27:50+00:00,"['Sándor Juhász', 'Gergő Pintér', 'Ádám Kovács', 'Endre Borza', 'Gergely Mónus', 'László Lőrincz', 'Balázs Lengyel']",physics.soc-ph,"Cities host diverse people and their mixing is the engine of prosperity. In
turn, segregation and inequalities are common features of most cities and
locations that enable the meeting of people with different socio-economic
status are key for urban inclusion. In this study, we adopt the concept of
economic complexity to quantify the sophistication of amenity supply at urban
locations. We propose that neighborhood complexity and amenity complexity are
connected to the ability of locations to attract diverse visitors from various
socio-economic backgrounds across the city. We construct the measures of
amenity complexity based on the local portfolio of diverse and non-ubiquitous
amenities in Budapest, Hungary. Socio-economic mixing at visited third places
is investigated by tracing the daily mobility of individuals and by
characterizing their status by the real-estate price of their home locations.
Results suggest that measures of ubiquity and diversity of amenities do not,
but neighborhood complexity and amenity complexity are correlated with the
urban centrality of locations. Urban centrality is a strong predictor of
socio-economic mixing, but both neighborhood complexity and amenity complexity
add further explanatory power to our models. Our work combines urban mobility
data with economic complexity thinking to show that the diversity of
non-ubiquitous amenities, central locations, and the potentials for
socio-economic mixing are interrelated.",2022
http://arxiv.org/abs/2212.07263v2,Robust Estimation of the non-Gaussian Dimension in Structural Linear Models,2022-12-14 14:57:22+00:00,['Miguel Cabello'],econ.EM,"Statistical identification of possibly non-fundamental SVARMA models requires
structural errors: (i) to be an i.i.d process, (ii) to be mutually independent
across components, and (iii) each of them must be non-Gaussian distributed.
Hence, provided the first two requisites, it is crucial to evaluate the
non-Gaussian identification condition. We address this problem by relating the
non-Gaussian dimension of structural errors vector to the rank of a matrix
built from the higher-order spectrum of reduced-form errors. This makes our
proposal robust to the roots location of the lag polynomials, and generalizes
the current procedures designed for the restricted case of a causal structural
VAR model. Simulation exercises show that our procedure satisfactorily
estimates the number of non-Gaussian components.",2022
http://arxiv.org/abs/2212.07246v4,Topology-Free Type Structures with Conditioning Events,2022-12-14 14:30:05+00:00,['Pierfrancesco Guarino'],econ.TH,"We establish the existence of the universal type structure in presence of
conditioning events without any topological assumption, namely, a type
structure that is terminal, belief-complete, and non-redundant, by performing a
construction \`a la Heifetz & Samet (1998). In doing so, we answer
affirmatively to a longstanding conjecture made by Battigalli & Siniscalchi
(1999) concerning the possibility of performing such a construction with
conditioning events. In particular, we obtain the result by exploiting
arguments from category theory and the theory of coalgebras, thus, making
explicit the mathematical structure underlining all the constructions of large
interactive structures and obtaining the belief-completeness of the structure
as an immediate corollary of known results from these fields.",2022
http://arxiv.org/abs/2212.07128v1,Influence of rationality levels on dynamics of heterogeneous Cournot duopolists with quadratic costs,2022-12-14 09:27:06+00:00,"['Xiaoliang Li', 'Yihuo Jiang']",econ.TH,"This paper is intended to investigate the dynamics of heterogeneous Cournot
duopoly games, where the first players adopt identical gradient adjustment
mechanisms but the second players are endowed with distinct rationality levels.
Based on tools of symbolic computations, we introduce a new approach and use it
to establish rigorous conditions of the local stability for these models. We
analytically investigate the bifurcations and prove that the period-doubling
bifurcation is the only possible bifurcation that may occur for all the
considered models. The most important finding of our study is regarding the
influence of players' rational levels on the stability of heterogeneous
duopolistic competition. It is derived that the stability region of the model
where the second firm is rational is the smallest, while that of the one where
the second firm is boundedly rational is the largest. This fact is
counterintuitive and contrasts with relative conclusions in the existing
literature. Furthermore, we also provide numerical simulations to demonstrate
the emergence of complex dynamics such as periodic solutions with different
orders and strange attractors.",2022
http://arxiv.org/abs/2212.07427v1,Limited Farsightedness in Priority-Based Matching,2022-12-14 09:02:41+00:00,"['Ata Atay', 'Ana Mauleon', 'Vincent Vannetelbosch']",econ.TH,"We consider priority-based matching problems with limited farsightedness. We
show that, once agents are sufficiently farsighted, the matching obtained from
the Top Trading Cycles (TTC) algorithm becomes stable: a singleton set
consisting of the TTC matching is a horizon-$k$ vNM stable set if the degree of
farsightedness is greater than three times the number of agents in the largest
cycle of the TTC. On the contrary, the matching obtained from the Deferred
Acceptance (DA) algorithm may not belong to any horizon-$k$ vNM stable set for
$k$ large enough.",2022
http://arxiv.org/abs/2212.07108v1,School Choice with Farsighted Students,2022-12-14 08:57:28+00:00,"['Ata Atay', 'Ana Mauleon', 'Vincent Vannetelbosch']",econ.TH,"We consider priority-based school choice problems with farsighted students.
We show that a singleton set consisting of the matching obtained from the Top
Trading Cycles (TTC) mechanism is a farsighted stable set. However, the
matching obtained from the Deferred Acceptance (DA) mechanism may not belong to
any farsighted stable set. Hence, the TTC mechanism provides an assignment that
is not only Pareto efficient but also farsightedly stable. Moreover, looking
forward three steps ahead is already sufficient for stabilizing the matching
obtained from the TTC.",2022
http://arxiv.org/abs/2212.07099v1,Research on The Cultivation Path of Craftsman Spirit in Higher Vocational Education Based on Survey Data,2022-12-14 08:45:56+00:00,"['Yufei Xie', 'Jing Cui', 'Mengdie Wang']",econ.GN,"With the development of China's economy and society, the importance of
""craftsman's spirit"" has become more and more prominent. As the main
educational institution for training technical talents, higher vocational
colleges vigorously promote the exploration of the cultivation path of
craftsman spirit in higher vocational education, which provides new ideas and
directions for the reform and development of higher vocational education, and
is the fundamental need of the national innovation driven development strategy.
Based on the questionnaire survey of vocational students in a certain range,
this paper analyzes the problems existing in the cultivation path of craftsman
spirit in Higher Vocational Education from multiple levels and the
countermeasures.",2022
http://arxiv.org/abs/2212.07052v2,On LASSO for High Dimensional Predictive Regression,2022-12-14 06:14:58+00:00,"['Ziwei Mei', 'Zhentao Shi']",econ.EM,"This paper examines LASSO, a widely-used $L_{1}$-penalized regression method,
in high dimensional linear predictive regressions, particularly when the number
of potential predictors exceeds the sample size and numerous unit root
regressors are present. The consistency of LASSO is contingent upon two key
components: the deviation bound of the cross product of the regressors and the
error term, and the restricted eigenvalue of the Gram matrix. We present new
probabilistic bounds for these components, suggesting that LASSO's rates of
convergence are different from those typically observed in cross-sectional
cases. When applied to a mixture of stationary, nonstationary, and cointegrated
predictors, LASSO maintains its asymptotic guarantee if predictors are
scale-standardized. Leveraging machine learning and macroeconomic domain
expertise, LASSO demonstrates strong performance in forecasting the
unemployment rate, as evidenced by its application to the FRED-MD database.",2022
http://arxiv.org/abs/2212.07019v1,Data-Driven Prediction and Evaluation on Future Impact of Energy Transition Policies in Smart Regions,2022-12-14 04:19:34+00:00,"['Chunmeng Yang', 'Siqi Bu', 'Yi Fan', 'Wayne Xinwei Wan', 'Ruoheng Wang', 'Aoife Foley']",econ.GN,"To meet widely recognised carbon neutrality targets, over the last decade
metropolitan regions around the world have implemented policies to promote the
generation and use of sustainable energy. Nevertheless, there is an
availability gap in formulating and evaluating these policies in a timely
manner, since sustainable energy capacity and generation are dynamically
determined by various factors along dimensions based on local economic
prosperity and societal green ambitions. We develop a novel data-driven
platform to predict and evaluate energy transition policies by applying an
artificial neural network and a technology diffusion model. Using Singapore,
London, and California as case studies of metropolitan regions at distinctive
stages of energy transition, we show that in addition to forecasting renewable
energy generation and capacity, the platform is particularly powerful in
formulating future policy scenarios. We recommend global application of the
proposed methodology to future sustainable energy transition in smart regions.",2022
http://arxiv.org/abs/2212.06984v2,Market Mechanisms for Low-Carbon Electricity Investments: A Game-Theoretical Analysis,2022-12-14 02:37:50+00:00,"['Dongwei Zhao', 'Sarah Coyle', 'Apurba Sakti', 'Audun Botterud']",eess.SY,"Electricity markets are transforming from the dominance of conventional
energy resources (CERs), e.g., fossil fuels, to low-carbon energy resources
(LERs), e.g., renewables and energy storage. This work examines market
mechanisms to incentivize LER investments, while ensuring adequate market
revenues for investors, guiding investors' strategic investments towards social
optimum, and protecting consumers from scarcity prices. To reduce the impact of
excessive scarcity prices, we present a new market mechanism, which consists of
a Penalty payment for lost load, a supply Incentive, and an energy price Uplift
(PIU). We establish a game-theoretical framework to analyze market equilibrium.
We prove that one Nash equilibrium under the penalty payment and supply
incentive can reach the social optimum given quadratic supply costs of CERs.
Although the price uplift can ensure adequate revenues, the resulting system
cost deviates from the social optimum while the gap decreases as more CERs
retire. Furthermore, under the traditional marginal-cost pricing (MCP)
mechanism, investors may withhold investments to cause scarcity prices, but
such behavior is absent under the PIU mechanism. Simulation results show that
the PIU mechanism can reduce consumers' costs by over 30% compared with the MCP
mechanism by reducing excessive revenues of low-cost CERs from scarcity prices.",2022
http://arxiv.org/abs/2212.06744v1,Demand-side policies for power generation in response to the energy crisis: a model analysis for Italy,2022-12-13 17:27:47+00:00,"['Alice Di Bella', 'Massimo Tavoni']",physics.soc-ph,"In order to mitigate the impacts of the energy crise, the European Union has
proposed various measures. For the power sector a directive prescribes a shift
of 5% of the demand in 10% of the peak hours, plus a voluntary 10% overall
demand reduction. Here we use a power system model to quantify the implications
of this policy for the Italian power sector, as it stands today and under the
transformation required to meet the climate goals of the Fit-for-55. We find
that policymakers would need to incentivize electricity consumption in the
middle of the day while discouraging it in the early morning and late
afternoon. We also highlight the benefits of the decarbonization strategy in
the context of uncertain gas prices: for a gas price at or above 50 euro/MWh,
power generation through gas is reduced by more than one third, approaching
what needed to comply with the Fit-for-55. Finally, we quantify the value of
demand side management strategies to curb fossil resource consumption and to
reduce curtailed electricity under a high renewable scenario.",2022
http://arxiv.org/abs/2212.06736v2,The Role of Mandated Mental Health Treatment in the Criminal Justice System,2022-12-13 17:16:16+00:00,['Rachel Nesbit'],econ.GN,"Mental health disorders are particularly prevalent among those in the
criminal justice system and may be a contributing factor in recidivism. Using
North Carolina court cases from 1994 to 2009, this paper evaluates how mandated
mental health treatment as a term of probation impacts the likelihood that
individuals return to the criminal justice system. I use random variation in
judge assignment to compare those who were required to seek weekly mental
health counseling to those who were not. The main findings are that being
assigned to seek mental health treatment decreases the likelihood of three-year
recidivism by about 12 percentage points, or 36 percent. This effect persists
over time, and is similar among various types of individuals on probation. In
addition, I show that mental health treatment operates distinctly from drug
addiction interventions in a multiple-treatment framework. I provide evidence
that mental health treatment's longer-term effectiveness is strongest among
more financially-advantaged probationers, consistent with this setting, in
which the cost of mandated treatment is shouldered by offenders. Finally,
conservative calculations result in a 5:1 benefit-to-cost ratio which suggests
that the treatment-induced decrease in future crime would be more than
sufficient to offset the costs of treatment.",2022
http://arxiv.org/abs/2212.06684v1,Identifying the regional drivers of influenza-like illness in Nova Scotia with dominance analysis,2022-12-13 15:59:56+00:00,"['Yigit Aydede', 'Jan Ditzen']",econ.GN,"The spread of viral pathogens is inherently a spatial process. While the
temporal aspects of viral spread at the epidemiological level have been
increasingly well characterized, the spatial aspects of viral spread are still
understudied due to a striking absence of theoretical expectations of how
spatial dynamics may impact the temporal dynamics of viral populations.
Characterizing the spatial transmission and understanding the factors driving
it are important for anticipating local timing of disease incidence and for
guiding more informed control strategies. Using a unique data set from Nova
Scotia, the objective of this study is to apply a new novel method that
recovers a spatial network of the influenza-like viral spread where the regions
in their dominance are identified and ranked. We, then, focus on identifying
regional predictors of those dominant regions.",2022
http://arxiv.org/abs/2212.06832v1,Multi-Target Decision Making under Conditions of Severe Uncertainty,2022-12-13 11:47:02+00:00,"['Christoph Jansen', 'Georg Schollmeyer', 'Thomas Augustin']",cs.AI,"The quality of consequences in a decision making problem under (severe)
uncertainty must often be compared among different targets (goals, objectives)
simultaneously. In addition, the evaluations of a consequence's performance
under the various targets often differ in their scale of measurement,
classically being either purely ordinal or perfectly cardinal. In this paper,
we transfer recent developments from abstract decision theory with incomplete
preferential and probabilistic information to this multi-target setting and
show how -- by exploiting the (potentially) partial cardinal and partial
probabilistic information -- more informative orders for comparing decisions
can be given than the Pareto order. We discuss some interesting properties of
the proposed orders between decision options and show how they can be
concretely computed by linear optimization. We conclude the paper by
demonstrating our framework in an artificial (but quite real-world) example in
the context of comparing algorithms under different performance measures.",2022
http://arxiv.org/abs/2401.10261v1,How industrial clusters influence the growth of the regional GDP: A spatial-approach,2023-12-31 22:29:57+00:00,"['Vahidin Jeleskovic', 'Steffen Loeber']",econ.GN,"In this paper, we employ spatial econometric methods to analyze panel data
from German NUTS 3 regions. Our goal is to gain a deeper understanding of the
significance and interdependence of industry clusters in shaping the dynamics
of GDP. To achieve a more nuanced spatial differentiation, we introduce
indicator matrices for each industry sector which allows for extending the
spatial Durbin model to a new version of it. This approach is essential due to
both the economic importance of these sectors and the potential issue of
omitted variables. Failing to account for industry sectors can lead to omitted
variable bias and estimation problems. To assess the effects of the major
industry sectors, we incorporate eight distinct branches of industry into our
analysis. According to prevailing economic theory, these clusters should have a
positive impact on the regions they are associated with. Our findings indeed
reveal highly significant impacts, which can be either positive or negative, of
specific sectors on local GDP growth. Spatially, we observe that direct and
indirect effects can exhibit opposite signs, indicative of heightened
competitiveness within and between industry sectors. Therefore, we recommend
that industry sectors should be taken into consideration when conducting
spatial analysis of GDP. Doing so allows for a more comprehensive understanding
of the economic dynamics at play.",2023
http://arxiv.org/abs/2401.00535v1,Actualised and future changes in regional economic growth through sea level rise,2023-12-31 16:42:45+00:00,"['Theodoros Chatzivasileiadis', 'Ignasi Cortes Arbues', 'Jochen Hinkel', 'Daniel Lincke', 'Richard S. J. Tol']",econ.GN,"This study investigates the long-term economic impact of sea-level rise (SLR)
on coastal regions in Europe, focusing on Gross Domestic Product (GDP). Using a
novel dataset covering regional SLR and economic growth from 1900 to 2020, we
quantify the relationships between SLR and regional GDP per capita across 79
coastal EU & UK regions. Our results reveal that the current SLR has already
negatively influenced GDP of coastal regions, leading to a cumulative 4.7% loss
at 39 cm of SLR. Over the 120 year period studied, the actualised impact of SLR
on the annual growth rate is between -0.02% and 0.04%. Extrapolating these
findings to future climate and socio-economic scenarios, we show that in the
absence of additional adaptation measures, GDP losses by 2100 could range
between -6.3% and -20.8% under the most extreme SLR scenario (SSP5-RCP8.5
High-end Ice, or -4.0% to -14.1% in SSP5-RCP8.5 High Ice). This statistical
analysis utilising a century-long dataset, provides an empirical foundation for
designing region-specific climate adaptation strategies to mitigate economic
damages caused by SLR. Our evidence supports the argument for strategically
relocating assets and establishing coastal setback zones when it is
economically preferable and socially agreeable, given that protection
investments have an economic impact.",2023
http://arxiv.org/abs/2401.00342v2,Equilibrium existence in a discrete-time endogenous growth model with physical and human capital,2023-12-30 22:52:59+00:00,['Luis Alcala'],econ.TH,"This paper studies a discrete-time version of the Lucas-Uzawa endogenous
growth model with physical and human capital in the presence of externalities.
Existence of an optimal equilibrium is proved using tools from dynamic
programming with bounded or unbounded returns. The proofs also rely on
properties of isoelastic utility and homogeneous production functions and apply
well-known inequalities in real analysis, seldom used in the literature, which
significantly simplify the task of verifying certain assumptions that are
rather technical in nature. Some advantages of adopting a parametric family of
isoelastic utility functions, instead of the ad hoc formulation typically used,
are also discussed.",2023
http://arxiv.org/abs/2401.00313v3,Matching of Users and Creators in Two-Sided Markets with Departures,2023-12-30 20:13:28+00:00,"['Daniel Huttenlocher', 'Hannah Li', 'Liang Lyu', 'Asuman Ozdaglar', 'James Siderius']",cs.GT,"Many online platforms of today, including social media sites, are two-sided
markets bridging content creators and users. Most of the existing literature on
platform recommendation algorithms largely focuses on user preferences and
decisions, and does not simultaneously address creator incentives. We propose a
model of content recommendation that explicitly focuses on the dynamics of
user-content matching, with the novel property that both users and creators may
leave the platform permanently if they do not experience sufficient engagement.
In our model, each player decides to participate at each time step based on
utilities derived from the current match: users based on alignment of the
recommended content with their preferences, and creators based on their
audience size. We show that a user-centric greedy algorithm that does not
consider creator departures can result in arbitrarily poor total engagement,
relative to an algorithm that maximizes total engagement while accounting for
two-sided departures. Moreover, in stark contrast to the case where only users
or only creators leave the platform, we prove that with two-sided departures,
approximating maximum total engagement within any constant factor is NP-hard.
We present two practical algorithms, one with performance guarantees under mild
assumptions on user preferences, and another that tends to outperform
algorithms that ignore two-sided departures in practice.",2023
http://arxiv.org/abs/2401.00307v4,Minimalist Market Design: A Framework for Economists with Policy Aspirations,2023-12-30 19:44:53+00:00,['Tayfun Sönmez'],econ.GN,"Minimalist market design is an economic design framework developed from the
perspective of an outsider -- one seeking to improve real institutions without
a commission or official mandate. It offers a structured, ""minimally invasive""
method for reforming institutions from within: identify their mission as
understood by stakeholders, diagnose the root causes of failure, and refine
only those elements that compromise that mission. By fixing what is broken and
leaving the rest intact, the framework respects the tacit knowledge embedded in
long-standing institutions, minimizes unintended consequences, and secures
legitimacy that facilitates adoption.
  Such targeted interventions often call for novel, use-inspired theory
tailored to the institutional context. In this way, minimalist market design
advances both theory and practice through a reciprocal process fostering
collaboration across disciplines and between academic research and real-world
practice.
  Tracing the framework's evolution over twenty-five years of intertwined
progress in theory and real-world implementation across a range of matching
market applications -- including housing allocation, school choice,
living-donor organ exchange for kidney and liver, military branch assignment in
the U.S. Army, the allocation of vaccines and therapies during the COVID-19
pandemic, and the allocation of public jobs and college seats under India's
reservation system -- this monograph reveals a consistent ""less is more"" ethos,
showing how restrained, precisely targeted reforms can yield substantial policy
improvements while advancing fundamental knowledge.",2023
http://arxiv.org/abs/2401.00264v4,Identification of Nonlinear Dynamic Panels under Partial Stationarity,2023-12-30 15:33:10+00:00,"['Wayne Yuan Gao', 'Rui Wang']",econ.EM,"This paper provides a general identification approach for a wide range of
nonlinear panel data models, including binary choice, ordered response, and
other types of limited dependent variable models. Our approach accommodates
dynamic models with any number of lagged dependent variables as well as other
types of endogenous covariates. Our identification strategy relies on a partial
stationarity condition, which allows for not only an unknown distribution of
errors, but also temporal dependencies in errors. We derive partial
identification results under flexible model specifications and establish
sharpness of our identified set in the binary choice setting. We demonstrate
the robust finite-sample performance of our approach using Monte Carlo
simulations, and apply the approach to analyze the empirical application of
income categories using various ordered choice models.",2023
http://arxiv.org/abs/2401.00249v2,Forecasting CPI inflation under economic policy and geopolitical uncertainties,2023-12-30 14:34:22+00:00,"['Shovon Sengupta', 'Tanujit Chakraborty', 'Sunny Kumar Singh']",econ.EM,"Forecasting consumer price index (CPI) inflation is of paramount importance
for both academics and policymakers at the central banks. This study introduces
a filtered ensemble wavelet neural network (FEWNet) to forecast CPI inflation,
which is tested on BRIC countries. FEWNet breaks down inflation data into high
and low-frequency components using wavelets and utilizes them along with other
economic factors (economic policy uncertainty and geopolitical risk) to produce
forecasts. All the wavelet-transformed series and filtered exogenous variables
are fed into downstream autoregressive neural networks to make the final
ensemble forecast. Theoretically, we show that FEWNet reduces the empirical
risk compared to fully connected autoregressive neural networks. FEWNet is more
accurate than other forecasting methods and can also estimate the uncertainty
in its predictions due to its capacity to effectively capture non-linearities
and long-range dependencies in the data through its adaptable architecture.
This makes FEWNet a valuable tool for central banks to manage inflation.",2023
http://arxiv.org/abs/2401.00227v1,Does the World Bank's Ease of Doing Business Index Matter for FDI? Findings from Africa,2023-12-30 13:27:31+00:00,['Bhaso Ndzendze'],econ.GN,"This paper investigates whether foreign investment (FDI) into Africa is at
least partially responsive to World Bank-measured market friendliness.
Specifically, I conducted analyses of four countries between 2009 and 2017,
using cases that represent two of the highest scorers on the bank's Doing
Business index as of 2008 (Mauritius and South Africa) and the two lowest
scorers (DRC and CAR), and subsequently traced all four for growths or declines
in FDI in relation to their scores in the index. The findings show that there
is a moderate association between decreased costs of starting a business and
growth of FDI. Mauritius, South Africa and the DRC reduced their total cost of
starting a business by 71.7%, 143.7% and 122.9% for the entire period, and saw
inward FDI increases of 167.6%, 79.8% and 152.21%, respectively. The CAR
increased the cost of starting businesses but still saw increases in FDI.
However, the country also saw the least amount of growth in FDI at only 13.3%.",2023
http://arxiv.org/abs/2312.17676v1,Robust Inference in Panel Data Models: Some Effects of Heteroskedasticity and Leveraged Data in Small Samples,2023-12-29 16:43:19+00:00,['Annalivia Polselli'],econ.EM,"With the violation of the assumption of homoskedasticity, least squares
estimators of the variance become inefficient and statistical inference
conducted with invalid standard errors leads to misleading rejection rates.
Despite a vast cross-sectional literature on the downward bias of robust
standard errors, the problem is not extensively covered in the panel data
framework. We investigate the consequences of the simultaneous presence of
small sample size, heteroskedasticity and data points that exhibit extreme
values in the covariates ('good leverage points') on the statistical inference.
Focusing on one-way linear panel data models, we examine asymptotic and finite
sample properties of a battery of heteroskedasticity-consistent estimators
using Monte Carlo simulations. We also propose a hybrid estimator of the
variance-covariance matrix. Results show that conventional standard errors are
always dominated by more conservative estimators of the variance, especially in
small samples. In addition, all types of HC standard errors have excellent
performances in terms of size and power tests under homoskedasticity.",2023
http://arxiv.org/abs/2312.17623v3,Decision Theory for Treatment Choice Problems with Partial Identification,2023-12-29 14:27:52+00:00,"['José Luis Montiel Olea', 'Chen Qiu', 'Jörg Stoye']",econ.EM,"We apply classical statistical decision theory to a large class of treatment
choice problems with partial identification. We show that, in a general class
of problems with Gaussian likelihood, all decision rules are admissible; it is
maximin-welfare optimal to ignore all data; and, for severe enough partial
identification, there are infinitely many minimax-regret optimal decision
rules, all of which sometimes randomize the policy recommendation. We uniquely
characterize the minimax-regret optimal rule that least frequently randomizes,
and show that, in some cases, it can outperform other minimax-regret optimal
rules in terms of what we term profiled regret. We analyze the implications of
our results in the aggregation of experimental estimates for policy adoption,
extrapolation of Local Average Treatment Effects, and policy making in the
presence of omitted variable bias.",2023
http://arxiv.org/abs/2401.13688v1,In the Aftermath of Oil Prices Fall of 2014/2015-Socioeconomic Facts and Changes in the Public Policies in the Sultanate of Oman,2023-12-29 10:38:58+00:00,['Osama A. Marzouk'],econ.GN,"Since the start of its national renaissance in 1970, the Sultanate of Oman
(Oman) has gone over a major development in several areas, such as education,
infrastructure, and urbanization. This has been powered by the revenues from
exporting crude oil and natural gas, which together form the skeleton of the
country's economy. In the second half of 2014, the oil prices declined strongly
to about 50% of its price. This was followed by another moderate decline in the
second half of 2015 and the beginning of 2016, leaving the barrel price at a
low level below 30 US$ in January 2016 (as compared to above 110 US$ in June
2014). This drop had direct impacts on the economy of Oman, manifested in a
large budget deficit, reduced governmental expenditure, reduced or cancelled
subsidy of fuels and electricity, increase in the water tariff, and decline in
deposits in banks. The country is coping with this through its 9th five-year
plan (2016-2020), which adopts a strategy of diversifying the income and
relying less on the traditional oil and gas sector. The country has also taken
measures to facilitate private businesses. This article sheds light on these
topics as well as miscellaneous data about Oman.",2023
http://arxiv.org/abs/2312.17529v2,Theorizing the Socio-Cultural Dynamics of Consumer Decision-Making for Participation in Community-Supported Agriculture,2023-12-29 09:24:23+00:00,"['Sota Takagi', 'Yusuke Numazawa', 'Kentaro Katsube', 'Wataru Omukai', 'Miki Saijo', 'Takumi Ohashi']",econ.TH,"In the context of the urgent need to establish sustainable food systems,
Community Supported Agriculture (CSA), in which consumers share risks with
producers, has gained increasing attention. Understanding the factors that
influence consumer participation in CSA is crucial, yet the complete picture
and interrelations of these factors remain unclear in existing studies. This
research adopts a scoping review and the KJ method to elucidate the factors
influencing consumer participation in CSA and to theorize the consumer
participation. In particular, we focus on the dynamics of individual
decision-making for participation, under the premise that individuals are
embedded in socio-cultural environments. We examine the decision-making process
based on the seesaw of expected gains and losses from participation, along with
the reflexivity to the individual and the process of updating decision-making
post-participation. Our study highlights how individual decision-making for
participation is influenced by relationships with others within the embedded
socio-cultural environment, as well as by attachment and connection to the
community. It also shows that discrepancies between expectations and
experiences post-participation, and the transformation of the social capital,
promote the updating of decision-making processes. In addition, among the
factors identified in this study for participation in CSA, the decision to
participate was heavily influenced by expectations of variety of ingredients,
suggesting that other factors such as food education and learning
opportunities, contribution to environmental and social issues, and connections
with people and nature had little impact. Although there are limitations, the
insights gained from this study offer profound implications for stakeholders
and provide valuable insights for more sustainable and efficient CSA practices.",2023
http://arxiv.org/abs/2312.17337v1,Exploring Nature: Datasets and Models for Analyzing Nature-Related Disclosures,2023-12-28 19:42:14+00:00,"['Tobias Schimanski', 'Chiara Colesanti Senni', 'Glen Gostlow', 'Jingwei Ni', 'Tingyu Yu', 'Markus Leippold']",cs.CL,"Nature is an amorphous concept. Yet, it is essential for the planet's
well-being to understand how the economy interacts with it. To address the
growing demand for information on corporate nature disclosure, we provide
datasets and classifiers to detect nature communication by companies. We ground
our approach in the guidelines of the Taskforce on Nature-related Financial
Disclosures (TNFD). Particularly, we focus on the specific dimensions of water,
forest, and biodiversity. For each dimension, we create an expert-annotated
dataset with 2,200 text samples and train classifier models. Furthermore, we
show that nature communication is more prevalent in hotspot areas and directly
effected industries like agriculture and utilities. Our approach is the first
to respond to calls to assess corporate nature communication on a large scale.",2023
http://arxiv.org/abs/2312.17167v1,"The Gatekeeper Effect: The Implications of Pre-Screening, Self-selection, and Bias for Hiring Processes",2023-12-28 17:54:39+00:00,['Moran Koren'],econ.TH,"We study the problem of screening in decision-making processes under
uncertainty, focusing on the impact of adding an additional screening stage,
commonly known as a 'gatekeeper.' While our primary analysis is rooted in the
context of job market hiring, the principles and findings are broadly
applicable to areas such as educational admissions, healthcare patient
selection, and financial loan approvals. The gatekeeper's role is to assess
applicants' suitability before significant investments are made. Our study
reveals that while gatekeepers are designed to streamline the selection process
by filtering out less likely candidates, they can sometimes inadvertently
affect the candidates' own decision-making process. We explore the conditions
under which the introduction of a gatekeeper can enhance or impede the
efficiency of these processes. Additionally, we consider how adjusting
gatekeeping strategies might impact the accuracy of selection decisions. Our
research also extends to scenarios where gatekeeping is influenced by
historical biases, particularly in competitive settings like hiring. We
discover that candidates confronted with a statistically biased gatekeeping
process are more likely to withdraw from applying, thereby perpetuating the
previously mentioned historical biases. The study suggests that measures such
as affirmative action can be effective in addressing these biases. While
centered on hiring, the insights and methodologies from our study have
significant implications for a wide range of fields where screening and
gatekeeping are integral.",2023
http://arxiv.org/abs/2312.17157v1,Discounting the distant future: What do historical bond prices imply about the long term discount rate?,2023-12-28 17:44:28+00:00,"['J. Doyne Farmer', 'John Geanakoplos', 'Matteo G. Richiardi', 'Miquel Montero', 'Josep Perelló', 'Jaume Masoliver']",q-fin.MF,"We present a thorough empirical study on real interest rates by also
including risk aversion through the introduction of the market price of risk.
With the view of complex systems science and its multidisciplinary approach, we
use the theory of bond pricing to study the long term discount rate.
Century-long historical records of 3 month bonds, 10 year bonds, and inflation
allow us to estimate real interest rates for the UK and the US. Real interest
rates are negative about a third of the time and the real yield curves are
inverted more than a third of the time, sometimes by substantial amounts. This
rules out most of the standard bond pricing models, which are designed for
nominal rates that are assumed to be positive. We therefore use the
Ornstein-Uhlenbeck model which allows negative rates and gives a good match to
inversions of the yield curve. We derive the discount function using the method
of Fourier transforms and fit it to the historical data. The estimated long
term discount rate is $1.7$ \% for the UK and $2.2$ \% for the US. The value of
$1.4$ \% used by Stern is less than a standard deviation from our estimated
long run return rate for the UK, and less than two standard deviations of the
estimated value for the US. All of this once more reinforces the support for
immediate and substantial spending to combat climate change.",2023
http://arxiv.org/abs/2312.17123v2,Further Education During Unemployment,2023-12-28 17:00:13+00:00,"['Pauline Leung', 'Zhuan Pei']",econ.GN,"Evidence on the effectiveness of retraining U.S. unemployed workers primarily
comes from evaluations of training programs, which represent one narrow avenue
for skill acquisition. We use high-quality records from Ohio and a matching
method to estimate the effects of retraining, broadly defined as enrollment in
postsecondary institutions. Our simple method bridges two strands of the
dynamic treatment effect literature that estimate the
treatment-now-versus-later and treatment-versus-no-treatment effects. We find
that enrollees experience earnings gains of six percent three to four years
after enrolling, after depressed earnings during the first two years. The
earnings effects are driven by industry-switchers, particularly to healthcare.",2023
http://arxiv.org/abs/2312.17061v2,Bayesian Analysis of High Dimensional Vector Error Correction Model,2023-12-28 15:20:29+00:00,"['Parley R Yang', 'Alexander Y Shestopaloff']",stat.ME,"Vector Error Correction Model (VECM) is a classic method to analyse
cointegration relationships amongst multivariate non-stationary time series. In
this paper, we focus on high dimensional setting and seek for
sample-size-efficient methodology to determine the level of cointegration. Our
investigation centres at a Bayesian approach to analyse the cointegration
matrix, henceforth determining the cointegration rank. We design two algorithms
and implement them on simulated examples, yielding promising results
particularly when dealing with high number of variables and relatively low
number of observations. Furthermore, we extend this methodology to empirically
investigate the constituents of the S&P 500 index, where low-volatility
portfolios can be found during both in-sample training and out-of-sample
testing periods.",2023
http://arxiv.org/abs/2312.16927v1,Development of Choice Model for Brand Evaluation,2023-12-28 09:51:46+00:00,"['Marina Kholod', 'Nikita Mokrenko']",econ.EM,"Consumer choice modeling takes center stage as we delve into understanding
how personal preferences of decision makers (customers) for products influence
demand at the level of the individual. The contemporary choice theory is built
upon the characteristics of the decision maker, alternatives available for the
choice of the decision maker, the attributes of the available alternatives and
decision rules that the decision maker uses to make a choice. The choice set in
our research is represented by six major brands (products) of laundry
detergents in the Japanese market. We use the panel data of the purchases of 98
households to which we apply the hierarchical probit model, facilitated by a
Markov Chain Monte Carlo simulation (MCMC) in order to evaluate the brand
values of six brands. The applied model also allows us to evaluate the tangible
and intangible brand values. These evaluated metrics help us to assess the
brands based on their tangible and intangible characteristics. Moreover,
consumer choice modeling also provides a framework for assessing the
environmental performance of laundry detergent brands as the model uses the
information on components (physical attributes) of laundry detergents.",2023
http://arxiv.org/abs/2312.16878v4,Voting power in the Council of the European Union: A comprehensive sensitivity analysis,2023-12-28 08:07:33+00:00,"['Dóra Gréta Petróczy', 'László Csató']",physics.soc-ph,"The Council of the European Union (EU) is one of the main decision-making
bodies of the EU. A number of decisions require a qualified majority, the
support of 55% of the member states (currently 15) that represent at least 65%
of the total population. We investigate how the power distribution -- based on
the Shapley--Shubik index -- and the proportion of winning coalitions change if
these criteria are modified within reasonable bounds. The power of the two
countries with about 4% of the total population each is found to be almost
flat. The decisiveness index decreases if the population criterion is above 68%
or the states criterion is at least 17. Some quota combinations contradict the
principles of double majority. The proportion of winning coalitions can be
increased from 13.2% to 20.8% (30.1%) such that the maximal relative change in
the Shapley--Shubik indices remains below 3.5% (5.5%). Our results are
indispensable in evaluating any proposal for reforming the qualified majority
voting system.",2023
http://arxiv.org/abs/2312.16789v2,Monitoring with Rich Data,2023-12-28 02:34:18+00:00,"['Mira Frick', 'Ryota Iijima', 'Yuhta Ishii']",econ.TH,"We consider moral hazard problems where a principal has access to rich
monitoring data about an agent's action. Rather than focusing on optimal
contracts (which are known to in general be complicated), we characterize the
optimal rate at which the principal's payoffs can converge to the first-best
payoff as the amount of data grows large. Our main result suggests a novel
rationale for the widely observed binary wage schemes, by showing that such
simple contracts achieve the optimal convergence rate. Notably, in order to
attain the optimal convergence rate, the principal must set a lenient cutoff
for when the agent receives a high vs. low wage. In contrast, we find that
other common contracts where wages vary more finely with observed data (e.g.,
linear contracts) approximate the first-best at a highly suboptimal rate.
Finally, we show that the optimal convergence rate depends only on a simple
summary statistic of the monitoring technology. This yields a detail-free
ranking over monitoring technologies that quantifies their value for incentive
provision in data-rich settings and applies regardless of the agent's specific
utility or cost functions.",2023
http://arxiv.org/abs/2312.16710v1,"Health-related Quality of life, Financial Toxicity, Productivity Loss and Catastrophic Health Expenditures After Lung Cancer Diagnosis in Argentina",2023-12-27 20:17:57+00:00,"['Lucas Gonzalez', 'Andrea Alcaraz', 'Carolina Gabay', 'Monica Castro', 'Silvina Vigo', 'Eduardo Carinci', 'Federico Augustovski']",econ.GN,"Objective: About 12,000 people are diagnosed with lung cancer (LC) each year
in Argentina, and the diagnosis has a significant personal and family. The
objective of this study was to characterize the Health-Related Quality of Life
(HRQoL) and the economic impact in patients with LC and in their households.
Methods: Observational cross-sectional study, through validated structured
questionnaires to patients with a diagnosis of Non-Small Cell Lung Cancer
(NSCLC) in two referral public health care centers in Argentina. Questionnaries
used: Health-Related Quality of life (EuroQol EQ-5D-3L questionnaire);
financial toxicity (COST questionnaire); productivity loss (WPAI-GH, Work
Productivity and Activity Impairment Questionnaire: General Health), and
out-of-pocket expenses. Results: We included 101 consecutive patients (mean age
67.5 years; 55.4% men; 57.4% with advanced disease -stage III/IV-). The mean
EQ-VAS was 68.8 (SD:18.3), with 82.2% describing fair or poor health. The most
affected dimensions were anxiety/depression, pain, and activities of daily
living. Patients reported an average 59% decrease in their ability to perform
regular daily activities as measured by WPAI-GH. 54.5% reported a reduction in
income due to the disease, and 19.8% lost their jobs. The annual economic
productivity loss was estimated at USD 2,465 per person. 70.3%) of patients
reported financial toxicity. The average out-of-pocket expenditure was USD
100.38 per person per month, which represented 18.5% of household income.
Catastrophic expenditures were present in 37.1% of households. When performing
subgroup analysis by disease severity, all outcomes were worse in the
subpopulation with advanced disease. Conclusions Patients with NSCLC treated in
public hospitals in Argentina have significant health-related quality of life
and economic impact, worsening in patients with advanced disease.",2023
http://arxiv.org/abs/2312.16707v1,Modeling Systemic Risk: A Time-Varying Nonparametric Causal Inference Framework,2023-12-27 20:09:57+00:00,"['Jalal Etesami', 'Ali Habibnia', 'Negar Kiyavash']",econ.EM,"We propose a nonparametric and time-varying directed information graph
(TV-DIG) framework to estimate the evolving causal structure in time series
networks, thereby addressing the limitations of traditional econometric models
in capturing high-dimensional, nonlinear, and time-varying interconnections
among series. This framework employs an information-theoretic measure rooted in
a generalized version of Granger-causality, which is applicable to both linear
and nonlinear dynamics. Our framework offers advancements in measuring systemic
risk and establishes meaningful connections with established econometric
models, including vector autoregression and switching models. We evaluate the
efficacy of our proposed model through simulation experiments and empirical
analysis, reporting promising results in recovering simulated time-varying
networks with nonlinear and multivariate structures. We apply this framework to
identify and monitor the evolution of interconnectedness and systemic risk
among major assets and industrial sectors within the financial network. We
focus on cryptocurrencies' potential systemic risks to financial stability,
including spillover effects on other sectors during crises like the COVID-19
pandemic and the Federal Reserve's 2020 emergency response. Our findings
reveals significant, previously underrecognized pre-2020 influences of
cryptocurrencies on certain financial sectors, highlighting their potential
systemic risks and offering a systematic approach in tracking evolving
cross-sector interactions within financial networks.",2023
http://arxiv.org/abs/2312.16698v1,The Green Advantage: Analyzing the Effects of Eco-Friendly Marketing on Consumer Loyalty,2023-12-27 19:30:58+00:00,"['Erfan Mohammadi', 'MohammadMahdi Barzegar', 'Mahdi Nohekhan']",econ.GN,"The idea that marketing, in addition to profitability and sales, should also
consider the consumer's health is not and has not been a far-fetched concept.
It can be stated that there is no longer a way back to producing
environmentally harmful products, and gradually, governmental pressures,
competition, and changing customer attitudes are obliging companies to adopt
and implement a green marketing approach. Over time, concepts such as green
marketing have penetrated marketing literature, making environmental
considerations one of the most important activities of companies. For this
purpose, this research examines the effects of green marketing strategy on
brand loyalty (case study: food exporting companies). The population of this
study consists of 345 employees and managers of companies like Kalleh, Solico,
Pemina, Sorben, Mac, Pol, and Casel, out of which 182 were randomly selected as
a sample using Cochran's formula. This research is practical; the required data
were collected through a survey and questionnaire. The research results
indicate that (1) green marketing strategy significantly affects brand loyalty.
(2) Green products have a significant positive effect on brand loyalty. (3)
Green promotion has a significant positive effect on brand loyalty. (4) Green
distribution has a significant positive effect on brand loyalty. (5) Green
pricing has a significant positive effect on brand loyalty.",2023
http://arxiv.org/abs/2312.16489v1,Best-of-Both-Worlds Linear Contextual Bandits,2023-12-27 09:32:18+00:00,"['Masahiro Kato', 'Shinji Ito']",cs.LG,"This study investigates the problem of $K$-armed linear contextual bandits,
an instance of the multi-armed bandit problem, under an adversarial corruption.
At each round, a decision-maker observes an independent and identically
distributed context and then selects an arm based on the context and past
observations. After selecting an arm, the decision-maker incurs a loss
corresponding to the selected arm. The decision-maker aims to minimize the
cumulative loss over the trial. The goal of this study is to develop a strategy
that is effective in both stochastic and adversarial environments, with
theoretical guarantees. We first formulate the problem by introducing a novel
setting of bandits with adversarial corruption, referred to as the contextual
adversarial regime with a self-bounding constraint. We assume linear models for
the relationship between the loss and the context. Then, we propose a strategy
that extends the RealLinExp3 by Neu & Olkhovskaya (2020) and the
Follow-The-Regularized-Leader (FTRL). The regret of our proposed algorithm is
shown to be upper-bounded by $O\left(\min\left\{\frac{(\log(T))^3}{\Delta_{*}}
+ \sqrt{\frac{C(\log(T))^3}{\Delta_{*}}},\ \
\sqrt{T}(\log(T))^2\right\}\right)$, where $T \in\mathbb{N}$ is the number of
rounds, $\Delta_{*} > 0$ is the constant minimum gap between the best and
suboptimal arms for any context, and $C\in[0, T] $ is an adversarial corruption
parameter. This regret upper bound implies
$O\left(\frac{(\log(T))^3}{\Delta_{*}}\right)$ in a stochastic environment and
by $O\left( \sqrt{T}(\log(T))^2\right)$ in an adversarial environment. We refer
to our strategy as the Best-of-Both-Worlds (BoBW) RealFTRL, due to its
theoretical guarantees in both stochastic and adversarial regimes.",2023
http://arxiv.org/abs/2312.16307v2,Incentive-Aware Synthetic Control: Accurate Counterfactual Estimation via Incentivized Exploration,2023-12-26 19:25:11+00:00,"['Daniel Ngo', 'Keegan Harris', 'Anish Agarwal', 'Vasilis Syrgkanis', 'Zhiwei Steven Wu']",econ.EM,"We consider the setting of synthetic control methods (SCMs), a canonical
approach used to estimate the treatment effect on the treated in a panel data
setting. We shed light on a frequently overlooked but ubiquitous assumption
made in SCMs of ""overlap"": a treated unit can be written as some combination --
typically, convex or linear combination -- of the units that remain under
control. We show that if units select their own interventions, and there is
sufficiently large heterogeneity between units that prefer different
interventions, overlap will not hold. We address this issue by proposing a
framework which incentivizes units with different preferences to take
interventions they would not normally consider. Specifically, leveraging tools
from information design and online learning, we propose a SCM that incentivizes
exploration in panel data settings by providing incentive-compatible
intervention recommendations to units. We establish this estimator obtains
valid counterfactual estimates without the need for an a priori overlap
assumption. We extend our results to the setting of synthetic interventions,
where the goal is to produce counterfactual outcomes under all interventions,
not just control. Finally, we provide two hypothesis tests for determining
whether unit overlap holds for a given panel dataset.",2023
http://arxiv.org/abs/2312.16161v1,Can the creation of separate bidding zones within countries create imbalances in PV uptake? Evidence from Sweden,2023-12-26 18:41:24+00:00,['Johanna Fink'],econ.GN,"This paper estimates how electricity price divergence within Sweden has
affected incentives to invest in photovoltaic (PV) generation between 2016 and
2022 based on a synthetic control approach. Sweden is chosen as the research
subject since it is together with Italy the only EU country with multiple
bidding zones and is facing dramatic divergence in electricity prices between
low-tariff bidding zones in Northern and high-tariff bidding zones in Southern
Sweden since 2020. The results indicate that PV uptake in municipalities
located north of the bidding zone border is reduced by 40.9-48% compared to
their Southern counterparts. Based on these results, the creation of separate
bidding zones within countries poses a threat to the expansion of PV generation
and other renewables since it disincentivizes investment in areas with low
electricity prices.",2023
http://arxiv.org/abs/2312.16099v1,Direct Multi-Step Forecast based Comparison of Nested Models via an Encompassing Test,2023-12-26 15:55:48+00:00,['Jean-Yves Pitarakis'],econ.EM,"We introduce a novel approach for comparing out-of-sample multi-step
forecasts obtained from a pair of nested models that is based on the forecast
encompassing principle. Our proposed approach relies on an alternative way of
testing the population moment restriction implied by the forecast encompassing
principle and that links the forecast errors from the two competing models in a
particular way. Its key advantage is that it is able to bypass the variance
degeneracy problem afflicting model based forecast comparisons across nested
models. It results in a test statistic whose limiting distribution is standard
normal and which is particularly simple to construct and can accommodate both
single period and longer-horizon prediction comparisons. Inferences are also
shown to be robust to different predictor types, including stationary,
highly-persistent and purely deterministic processes. Finally, we illustrate
the use of our proposed approach through an empirical application that explores
the role of global inflation in enhancing individual country specific inflation
forecasts.",2023
http://arxiv.org/abs/2312.15999v1,Pricing with Contextual Elasticity and Heteroscedastic Valuation,2023-12-26 11:07:37+00:00,"['Jianyu Xu', 'Yu-Xiang Wang']",cs.LG,"We study an online contextual dynamic pricing problem, where customers decide
whether to purchase a product based on its features and price. We introduce a
novel approach to modeling a customer's expected demand by incorporating
feature-based price elasticity, which can be equivalently represented as a
valuation with heteroscedastic noise. To solve the problem, we propose a
computationally efficient algorithm called ""Pricing with Perturbation (PwP)"",
which enjoys an $O(\sqrt{dT\log T})$ regret while allowing arbitrary
adversarial input context sequences. We also prove a matching lower bound at
$\Omega(\sqrt{dT})$ to show the optimality regarding $d$ and $T$ (up to $\log
T$ factors). Our results shed light on the relationship between contextual
elasticity and heteroscedastic valuation, providing insights for effective and
practical pricing strategies.",2023
http://arxiv.org/abs/2312.15865v1,Mental Perception of Quality: Green Marketing as a Catalyst for Brand Quality Enhancement,2023-12-26 03:10:43+00:00,"['Saleh Ghobbe', 'Mahdi Nohekhan']",econ.GN,"The environmental conservation issue has led consumers to rethink the
products they purchase. Nowadays, many consumers are willing to pay more for
products that genuinely adhere to environmental standards to support the
environment. Consequently, concepts like green marketing have gradually
infiltrated marketing literature, making environmental considerations one of
the most important activities for companies. Accordingly, this research
investigates the impacts of green marketing strategy on perceived brand quality
(case study: food exporting companies). The study population comprises 345
employees and managers from companies such as Kalleh, Solico, Pemina, Sorbon,
Mac, Pol, and Casel. Using Cochran's formula, a sample of 182 individuals was
randomly selected. This research is practical; the required data were collected
through surveys and questionnaires. The findings indicate that (1) green
marketing strategy has a significant positive effect on perceived brand
quality, (2) green products have a significant positive effect on perceived
brand quality, (3) green promotion has a significant positive effect on
perceived brand quality, (4) green distribution has a significant positive
effect on perceived brand quality, and (5) green pricing has a significant
positive effect on perceived brand quality.",2023
http://arxiv.org/abs/2312.15646v2,A graph-based multimodal framework to predict gentrification,2023-12-25 08:20:50+00:00,"['Javad Eshtiyagh', 'Baotong Zhang', 'Yujing Sun', 'Linhui Wu', 'Zhao Wang']",cs.CY,"Gentrification--the transformation of a low-income urban area caused by the
influx of affluent residents--has many revitalizing benefits. However, it also
poses extremely concerning challenges to low-income residents. To help
policymakers take targeted and early action in protecting low-income residents,
researchers have recently proposed several machine learning models to predict
gentrification using socioeconomic and image features. Building upon previous
studies, we propose a novel graph-based multimodal deep learning framework to
predict gentrification based on urban networks of tracts and essential
facilities (e.g., schools, hospitals, and subway stations). We train and test
the proposed framework using data from Chicago, New York City, and Los Angeles.
The model successfully predicts census-tract level gentrification with 0.9
precision on average. Moreover, the framework discovers a previously unexamined
strong relationship between schools and gentrification, which provides a basis
for further exploration of social factors affecting gentrification.",2023
http://arxiv.org/abs/2312.15624v3,Negative Control Falsification Tests for Instrumental Variable Designs,2023-12-25 06:14:40+00:00,"['Oren Danieli', 'Daniel Nevo', 'Itai Walk', 'Bar Weinstein', 'Dan Zeltzer']",econ.EM,"The validity of instrumental variable (IV) designs is typically tested using
two types of falsification tests. We characterize these tests as conditional
independence tests between negative control variables -- proxies for unobserved
variables posing a threat to the identification -- and the IV or the outcome.
We describe the conditions that variables must satisfy in order to serve as
negative controls. We show that these falsification tests examine not only
independence and the exclusion restriction, but also functional form
assumptions. Our analysis reveals that conventional applications of these tests
may flag problems even in valid IV designs. We offer implementation guidance to
address these issues.",2023
http://arxiv.org/abs/2312.15595v3,Zero-Inflated Bandits,2023-12-25 03:13:21+00:00,"['Haoyu Wei', 'Runzhe Wan', 'Lei Shi', 'Rui Song']",stat.ML,"Many real-world bandit applications are characterized by sparse rewards,
which can significantly hinder learning efficiency. Leveraging problem-specific
structures for careful distribution modeling is recognized as essential for
improving estimation efficiency in statistics. However, this approach remains
under-explored in the context of bandits. To address this gap, we initiate the
study of zero-inflated bandits, where the reward is modeled using a classic
semi-parametric distribution known as the zero-inflated distribution. We
develop algorithms based on the Upper Confidence Bound and Thompson Sampling
frameworks for this specific structure. The superior empirical performance of
these methods is demonstrated through extensive numerical studies.",2023
http://arxiv.org/abs/2312.15563v3,Dynamics of Global Emission Permit Prices and Regional Social Cost of Carbon under Noncooperation,2023-12-24 23:10:30+00:00,"['Yongyang Cai', 'Khyati Malik', 'Hyeseon Shin']",econ.GN,"We build a dynamic multi-region model of climate and economy with emission
permit trading among 12 aggregated regions in the world. We solve for the
dynamic Nash equilibrium under noncooperation, wherein each region adheres to
the emission cap constraints following commitments that were first outlined in
the 2015 Paris Agreement and later strengthened under the Glasgow Pact. Our
model shows that the emission permit price reaches $845 per ton of carbon by
2050, and global average temperature is expected to reach 1.7{\deg}C above the
pre-industrial level by the end of this century. We demonstrate, both
theoretically and numerically, that a regional carbon tax is complementary to
the global cap-and-trade system, and the optimal regional carbon tax is equal
to the difference between the regional marginal abatement cost and the permit
price. We show that optimal regional carbon tax has significant heterogeneity
between regions, and the tax needs to be implemented in both the developed and
the developing regions.",2023
http://arxiv.org/abs/2401.10255v1,Nowcasting Madagascar's real GDP using machine learning algorithms,2023-12-24 20:40:54+00:00,"['Franck Ramaharo', 'Gerzhino Rasolofomanana']",econ.GN,"We investigate the predictive power of different machine learning algorithms
to nowcast Madagascar's gross domestic product (GDP). We trained popular
regression models, including linear regularized regression (Ridge, Lasso,
Elastic-net), dimensionality reduction model (principal component regression),
k-nearest neighbors algorithm (k-NN regression), support vector regression
(linear SVR), and tree-based ensemble models (Random forest and XGBoost
regressions), on 10 Malagasy quarterly macroeconomic leading indicators over
the period 2007Q1--2022Q4, and we used simple econometric models as a
benchmark. We measured the nowcast accuracy of each model by calculating the
root mean square error (RMSE), mean absolute error (MAE), and mean absolute
percentage error (MAPE). Our findings reveal that the Ensemble Model, formed by
aggregating individual predictions, consistently outperforms traditional
econometric models. We conclude that machine learning models can deliver more
accurate and timely nowcasts of Malagasy economic performance and provide
policymakers with additional guidance for data-driven decision making.",2023
http://arxiv.org/abs/2312.15535v1,Forecasting exports in selected OECD countries and Iran using MLP Artificial Neural Network,2023-12-24 18:38:51+00:00,"['Soheila Khajoui', 'Saeid Dehyadegari', 'Sayyed Abdolmajid Jalaee']",econ.GN,"The present study aimed to forecast the exports of a select group of
Organization for Economic Co-operation and Development (OECD) countries and
Iran using the neural networks. The data concerning the exports of the above
countries from 1970 to 2019 were collected. The collected data were implemented
to forecast the exports of the investigated countries for 2021 to 2025. The
analysis was performed using the Multi-Layer-Perceptron (MLP) neural network in
Python. Out of the total number, 75 percent were used as training data, and 25
percent were used as the test data. The findings of the study were evaluated
with 99% accuracy, which indicated the reliability of the output of the
network. The Results show that Covid-19 has affected exports over time.
However, long-term export contracts are less affected by tensions and crises,
due to the effect of exports on economic growth, per capita income and it is
better for economic policies of countries to use long-term export contracts.",2023
http://arxiv.org/abs/2312.15524v2,The Challenge of Using LLMs to Simulate Human Behavior: A Causal Inference Perspective,2023-12-24 16:32:35+00:00,"['George Gui', 'Olivier Toubia']",cs.AI,"Large Language Models (LLMs) have shown impressive potential to simulate
human behavior. We identify a fundamental challenge in using them to simulate
experiments: when LLM-simulated subjects are blind to the experimental design
(as is standard practice with human subjects), variations in treatment
systematically affect unspecified variables that should remain constant,
violating the unconfoundedness assumption. Using demand estimation as a context
and an actual experiment as a benchmark, we show this can lead to implausible
results. While confounding may in principle be addressed by controlling for
covariates, this can compromise ecological validity in the context of LLM
simulations: controlled covariates become artificially salient in the simulated
decision process, which introduces focalism. This trade-off between
unconfoundedness and ecological validity is usually absent in traditional
experimental design and represents a unique challenge in LLM simulations. We
formalize this challenge theoretically, showing it stems from ambiguous
prompting strategies, and hence cannot be fully addressed by improving training
data or by fine-tuning. Alternative approaches that unblind the experimental
design to the LLM show promise. Our findings suggest that effectively
leveraging LLMs for experimental simulations requires fundamentally rethinking
established experimental design practices rather than simply adapting protocols
developed for human subjects.",2023
http://arxiv.org/abs/2312.15494v2,Variable Selection in High Dimensional Linear Regressions with Parameter Instability,2023-12-24 14:52:23+00:00,"['Alexander Chudik', 'M. Hashem Pesaran', 'Mahrad Sharifvaghefi']",econ.EM,"This paper considers the problem of variable selection allowing for parameter
instability. It distinguishes between signal and pseudo-signal variables that
are correlated with the target variable, and noise variables that are not, and
investigate the asymptotic properties of the One Covariate at a Time Multiple
Testing (OCMT) method proposed by Chudik et al. (2018) under parameter
insatiability. It is established that OCMT continues to asymptotically select
an approximating model that includes all the signals and none of the noise
variables. Properties of post selection regressions are also investigated, and
in-sample fit of the selected regression is shown to have the oracle property.
The theoretical results support the use of unweighted observations at the
selection stage of OCMT, whilst applying down-weighting of observations only at
the forecasting stage. Monte Carlo and empirical applications show that OCMT
without down-weighting at the selection stage yields smaller mean squared
forecast errors compared to Lasso, Adaptive Lasso, and boosting.",2023
http://arxiv.org/abs/2312.16214v4,Stochastic Equilibrium the Lucas Critique and Keynesian Economics,2023-12-23 22:59:33+00:00,['David Staines'],econ.TH,"In this paper, a mathematically rigorous solution overturns existing wisdom
regarding New Keynesian Dynamic Stochastic General Equilibrium. I develop a
formal concept of stochastic equilibrium. I prove uniqueness and necessity,
when agents are patient, with general application. Existence depends on
appropriately specified eigenvalue conditions. Otherwise, no solution of any
kind exists. I construct the equilibrium with Calvo pricing. I provide novel
comparative statics with the non-stochastic model of mathematical significance.
I uncover a bifurcation between neighbouring stochastic systems and
approximations taken from the Zero Inflation Non-Stochastic Steady State
(ZINSS). The correct Phillips curve agrees with the zero limit from the trend
inflation framework. It contains a large lagged inflation coefficient and a
small response to expected inflation. Price dispersion can be first or second
order depending how shocks are scaled. The response to the output gap is always
muted and is zero at standard parameters. A neutrality result is presented to
explain why and align Calvo with Taylor pricing. Present and lagged demand
shocks enter the Phillips curve so there is no Divine Coincidence and the
system is identified from structural shocks alone. The lagged inflation slope
is increasing in the inflation response, embodying substantive policy
trade-offs. The Taylor principle is reversed, inactive settings are necessary,
pointing towards inertial policy. The observational equivalence idea of the
Lucas critique is disproven. The bifurcation results from the breakdown of the
constraints implied by lagged nominal rigidity, associated with cross-equation
cancellation possible only at ZINSS. There is a dual relationship between
restrictions on the econometrician and constraints on repricing firms. Thus, if
the model is correct, goodness of fit will jump.",2023
http://arxiv.org/abs/2312.15362v1,Interdependent Total Factor Productivity in an Input-Output model,2023-12-23 22:10:05+00:00,"['Thomas M. Bombarde', 'Andrew L. Krause']",econ.TH,"Industries learn productivity improvements from their suppliers. The observed
empirical importance of these interactions, often omitted by input-output
models, mandates larger attention. This article embeds interdependent total
factor productivity (TFP) growth into a general non-parametric input-output
model. TFP growth is assumed to be Cobb-Douglas in TFP-stocks of adjacent
sectors, where elasticities are the input-output coefficients. Studying how the
steady state of the system reacts to changes in research effort bears insight
for policy and the input-output literature. First, industries higher in the
supply chain see a greater multiplication of their productivity gains. Second,
the presence of `laggard' industries can bottleneck the the rest of the
economy. By deriving these insights formally, we review a canonical method for
aggregating TFP -- Hulten's Theorem -- and show the potential importance of
backward linkages.",2023
http://arxiv.org/abs/2312.15326v4,On Connected Strongly-Proportional Cake-Cutting,2023-12-23 19:08:46+00:00,"['Zsuzsanna Jankó', 'Attila Joó', 'Erel Segal-Halevi', 'Sheung Man Yuen']",math.CO,"We investigate the problem of fairly dividing a divisible heterogeneous
resource, also known as a cake, among a set of agents who may have different
entitlements. We characterize the existence of a connected
strongly-proportional allocation -- one in which every agent receives a
contiguous piece worth strictly more than their proportional share. The
characterization is supplemented with an algorithm that determines its
existence using O(n * 2^n) queries. We devise a simpler characterization for
agents with strictly positive valuations and with equal entitlements, and
present an algorithm to determine the existence of such an allocation using
O(n^2) queries. We provide matching lower bounds in the number of queries for
both algorithms. When a connected strongly-proportional allocation exists, we
show that it can also be computed using a similar number of queries. We also
consider the problem of deciding the existence of a connected allocation of a
cake in which each agent receives a piece worth a small fixed value more than
their proportional share, and the problem of deciding the existence of a
connected strongly-proportional allocation of a pie.",2023
http://arxiv.org/abs/2312.16205v1,Valuing Open Defecation Free Surroundings: Experimental Evidence from a Norm-Based Intervention in India,2023-12-23 11:20:47+00:00,"['Sania Ashraf', 'Cristina Bicchieri', 'Upasak Das. Alex Shpenev']",econ.GN,"Open defecation, which is linked with poor health outcomes, lower cognitive
ability and productivity, has been widespread in India. This paper assesses the
impact of a randomized norm-centric intervention implemented in peri-urban
areas of Tamil Nadu in India on raising the value attached to residence in
areas with a lower prevalence of open defecation, measured through Willingness
to Pay (WTP). The intervention aimed to change social expectations about toilet
usage through audio announcements, wall paintings, household visits, and
community meetings. The findings indicate a significant increase in the WTP for
relocating to areas with lower prevalence of open defecation. The results are
consistent when using local average treatment effect estimations wherein the
possibility of spillovers in the control areas is accounted for. They are also
robust to potential bias due to local socio-political events during the study
period and COVID-led attrition. We further observe a significant increase in
toilet ownership and usage. While assessing the mechanism, we find that change
in empirical expectations through the intervention (what one believes about the
prevalence of toilet usage in the community) is one of the primary mediating
channels. Normative expectations (what one believes about community approval of
toilet usage) are found to have limited effect. The findings underscore the
need for norm-centric interventions to propel change in beliefs and achieve
long-term and sustainable sanitation behavior.",2023
http://arxiv.org/abs/2312.15221v1,"Learning from diversity: jati fractionalization, social expectations and improved sanitation practices in India",2023-12-23 11:04:02+00:00,"['Sania Ashraf', 'Cristina Bicchieri', 'Upasak Das', 'Tanu Gupta', 'Alex Shpenev']",econ.GN,"Prevalence of open defecation is associated with adverse health effects,
detrimental not only for the individual but also the community. Therefore,
neighborhood characteristics can influence collective progressive behavior such
as improved sanitation practices. This paper uses primary data collected from
rural and urban areas of Bihar to study the relationship between jati
(sub-castes) level fractionalization within the community and toilet ownership
and its usage for defecation. The findings indicate a diversity dividend
wherein jati fractionalization is found to improve toilet ownership and usage
significantly. While exploring the channels, we find social expectations to
play an important role, where individuals from diverse communities tend to
believe that there is a higher prevalence of toilet usage within the community.
To assess the reasons for the existence of these social expectations, we use
data from an egocentric network survey on a sub-sample of the households. The
findings reveal that in fractionalized communities, the neighbors with whom our
respondents interacted are more likely to be from different jatis. They are
also more likely to use toilets and approve of its usage due to health reasons.
Discussions about toilets are more common among neighbors from fractionalized
communities, which underscore the discernible role of social learning. The
inferences drawn from the paper have significant implications for community
level behavioral change interventions that aim at reducing open defecation.",2023
http://arxiv.org/abs/2312.15198v3,Do LLM Agents Exhibit Social Behavior?,2023-12-23 08:46:53+00:00,"['Yan Leng', 'Yuan Yuan']",cs.AI,"As LLMs increasingly take on roles in human-AI interactions and autonomous AI
systems, understanding their social behavior becomes important for informed use
and continuous improvement. However, their behaviors in social interactions
with humans and other agents, as well as the mechanisms shaping their
responses, remain underexplored. To address this gap, we introduce a novel
probabilistic framework, State-Understanding-Value-Action (SUVA), to
systematically analyze LLM responses in social contexts based on their textual
outputs (i.e., utterances). Using canonical behavioral economics games and
social preference concepts relatable to LLM users, SUVA assesses LLMs' social
behavior through both their final decisions and the response generation
processes leading to those decisions. Our analysis of eight LLMs -- including
two GPT, four LLaMA, and two Mistral models -- suggests that most models do not
generate decisions aligned solely with self-interest; instead, they often
produce responses that reflect social welfare considerations and display
patterns consistent with direct and indirect reciprocity. Additionally,
higher-capacity models more frequently display group identity effects. The SUVA
framework also provides explainable tools -- including tree-based
visualizations and probabilistic dependency analysis -- to elucidate how
factors in LLMs' utterance-based reasoning influence their decisions. We
demonstrate that utterance-based reasoning reliably predicts LLMs' final
actions; references to altruism, fairness, and cooperation in the reasoning
increase the likelihood of prosocial actions, while mentions of self-interest
and competition reduce them. Overall, our framework enables practitioners to
assess LLMs for applications involving social interactions, and provides
researchers with a structured method to interpret how LLM behavior arises from
utterance-based reasoning.",2023
http://arxiv.org/abs/2312.15119v2,"Functional CLTs for subordinated Lévy models in physics, finance, and econometrics",2023-12-22 23:39:26+00:00,"['Andreas Søjmark', 'Fabrice Wunderlich']",math.PR,"We present a simple unifying treatment of a broad class of applications from
statistical mechanics, econometrics, mathematical finance, and insurance
mathematics, where (possibly subordinated) L\'evy noise arises as a scaling
limit of some form of continuous-time random walk (CTRW). For each application,
it is natural to rely on weak convergence results for stochastic integrals on
Skorokhod space in Skorokhod's J1 or M1 topologies. As compared to earlier and
entirely separate works, we are able to give a more streamlined account while
also allowing for greater generality and providing important new insights. For
each application, we first elucidate how the fundamental conclusions for J1
convergent CTRWs emerge as special cases of the same general principles, and we
then illustrate how the specific settings give rise to different results for
strictly M1 convergent CTRWs.",2023
http://arxiv.org/abs/2401.06163v2,Grassroots Innovation Actors: Their Role and Positioning in Economic Ecosystems -- A Comparative Study Through Complex Network Analysis,2023-12-22 20:05:15+00:00,"['Marcelo S. Tedesco', 'Francisco Javier Ramos Soria']",econ.GN,"This study offers an examination of grassroots innovation actors and their
integration within larger economic ecosystems. Through a comparative analysis
in Oaxaca, Mexico; La Plata, Argentina; and Araucania, Chile, this research
sheds light on the vital role that grassroots innovation plays in broader
economic ecosystems. Using Complex Network Analysis and the TE-SER model, the
study unveils how these actors interact, collaborate, and influence major
economic ecosystems in the context of complex social challenges. The findings
highlight that actors from the grassroots innovation ecosystem make up a
significant portion of the larger innovation-driven entrepreneurial economic
ecosystem, accounting for between 20% and 30% in all three cases and are
strategically positioned within the ecosystem's structural network.
Additionally, this study emphasizes the potential for greater integration of
grassroots innovation actors to leverage resources and foster socio-economic
development. The research concludes by advocating for further studies in
similar socio-economic contexts to enhance our understanding of integration
dynamics and mutual benefits between grassroots innovation ecosystems and other
larger economic systems.",2023
http://arxiv.org/abs/2312.14853v1,In the Line of Fire: A Systematic Review and Meta-Analysis of Job Burnout Among Nurses,2023-12-22 17:25:55+00:00,"['Zahra Ghasemi Kooktapeh', 'Hakimeh Dustmohammadloo', 'Hooman Mehrdoost', 'Farivar Fatehi']",econ.GN,"Using a systematic review and meta-analysis, this study investigates the
impact of the COVID-19 pandemic on job burnout among nurses. We review
healthcare articles following the PRISMA 2020 guidelines and identify the main
aspects and factors of burnout among nurses during the pandemic. Using the
Maslach Burnout questionnaire, we searched PubMed, ScienceDirect, and Google
Scholar, three open-access databases, for relevant sources measuring emotional
burnout, personal failure, and nurse depersonalization. Two reviewers extract
and screen data from the sources and evaluate the risk of bias. The analysis
reveals that 2.75% of nurses experienced job burnout during the pandemic, with
a 95% confidence interval and rates varying from 1.87% to 7.75%. These findings
emphasize the need for interventions to address the pandemic's effect on job
burnout among nurses and enhance their well-being and healthcare quality. We
recommend considering individual, organizational, and contextual factors
influencing healthcare workers' burnout. Future research should focus on
identifying effective interventions to lower burnout in nurses and other
healthcare professionals during pandemics and high-stress situations.",2023
http://arxiv.org/abs/2401.13687v1,Econometric Approach to Analyzing Determinants of Sustained Prosperity,2023-12-22 14:13:45+00:00,['Anika Dixit'],econ.GN,"Every year, substantial resources are allocated to foreign aid with the aim
of catalyzing prosperity and development in recipient countries. The diverse
body of research on the relationship between aid and gross domestic product
(GDP) has yielded varying results, finding evidence of both positive, negative,
and negligible associations between the two. This study employs econometric
techniques, namely Fully Modified Ordinary Least Squares Regression (FMOLS) and
the Generalized Method of Moments (GMM), to explore the intricate links between
innovation and different types of official development assistance (ODA) with
the overarching construct of prosperity. The paper also reviews the linkages
between foundational metrics, such as the rule of law, education, and economic
infrastructure and services, in enabling self-sustaining prosperity. Drawing
upon panel data of relevant determinants for 74 countries across the years 2013
to 2021, the study found that there was a negligible relationship between both
ODA and innovation indices with prosperity. Notably, foreign aid targeted
specifically toward education was observed to have a positive impact on
prosperity, as was the presence of rule of law in a state. The results of the
study are then examined through the lens of a case-study on Reliance Jio,
exemplifying how the company engineered an ecosystem that harnessed resources
and facilitated infrastructure development, thereby contributing to
self-sustaining economic growth and prosperity in India.",2023
http://arxiv.org/abs/2312.14648v2,Inconsistency of Score-Elevated Reserve Policy for Indian Affirmative Action,2023-12-22 12:34:22+00:00,"['Orhan Aygn', 'Bertan Turhan']",econ.TH,"India has enacted an intricate affirmative action program through a
reservation system since the 1950s. Notably, in 2008, a historic judgment by
the Supreme Court of India (SCI) in the case of Ashoka Kumar Thakur vs. Union
of India mandated a 27 percent reservation to the Other Backward Classes (OBC).
The SCI's ruling suggested implementing the OBC reservation as a soft reserve
without defining a procedural framework. The SCI recommended a maximum of 10
points difference between the cutoff scores of the open-category and OBC
positions. We show that this directive conflicts with India's fundamental
Supreme Court mandates on reservation policy. Moreover, we show that the
score-elevated reserve policy proposed by S\""onmez and Yenmez (2022) is
inconsistent with this directive.",2023
http://arxiv.org/abs/2312.14565v2,Improving Task Instructions for Data Annotators: How Clear Rules and Higher Pay Increase Performance in Data Annotation in the AI Economy,2023-12-22 09:50:57+00:00,"['Johann Laux', 'Fabian Stephany', 'Alice Liefgreen']",econ.GN,"The global surge in AI applications is transforming industries, leading to
displacement and complementation of existing jobs, while also giving rise to
new employment opportunities. Data annotation, encompassing the labelling of
images or annotating of texts by human workers, crucially influences the
quality of a dataset directly influences the quality of AI models trained on
it. This paper delves into the economics of data annotation, with a specific
focus on the impact of task instruction design (that is, the choice between
rules and standards as theorised in law and economics) and monetary incentives
on data quality and costs. An experimental study involving 307 data annotators
examines six groups with varying task instructions (norms) and monetary
incentives. Results reveal that annotators provided with clear rules exhibit
higher accuracy rates, outperforming those with vague standards by 14%.
Similarly, annotators receiving an additional monetary incentive perform
significantly better, with the highest accuracy rate recorded in the group
working with both clear rules and incentives (87.5% accuracy). In addition, our
results show that rules are perceived as being more helpful by annotators than
standards and reduce annotators' difficulty in annotating images. These
empirical findings underscore the double benefit of rule-based instructions on
both data quality and worker wellbeing. Our research design allows us to reveal
that, in our study, rules are more cost-efficient in increasing accuracy than
monetary incentives. The paper contributes experimental insights to discussions
on the economical, ethical, and legal considerations of AI technologies.
Addressing policymakers and practitioners, we emphasise the need for a balanced
approach in optimising data annotation processes for efficient and ethical AI
development and usage.",2023
http://arxiv.org/abs/2312.14562v1,Measuring the Concentration of Control in Contemporary Ethereum,2023-12-22 09:47:52+00:00,['Simon Brown'],econ.TH,"Ethereum is undergoing significant changes to its architecture as it evolves.
These changes include its switch to PoS consensus and the introduction of
significant infrastructural changes that do not require a change to the core
protocol, but that fundamentally affect the way users interact with the
network. These changes represent an evolution toward a more modular
architecture, in which there exists new exogenous vectors for centralization.
This paper builds on previous studies of decentralization of Ethereum to
reflect these recent significant changes, and Ethereum's new modular paradigm.",2023
http://arxiv.org/abs/2401.13686v1,Capturing the Tax-Revenue Bracketing System via a predator-prey model: Evidence from South Africa,2023-12-22 04:51:39+00:00,['Leonard Mushunje'],econ.GN,"Revenues obtained from the corporate tax heads play significant roles in any
economy as they can be prioritized for producing public goods and employment
creations, among others. As such, corporate tax revenue should be paid enough
attention. This study, therefore, explores the tax-revenue harvesting system of
an economy where we focused on the corporate tax head. The system comprises
three players; the government and formal and informal firms. We applied the
predator-prey model to model the effect of the government-gazetted tax rate on
corporate survivability. It is a new approach to modeling economic system
relations and games. Critical combinatory points are derived, with stability
analysis provided after that. Dynamics associated with the tax-revenue system
are established and critically analyzed. Lastly, we provide the mathematical
way the system can be optimized for the government to harvest as much Revenue
as possible, including optimal conditions.",2023
http://arxiv.org/abs/2312.14402v1,The Fairness Fair: Bringing Human Perception into Collective Decision-Making,2023-12-22 03:06:24+00:00,['Hadi Hosseini'],cs.AI,"Fairness is one of the most desirable societal principles in collective
decision-making. It has been extensively studied in the past decades for its
axiomatic properties and has received substantial attention from the multiagent
systems community in recent years for its theoretical and computational aspects
in algorithmic decision-making. However, these studies are often not
sufficiently rich to capture the intricacies of human perception of fairness in
the ambivalent nature of the real-world problems. We argue that not only fair
solutions should be deemed desirable by social planners (designers), but they
should be governed by human and societal cognition, consider perceived outcomes
based on human judgement, and be verifiable. We discuss how achieving this goal
requires a broad transdisciplinary approach ranging from computing and AI to
behavioral economics and human-AI interaction. In doing so, we identify
shortcomings and long-term challenges of the current literature of fair
division, describe recent efforts in addressing them, and more importantly,
highlight a series of open research directions.",2023
http://arxiv.org/abs/2312.14355v2,Optimal Strategies for the Decumulation of Retirement Savings under Differing Appetites for Liquidity and Investment Risks,2023-12-22 01:06:47+00:00,"['Benjamin Avanzi', 'Lewis de Felice']",econ.GN,"A retiree's appetite for risk is a common input into the lifetime utility
models that are traditionally used to find optimal strategies for the
decumulation of retirement savings.
  In this work, we consider a retiree with potentially differing appetites for
the key financial risks of decumulation: liquidity risk and investment risk. We
set out to determine whether these differing risk appetites have a significant
impact on the retiree's optimal choice of decumulation strategy. To do so, we
design and implement a framework which selects the optimal decumulation
strategy from a general set of admissible strategies in line with a retiree's
goals, and under differing appetites for the key risks of decumulation.
  Overall, we find significant evidence to suggest that a retiree's differing
appetites for different decumulation risks will impact their optimal choice of
strategy at retirement. Through an illustrative example calibrated to the
Australian context, we find results which are consistent with actual behaviours
in this jurisdiction (in particular, a shallow market for annuities), which
lends support to our framework and may provide some new insight into the
so-called annuity puzzle.",2023
http://arxiv.org/abs/2312.14325v1,Exploring Distributions of House Prices and House Price Indices,2023-12-21 22:38:24+00:00,"['Jiong Liu', 'Hamed Farahani', 'R. A. Serota']",econ.EM,"We use house prices (HP) and house price indices (HPI) as a proxy to income
distribution. Specifically, we analyze sale prices in the 1970-2010 window of
over 116,000 single-family homes in Hamilton County, Ohio, including Cincinnati
metro area of about 2.2 million people. We also analyze HPI, published by
Federal Housing Finance Agency (FHFA), for nearly 18,000 US ZIP codes that
cover a period of over 40 years starting in 1980's. If HP can be viewed as a
first derivative of income, HPI can be viewed as its second derivative. We use
generalized beta (GB) family of functions to fit distributions of HP and HPI
since GB naturally arises from the models of economic exchange described by
stochastic differential equations. Our main finding is that HP and multi-year
HPI exhibit a negative Dragon King (nDK) behavior, wherein power-law
distribution tail gives way to an abrupt decay to a finite upper limit value,
which is similar to our recent findings for realized volatility of S\&P500
index in the US stock market. This type of tail behavior is best fitted by a
modified GB (mGB) distribution. Tails of single-year HPI appear to show more
consistency with power-law behavior, which is better described by a GB Prime
(GB2) distribution. We supplement full distribution fits by mGB and GB2 with
direct linear fits (LF) of the tails. Our numerical procedure relies on
evaluation of confidence intervals (CI) of the fits, as well as of p-values
that give the likelihood that data come from the fitted distributions.",2023
http://arxiv.org/abs/2312.14289v2,The Returns to Science in the Presence of Technological Risk,2023-12-21 20:45:20+00:00,['Matt Clancy'],econ.GN,"Scientific and technological progress has historically been very beneficial
to humanity but this does not always need to be true. Going forward, science
may enable bad actors to cause genetically engineered pandemics that are more
frequent and deadly than prior pandemics. I develop a quantitative economic
model to assess the social returns to science, taking into account benefits to
health and income, and forecast damages from new biological capabilities
enabled by science. I set parameters for this model based on historical trends
and forecasts from a large forecasting tournament of domain experts and
superforecasters, which included forecasts about genetically engineered
pandemic events. The results depend on the forecast likelihood that new
scientific capabilities might lead to the end of our advanced civilization -
there is substantial disagreement about this probability from participants in
the forecasting tournament I use. If I set aside this remote possibility, I
find the expected future social returns to science are strongly positive.
Otherwise, the desirability of accelerating science depends on the value placed
on the long-run future, in addition to which set of (quite different) forecasts
of extinction risk are preferred. I also explore the sensitivity of these
conclusions to a range of alternative assumptions.",2023
http://arxiv.org/abs/2312.14256v4,An extension of May's Theorem to three alternatives: axiomatizing Minimax voting,2023-12-21 19:18:28+00:00,"['Wesley H. Holliday', 'Eric Pacuit']",econ.TH,"May's Theorem [K. O. May, Econometrica 20 (1952) 680-684] characterizes
majority voting on two alternatives as the unique preferential voting method
satisfying several simple axioms. Here we show that by adding some desirable
axioms to May's axioms, we can uniquely determine how to vote on three
alternatives (setting aside tiebreaking). In particular, we add two axioms
stating that the voting method should mitigate spoiler effects and avoid the
so-called strong no show paradox. We prove a theorem stating that any
preferential voting method satisfying our enlarged set of axioms, which
includes some weak homogeneity and preservation axioms, must choose from among
the Minimax winners in all three-alternative elections. When applied to more
than three alternatives, our axioms also distinguish Minimax from other known
voting methods that coincide with or refine Minimax for three alternatives.",2023
http://arxiv.org/abs/2312.14120v1,Name order and the top elite: Long-term effects of a hidden cur-riculum,2023-12-21 18:42:55+00:00,['Eiji Yamamura'],econ.GN,"In Japanese primary and secondary schools, an alphabetical name list is used
in various situations. Generally, students are called on by the teacher during
class and in the cer-emony if their family name is early on the list.
Therefore, students whose surname ini-tials are earlier in the Japanese
alphabet acquire more experience. Surname advantages are considered to have a
long-term positive effect on life in adulthood. This study ex-amines the
surname effect. The data set is constructed by gathering lists of
representative figures from various fields. Based on the list, we calculate the
proportion of surname groups according to Japanese alphabetical column lists.
The major findings are as follows: (1) people whose surnames are in the
A-column (the first column among 10 Japanese name col-umns) are 20% more likely
to appear among the ruling elite but are less likely to ap-pear in
entertainment and sports lists. (2) This tendency is rarely observed in the
Uni-versity of Tokyo entrance examination pass rate. Consequently, the A-column
sur-names are advantageous in helping students succeed as part of the elite
after graduating from universities but not when gaining entry into
universities. The surname helps form non-cognitive skills that help students
become part of the ruling elite instead of specif-ic cognitive skills that help
students enter elite universities. Keywords: Surname, Education, Name order,
Hidden curriculum, Cognitive skill, Non-cognitive skill, Elite, Vice Minister,
Academic, Prestigious university, Enter-tainment, Sports.",2023
http://arxiv.org/abs/2312.14095v1,RetailSynth: Synthetic Data Generation for Retail AI Systems Evaluation,2023-12-21 18:17:16+00:00,"['Yu Xia', 'Ali Arian', 'Sriram Narayanamoorthy', 'Joshua Mabry']",stat.AP,"Significant research effort has been devoted in recent years to developing
personalized pricing, promotions, and product recommendation algorithms that
can leverage rich customer data to learn and earn. Systematic benchmarking and
evaluation of these causal learning systems remains a critical challenge, due
to the lack of suitable datasets and simulation environments. In this work, we
propose a multi-stage model for simulating customer shopping behavior that
captures important sources of heterogeneity, including price sensitivity and
past experiences. We embedded this model into a working simulation environment
-- RetailSynth. RetailSynth was carefully calibrated on publicly available
grocery data to create realistic synthetic shopping transactions. Multiple
pricing policies were implemented within the simulator and analyzed for impact
on revenue, category penetration, and customer retention. Applied researchers
can use RetailSynth to validate causal demand models for multi-category retail
and to incorporate realistic price sensitivity into emerging benchmarking
suites for personalized pricing, promotions, and product recommendations.",2023
http://arxiv.org/abs/2312.13939v1,Binary Endogenous Treatment in Stochastic Frontier Models with an Application to Soil Conservation in El Salvador,2023-12-21 15:32:31+00:00,"['Samuele Centorrino', 'Maria Pérez-Urdiales', 'Boris Bravo-Ureta', 'Alan J. Wall']",econ.EM,"Improving the productivity of the agricultural sector is part of one of the
Sustainable Development Goals set by the United Nations. To this end, many
international organizations have funded training and technology transfer
programs that aim to promote productivity and income growth, fight poverty and
enhance food security among smallholder farmers in developing countries.
Stochastic production frontier analysis can be a useful tool when evaluating
the effectiveness of these programs. However, accounting for treatment
endogeneity, often intrinsic to these interventions, only recently has received
any attention in the stochastic frontier literature. In this work, we extend
the classical maximum likelihood estimation of stochastic production frontier
models by allowing both the production frontier and inefficiency to depend on a
potentially endogenous binary treatment. We use instrumental variables to
define an assignment mechanism for the treatment, and we explicitly model the
density of the first and second-stage composite error terms. We provide
empirical evidence of the importance of controlling for endogeneity in this
setting using farm-level data from a soil conservation program in El Salvador.",2023
http://arxiv.org/abs/2312.13774v2,Investigating Assumptions and Proposals for Blockchain Integration in the Circular Economy. A Delphi Study,2023-12-21 11:59:35+00:00,['Giulio Caldarelli'],cs.CY,"Given the rising interest in the circular economy and blockchain hype,
numerous integrations were proposed. However, studies on the practical
feasibility were scarce, and the assumptions of blockchain potential in the
circular economy were rarely questioned. With the help of eleven of the most
prominent blockchain experts, the present study critically analyzed technology
integration in many areas of the circular economy to forecast their possible
outcomes. Delphi's technique is leveraged to reach a consensus among experts'
visions and opinions. Results support the view that some circular economy
integrations are unlikely to succeed, while others if specific conditions are
met, may prove to be successful in the long run.",2023
http://arxiv.org/abs/2312.13564v2,The Effect of Antitrust Enforcement on Venture Capital Investments,2023-12-21 04:19:00+00:00,['Wentian Zhang'],econ.GN,"This paper studies the effect of antitrust enforcement on venture capital
(VC) investments and VC-backed companies. To establish causality, I exploit the
DOJ's decision to close several antitrust field offices in 2013, which reduced
the antitrust enforcement in areas near the closed offices. I find that the
reduction in antitrust enforcement causes a significant decrease in VC
investments in startups located in the affected areas. Furthermore, these
affected VC-backed startups exhibit a reduced likelihood of successful exits
and diminished innovation performance. These negative results are mainly driven
by startups in concentrated industries, where incumbents tend to engage in
anticompetitive behaviors more frequently. To mitigate the adverse effect,
startups should innovate more to differentiate their products. This paper sheds
light on the importance of local antitrust enforcement in fostering competition
and innovation.",2023
http://arxiv.org/abs/2312.13515v1,Recognising natural capital on the balance sheet: options for water utilities,2023-12-21 01:34:09+00:00,"['Marie-Chantale Pelletier', 'Claire Horner', 'Mathew Vickers', 'Aliya Gul', 'Eren Turak', 'Christine Turner']",econ.GN,"Purpose: The aim of this study was to explore the feasibility of natural
capital accounting for the purpose of strengthening sustainability claims by
reporting entities. The study linked riparian land improvement to ecosystem
services and tested options for incorporating natural capital into financial
accounting practices, specifically on the balance sheet. Methodology: To test
the approach, the study used a public asset manager (a water utility) with
accountabilities to protect the environment including maintaining and enhancing
riparian land assets. Research activities included stakeholder engagement,
physical asset measurement, monetary valuation and financial recognition of
natural capital income and assets. Natural capital income was estimated by
modelling and valuing ecosystem services relating to stormwater filtration and
carbon storage. Findings: This research described how a water utility could
disclose changes in the natural capital assets they manage either through
voluntary disclosures, in notes to the financial statements or as balance sheet
items. We found that current accounting standards allowed the recognition of
some types of environmental income and assets where ecosystem services were
associated with cost savings. The proof-of-concept employed to estimate
environmental income through ecosystem service modelling proved useful to
strengthen sustainability claims or report financial returns on natural capital
investment. Originality/value: This study applied financial accounting
processes and principles to a realistic public asset management scenario with
direct participation by the asset manager working together with academic
researchers and a sub-national government environment management agency.
Importantly it established that natural assets could be included in financial
statements, proposing a new approach to measuring and reporting on natural
capital.",2023
http://arxiv.org/abs/2312.13432v4,Can Digital Aid Deliver During Humanitarian Crises?,2023-12-20 21:17:57+00:00,"['Michael Callen', 'Miguel Fajardo-Steinhäuser', 'Michael G. Findley', 'Tarek Ghani']",econ.GN,"Can digital payments systems help reduce extreme hunger? Humanitarian needs
are at their highest since 1945, aid budgets are falling behind, and hunger is
concentrating in fragile states where repression and aid diversion present
major obstacles. In such contexts, partnering with governments is often neither
feasible nor desirable, making private digital platforms a potentially useful
means of delivering assistance. We experimentally evaluated digital payments to
extremely poor, female-headed households in Afghanistan, as part of a
partnership between community, nonprofit, and private organizations. The
payments led to substantial improvements in food security and mental
well-being. Despite beneficiaries' limited tech literacy, 99.75\% used the
payments, and stringent checks revealed no evidence of diversion. Before seeing
our results, policymakers and experts are uncertain and skeptical about digital
aid, consistent with the lack of prior evidence on digital payments for
humanitarian response. Delivery costs are under 7 cents per dollar, which is 10
cents per dollar less than the World Food Programme's global figure for
cash-based transfers. These savings can help reduce hunger without additional
resources, demonstrating how hybrid partnerships utilizing digital platform
technologies can help address grand challenges in difficult contexts.",2023
http://arxiv.org/abs/2312.13195v3,Principal Component Copulas for Capital Modelling and Systemic Risk,2023-12-20 17:11:34+00:00,"['K. B. Gubbels', 'J. Y. Ypma', 'C. W. Oosterlee']",q-fin.RM,"We introduce a class of copulas that we call Principal Component Copulas
(PCCs). This class combines the strong points of copula-based techniques with
principal component analysis (PCA), which results in flexibility when modelling
tail dependence along the most important directions in high-dimensional data.
We obtain theoretical results for PCCs that are important for practical
applications. In particular, we derive tractable expressions for the
high-dimensional copula density, which can be represented in terms of
characteristic functions. We also develop algorithms to perform Maximum
Likelihood and Generalized Method of Moment estimation in high-dimensions and
show very good performance in simulation experiments. Finally, we apply the
copula to the international stock market to study systemic risk. We find that
PCCs lead to excellent performance on measures of systemic risk due to their
ability to distinguish between parallel and orthogonal movements in the global
market, which have a different impact on systemic risk and diversification. As
a result, we consider the PCC promising for capital models, which financial
institutions use to protect themselves against systemic risk.",2023
http://arxiv.org/abs/2312.14191v2,"Noisy Measurements Are Important, the Design of Census Products Is Much More Important",2023-12-20 15:43:04+00:00,['John M. Abowd'],cs.CR,"McCartan et al. (2023) call for ""making differential privacy work for census
data users."" This commentary explains why the 2020 Census Noisy Measurement
Files (NMFs) are not the best focus for that plea. The August 2021 letter from
62 prominent researchers asking for production of the direct output of the
differential privacy system deployed for the 2020 Census signaled the
engagement of the scholarly community in the design of decennial census data
products. NMFs, the raw statistics produced by the 2020 Census Disclosure
Avoidance System before any post-processing, are one component of that
design-the query strategy output. The more important component is the query
workload output-the statistics released to the public. Optimizing the query
workload-the Redistricting Data (P.L. 94-171) Summary File, specifically-could
allow the privacy-loss budget to be more effectively managed. There could be
fewer noisy measurements, no post-processing bias, and direct estimates of the
uncertainty from disclosure avoidance for each published statistic.",2023
http://arxiv.org/abs/2312.12741v2,Locally Optimal Fixed-Budget Best Arm Identification in Two-Armed Gaussian Bandits with Unknown Variances,2023-12-20 03:28:49+00:00,['Masahiro Kato'],cs.LG,"We address the problem of best arm identification (BAI) with a fixed budget
for two-armed Gaussian bandits. In BAI, given multiple arms, we aim to find the
best arm, an arm with the highest expected reward, through an adaptive
experiment. Kaufmann et al. (2016) develops a lower bound for the probability
of misidentifying the best arm. They also propose a strategy, assuming that the
variances of rewards are known, and show that it is asymptotically optimal in
the sense that its probability of misidentification matches the lower bound as
the budget approaches infinity. However, an asymptotically optimal strategy is
unknown when the variances are unknown. For this open issue, we propose a
strategy that estimates variances during an adaptive experiment and draws arms
with a ratio of the estimated standard deviations. We refer to this strategy as
the Neyman Allocation (NA)-Augmented Inverse Probability weighting (AIPW)
strategy. We then demonstrate that this strategy is asymptotically optimal by
showing that its probability of misidentification matches the lower bound when
the budget approaches infinity, and the gap between the expected rewards of two
arms approaches zero (small-gap regime). Our results suggest that under the
worst-case scenario characterized by the small-gap regime, our strategy, which
employs estimated variance, is asymptotically optimal even when the variances
are unknown.",2023
http://arxiv.org/abs/2312.12700v1,"Performance rating in chess, tennis, and other contexts",2023-12-20 01:47:55+00:00,['Mehmet S. Ismail'],econ.TH,"In this note, I introduce Estimated Performance Rating (PR$^e$), a novel
system for evaluating player performance in sports and games. PR$^e$ addresses
a key limitation of the Tournament Performance Rating (TPR) system, which is
undefined for zero or perfect scores in a series of games. PR$^e$ is defined as
the rating that solves an optimization problem related to scoring probability,
making it applicable for any performance level. The main theorem establishes
that the PR$^e$ of a player is equivalent to the TPR whenever the latter is
defined. I then apply this system to historically significant win-streaks in
association football, tennis, and chess. Beyond sports, PR$^e$ has broad
applicability in domains where Elo ratings are used, from college rankings to
the evaluation of large language models.",2023
http://arxiv.org/abs/2312.12612v2,Stochastic Control Barrier Functions for Economics,2023-12-19 21:34:54+00:00,['David van Wijk'],econ.TH,"Control barrier functions (CBFs) and safety-critical control have seen a
rapid increase in popularity in recent years, predominantly applied to systems
in aerospace, robotics and neural network controllers. Control barrier
functions can provide a computationally efficient method to monitor arbitrary
primary controllers and enforce state constraints to ensure overall system
safety. One area that has yet to take advantage of the benefits offered by CBFs
is the field of finance and economics. This manuscript re-introduces three
applications of traditional control to economics, and develops and implements
CBFs for such problems. We consider the problem of optimal advertising for the
deterministic and stochastic case and Merton's portfolio optimization problem.
Numerical simulations are used to demonstrate the effectiveness of using
traditional control solutions in tandem with CBFs and stochastic CBFs to solve
such problems in the presence of state constraints.",2023
http://arxiv.org/abs/2401.05395v1,SRNI-CAR: A comprehensive dataset for analyzing the Chinese automotive market,2023-12-19 09:32:32+00:00,"['Ruixin Ding', 'Bowei Chen', 'James M. Wilson', 'Zhi Yan', 'Yufei Huang']",econ.GN,"The automotive industry plays a critical role in the global economy, and
particularly important is the expanding Chinese automobile market due to its
immense scale and influence. However, existing automotive sector datasets are
limited in their coverage, failing to adequately consider the growing demand
for more and diverse variables. This paper aims to bridge this data gap by
introducing a comprehensive dataset spanning the years from 2016 to 2022,
encompassing sales data, online reviews, and a wealth of information related to
the Chinese automotive industry. This dataset serves as a valuable resource,
significantly expanding the available data. Its impact extends to various
dimensions, including improving forecasting accuracy, expanding the scope of
business applications, informing policy development and regulation, and
advancing academic research within the automotive sector. To illustrate the
dataset's potential applications in both business and academic contexts, we
present two application examples. Our developed dataset enhances our
understanding of the Chinese automotive market and offers a valuable tool for
researchers, policymakers, and industry stakeholders worldwide.",2023
http://arxiv.org/abs/2312.11942v2,Skills or Degree? The Rise of Skill-Based Hiring for AI and Green Jobs,2023-12-19 08:40:45+00:00,"['Matthew Bone', 'Eugenia Ehlinger', 'Fabian Stephany']",econ.GN,"Emerging professions in fields like Artificial Intelligence (AI) and
sustainability (green jobs) are experiencing labour shortages as industry
demand outpaces labour supply. In this context, our study aims to understand
whether employers have begun focusing more on individual skills rather than
formal qualifications in their recruitment processes. We analysed a large
time-series dataset of approximately eleven million online job vacancies in the
UK from 2018 to mid-2024, drawing on diverse literature on technological change
and labour market signalling. Our findings provide evidence that employers have
initiated ""skill-based hiring"" for AI roles, adopting more flexible hiring
practices to expand the available talent pool. From 2018-2023, demand for AI
roles grew by 21% as a proportion of all postings (and accelerated into 2024).
Simultaneously, mentions of university education requirements for AI roles
declined by 15%. Our regression analysis shows that university degrees have a
significantly lower wage premium for both AI and green roles. In contrast, AI
skills command a wage premium of 23%, exceeding the value of degrees up until
the PhD-level (33%). In occupations with high demand for AI skills, the premium
for skills is high, and the reward for degrees is relatively low. We recommend
leveraging alternative skill-building formats such as apprenticeships,
on-the-job training, MOOCs, vocational education and training,
micro-certificates, and online bootcamps to fully utilise human capital and
address talent shortages.",2023
http://arxiv.org/abs/2401.05393v3,"RIVCoin: an alternative, integrated, CeFi/DeFi-Vaulted Cryptocurrency",2023-12-19 07:46:47+00:00,"['Roberto Rivera', 'Guido Rocco', 'Massimiliano Marzo', 'Enrico Talin', 'Ammar Elsabe']",q-fin.GN,"This whitepaper introduces RIV Coin, a cryptocurrency that is fully
stabilized by a diversified portfolio of invested reserves that are evaluated
by professional independent third parties, and auditable and provable by the
protocol. It is born and managed as a decentralized token, minted by a
Decentralized Autonomous Organization (DAO). All wealthier Users are then
accepting a redistribution of income, to the benefit of those who have
purchased less tokens. In cooperative Game Theory, maximization of the economic
benefit of the ecosystem is achieved when players' incentives are perfectly
aligned. The proposed model allows for alignment of incentives: decreasing the
risk exposure by wealthier Users, but implicitly increasing that of smaller
ones to a level perceived by them as still sustainable and never creating
ultra-speculative positions. In other words, wealthier Users stabilize the risk
associated with the market value of portfolios in which the reserves are
invested in Centralized and Decentralized Finance, without falling into the bet
scheme. Users indirectly benefit from the access to the rewards of
sophisticated cryptocurrency portfolios hitherto precluded to them, as well as
having access to a real redistribution of wealth, without this turning into a
disadvantage for the wealthy User, who benefits from the greater stability
created by the huge influx of smaller Users. Therefore, the progressive growth
becomes additional value that tends to stabilize over time, optimizing RIV Coin
on the systemic risk level.",2023
http://arxiv.org/abs/2312.11806v1,Managing Demographic Transitions: A Comprehensive Analysis of China's Path to Economic Sustainability,2023-12-19 02:40:37+00:00,['Yuxin Hu'],econ.GN,"This article presents an analysis of China's economic evolution amidst
demographic changes from 1990 to 2050, offering valuable insights for academia
and policymakers. It uniquely intertwines various economic theories with
empirical data, examining the impact of an aging population, urbanization, and
family dynamics on labor, demand, and productivity. The study's novelty lies in
its integration of Classical, Neoclassical, and Endogenous Growth theories,
alongside models like Barro and Sala-i-Martin, to contextualize China's
economic trajectory. It provides a forward-looking perspective, utilizing
econometric methods to predict future trends, and suggests practical policy
implications. This comprehensive approach sheds light on managing demographic
transitions in a global context, making it a significant contribution to the
field of demographic economics.",2023
http://arxiv.org/abs/2312.11767v1,"Least-cost diets to teach optimization and consumer behavior, with applications to health equity, poverty measurement and international development",2023-12-19 00:56:44+00:00,"['Jessica K. Wallingford', 'William A. Masters']",econ.GN,"The least-cost diet problem introduces students to optimization and linear
programming, using the health consequences of food choice. We provide a
graphical example, Excel workbook and Word template using actual data on item
prices, food composition and nutrient requirements for a brief exercise in
which students guess at and then solve for nutrient adequacy at lowest cost,
before comparing modeled diets to actual consumption which has varying degrees
of nutrient adequacy. The graphical example is a 'three sisters' diet of corn,
beans and squash, and the full multidimensional model is compared to current
food consumption in Ethiopia. This updated Stigler diet shows how cost
minimization relates to utility maximization, and links to ongoing research and
policy debates about the affordability of healthy diets worldwide.",2023
http://arxiv.org/abs/2312.11711v2,Energy-saving technologies and energy efficiency in the post-pandemic world,2023-12-18 21:15:07+00:00,"['Wadim Strielkowski', 'Larisa Gorina', 'Elena Korneeva', 'Olga Kovaleva']",econ.GN,"This paper explores the role of energy-saving technologies and energy
efficiency in the post-COVID era. The pandemic meant major rethinking of the
entrenched patterns in energy saving and efficiency. It also provided
opportunities for reevaluating energy consumption for households and
industries. In addition, it highlighted the importance of employing digital
tools and technologies in energy networks and smart grids (e.g. Internet of
Energy (IoE), peer-to-peer (P2P) prosumer networks, or the AI-powered
autonomous power systems (APS)). In addition, the pandemic added novel legal
aspects to the energy efficiency and energy saving and enhanced inter-national
collaborations and partnerships. The paper highlights the importance of energy
efficiency measures and examines various technologies that can contribute to a
sustainable and resilient energy future. Using the bibliometric network
analysis of 12960 publications indexed in Web of Science databases, it
demonstrates the potential benefits and challenges associated with implementing
energy-saving technologies and autonomic power systems in a post-COVID world.
Our findings emphasize the need for robust policies, technological
advancements, and public engagement to foster energy efficiency and mitigate
the environmental impacts of energy consumption.",2023
http://arxiv.org/abs/2312.11710v1,Real-time monitoring with RCA models,2023-12-18 21:10:10+00:00,"['Lajos Horváth', 'Lorenzo Trapani']",stat.ME,"We propose a family of weighted statistics based on the CUSUM process of the
WLS residuals for the online detection of changepoints in a Random Coefficient
Autoregressive model, using both the standard CUSUM and the Page-CUSUM process.
We derive the asymptotics under the null of no changepoint for all possible
weighing schemes, including the case of the standardised CUSUM, for which we
derive a Darling-Erdos-type limit theorem; our results guarantee the
procedure-wise size control under both an open-ended and a closed-ended
monitoring. In addition to considering the standard RCA model with no
covariates, we also extend our results to the case of exogenous regressors. Our
results can be applied irrespective of (and with no prior knowledge required as
to) whether the observations are stationary or not, and irrespective of whether
they change into a stationary or nonstationary regime. Hence, our methodology
is particularly suited to detect the onset, or the collapse, of a bubble or an
epidemic. Our simulations show that our procedures, especially when
standardising the CUSUM process, can ensure very good size control and short
detection delays. We complement our theory by studying the online detection of
breaks in epidemiological and housing prices series.",2023
http://arxiv.org/abs/2312.11408v2,Approval-Based Committee Voting in Practice: A Case Study of (Over-)Representation in the Polkadot Blockchain,2023-12-18 18:15:38+00:00,"['Niclas Boehmer', 'Markus Brill', 'Alfonso Cevallos', 'Jonas Gehrlein', 'Luis Sánchez-Fernández', 'Ulrike Schmidt-Kraepelin']",cs.GT,"We provide the first large-scale data collection of real-world approval-based
committee elections. These elections have been conducted on the Polkadot
blockchain as part of their Nominated Proof-of-Stake mechanism and contain
around one thousand candidates and tens of thousands of (weighted) voters each.
We conduct an in-depth study of application-relevant questions, including a
quantitative and qualitative analysis of the outcomes returned by different
voting rules. Besides considering proportionality measures that are standard in
the multiwinner voting literature, we pay particular attention to less-studied
measures of overrepresentation, as these are closely related to the security of
the Polkadot network. We also analyze how different design decisions such as
the committee size affect the examined measures.",2023
http://arxiv.org/abs/2312.11589v1,Moral Uncertainty and the Problem of Fanaticism,2023-12-18 16:09:09+00:00,"['Jazon Szabo', 'Jose Such', 'Natalia Criado', 'Sanjay Modgil']",cs.AI,"While there is universal agreement that agents ought to act ethically, there
is no agreement as to what constitutes ethical behaviour. To address this
problem, recent philosophical approaches to `moral uncertainty' propose
aggregation of multiple ethical theories to guide agent behaviour. However, one
of the foundational proposals for aggregation - Maximising Expected
Choiceworthiness (MEC) - has been criticised as being vulnerable to fanaticism;
the problem of an ethical theory dominating agent behaviour despite low
credence (confidence) in said theory. Fanaticism thus undermines the
`democratic' motivation for accommodating multiple ethical perspectives. The
problem of fanaticism has not yet been mathematically defined. Representing
moral uncertainty as an instance of social welfare aggregation, this paper
contributes to the field of moral uncertainty by 1) formalising the problem of
fanaticism as a property of social welfare functionals and 2) providing
non-fanatical alternatives to MEC, i.e. Highest k-trimmed Mean and Highest
Median.",2023
http://arxiv.org/abs/2312.11283v3,A Simulated Reconstruction and Reidentification Attack on the 2010 U.S. Census,2023-12-18 15:23:12+00:00,"['John M. Abowd', 'Tamara Adams', 'Robert Ashmead', 'David Darais', 'Sourya Dey', 'Simson L. Garfinkel', 'Nathan Goldschlag', 'Michael B. Hawes', 'Daniel Kifer', 'Philip Leclerc', 'Ethan Lew', 'Scott Moore', 'Rolando A. Rodríguez', 'Ramy N. Tadros', 'Lars Vilhuber']",stat.AP,"We show that individual, confidential microdata records from the 2010 U.S.
Census of Population and Housing can be accurately reconstructed from the
published tabular summaries. Ninety-seven million person records (every
resident in 70% of all census blocks) are exactly reconstructed with provable
certainty using only public information. We further show that a hypothetical
attacker using our methods can reidentify with 95% accuracy population unique
individuals who are perfectly reconstructed and not in the modal race and
ethnicity category in their census block (3.4 million persons)--a result that
is only possible because their confidential records were used in the published
tabulations. Finally, we show that the methods used for the 2020 Census, based
on a differential privacy framework, provide better protection against this
type of attack, with better published data accuracy, than feasible
alternatives.",2023
http://arxiv.org/abs/2401.13685v1,Determinants of Hotels and Restaurants entrepreneurship: A study using GEM data,2023-12-18 11:26:38+00:00,"['Antonio Rafael Ramos-Rodriguez', 'Jose Aurelio Medina-Garrido', 'Jose Ruiz-Navarro']",econ.GN,"The objective of this work is to assess the influence of certain factors on
the likelihood of being a Hotels and Restaurants (H&R) entrepreneur. The
factors evaluated are demographic and economic variables, variables related to
perceptions of the environment and personal traits, and variables measuring the
individual's intellectual and social capital. The work uses logistic regression
techniques to analyze a sample of 33,711 individuals in the countries
participating in the GEM project in 2008. The findings show that age, gender,
income, perception of opportunities, fear of failure, entrepreneurial ability,
knowing other entrepreneurs and being a business angel are explanatory factors
of the probability of being an H&R entrepreneur.",2023
http://arxiv.org/abs/2312.11063v1,A survey on algorithms for Nash equilibria in finite normal-form games,2023-12-18 10:00:47+00:00,"['Hanyu Li', 'Wenhan Huang', 'Zhijian Duan', 'David Henry Mguni', 'Kun Shao', 'Jun Wang', 'Xiaotie Deng']",cs.GT,"Nash equilibrium is one of the most influential solution concepts in game
theory. With the development of computer science and artificial intelligence,
there is an increasing demand on Nash equilibrium computation, especially for
Internet economics and multi-agent learning. This paper reviews various
algorithms computing the Nash equilibrium and its approximation solutions in
finite normal-form games from both theoretical and empirical perspectives. For
the theoretical part, we classify algorithms in the literature and present
basic ideas on algorithm design and analysis. For the empirical part, we
present a comprehensive comparison on the algorithms in the literature over
different kinds of games. Based on these results, we provide practical
suggestions on implementations and uses of these algorithms. Finally, we
present a series of open problems from both theoretical and practical
considerations.",2023
http://arxiv.org/abs/2401.13684v1,Global Entrepreneurship Monitor versus Panel Study of Entrepreneurial Dynamics: comparing their intellectual structures,2023-12-18 08:20:21+00:00,"['Antonio Rafael Ramos-Rodriguez', 'Salustiano Martinez-Fierro', 'Jose Aurelio Medina-Garrido', 'Jose Ruiz-Navarro']",econ.GN,"In the past 15 years, two international observatories have been intensively
studying entrepreneurship using empirical studies with different methodologies:
GEM and PSED. Both projects have generated a considerable volume of scientific
production, and their intellectual structures are worth analyzing. The current
work is an exploratory study of the knowledge base of the articles generated by
each of these two observatories and published in prestigious journals. The
value added of this work lies in its novel characterization of the intellectual
structure of entrepreneurship according to the academic production of these two
initiatives. The results may be of interest to the managers and members of
these observatories, as well as to academics, researchers, sponsors and
policymakers interested in entrepreneurship.",2023
http://arxiv.org/abs/2312.11010v1,Endogenous preference for non-market goods in carbon abatement decision,2023-12-18 08:15:32+00:00,"['Fangzhi Wang', 'Hua Liao', 'Richard S. J. Tol', 'Changjing Ji']",econ.GN,"Carbon abatement decisions are usually based on the implausible assumption of
constant social preference. This paper focuses on a specific case of market and
non-market goods, and investigates the optimal climate policy when social
preference for them is also changed by climate policy in the DICE model. The
relative price of non-market goods grows over time due to increases in both
relative scarcity and appreciation of it. Therefore, climbing relative price
brings upward the social cost of carbon denominated in terms of market goods.
Because abatement decision affects the valuation of non-market goods in the
utility function, unlike previous climate-economy models, we solve the model
iteratively by taking the obtained abatement rates from the last run as inputs
in the current run. The results in baseline calibration advocate a more
stringent climate policy, where endogenous social preference to climate policy
raises the social cost of carbon further by roughly 12%-18% this century.
Moreover, neglecting changing social preference leads to an underestimate of
non-market goods damages by 15%. Our results support that climate policy is
self-reinforced if it favors more expensive consumption type.",2023
http://arxiv.org/abs/2312.10984v1,Predicting Financial Literacy via Semi-supervised Learning,2023-12-18 07:12:51+00:00,"['David Hason Rudd', 'Huan Huo', 'Guandong Xu']",cs.LG,"Financial literacy (FL) represents a person's ability to turn assets into
income, and understanding digital currencies has been added to the modern
definition. FL can be predicted by exploiting unlabelled recorded data in
financial networks via semi-supervised learning (SSL). Measuring and predicting
FL has not been widely studied, resulting in limited understanding of customer
financial engagement consequences. Previous studies have shown that low FL
increases the risk of social harm. Therefore, it is important to accurately
estimate FL to allocate specific intervention programs to less financially
literate groups. This will not only increase company profitability, but will
also reduce government spending. Some studies considered predicting FL in
classification tasks, whereas others developed FL definitions and impacts. The
current paper investigated mechanisms to learn customer FL level from their
financial data using sampling by synthetic minority over-sampling techniques
for regression with Gaussian noise (SMOGN). We propose the SMOGN-COREG model
for semi-supervised regression, applying SMOGN to deal with unbalanced datasets
and a nonparametric multi-learner co-regression (COREG) algorithm for labeling.
We compared the SMOGN-COREG model with six well-known regressors on five
datasets to evaluate the proposed models effectiveness on unbalanced and
unlabelled financial data. Experimental results confirmed that the proposed
method outperformed the comparator models for unbalanced and unlabelled
financial data. Therefore, SMOGN-COREG is a step towards using unlabelled data
to estimate FL level.",2023
http://arxiv.org/abs/2312.10896v1,The Market for Lemons and the Regulator's Signalling Problem,2023-12-18 02:55:01+00:00,['Roy Long'],econ.TH,"The Market for Lemons is a classic model of asymmetric information first
studied by Nobel Prize economist George Akerlof. It shows that information
asymmetry between the seller and buyer may result in market collapse or some
sellers leaving the market. ""Lemons"" in the used car market are cars of poor
quality. The information asymmetry present is that the buyer is uncertain of
the cars' true quality. I first offer a simple baseline model that illustrates
the market collapse, and then examine what happens when regulation, ie. a DMV
is introduced to reveal (signal) the true car quality to the buyer. The effect
on the market varies based on the assumptions about the regulator. The central
focus is on the DMV's signal structure, which can have interesting effects on
the market and the information asymmetry. I show that surprisingly, when the
DMV actually decreases the quality of their signal in a well constructed way,
it can substantially increase their profit. On the other hand, this negatively
effects overall welfare.",2023
http://arxiv.org/abs/2312.10749v1,A new behavioral model for portfolio selection using the Half-Full/Half-Empty approach,2023-12-17 15:50:10+00:00,"['Francesco Cesarone', 'Massimiliano Corradini', 'Lorenzo Lampariello', 'Jessica Riccioni']",q-fin.PM,"We focus on a behavioral model, that has been recently proposed in the
literature, whose rational can be traced back to the Half-Full/Half-Empty glass
metaphor. More precisely, we generalize the Half-Full/Half-Empty approach to
the context of positive and negative lotteries and give financial and
behavioral interpretations of the Half-Full/Half-Empty parameters. We develop a
portfolio selection model based on the Half-Full/Half-Empty strategy, resulting
in a nonconvex optimization problem, which, nonetheless, is proven to be
equivalent to an alternative Mixed-Integer Linear Programming formulation. By
means of the ensuing empirical analysis, based on three real-world datasets,
the Half-Full/Half-Empty model is shown to be very versatile by appropriately
varying its parameters, and to provide portfolios displaying promising
performances in terms of risk and profitability, compared with Prospect Theory,
risk minimization approaches and Equally-Weighted portfolios.",2023
http://arxiv.org/abs/2312.10695v5,Nonparametric Strategy Test,2023-12-17 12:09:42+00:00,['Sam Ganzfried'],stat.ME,"We present a nonparametric statistical test for determining whether an agent
is following a given mixed strategy in a repeated strategic-form game given
samples of the agent's play. This involves two components: determining whether
the agent's frequencies of pure strategies are sufficiently close to the target
frequencies, and determining whether the pure strategies selected are
independent between different game iterations. Our integrated test involves
applying a chi-squared goodness of fit test for the first component and a
generalized Wald-Wolfowitz runs test for the second component. The results from
both tests are combined using Bonferroni correction to produce a complete test
for a given significance level $\alpha.$ We applied the test to publicly
available data of human rock-paper-scissors play. The data consists of 50
iterations of play for 500 human players. We test with a null hypothesis that
the players are following a uniform random strategy independently at each game
iteration. Using a significance level of $\alpha = 0.05$, we conclude that 305
(61%) of the subjects are following the target strategy.",2023
http://arxiv.org/abs/2401.08645v1,Automated Design Appraisal: Estimating Real Estate Price Growth and Value at Risk due to Local Development,2023-12-17 11:44:07+00:00,['Adam R. Swietek'],econ.GN,"Financial criteria in architectural design evaluation are limited to cost
performance. Here, I introduce a method, Automated Design Appraisal (ADA), to
predict the market price of a generated building design concept within a local
urban context. Integrating ADA with 3D building performance simulations enables
financial impact assessment that exceeds the spatial resolution of previous
work. Within an integrated impact assessment, ADA measures the direct and
localized effect of urban development. To demonstrate its practical utility, I
study local devaluation risk due to nearby development associated with changes
to visual landscape quality. The results shed light on the relationship between
amenities and property value, identifying clusters of properties physically
exposed or financially sensitive to local land-use change. Beyond its
application as a financial sensitivity tool, ADA serves as a blueprint for
architectural design optimization procedures, in which economic performance is
evaluated based on learned preferences derived from financial market data.",2023
http://arxiv.org/abs/2312.10558v1,Some Finite-Sample Results on the Hausman Test,2023-12-16 23:14:02+00:00,"['Jinyong Hahn', 'Zhipeng Liao', 'Nan Liu', 'Shuyang Sheng']",econ.EM,"This paper shows that the endogeneity test using the control function
approach in linear instrumental variable models is a variant of the Hausman
test. Moreover, we find that the test statistics used in these tests can be
numerically ordered, indicating their relative power properties in finite
samples.",2023
http://arxiv.org/abs/2312.10487v2,The Dynamic Triple Gamma Prior as a Shrinkage Process Prior for Time-Varying Parameter Models,2023-12-16 15:46:32+00:00,"['Peter Knaus', 'Sylvia Frühwirth-Schnatter']",econ.EM,"Many existing shrinkage approaches for time-varying parameter (TVP) models
assume constant innovation variances across time points, inducing sparsity by
shrinking these variances toward zero. However, this assumption falls short
when states exhibit large jumps or structural changes, as often seen in
empirical time series analysis. To address this, we propose the dynamic triple
gamma prior -- a stochastic process that induces time-dependent shrinkage by
modeling dependence among innovations while retaining a well-known triple gamma
marginal distribution. This framework encompasses various special and limiting
cases, including the horseshoe shrinkage prior, making it highly flexible. We
derive key properties of the dynamic triple gamma that highlight its dynamic
shrinkage behavior and develop an efficient Markov chain Monte Carlo algorithm
for posterior sampling. The proposed approach is evaluated through sparse
covariance modeling and forecasting of the returns of the EURO STOXX 50 index,
demonstrating favorable forecasting performance.",2023
http://arxiv.org/abs/2312.10476v1,Sails and Anchors: The Complementarity of Exploratory and Exploitative Scientists in Knowledge Creation,2023-12-16 15:13:06+00:00,"['Pierre Pelletier', 'Kevin Wirtz']",econ.GN,"This paper investigates the relationship between scientists' cognitive
profile and their ability to generate innovative ideas and gain scientific
recognition. We propose a novel author-level metric based on the semantic
representation of researchers' past publications to measure cognitive diversity
both at individual and team levels. Using PubMed Knowledge Graph (PKG), we
analyze the impact of cognitive diversity on novelty, as measured by
combinatorial novelty indicators and peer labels on Faculty Opinion. We
assessed scientific impact through citations and disruption indicators. We show
that the presence of exploratory individuals (i.e., cognitively diverse) is
beneficial in generating distant knowledge combinations, but only when balanced
by a significant proportion of exploitative individuals (i.e., cognitively
specialized). Furthermore, teams with a high proportion of exploitative
profiles tend to consolidate science, whereas those with a significant share of
both profiles tend to disrupt it. Cognitive diversity between team members
appears to be always beneficial to combining more distant knowledge. However,
to maximize the relevance of these distant combinations of knowledge,
maintaining a limited number of exploratory individuals is essential, as
exploitative individuals must question and debate their novel perspectives.
These specialized individuals are the most qualified to extract the full
potential of novel ideas and integrate them within the existing scientific
paradigm.",2023
http://arxiv.org/abs/2312.10405v1,The Fallacy of Borda Count Method -- Why it is Useless with Group Intelligence and Shouldn't be Used with Big Data including Banking Customer Services,2023-12-16 10:08:03+00:00,['Hao Wang'],econ.GN,"Borda Count Method is an important theory in the field of voting theory. The
basic idea and implementation methodology behind the approach is simple and
straight forward. Borda Count Method has been used in sports award evaluations
and many other scenarios, and therefore is an important aspect of our society.
An often ignored ground truth is that online cultural rating platforms such as
Douban.com and Goodreads.com often adopt integer rating values for large scale
public audience, and therefore leading to Poisson/Pareto behavior. In this
paper, we rely on the theory developed by Wang from 2021 to 2023 to demonstrate
that online cultural rating platform rating data often evolve into
Poisson/Pareto behavior, and individualistic voting preferences are predictable
without any data input, so Borda Count Method (or, Range Voting Method) has
intrinsic fallacy and should not be used as a voting theory method.",2023
http://arxiv.org/abs/2312.10333v1,Logit-based alternatives to two-stage least squares,2023-12-16 05:47:43+00:00,"['Denis Chetverikov', 'Jinyong Hahn', 'Zhipeng Liao', 'Shuyang Sheng']",econ.EM,"We propose logit-based IV and augmented logit-based IV estimators that serve
as alternatives to the traditionally used 2SLS estimator in the model where
both the endogenous treatment variable and the corresponding instrument are
binary. Our novel estimators are as easy to compute as the 2SLS estimator but
have an advantage over the 2SLS estimator in terms of causal interpretability.
In particular, in certain cases where the probability limits of both our
estimators and the 2SLS estimator take the form of weighted-average treatment
effects, our estimators are guaranteed to yield non-negative weights whereas
the 2SLS estimator is not.",2023
http://arxiv.org/abs/2312.09843v2,Drivers and Barriers of AI Adoption and Use in Scientific Research,2023-12-15 14:49:13+00:00,"['Stefano Bianchini', 'Moritz Müller', 'Pierre Pelletier']",cs.CY,"New technologies have the power to revolutionize science. It has happened in
the past and is happening again with the emergence of new computational tools,
such as artificial intelligence and machine learning. Despite the documented
impact of these technologies, there remains a significant gap in understanding
the process of their adoption within the scientific community. In this paper,
we draw on theories of scientific and technical human capital to study the
integration of AI in scientific research, focusing on the human capital of
scientists and the external resources available within their network of
collaborators and institutions. We validate our hypotheses on a large sample of
publications from OpenAlex, covering all sciences from 1980 to 2020, and
identify a set key drivers and inhibitors of AI adoption and use in science.
Our results suggest that AI is pioneered by domain scientists with a `taste for
exploration' and who are embedded in a network rich of computer scientists,
experienced AI scientists and early-career researchers; they come from
institutions with high citation impact and a relatively strong publication
history on AI. The access to computing resources only matters for a few
scientific disciplines, such as chemistry and medical sciences. Once AI is
integrated into research, most adoption factors continue to influence its
subsequent reuse. Implications for the organization and management of science
in the evolving era of AI-driven discovery are discussed.",2023
http://arxiv.org/abs/2312.09841v1,Monoculture in Matching Markets,2023-12-15 14:46:54+00:00,"['Kenny Peng', 'Nikhil Garg']",cs.GT,"Algorithmic monoculture arises when many decision-makers rely on the same
algorithm to evaluate applicants. An emerging body of work investigates
possible harms of this kind of homogeneity, but has been limited by the
challenge of incorporating market effects in which the preferences and behavior
of many applicants and decision-makers jointly interact to determine outcomes.
  Addressing this challenge, we introduce a tractable theoretical model of
algorithmic monoculture in a two-sided matching market with many participants.
We use the model to analyze outcomes under monoculture (when decision-makers
all evaluate applicants using a common algorithm) and under polyculture (when
decision-makers evaluate applicants independently). All else equal, monoculture
(1) selects less-preferred applicants when noise is well-behaved, (2) matches
more applicants to their top choice, though individual applicants may be worse
off depending on their value to decision-makers and risk tolerance, and (3) is
more robust to disparities in the number of applications submitted.",2023
http://arxiv.org/abs/2312.09796v1,Better Foundations for Subjective Probability,2023-12-15 13:52:17+00:00,['Sven Neth'],stat.OT,"How do we ascribe subjective probability? In decision theory, this question
is often addressed by representation theorems, going back to Ramsey (1926),
which tell us how to define or measure subjective probability by observable
preferences. However, standard representation theorems make strong rationality
assumptions, in particular expected utility maximization. How do we ascribe
subjective probability to agents which do not satisfy these strong rationality
assumptions? I present a representation theorem with weak rationality
assumptions which can be used to define or measure subjective probability for
partly irrational agents.",2023
http://arxiv.org/abs/2401.13683v1,"Relationship between work-family balance, employee well-being and job performance",2023-12-15 13:43:31+00:00,"['Jose Aurelio Medina-Garrido', 'Jose Maria Biedma-Ferrer', 'Antonio Rafael Ramos-Rodriguez']",econ.GN,"Purpose: To assess the impact of the existence of and access to different
work-family policies on employee well-being and job performance.
  Design-methodology-approach: Hypothesis testing was performed using a
structural equation model based on a PLS-SEM approach applied to a sample of
1,511 employees of the Spanish banking sector.
  Findings: The results obtained demonstrate that the existence and true access
to different types of work-family policies such as flexible working hours
(flexi-time), long leaves, and flexible work location (flexi-place) are not
directly related to job performance, but indirectly so, when mediated by the
well-being of employees generated by work-family policies. In a similar vein,
true access to employee and family support services also has an indirect
positive impact on job performance mediated by the well-being produced. In
contrast, the mere existence of employee and family support services does not
have any direct or indirect effect on job performance.
  Originality-value: This study makes a theoretical and empirical contribution
to better understand the impact that of the existence of and access to
work-family policies on job performance mediated by employee well-being. In
this sense, we posited and tested an unpublished theoretical model where the
concept of employee well-being gains special relevance at academic and
organizational level due to its implications for human resource management.",2023
http://arxiv.org/abs/2401.13682v1,Why not now? Intended timing in entrepreneurial intentions,2023-12-15 10:44:16+00:00,"['Antonio Rafael Ramos-Rodriguez', 'Jose Aurelio Medina-Garrido', 'Jose Ruiz-Navarro']",econ.GN,"Purpose: Understanding the formation of entrepreneurial intentions is
critical, given that it is the first step in the entrepreneurial process.
Although entrepreneurial intention has been extensively studied, little
attention has been paid on the intended timing of future entrepreneurial
projects. This paper analyses entrepreneurial intentions among final-year
university students after graduation in terms of the timeframe to start a
business. Potentially rapid entrepreneurs and entrepreneurs-in-waiting were
compared using the Theory of Planned Behaviour (TPB). Methodology: A
variance-based structural equation modelling approach was used for the sample
of 851 final-year university students with entrepreneurial intentions who
participated in GUESSS project. Findings: The results obtained contribute to
the understanding of how entrepreneurial intentions are formed, particularly,
how intended timing plays a moderating role in the relationships of the
variables of the theoretical model of TPB. This study provides empirical
evidence that significant differences exist between potential rapid
entrepreneurs and entrepreneurs-in-waiting. Practical implications: The
findings of this study have practical implications for entrepreneurship
education, and they can help policy makers develop more effective policies and
programs to promote entrepreneurship. Originality: Intention-based models have
traditionally examined the intent -- but not the timing -- of new venture
creation. However, the time elapsed between the formation of the
entrepreneurial intent and the identification of a business opportunity can
vary considerably. Therefore, analysing the moderating role of intended timing
could be relevant to entrepreneurial intention research.",2023
http://arxiv.org/abs/2312.09479v4,LQG Information Design,2023-12-15 01:36:36+00:00,"['Masaki Miyashita', 'Takashi Ui']",econ.TH,"This paper addresses information design in a workhorse model of network
games, where agents have linear best responses, the information designer
maximizes a quadratic objective, and the payoff-relevant state follows a
multivariate Gaussian distribution. We formulate the problem as a semidefinite
program and establish strong duality to characterize the optimal information
structure. A necessary and sufficient condition for optimality is given by a
simple linear relationship between the induced equilibrium strategy profile and
the state. Leveraging this characterization, we show that the state is fully
revealed in an aggregative form for welfare maximization, while individual
agents may remain only partially informed. When agent roles are
interchangeable, the optimal information structure inherits the same degree of
symmetry, which facilitates computation. In such cases, we show that the
optimal amount of information revealed to each agent is closely linked to the
network's chromatic number.",2023
http://arxiv.org/abs/2312.09081v2,Forecasting skill of a crowd-prediction platform: A comparison of exchange rate forecasts,2023-12-14 16:15:16+00:00,['Niklas Valentin Lehmann'],econ.GN,"Open online crowd-prediction platforms are increasingly used to forecast
trends and complex events. Despite the large body of research on
crowd-prediction and forecasting tournaments, online crowd-prediction platforms
have never been directly compared to other forecasting methods. In this
analysis, exchange rate crowd-predictions made on Metaculus are compared to
predictions made by the random-walk, a statistical model considered extremely
hard-to-beat. The random-walk provides less erroneous forecasts, but the
crowd-prediction does very well. By using the random-walk as a benchmark, this
analysis provides a rare glimpse into the forecasting skill displayed on open
online crowd-prediction platforms.",2023
http://arxiv.org/abs/2312.08799v2,Refined Characterizations of Approval-based Committee Scoring Rules,2023-12-14 10:34:07+00:00,"['Chris Dong', 'Patrick Lederer']",cs.GT,"In approval-based committee (ABC) elections, the goal is to select a
fixed-size subset of the candidates, a so-called committee, based on the
voters' approval ballots over the candidates. One of the most popular classes
of ABC voting rules are ABC scoring rules, which have recently been
characterized by Lackner and Skowron (2021). However, this characterization
relies on a model where the output is a ranking of committees instead of a set
of winning committees and no full characterization of ABC scoring rules exists
in the latter standard setting. We address this issue by characterizing two
important subclasses of ABC scoring rules in the standard ABC election model,
thereby both extending the result of Lackner and Skowron (2021) to the standard
setting and refining it to subclasses. In more detail, by relying on a
consistency axiom for variable electorates, we characterize (i) the prominent
class of Thiele rules and (ii) a new class of ABC voting rules called ballot
size weighted approval voting. Based on these theorems, we also infer
characterizations of three well-known ABC voting rules, namely multi-winner
approval voting, proportional approval voting, and satisfaction approval
voting.",2023
http://arxiv.org/abs/2501.00634v2,Copula Central Asymmetry of Equity Portfolios,2024-12-31 20:18:21+00:00,['Lorenzo Frattarolo'],econ.EM,"Financial crises are usually associated with increased cross-sectional
dependence between asset returns, causing asymmetry between the lower and upper
tail of return distribution. The detection of asymmetric dependence is now
understood to be essential for market supervision, risk management, and
portfolio allocation. I propose a non-parametric test procedure for the
hypothesis of copula central symmetry based on the Cram\'er-von Mises distance
of the empirical copula and its survival counterpart, deriving the asymptotic
properties of the test under standard assumptions for stationary time series. I
use the powerful tie-break bootstrap that, as the included simulation study
implies, allows me to detect asymmetries with up to 25 series and the number of
observations corresponding to one year of daily returns. Applying the procedure
to US portfolio returns separately for each year shows that the amount of
copula central asymmetry is time-varying and less present in the recent past.
Asymmetry is more critical in portfolios based on size and less in portfolios
based on book-to-market and momentum. In portfolios based on industry
classification, asymmetry is present during market downturns, coherently with
the financial contagion narrative.",2024
http://arxiv.org/abs/2501.00633v1,Panel Estimation of Taxable Income Elasticities with Heterogeneity and Endogenous Budget Sets,2024-12-31 20:17:32+00:00,"['Soren Blomquist', 'Anil Kumar', 'Whitney K. Newey']",econ.EM,"This paper introduces an estimator for the average of heterogeneous
elasticities of taxable income (ETI), addressing key econometric challenges
posed by nonlinear budget sets. Building on an isoelastic utility framework, we
derive a linear-in-logs taxable income specification that incorporates the
entire budget set while allowing for individual-specific ETI and productivity
growth. To account for endogenous budget sets, we employ panel data and
estimate individual-specific ridge regressions, constructing a debiased average
of ridge coefficients to obtain the average ETI.",2024
http://arxiv.org/abs/2501.00618v2,An Evaluation of Borda Count Variations Using Ranked Choice Voting Data,2024-12-31 19:36:15+00:00,"['N. Bradley Fox', 'Benjamin Bruyns']",econ.GN,"The standard voting methods in the United States, plurality and ranked choice
(or instant runoff) voting, are susceptible to significant voting failures.
These flaws include Condorcet and majority failures as well as monotonicity and
no-show paradoxes. We investigate alternative ranked choice voting systems
using variations of the points-based Borda count which avoid monotonicity
paradoxes. These variations are based on the way partial ballots are counted
and on extending the values of the points assigned to each rank in the ballot.
In particular, we demonstrate which voting failures are possible for each
variation and then empirically study 421 U.S. ranked choice elections conducted
from 2004 to 2023 to determine the frequency of voting failures when using five
Borda variations. Our analysis demonstrates that the primary vulnerability of
majority failures is rare or nonexistent depending on the variation. Other
voting failures such as truncation or compromise failures occur more frequently
compared to instant runoff voting as a trade-off for avoiding monotonicity
paradoxes.",2024
http://arxiv.org/abs/2501.00578v1,The Limits of Tolerance,2024-12-31 18:09:59+00:00,['Alan D. Miller'],econ.TH,"I propose a model of aggregation of intervals relevant to the study of legal
standards of tolerance. Seven axioms: responsiveness, anonymity, continuity,
strategyproofness, and three variants of neutrality are then used to prove
several important results about a new class of aggregation methods called
endpoint rules. The class of endpoint rules includes extreme tolerance
(allowing anything permitted by anyone) and a form of majoritarianism (the
median rule).",2024
http://arxiv.org/abs/2501.00428v1,"Regression discontinuity aggregation, with an application to the union effects on inequality",2024-12-31 13:10:57+00:00,"['Kirill Borusyak', 'Matan Kolerman-Shemer']",econ.EM,"We extend the regression discontinuity (RD) design to settings where each
unit's treatment status is an average or aggregate across multiple
discontinuity events. Such situations arise in many studies where the outcome
is measured at a higher level of spatial or temporal aggregation (e.g., by
state with district-level discontinuities) or when spillovers from
discontinuity events are of interest. We propose two novel estimation
procedures - one at the level at which the outcome is measured and the other in
the sample of discontinuities - and show that both identify a local average
causal effect under continuity assumptions similar to those of standard RD
designs. We apply these ideas to study the effect of unionization on inequality
in the United States. Using credible variation from close unionization
elections at the establishment level, we show that a higher rate of newly
unionized workers in a state-by-industry cell reduces wage inequality within
the cell.",2024
http://arxiv.org/abs/2501.00382v1,Adventures in Demand Analysis Using AI,2024-12-31 10:33:10+00:00,"['Philipp Bach', 'Victor Chernozhukov', 'Sven Klaassen', 'Martin Spindler', 'Jan Teichert-Kluge', 'Suhas Vijaykumar']",econ.GN,"This paper advances empirical demand analysis by integrating multimodal
product representations derived from artificial intelligence (AI). Using a
detailed dataset of toy cars on \textit{Amazon.com}, we combine text
descriptions, images, and tabular covariates to represent each product using
transformer-based embedding models. These embeddings capture nuanced
attributes, such as quality, branding, and visual characteristics, that
traditional methods often struggle to summarize. Moreover, we fine-tune these
embeddings for causal inference tasks. We show that the resulting embeddings
substantially improve the predictive accuracy of sales ranks and prices and
that they lead to more credible causal estimates of price elasticity. Notably,
we uncover strong heterogeneity in price elasticity driven by these
product-specific features. Our findings illustrate that AI-driven
representations can enrich and modernize empirical demand analysis. The
insights generated may also prove valuable for applied causal inference more
broadly.",2024
http://arxiv.org/abs/2501.00235v2,Robust Intervention in Networks,2024-12-31 03:00:40+00:00,"['Daeyoung Jeong', 'Tongseok Lim', 'Euncheol Shin']",econ.TH,"In economic settings such as learning, social behavior, and financial
contagion, agents interact through interdependent networks. This paper examines
how a decision maker (DM) can design an optimal intervention strategy under
network uncertainty, modeled as a zero-sum game against an adversarial
``Nature'' that reconfigures the network within an uncertainty set. Using
duality, we characterize the DM's unique robust intervention and identify the
worst-case network structure, which exhibits a rank-1 property, concentrating
risk along the intervention strategy. We analyze the costs of robustness,
distinguishing between global and local uncertainty, and examine the role of
higher-order uncertainties in shaping intervention outcomes. Our findings
highlight key trade-offs between maximizing influence and mitigating
uncertainty, offering insights into robust decision-making. This framework has
applications in policy design, economic regulation, and strategic interventions
in dynamic networks, ensuring their resilience against uncertainty in network
structures.",2024
http://arxiv.org/abs/2501.00218v1,How Well Did U.S. Rail and Intermodal Freight Respond to the COVID-19 Pandemic vs. the Great Recession?,2024-12-31 02:12:29+00:00,"['Max T. M. Ng', 'Joseph Schofer', 'Hani S. Mahmassani']",econ.GN,"This paper analyzes and compares patterns of U.S. domestic rail freight
volumes during, and after the disruptions caused by the 2007-2009 Great
Recession and the COVID-19 pandemic in 2020. Trends in rail and intermodal
shipment data are examined in conjunction with economic indicators, focusing on
the extent of drop and recovery of freight volumes of various commodities and
intermodal shipments, and the lead/lag time with respect to economic drivers.
While impacts from and the rebound from the Great Recessions were slow to
develop, COVID-19 produced both profound disruptions in the freight market and
rapid rebound, with important variations across commodity types.
  Energy-related commodities (i.e., coal, petroleum, and fracking sand),
dropped during the pandemic while demand for other commodities (i.e., grain
products and lumber, and intermodal freight). rebounded rapidly and in some
cases grew. Overall rail freight experienced a rapid rebound following the
precipitous drop in traffic in March and April 2020, achieving a near-full
recovery in five months. As the recovery proceeded through 2020, intermodal
flow, containers moving by rail for their longest overland trips, rebounded
strongly, some exceeding 2019 levels. In contrast, rail flows during the Great
Recession changed slowly with the onset and recovery, extending over multiple
years. Pandemic response reflected the impacts of quick shutdowns and a rapid
shift in consumer purchasing patterns. Results for the pandemic illustrate the
resilience of U.S. rail freight industry and the multifaceted role it plays in
the overall logistics system. Amid a challenging logistical environment,
freight rail kept goods moving when other methods of transport were
constrained.",2024
http://arxiv.org/abs/2412.21181v1,Causal Hangover Effects,2024-12-30 18:52:48+00:00,"['Andreas Santucci', 'Eric Lax']",econ.EM,"It's not unreasonable to think that in-game sporting performance can be
affected partly by what takes place off the court. We can't observe what
happens between games directly. Instead, we proxy for the possibility of
athletes partying by looking at play following games in party cities. We are
interested to see if teams exhibit a decline in performance the day following a
game in a city with active nightlife; we call this a ""hangover effect"". Part of
the question is determining a reasonable way to measure levels of nightlife,
and correspondingly which cities are notorious for it; we colloquially refer to
such cities as ""party cities"". To carry out this study, we exploit data on
bookmaker spreads: the expected score differential between two teams after
conditioning on observable performance in past games and expectations about the
upcoming game. We expect a team to meet the spread half the time, since this is
one of the easiest ways for bookmakers to guarantee a profit. We construct a
model which attempts to estimate the causal effect of visiting a ""party city""
on subsequent day performance as measured by the odds of beating the spread. In
particular, we only consider the hangover effect on games played back-to-back
within 24 hours of each other. To the extent that odds of beating the spread
against next day opponent is uncorrelated with playing in a party city the day
before, which should be the case under an efficient betting market, we have
identification in our variable of interest. We find that visiting a city with
active nightlife the day prior to a game does have a statistically significant
negative effect on a team's likelihood of meeting bookmakers' expectations for
both NBA and MLB.",2024
http://arxiv.org/abs/2412.20853v1,Incentive-Compatible Collusion-Resistance via Posted Prices,2024-12-30 10:47:00+00:00,"['Matheus V. X. Ferreira', 'Yotam Gafni', 'Max Resnick']",cs.GT,"We consider a refinement to the notions of collusion-resistance in
transaction fee mechanisms. In particular, we require that the collusion is by
itself incentive-compatible and individually rational to all of its
participants. We then study the structural properties of these notions, and
importantly, characterize the class of collusion-resistant and
incentive-compatible transaction fee mechanisms in the single bidder case, and
show that this is exactly the class of posted-price where the price is not too
prohibitive. We analyze welfare and revenue implications, as well as the shape
of the solution space, for both regular and non-regular distributions.",2024
http://arxiv.org/abs/2501.01448v1,Agency-Driven Labor Theory: A Framework for Understanding Human Work in the AI Age,2024-12-30 04:55:06+00:00,['Venkat Ram Reddy Ganuthula'],econ.GN,"This paper introduces Agency-Driven Labor Theory as a new theoretical
framework for understanding human work in AI-augmented environments. While
traditional labor theories have focused primarily on task execution and labor
time, ADLT proposes that human labor value is increasingly derived from agency
- the capacity to make informed judgments, provide strategic direction, and
design operational frameworks for AI systems. The paper presents a mathematical
framework expressing labor value as a function of agency quality, direction
effectiveness, and outcomes, providing a quantifiable approach to analyzing
human value creation in AI-augmented workplaces. Drawing on recent work in
organizational economics and knowledge worker productivity, ADLT explains how
human workers create value by orchestrating complex systems that combine human
and artificial intelligence. The theory has significant implications for job
design, compensation structures, professional development, and labor market
dynamics. Through applications across various sectors, the paper demonstrates
how ADLT can guide organizations in managing the transition to AI-augmented
operations while maximizing human value creation. The framework provides
practical tools for policymakers and educational institutions as they prepare
workers for a labor market where value creation increasingly centers on agency
and direction rather than execution.",2024
http://arxiv.org/abs/2501.01447v2,Analyzing Country-Level Vaccination Rates and Determinants of Practical Capacity to Administer COVID-19 Vaccines,2024-12-30 02:48:40+00:00,"['Sharika J. Hegde', 'Max T. M. Ng', 'Marcos Rios', 'Hani S. Mahmassani', 'Ying Chen', 'Karen Smilowitz']",econ.GN,"The COVID-19 vaccine development, manufacturing, transportation, and
administration proved an extreme logistics operation of global magnitude.
Global vaccination levels, however, remain a key concern in preventing the
emergence of new strains and minimizing the impact of the pandemic's disruption
of daily life. In this paper, country-level vaccination rates are analyzed
through a queuing framework to extract service rates that represent the
practical capacity of a country to administer vaccines. These rates are further
characterized through regression and interpretable machine learning methods
with country-level demographic, governmental, and socio-economic variates.
Model results show that participation in multi-governmental collaborations such
as COVAX may improve the ability to vaccinate. Similarly, improved
transportation and accessibility variates such as roads per area for low-income
countries and rail lines per area for high-income countries can improve rates.
It was also found that for low-income countries specifically, improvements in
basic and health infrastructure (as measured through spending on healthcare,
number of doctors and hospital beds per 100k, population percent with access to
electricity, life expectancy, and vehicles per 1000 people) resulted in higher
vaccination rates. Of the high-income countries, those with larger 65-plus
populations struggled to vaccinate at high rates, indicating potential
accessibility issues for the elderly. This study finds that improving basic and
health infrastructure, focusing on accessibility in the last mile, particularly
for the elderly, and fostering global partnerships can improve logistical
operations of such a scale. Such structural impediments and inequities in
global health care must be addressed in preparation for future global public
health crises.",2024
http://arxiv.org/abs/2412.20669v1,Econometric Analysis of Pandemic Disruption and Recovery Trajectory in the U.S. Rail Freight Industry,2024-12-30 02:48:34+00:00,"['Max T. M. Ng', 'Hani S. Mahmassani', 'Joseph L. Schofer']",econ.EM,"To measure the impacts on U.S. rail and intermodal freight by economic
disruptions of the 2007-09 Great Recession and the COVID-19 pandemic, this
paper uses time series analysis with the AutoRegressive Integrated Moving
Average (ARIMA) family of models and covariates to model intermodal and
commodity-specific rail freight volumes based on pre-disruption data. A
framework to construct scenarios and select parameters and variables is
demonstrated. By comparing actual freight volumes during the disruptions
against three counterfactual scenarios, Trend Continuation, Covariate-adapted
Trend Continuation, and Full Covariate-adapted Prediction, the characteristics
and differences in magnitude and timing between the two disruptions and their
effects across nine freight components are examined.
  Results show the disruption impacts differ from measurement by simple
comparison with pre-disruption levels or year-on-year comparison depending on
the structural trend and seasonal pattern. Recovery Pace Plots are introduced
to support comparison in recovery speeds across freight components. Accounting
for economic variables helps improve model fitness. It also enables evaluation
of the change in association between freight volumes and covariates, where
intermodal freight was found to respond more slowly during the pandemic,
potentially due to supply constraint.",2024
http://arxiv.org/abs/2412.20447v2,"Cool, But What About Oracles? An Oracle-Based Perspective on Blockchain Integration in the Accounting Field",2024-12-29 12:09:19+00:00,['Giulio Caldarelli'],cs.CR,"The Bitcoin Network is a sophisticated accounting system that allows its
underlying cryptocurrency to be trusted even in the absence of a reliable
financial authority. Given its undeniable success, the technology, generally
referred to as blockchain, has also been proposed as a means to improve legacy
accounting systems. Accounting for real-world data, however, requires the
intervention of a third party known as an Oracle, which, having not the same
characteristics as a blockchain, could potentially reduce the expected
integration benefit. Through a systematic review of the literature, this study
aims to investigate whether the papers concerning blockchain integration in
accounting consider and address the limitations posed by oracles. A broad
overview of the limitations that emerged in the literature is provided and
distinguished according to the specific accounting integration. Results support
the view that although research on the subject counts numerous articles, actual
studies considering oracle limitations are lacking. Interestingly, despite the
scarce production of papers addressing oracles in various accounting sectors,
reporting for ESG already shows interesting workarounds for oracle limitations,
with permissioned chains envisioned as a valid support for the safe storage of
sustainability data.",2024
http://arxiv.org/abs/2412.20438v1,Integrating Natural Language Processing Techniques of Text Mining Into Financial System: Applications and Limitations,2024-12-29 11:25:03+00:00,"['Denisa Millo', 'Blerina Vika', 'Nevila Baci']",cs.CL,"The financial sector, a pivotal force in economic development, increasingly
uses the intelligent technologies such as natural language processing to
enhance data processing and insight extraction. This research paper through a
review process of the time span of 2018-2023 explores the use of text mining as
natural language processing techniques in various components of the financial
system including asset pricing, corporate finance, derivatives, risk
management, and public finance and highlights the need to address the specific
problems in the discussion section. We notice that most of the research
materials combined probabilistic with vector-space models, and text-data with
numerical ones. The most used technique regarding information processing is the
information classification technique and the most used algorithms include the
long-short term memory and bidirectional encoder models. The research noticed
that new specific algorithms are developed and the focus of the financial
system is mainly on asset pricing component. The research also proposes a path
from engineering perspective for researchers who need to analyze financial
text. The challenges regarding text mining perspective such as data quality,
context-adaption and model interpretability need to be solved so to integrate
advanced natural language processing models and techniques in enhancing
financial analysis and prediction. Keywords: Financial System (FS), Natural
Language Processing (NLP), Software and Text Engineering, Probabilistic,
Vector-Space, Models, Techniques, TextData, Financial Analysis.",2024
http://arxiv.org/abs/2412.20420v1,Automated Demand Forecasting in small to medium-sized enterprises,2024-12-29 10:05:47+00:00,"['Thomas Gaertner', 'Christoph Lippert', 'Stefan Konigorski']",econ.EM,"In response to the growing demand for accurate demand forecasts, this
research proposes a generalized automated sales forecasting pipeline tailored
for small- to medium-sized enterprises (SMEs). Unlike large corporations with
dedicated data scientists for sales forecasting, SMEs often lack such
resources. To address this, we developed a comprehensive forecasting pipeline
that automates time series sales forecasting, encompassing data preparation,
model training, and selection based on validation results.
  The development included two main components: model preselection and the
forecasting pipeline. In the first phase, state-of-the-art methods were
evaluated on a showcase dataset, leading to the selection of ARIMA, SARIMAX,
Holt-Winters Exponential Smoothing, Regression Tree, Dilated Convolutional
Neural Networks, and Generalized Additive Models. An ensemble prediction of
these models was also included. Long-Short-Term Memory (LSTM) networks were
excluded due to suboptimal prediction accuracy, and Facebook Prophet was
omitted for compatibility reasons.
  In the second phase, the proposed forecasting pipeline was tested with SMEs
in the food and electric industries, revealing variable model performance
across different companies. While one project-based company derived no benefit,
others achieved superior forecasts compared to naive estimators.
  Our findings suggest that no single model is universally superior. Instead, a
diverse set of models, when integrated within an automated validation
framework, can significantly enhance forecasting accuracy for SMEs. These
results emphasize the importance of model diversity and automated validation in
addressing the unique needs of each business. This research contributes to the
field by providing SMEs access to state-of-the-art sales forecasting tools,
enabling data-driven decision-making and improving operational efficiency.",2024
http://arxiv.org/abs/2412.20285v1,Consumption Periods in Advance Selling Auctions: Evidence from US Timber Market,2024-12-28 21:46:34+00:00,"['Shosuke Noguchi', 'Suguru Otani']",econ.GN,"This study investigates products sold before consumption and examines how the
duration of the consumption periods and the choice of selling mechanism
influence sellers' revenue. Using empirical data from timber auctions, we
identify buyers' tendency to delay consumption to resolve payoff uncertainty
and reveal heterogeneous motivations among buyers. Through structural
estimation, we uncover key parameters for each buyer type, including
sensitivity to realized payoffs and consumption-related costs. Leveraging these
estimates, we perform counterfactual analyses to propose revenue-enhancing
consumption periods and selling mechanisms. Our findings suggest that extending
the consumption periods is likely to increase revenue, with the magnitude
depending on the selling mechanism, the composition of buyer types, and the
number of interested buyers.",2024
http://arxiv.org/abs/2501.00058v2,European Defence Readiness,2024-12-28 17:41:46+00:00,"[""d'Artis Kancs""]",econ.GN,"The preparedness and readiness of Europe is currently being challenged not
only by Russia, but since recently also by its long-standing allies. In
response to the evolving external security environment, the EU's White Paper on
European Defence Readiness 2030 outlines the key defence issues in Europe -
including critical capability gaps of forces, challenges of the defence
industry such as fragmented defence market and military mobility. Leveraging an
empirically validated general equilibrium multi-sector model with ambiguity and
risk, this study assesses how prepared is Europe to address protracted
conflicts and systemic shocks. Exploring what strategies could enhance European
readiness, scenario analysis of a hypothetical total trade war imply that
today's existing problems will only be amplified by systemic shocks, if not
addressed timely and in a targeted way.",2024
http://arxiv.org/abs/2412.20204v2,Fitting Dynamically Misspecified Models: An Optimal Transportation Approach,2024-12-28 16:34:21+00:00,"['Jean-Jacques Forneron', 'Zhongjun Qu']",econ.EM,"This paper considers filtering, parameter estimation, and testing for
potentially dynamically misspecified state-space models. When dynamics are
misspecified, filtered values of state variables often do not satisfy model
restrictions, making them hard to interpret, and parameter estimates may fail
to characterize the dynamics of filtered variables. To address this, a
sequential optimal transportation approach is used to generate a
model-consistent sample by mapping observations from a flexible reduced-form to
the structural conditional distribution iteratively. Filtered series from the
generated sample are model-consistent. Specializing to linear processes, a
closed-form Optimal Transport Filtering algorithm is derived. Minimizing the
discrepancy between generated and actual observations defines an Optimal
Transport Estimator. Its large sample properties are derived. A specification
test determines if the model can reproduce the sample path, or if the
discrepancy is statistically significant. Empirical applications to trend-cycle
decomposition, DSGE models, and affine term structure models illustrate the
methodology and the results.",2024
http://arxiv.org/abs/2412.20176v1,The impact of China's economic growth on poverty alleviation: From absolute to relative poverty,2024-12-28 15:12:44+00:00,"['Yixun Kang', 'Ying Li']",econ.GN,"This paper investigates the extent to which China's economic growth and
development influence poverty levels, focusing on the dichotomy between
absolute and relative poverty. Leveraging data from sources like the World
Bank, Statista, and Macrotrends, and employing economic frameworks such as the
Lewis Model, Poverty Headcount Ratio, and Gini Coefficient, the study examines
China's transformation from combating absolute poverty to addressing relative
poverty. The findings highlight that robust economic growth from 2011 to 2022,
driven by urban development and rural infrastructure investments, successfully
eradicated absolute poverty and elevated rural incomes. However, this progress
also exacerbated income inequality, as evidenced by a rising Gini Coefficient,
complicating efforts to alleviate relative poverty. Through multidimensional
analyses encompassing regional disparities, migration patterns, educational
access, and societal factors, the paper underscores the dual impact of economic
development on poverty alleviation. It concludes by advocating for policies
that balance economic growth with equitable resource distribution to tackle
persistent relative poverty and foster sustainable development.",2024
http://arxiv.org/abs/2412.20173v3,Debiased Nonparametric Regression for Statistical Inference and Distributionally Robustness,2024-12-28 15:01:19+00:00,['Masahiro Kato'],stat.ME,"This study proposes a debiasing method for smooth nonparametric estimators.
While machine learning techniques such as random forests and neural networks
have demonstrated strong predictive performance, their theoretical properties
remain relatively underexplored. In particular, many modern algorithms lack
guarantees of pointwise and uniform risk convergence, as well as asymptotic
normality. These properties are essential for statistical inference and robust
estimation and have been well-established for classical methods such as
Nadaraya-Watson regression. To ensure these properties for various
nonparametric regression estimators, we introduce a model-free debiasing
method. By incorporating a correction term that estimates the conditional
expected residual of the original estimator, or equivalently, its estimation
error, into the initial nonparametric regression estimator, we obtain a
debiased estimator that satisfies pointwise and uniform risk convergence, along
with asymptotic normality, under mild smoothness conditions. These properties
facilitate statistical inference and enhance robustness to covariate shift,
making the method broadly applicable to a wide range of nonparametric
regression problems.",2024
http://arxiv.org/abs/2412.19931v1,Pivoting B2B platform business models: From platform experimentation to multi-platform integration to ecosystem envelopment,2024-12-27 21:34:05+00:00,"['Clara Filosa', 'Marin Jovanovic', 'Lara Agostini', 'Anna Nosella']",econ.GN,"The landscape of digital servitization in the manufacturing sector is
evolving, marked by a strategic shift from traditional product-centric to
platform business models (BMs). Manufacturing firms often employ a blend of
approaches to develop business-to-business (B2B) platforms, leading to
significant reconfigurations in their BMs. However, they frequently encounter
failures in their B2B platform development initiatives, leading them to abandon
initial efforts and pivot to alternative platform strategies. Therefore, this
study, through an in-depth case study of a manufacturer in the energy sector,
articulates a three-phase pivoting framework for B2B platform BMs, including
platform development and platform strategy. Initially, the manufacturer focused
on asset-based product sales supplemented by asset maintenance services and
followed an emergent platformization strategy characterized by the rise of
multiple, independent B2B platforms catering to diverse functions. Next,
focusing on the imposed customer journey strategy, the firm shifted towards a
strategic multi-platform integration into an all-encompassing platform
supported by artificial intelligence (AI), signaling a maturation of the
platform BM to combine a wide range of services into an
energy-performance-based contract. Finally, the last step of the firm's
platform BM evolution consisted of a deliberate platform strategy open to
external stakeholders and enveloping its data-driven offerings within a broader
platform ecosystem. This article advances B2B platform BMs and digital
servitization literature, highlighting the efficacy of a progressive approach
and strategic pivoting.",2024
http://arxiv.org/abs/2501.07580v1,Assets Forecasting with Feature Engineering and Transformation Methods for LightGBM,2024-12-27 18:37:08+00:00,['Konstantinos-Leonidas Bisdoulis'],q-fin.ST,"Fluctuations in the stock market rapidly shape the economic world and
consumer markets, impacting millions of individuals. Hence, accurately
forecasting it is essential for mitigating risks, including those associated
with inactivity. Although research shows that hybrid models of Deep Learning
(DL) and Machine Learning (ML) yield promising results, their computational
requirements often exceed the capabilities of average personal computers,
rendering them inaccessible to many. In order to address this challenge in this
paper we optimize LightGBM (an efficient implementation of gradient-boosted
decision trees (GBDT)) for maximum performance, while maintaining low
computational requirements. We introduce novel feature engineering techniques
including indicator-price slope ratios and differences of close and open prices
divided by the corresponding 14-period Exponential Moving Average (EMA),
designed to capture market dynamics and enhance predictive accuracy.
Additionally, we test seven different feature and target variable
transformation methods, including returns, logarithmic returns, EMA ratios and
their standardized counterparts as well as EMA difference ratios, so as to
identify the most effective ones weighing in both efficiency and accuracy. The
results demonstrate Log Returns, Returns and EMA Difference Ratio constitute
the best target variable transformation methods, with EMA ratios having a lower
percentage of correct directional forecasts, and standardized versions of
target variable transformations requiring significantly more training time.
Moreover, the introduced features demonstrate high feature importance in
predictive performance across all target variable transformation methods. This
study highlights an accessible, computationally efficient approach to stock
market forecasting using LightGBM, making advanced forecasting techniques more
widely attainable.",2024
http://arxiv.org/abs/2412.19784v4,Can AI Help with Your Personal Finances?,2024-12-27 18:25:27+00:00,"['Oudom Hean', 'Utsha Saha', 'Binita Saha']",cs.AI,"In recent years, Large Language Models (LLMs) have emerged as a
transformative development in artificial intelligence (AI), drawing significant
attention from industry and academia. Trained on vast datasets, these
sophisticated AI systems exhibit impressive natural language processing and
content generation capabilities. This paper explores the potential of LLMs to
address key challenges in personal finance, focusing on the United States. We
evaluate several leading LLMs, including OpenAI's ChatGPT, Google's Gemini,
Anthropic's Claude, and Meta's Llama, to assess their effectiveness in
providing accurate financial advice on topics such as mortgages, taxes, loans,
and investments. Our findings show that while these models achieve an average
accuracy rate of approximately 70%, they also display notable limitations in
certain areas. Specifically, LLMs struggle to provide accurate responses for
complex financial queries, with performance varying significantly across
different topics. Despite these limitations, the analysis reveals notable
improvements in newer versions of these models, highlighting their growing
utility for individuals and financial advisors. As these AI systems continue to
evolve, their potential for advancing AI-driven applications in personal
finance becomes increasingly promising.",2024
http://arxiv.org/abs/2412.19754v3,Complement or substitute? How AI increases the demand for human skills,2024-12-27 17:26:30+00:00,"['Elina Mäkelä', 'Fabian Stephany']",econ.GN,"This paper examines whether artificial intelligence (AI) acts as a substitute
or complement to human labour, drawing on 12 million online job vacancies from
the United States spanning 2018-2023. We adopt a two-pronged approach: first,
analysing ""internal effects"" within roles explicitly requiring AI, and second,
investigating ""external effects"" that arise when industries, occupations, and
regions experience increases in AI demand. Our focus centres on whether
complementary skills-such as digital literacy, teamwork, resilience, agility,
or analytical thinking-become more prevalent and valuable as AI adoption grows.
Results show that AI-focused roles are nearly twice as likely to require skills
like resilience, agility, or analytical thinking compared to non-AI roles.
Furthermore, these skills command a significant wage premium; data scientists,
for instance, are offered 5-10% higher salaries if they also possess resilience
or ethics capabilities. We observe positive spillover effects: a doubling of
AI-specific demand across industries correlates with a 5% increase in demand
for complementary skills, even outside AI-related roles. Conversely, tasks
vulnerable to AI substitution, such as basic data skills or translation,
exhibit modest declines in demand. However, the external effect is clearly net
positive: Complementary effects are up to 1.7x larger than substitution
effects. These results are consistent across economies, including the United
Kingdom and Australia. Our findings highlight the necessity of reskilling
workers in areas where human expertise remains increasingly valuable and
ensuring workers can effectively complement and leverage emerging AI
technologies.",2024
http://arxiv.org/abs/2412.19555v2,Asymptotic Properties of the Maximum Likelihood Estimator for Markov-switching Observation-driven Models,2024-12-27 09:57:59+00:00,['Frederik Krabbe'],econ.EM,"A Markov-switching observation-driven model is a stochastic process
$((S_t,Y_t))_{t \in \mathbb{Z}}$ where (i) $(S_t)_{t \in \mathbb{Z}}$ is an
unobserved Markov process taking values in a finite set and (ii) $(Y_t)_{t \in
\mathbb{Z}}$ is an observed process such that the conditional distribution of
$Y_t$ given all past $Y$'s and the current and all past $S$'s depends only on
all past $Y$'s and $S_t$. In this paper, we prove the consistency and
asymptotic normality of the maximum likelihood estimator for such model. As a
special case hereof, we give conditions under which the maximum likelihood
estimator for the widely applied Markov-switching generalised autoregressive
conditional heteroscedasticity model introduced by Haas et al. (2004b) is
consistent and asymptotic normal.",2024
http://arxiv.org/abs/2412.19506v1,A Certain Notion of Strategy Freedom under Retail Competition in Claims Problems,2024-12-27 07:46:45+00:00,['Kentarô Yamamoto'],econ.TH,"A new axiom for rules for claims problems is introduced. It strengthens a
condition studied in supply chain literature, which forces rules to
disincentivize order inflation under capacity allocation and retail
competition. The relevance of the axiom is further demonstrated by one of the
main results of the present article: it characterizes the weighted constrained
equal awards rule together with known natural axioms.",2024
http://arxiv.org/abs/2412.19301v1,Sanctions and Venezuelan Migration,2024-12-26 17:51:46+00:00,['Francisco Rodríguez'],econ.GN,"This paper examines the potential impact of different US economic sanctions
policies on Venezuelan migration flows. I consider three possible departures
from the current status quo in which selected oil companies are permitted to
conduct transactions with Venezuela's state-owned oil sector: a return to
maximum pressure, characterized by intensive use of secondary sanctions, a more
limited tightening that would revoke only the current Chevron license, and a
complete lifting of economic sanctions. I find that sanctions significantly
influence migration patterns by disrupting oil revenues, which fund imports
critical to productivity in the non-oil sector. Reimposing maximum pressure
sanctions would lead to an estimated one million additional Venezuelans
emigrating over the next five years compared to a baseline scenario of no
economic sanctions. If the US aims to address the Venezuelan migrant crisis
effectively, a policy of engagement and lifting economic sanctions appears more
likely to stabilize migration flows than a return to maximum pressure
strategies.",2024
http://arxiv.org/abs/2412.19245v1,Sentiment trading with large language models,2024-12-26 15:01:24+00:00,"['Kemal Kirtac', 'Guido Germano']",q-fin.CP,"We investigate the efficacy of large language models (LLMs) in sentiment
analysis of U.S. financial news and their potential in predicting stock market
returns. We analyze a dataset comprising 965,375 news articles that span from
January 1, 2010, to June 30, 2023; we focus on the performance of various LLMs,
including BERT, OPT, FINBERT, and the traditional Loughran-McDonald dictionary
model, which has been a dominant methodology in the finance literature. The
study documents a significant association between LLM scores and subsequent
daily stock returns. Specifically, OPT, which is a GPT-3 based LLM, shows the
highest accuracy in sentiment prediction with an accuracy of 74.4%, slightly
ahead of BERT (72.5%) and FINBERT (72.2%). In contrast, the Loughran-McDonald
dictionary model demonstrates considerably lower effectiveness with only 50.1%
accuracy. Regression analyses highlight a robust positive impact of OPT model
scores on next-day stock returns, with coefficients of 0.274 and 0.254 in
different model specifications. BERT and FINBERT also exhibit predictive
relevance, though to a lesser extent. Notably, we do not observe a significant
relationship between the Loughran-McDonald dictionary model scores and stock
returns, challenging the efficacy of this traditional method in the current
financial context. In portfolio performance, the long-short OPT strategy excels
with a Sharpe ratio of 3.05, compared to 2.11 for BERT and 2.07 for FINBERT
long-short strategies. Strategies based on the Loughran-McDonald dictionary
yield the lowest Sharpe ratio of 1.23. Our findings emphasize the superior
performance of advanced LLMs, especially OPT, in financial market prediction
and portfolio management, marking a significant shift in the landscape of
financial analysis tools with implications to financial regulation and policy
analysis.",2024
http://arxiv.org/abs/2412.19861v1,Measurement of coupling development level of new infrastructure investment and digital transformation and its temporal and spatial evolution trend,2024-12-26 07:52:11+00:00,"['Sanglin Zhao', 'Jikang Cao']",econ.GN,"Based on the coupling mechanism between new infrastructure investment level
and digital transformation level, a comprehensive index system is constructed.
By using entropy weight method, coupling coordination evaluation model,
exploratory spatial data analysis (ESDA) and standard deviation ellipse model,
the coupling coordination development level of new infrastructure investment
level and digital transformation level in 31 provinces and cities in China from
2014 to 2021 is calculated, and its spatial agglomeration level and
spatial-temporal evolution characteristics are analyzed.",2024
http://arxiv.org/abs/2412.19024v2,Nonparametric Estimation of Matching Efficiency and Elasticity in a Spot Gig Work Platform: 2019-2023,2024-12-26 02:16:58+00:00,"['Hayato Kanayama', 'Suguru Otani']",econ.GN,"This paper provides new evidence on spot gig work platforms for unemployed
workers searching for occupations with minimal educational or experience
requirements in Japan. Using proprietary data from a private online spot work
matching platform, Timee, it examines trends in key variables such as the
numbers of unemployed users, vacancies, hires, and labor market tightness. The
study compares these trends with part-time worker data from the public
employment platform, Hello Work. The private platform shows a significant
market expansion from December 2019 to December 2023. Applying a novel
nonparametric approach, the paper finds greater variability in efficiency and
higher elasticity, with elasticity with respect to the number of users
fluctuating from below 0.7 to above 1.5, and elasticity with respect to the
number of vacancies often exceeding 1.0, which is higher than Hello Work.
Lastly, the study highlights less geographical heterogeneity of the spot work
compared to Hello Work.",2024
http://arxiv.org/abs/2412.19850v1,The Patterns of Digital Deception,2024-12-25 15:55:17+00:00,['Gregory M. Dickinson'],econ.GN,"Current consumer-protection debates focus on the powerful new data-analysis
techniques that have disrupted the balance of power between companies and their
customers. Online tracking enables sellers to amass troves of historical data,
apply machine-learning tools to construct detailed customer profiles, and
target those customers with tailored offers that best suit their interests. It
is often a win-win. Sellers avoid pumping dud products and consumers see ads
for things they actually want to buy. But the same tools are also used for ill
-- to target vulnerable members of the population with scams specially tailored
to prey on their weaknesses. The result has been a dramatic rise in online
fraud that disproportionately impacts those least able to bear the loss.
  The law's response has been technology centric. Lawmakers race to identify
those technologies that drive consumer deception and target them for regulatory
restrictions. But that approach comes at a major cost. General-purpose
data-analysis and communications tools have both desirable and undesirable
uses, and uniform restrictions on their use impede the good along with the bad.
A superior approach would focus not on the technological tools of deception but
on what this Article identifies as the legal patterns of digital deception --
those aspects of digital technology that have outflanked the law's existing
mechanisms for redressing consumer harm. This Article reorients the discussion
from the power of new technologies to the shortcomings in existing regulatory
structures that have allowed for their abuse. Focus on these patterns of
deception will allow regulators to reallocate resources to offset those
shortcomings and thereby enhance efforts to combat online fraud without
impeding technological innovation.",2024
http://arxiv.org/abs/2412.18875v2,Market allocations under conflation of goods,2024-12-25 11:19:32+00:00,"['Niccolò Urbinat', 'Marco LiCalzi']",econ.TH,"We study competitive equilibria in exchange economies when a continuum of
goods is conflated into a finite set of commodities. The design of conflation
choices affects the allocation of scarce resources among agents, by
constraining trading opportunities and shifting competitive pressures. We
demonstrate the consequences on relative prices, trading positions, and
welfare.",2024
http://arxiv.org/abs/2412.18714v1,Using Ordinal Voting to Compare the Utilitarian Welfare of a Status Quo and A Proposed Policy: A Simple Nonparametric Analysis,2024-12-25 00:26:36+00:00,['Charles F. Manski'],econ.TH,"The relationship of policy choice by majority voting and by maximization of
utilitarian welfare has long been discussed. I consider choice between a status
quo and a proposed policy when persons have interpersonally comparable cardinal
utilities taking values in a bounded interval, voting is compulsory, and each
person votes for a policy that maximizes utility. I show that knowledge of the
attained status quo welfare and the voting outcome yields an informative bound
on welfare with the proposed policy. The bound contains the value of status quo
welfare, so the better utilitarian policy is not known. The minimax-regret
decision and certain Bayes decisions choose the proposed policy if its vote
share exceeds the known value of status quo welfare. This procedure differs
from majority rule, which chooses the proposed policy if its vote share exceeds
1/2.",2024
http://arxiv.org/abs/2412.18699v1,Market Basket Analysis Using Rule-Based Algorithms and Data Mining Techniques,2024-12-24 23:17:57+00:00,"['Marina Kholod', 'Nikita Mokrenko']",cs.DB,"The research identifies association rules that can inform marketing
strategies and enhance operational efficiency. A structured methodology is
applied to extract and interpret meaningful relationships within transactional
data, emphasizing their implications for managerial decision-making. By
demonstrating the potential of data mining to transform raw data into valuable
business insights, this paper provides a framework for using analytical tools
to improve customer engagement and competitive positioning.",2024
http://arxiv.org/abs/2412.18486v2,Calibrating the Subjective,2024-12-24 15:18:27+00:00,['Mark Whitmeyer'],econ.TH,"I conduct Rabin's (2000) calibration exercise in the subjective expected
utility realm. I show that the rejection of some risky bet by a risk-averse
agent only implies the rejection of more extreme and less desirable bets and
nothing more.",2024
http://arxiv.org/abs/2412.18449v2,Robust Equilibria in Generic Extensive form Games,2024-12-24 14:19:32+00:00,"['Lucas Pahl', 'Carlos Pimienta']",econ.TH,"We prove the 2-player, generic extensive-form case of the conjecture of
Govindan and Wilson (1997a,b) and Hauk and Hurkens (2002) stating that an
equilibrium component is essential in every equivalent game if and only if the
index of the component is nonzero. This provides an index-theoretic
characterization of the concept of hyperstable components of equilibria in
generic extensive-form games, first formulated by Kohlberg and Mertens (1986).
We also illustrate how to compute hyperstable equilibria in multiple
economically relevant examples and show how the predictions of hyperstability
compare with other solution concepts.",2024
http://arxiv.org/abs/2412.18346v1,ORAN Drives Higher Returns on Investments in Urban and Suburban Regions,2024-12-24 11:08:15+00:00,"['Priyanka Sharma', 'Edward J. Oughton', 'Aleksan Shanoyan']",econ.GN,"This paper provides the first incentive analysis of open radio access
networks (ORAN) using game theory. We assess strategic interactions between
telecom supply chain stakeholders: mobile network operators (MNOs), network
infrastructure suppliers (NIS), and original equipment manufacturers (OEMs)
across three procurement scenarios: (i) Traditional, (ii) Predatory as
monolithic radio access networks (MRAN), and (iii) DirectOEM as ORAN. We use
random forest and gradient boosting models to evaluate the optimal margins
across urban, suburban, and rural U.S. regions. Results suggest that ORAN
deployment consistently demonstrates higher net present value (NPV) of profits
in urban and suburban regions, outperforming the traditional procurement
strategy by 11% to 31%. However, rural areas present lower NPVs across all
scenarios, with significant variability at the county level. This analysis
offers actionable insights for telecom investment strategies, bridging
technical innovation with economic outcomes and addressing strategic supply
chain dynamics through a game-theoretic lens.",2024
http://arxiv.org/abs/2412.18337v1,The Value of AI-Generated Metadata for UGC Platforms: Evidence from a Large-scale Field Experiment,2024-12-24 10:47:27+00:00,"['Xinyi Zhang', 'Chenshuo Sun', 'Renyu Zhang', 'Khim-Yong Goh']",econ.GN,"AI-generated content (AIGC), such as advertisement copy, product
descriptions, and social media posts, is becoming ubiquitous in business
practices. However, the value of AI-generated metadata, such as titles, remains
unclear on user-generated content (UGC) platforms. To address this gap, we
conducted a large-scale field experiment on a leading short-video platform in
Asia to provide about 1 million users access to AI-generated titles for their
uploaded videos. Our findings show that the provision of AI-generated titles
significantly boosted content consumption, increasing valid watches by 1.6% and
watch duration by 0.9%. When producers adopted these titles, these increases
jumped to 7.1% and 4.1%, respectively. This viewership-boost effect was largely
attributed to the use of this generative AI (GAI) tool increasing the
likelihood of videos having a title by 41.4%. The effect was more pronounced
for groups more affected by metadata sparsity. Mechanism analysis revealed that
AI-generated metadata improved user-video matching accuracy in the platform's
recommender system. Interestingly, for a video for which the producer would
have posted a title anyway, adopting the AI-generated title decreased its
viewership on average, implying that AI-generated titles may be of lower
quality than human-generated ones. However, when producers chose to co-create
with GAI and significantly revised the AI-generated titles, the videos
outperformed their counterparts with either fully AI-generated or
human-generated titles, showcasing the benefits of human-AI co-creation. This
study highlights the value of AI-generated metadata and human-AI metadata
co-creation in enhancing user-content matching and content consumption for UGC
platforms.",2024
http://arxiv.org/abs/2412.18130v1,Profit Allocation in the We Media Value Chain: A Shapley Value-Based Approach,2024-12-24 03:38:36+00:00,"['Jianfei Xu', 'Rui Zhang', 'Junhui Fan']",econ.TH,"The study takes the social media industry as its research subject and
examines the impact of scientific innovation capabilities on profit
distribution within the value chain of the social media industry. It proposes a
specific solution to the profit distribution problem using an improved Shapley
value method. Additionally, the AHP (Analytic Hierarchy Process) is employed to
evaluate the profit distribution model, allowing the improved Shapley value
method to better address the issue of profit allocation within the value chain
of the social media industry. This approach ensures that each member receives a
fair share of the profits, fostering strong cooperative relationships among
members. Moreover, it compensates for the shortcomings of the traditional
Shapley value method in addressing such problems to a certain extent.",2024
http://arxiv.org/abs/2412.18080v1,Conditional Influence Functions,2024-12-24 01:29:26+00:00,"['Victor Chernozhukov', 'Whitney K. Newey', 'Vasilis Syrgkanis']",math.ST,"There are many nonparametric objects of interest that are a function of a
conditional distribution. One important example is an average treatment effect
conditional on a subset of covariates. Many of these objects have a conditional
influence function that generalizes the classical influence function of a
functional of a (unconditional) distribution. Conditional influence functions
have important uses analogous to those of the classical influence function.
They can be used to construct Neyman orthogonal estimating equations for
conditional objects of interest that depend on high dimensional regressions.
They can be used to formulate local policy effects and describe the effect of
local misspecification on conditional objects of interest. We derive
conditional influence functions for functionals of conditional means and other
features of the conditional distribution of an outcome variable. We show how
these can be used for locally linear estimation of conditional objects of
interest. We give rate conditions for first step machine learners to have no
effect on asymptotic distributions of locally linear estimators. We also give a
general construction of Neyman orthogonal estimating equations for conditional
objects of interest.",2024
http://arxiv.org/abs/2412.18032v1,A physics-engineering-economic model coupling approach for estimating the socio-economic impacts of space weather scenarios,2024-12-23 23:03:58+00:00,"['Edward J. Oughton', 'Dennies K. Bor', 'Michael Wiltberger', 'Robert Weigel', 'C. Trevor Gaunt', 'Ridvan Dogan', 'Liling Huang']",physics.geo-ph,"There is growing concern about our vulnerability to space weather hazards and
the disruption critical infrastructure failures could cause to society and the
economy. However, the socio-economic impacts of space weather hazards, such as
from geomagnetic storms, remain under-researched. This study introduces a novel
framework to estimate the economic impacts of electricity transmission
infrastructure failure due to space weather. By integrating existing
geophysical and geomagnetically induced current (GIC) estimation models with a
newly developed geospatial model of the Continental United States power grid,
GIC vulnerabilities are assessed for a range of space weather scenarios. The
approach evaluates multiple power network architectures, incorporating
input-output economic modeling to translate business and population disruptions
into macroeconomic impacts from GIC-related thermal heating failures. The
results indicate a daily GDP loss from 6 billion USD to over 10 billion USD.
Even under conservative GIC thresholds (75 A/ph) aligned with thermal withstand
limits from the North American Electric Reliability Corporation (NERC),
significant economic disruptions are evident. This study is limited by its
restriction to thermal heating analysis, though GICs can also affect the grid
through other pathways, such as voltage instability and harmonic distortions.
Addressing these other failure mechanisms need to be the focus of future
research.",2024
http://arxiv.org/abs/2412.17753v2,Minimax Optimal Simple Regret in Two-Armed Best-Arm Identification,2024-12-23 18:06:20+00:00,['Masahiro Kato'],stat.ML,"This study investigates an asymptotically minimax optimal algorithm in the
two-armed fixed-budget best-arm identification (BAI) problem. Given two
treatment arms, the objective is to identify the arm with the highest expected
outcome through an adaptive experiment. We focus on the Neyman allocation,
where treatment arms are allocated following the ratio of their outcome
standard deviations. Our primary contribution is to prove the minimax
optimality of the Neyman allocation for the simple regret, defined as the
difference between the expected outcomes of the true best arm and the estimated
best arm. Specifically, we first derive a minimax lower bound for the expected
simple regret, which characterizes the worst-case performance achievable under
the location-shift distributions, including Gaussian distributions. We then
show that the simple regret of the Neyman allocation asymptotically matches
this lower bound, including the constant term, not just the rate in terms of
the sample size, under the worst-case distribution. Notably, our optimality
result holds without imposing locality restrictions on the distribution, such
as the local asymptotic normality. Furthermore, we demonstrate that the Neyman
allocation reduces to the uniform allocation, i.e., the standard randomized
controlled trial, under Bernoulli distributions.",2024
http://arxiv.org/abs/2412.17598v1,A large non-Gaussian structural VAR with application to Monetary Policy,2024-12-23 14:17:19+00:00,['Jan Prüser'],econ.EM,"We propose a large structural VAR which is identified by higher moments
without the need to impose economically motivated restrictions. The model
scales well to higher dimensions, allowing the inclusion of a larger number of
variables. We develop an efficient Gibbs sampler to estimate the model. We also
present an estimator of the deviance information criterion to facilitate model
comparison. Finally, we discuss how economically motivated restrictions can be
added to the model. Experiments with artificial data show that the model
possesses good estimation properties. Using real data we highlight the benefits
of including more variables in the structural analysis. Specifically, we
identify a monetary policy shock and provide empirical evidence that prices and
economic output respond with a large delay to the monetary policy shock.",2024
http://arxiv.org/abs/2412.17470v2,A Necessary and Sufficient Condition for Size Controllability of Heteroskedasticity Robust Test Statistics,2024-12-23 10:52:27+00:00,"['Benedikt M. Pötscher', 'David Preinerstorfer']",math.ST,"We revisit size controllability results in P\""otscher and Preinerstorfer
(2025) concerning heteroskedasticity robust test statistics in regression
models. For the special, but important, case of testing a single restriction
(e.g., a zero restriction on a single coefficient), we povide a necessary and
sufficient condition for size controllability, whereas the condition in
P\""otscher and Preinerstorfer (2025) is, in general, only sufficient (even in
the case of testing a single restriction).",2024
http://arxiv.org/abs/2412.17379v1,Advanced Models for Hourly Marginal CO2 Emission Factor Estimation: A Synergy between Fundamental and Statistical Approaches,2024-12-23 08:27:42+00:00,"['Souhir Ben Amor', 'Smaranda Sgarciu', 'Taimyra BatzLineiro', 'Felix Muesgens']",econ.EM,"Global warming is caused by increasing concentrations of greenhouse gases,
particularly carbon dioxide (CO2). A metric used to quantify the change in CO2
emissions is the marginal emission factor, defined as the marginal change in
CO2 emissions resulting from a marginal change in electricity demand over a
specified period. This paper aims to present two methodologies to estimate the
marginal emission factor in a decarbonized electricity system with high
temporal resolution. First, we present an energy systems model that
incrementally calculates the marginal emission factors. Second, we examine a
Markov Switching Dynamic Regression model, a statistical model designed to
estimate marginal emission factors faster and use an incremental marginal
emission factor as a benchmark to assess its precision. For the German
electricity market, we estimate the marginal emissions factor time series
historically (2019, 2020) using Agora Energiewende and for the future (2025,
2030, and 2040) using estimated energy system data. The results indicate that
the Markov Switching Dynamic Regression model is more accurate in estimating
marginal emission factors than the Dynamic Linear Regression models, which are
frequently used in the literature. Hence, the Markov Switching Dynamic
Regression model is a simpler alternative to the computationally intensive
incremental marginal emissions factor, especially when short-term marginal
emissions factor estimation is needed. The results of the marginal emission
factor estimation are applied to an exemplary low-emission vehicle charging
scenario to estimate CO2 savings by shifting the charge hours to those
corresponding to the lower marginal emissions factor. By implementing this
emission-minimized charging approach, an average reduction of 31% in the
marginal emission factor was achieved over the 5 years.",2024
http://arxiv.org/abs/2412.17354v3,Bayesian penalized empirical likelihood and Markov Chain Monte Carlo sampling,2024-12-23 07:29:14+00:00,"['Jinyuan Chang', 'Cheng Yong Tang', 'Yuanzheng Zhu']",stat.ME,"In this study, we introduce a novel methodological framework called Bayesian
Penalized Empirical Likelihood (BPEL), designed to address the computational
challenges inherent in empirical likelihood (EL) approaches. Our approach has
two primary objectives: (i) to enhance the inherent flexibility of EL in
accommodating diverse model conditions, and (ii) to facilitate the use of
well-established Markov Chain Monte Carlo (MCMC) sampling schemes as a
convenient alternative to the complex optimization typically required for
statistical inference using EL. To achieve the first objective, we propose a
penalized approach that regularizes the Lagrange multipliers, significantly
reducing the dimensionality of the problem while accommodating a comprehensive
set of model conditions. For the second objective, our study designs and
thoroughly investigates two popular sampling schemes within the BPEL context.
We demonstrate that the BPEL framework is highly flexible and efficient,
enhancing the adaptability and practicality of EL methods. Our study highlights
the practical advantages of using sampling techniques over traditional
optimization methods for EL problems, showing rapid convergence to the global
optima of posterior distributions and ensuring the effective resolution of
complex statistical inference challenges.",2024
http://arxiv.org/abs/2412.17181v1,Gaussian and Bootstrap Approximation for Matching-based Average Treatment Effect Estimators,2024-12-22 22:47:31+00:00,"['Zhaoyang Shi', 'Chinmoy Bhattacharjee', 'Krishnakumar Balasubramanian', 'Wolfgang Polonik']",math.ST,"We establish Gaussian approximation bounds for covariate and
rank-matching-based Average Treatment Effect (ATE) estimators. By analyzing
these estimators through the lens of stabilization theory, we employ the
Malliavin-Stein method to derive our results. Our bounds precisely quantify the
impact of key problem parameters, including the number of matches and treatment
balance, on the accuracy of the Gaussian approximation. Additionally, we
develop multiplier bootstrap procedures to estimate the limiting distribution
in a fully data-driven manner, and we leverage the derived Gaussian
approximation results to further obtain bootstrap approximation bounds. Our
work not only introduces a novel theoretical framework for commonly used ATE
estimators, but also provides data-driven methods for constructing
non-asymptotically valid confidence intervals.",2024
http://arxiv.org/abs/2412.17021v1,Competitive Facility Location with Market Expansion and Customer-centric Objective,2024-12-22 13:53:58+00:00,"['Cuong Le', 'Tien Mai', 'Ngan Ha Duong', 'Minh Hoang Ha']",math.OC,"We study a competitive facility location problem, where customer behavior is
modeled and predicted using a discrete choice random utility model. The goal is
to strategically place new facilities to maximize the overall captured customer
demand in a competitive marketplace. In this work, we introduce two novel
considerations. First, the total customer demand in the market is not fixed but
is modeled as an increasing function of the customers' total utilities. Second,
we incorporate a new term into the objective function, aiming to balance the
firm's benefits and customer satisfaction. Our new formulation exhibits a
highly nonlinear structure and is not directly solved by existing approaches.
To address this, we first demonstrate that, under a concave market expansion
function, the objective function is concave and submodular, allowing for a
$(1-1/e)$ approximation solution by a simple polynomial-time greedy algorithm.
We then develop a new method, called Inner-approximation, which enables us to
approximate the mixed-integer nonlinear problem (MINLP), with arbitrary
precision, by an MILP without introducing additional integer variables. We
further demonstrate that our inner-approximation method consistently yields
lower approximations than the outer-approximation methods typically used in the
literature. Moreover, we extend our settings by considering a\textit{ general
(non-concave)} market-expansion function and show that the Inner-approximation
mechanism enables us to approximate the resulting MINLP, with arbitrary
precision, by an MILP. To further enhance this MILP, we show how to
significantly reduce the number of additional binary variables by leveraging
concave areas of the objective function. Extensive experiments demonstrate the
efficiency of our approaches.",2024
http://arxiv.org/abs/2412.16927v1,The impact of Egypt's accession to the BRICS group on the foreign exchange crisis in Egypt,2024-12-22 08:57:16+00:00,"['Yasmeen Fekery Yaseen El Khodary', 'Mousa Gowfal Selmey Gowfal Selmey', 'Elsayed Farrag Elsaid Mohamed Elsayed']",econ.GN,"This paper investigated the impact of Egypt's accession to the BRICS bloc by
studying the participation rate of BRICS countries in Egypt, as well as the
exposure rate and the ratio of foreign investment of BRICS countries in Egypt
to the total foreign investment in Egypt, as well as Egypt's export
opportunities in the BRICS markets and the impact of these variables on the
ratio of foreign assets in Egypt during the period from 2000 to 2024, In order
to predict the impact of Egypt's accession to the BRICS bloc in the coming
years as well. The study used the ARDL model to determine the impact of the
four independent variables on the dependent variable (foreign assets), which is
the ratio of foreign assets. It also used automatic ARIMA model to predict the
values of the study variables during the period from 2024 to 2030.",2024
http://arxiv.org/abs/2412.16591v1,Data-Driven Economic Agent-Based Models,2024-12-21 11:49:50+00:00,"['Marco Pangallo', 'R. Maria del Rio-Chanona']",econ.GN,"Economic agent-based models (ABMs) are becoming more and more data-driven,
establishing themselves as increasingly valuable tools for economic research
and policymaking. We propose to classify the extent to which an ABM is
data-driven based on whether agent-level quantities are initialized from
real-world micro-data and whether the ABM's dynamics track empirical time
series. This paper discusses how making ABMs data-driven helps overcome
limitations of traditional ABMs and makes ABMs a stronger alternative to
equilibrium models. We review state-of-the-art methods in parameter
calibration, initialization, and data assimilation, and then present successful
applications that have generated new scientific knowledge and informed policy
decisions. This paper serves as a manifesto for data-driven ABMs, introducing a
definition and classification and outlining the state of the field, and as a
guide for those new to the field.",2024
http://arxiv.org/abs/2412.17866v2,"Artificial Intelligence, Scientific Discovery, and Product Innovation",2024-12-21 05:38:10+00:00,['Aidan Toner-Rodgers'],econ.GN,"This paper studies the impact of artificial intelligence on innovation,
exploiting the randomized introduction of a new materials discovery technology
to 1,018 scientists in the R&D lab of a large U.S. firm. AI-assisted
researchers discover 44% more materials, resulting in a 39% increase in patent
filings and a 17% rise in downstream product innovation. These compounds
possess more novel chemical structures and lead to more radical inventions.
However, the technology has strikingly disparate effects across the
productivity distribution: while the bottom third of scientists see little
benefit, the output of top researchers nearly doubles. Investigating the
mechanisms behind these results, I show that AI automates 57% of
""idea-generation"" tasks, reallocating researchers to the new task of evaluating
model-produced candidate materials. Top scientists leverage their domain
knowledge to prioritize promising AI suggestions, while others waste
significant resources testing false positives. Together, these findings
demonstrate the potential of AI-augmented research and highlight the
complementarity between algorithms and expertise in the innovative process.
Survey evidence reveals that these gains come at a cost, however, as 82% of
scientists report reduced satisfaction with their work due to decreased
creativity and skill underutilization.",2024
http://arxiv.org/abs/2412.16452v1,Sharp Results for Hypothesis Testing with Risk-Sensitive Agents,2024-12-21 02:51:56+00:00,"['Flora C. Shi', 'Stephen Bates', 'Martin J. Wainwright']",stat.ME,"Statistical protocols are often used for decision-making involving multiple
parties, each with their own incentives, private information, and ability to
influence the distributional properties of the data. We study a game-theoretic
version of hypothesis testing in which a statistician, also known as a
principal, interacts with strategic agents that can generate data. The
statistician seeks to design a testing protocol with controlled error, while
the data-generating agents, guided by their utility and prior information,
choose whether or not to opt in based on expected utility maximization. This
strategic behavior affects the data observed by the statistician and,
consequently, the associated testing error. We analyze this problem for general
concave and monotonic utility functions and prove an upper bound on the Bayes
false discovery rate (FDR). Underlying this bound is a form of prior
elicitation: we show how an agent's choice to opt in implies a certain upper
bound on their prior null probability. Our FDR bound is unimprovable in a
strong sense, achieving equality at a single point for an individual agent and
at any countable number of points for a population of agents. We also
demonstrate that our testing protocols exhibit a desirable maximin property
when the principal's utility is considered. To illustrate the qualitative
predictions of our theory, we examine the effects of risk aversion, reward
stochasticity, and signal-to-noise ratio, as well as the implications for the
Food and Drug Administration's testing protocols.",2024
http://arxiv.org/abs/2412.16384v1,Algorithmic Contract Theory: A Survey,2024-12-20 22:40:02+00:00,"['Paul Duetting', 'Michal Feldman', 'Inbal Talgam-Cohen']",cs.GT,"A contract is an economic tool used by a principal to incentivize one or more
agents to exert effort on her behalf, by defining payments based on observable
performance measures. A key challenge addressed by contracts -- known in
economics as moral hazard -- is that, absent a properly set up contract, agents
might engage in actions that are not in the principal's best interest. Another
common feature of contracts is limited liability, which means that payments can
go only from the principal -- who has the deep pocket -- to the agents.
  With classic applications of contract theory moving online, growing in scale,
and becoming more data-driven, tools from contract theory become increasingly
important for incentive-aware algorithm design. At the same time, algorithm
design offers a whole new toolbox for reasoning about contracts, ranging from
additional tools for studying the tradeoff between simple and optimal
contracts, through a language for discussing the computational complexity of
contracts in combinatorial settings, to a formalism for analyzing data-driven
contracts.
  This survey aims to provide a computer science-friendly introduction to the
basic concepts of contract theory. We give an overview of the emerging field of
""algorithmic contract theory"" and highlight work that showcases the potential
for interaction between the two areas. We also discuss avenues for future
research.",2024
http://arxiv.org/abs/2412.16352v3,Counting Defiers: A Design-Based Model of an Experiment Can Reveal Evidence Beyond the Average Effect,2024-12-20 21:27:02+00:00,"['Neil Christy', 'Amanda Ellen Kowalski']",econ.EM,"Leveraging structure from the randomization process, a design-based model of
an experiment with a binary intervention and outcome can reveal evidence beyond
the average effect without additional data. Our proposed statistical decision
rule yields a design-based maximum likelihood estimate (MLE) of the joint
distribution of potential outcomes in intervention and control, specified by
the numbers of always takers, compliers, defiers, and never takers in the
sample. With a visualization, we explain why the likelihood varies with the
number of defiers within the Frechet bounds determined by the estimated
marginal distributions. We illustrate how the MLE varies with all possible data
in samples of 50 and 200: when the estimated average effect is positive, the
MLE includes defiers if takeup is below half in control and above half in
intervention, unless takeup is zero in control or full in intervention. Under
optimality conditions, for increasing sample sizes in which exhaustive grid
search is possible, our rule's performance increases relative to a rule that
places equal probability on all numbers of defiers within the estimated Frechet
bounds. We offer insights into effect heterogeneity in two published
experiments with positive, statistically significant average effects on takeup
of desired health behaviors and plausible defiers. Our 95% smallest credible
sets for defiers include zero and the estimated upper Frechet bound,
demonstrating that evidence is weak. Yet, our rule yields no defiers in one
experiment. In the other, our rule yields the estimated upper Frechet bound on
defiers -- a count representing over 18% of the sample.",2024
http://arxiv.org/abs/2412.16132v1,Data-Driven Mechanism Design: Jointly Eliciting Preferences and Information,2024-12-20 18:29:49+00:00,"['Dirk Bergemann', 'Marek Bojko', 'Paul Dütting', 'Renato Paes Leme', 'Haifeng Xu', 'Song Zuo']",econ.TH,"We study mechanism design when agents hold private information about both
their preferences and a common payoff-relevant state. We show that standard
message-driven mechanisms cannot implement socially efficient allocations when
agents have multidimensional types, even under favorable conditions. To
overcome this limitation, we propose data-driven mechanisms that leverage
additional post-allocation information, modeled as an estimator of the
payoff-relevant state. Our data-driven mechanisms extend the classic
Vickrey-Clarke-Groves class. We show that they achieve exact implementation in
posterior equilibrium when the state is either fully revealed or the utility is
linear in an unbiased estimator. We also show that they achieve approximate
implementation with a consistent estimator, converging to exact implementation
as the estimator converges, and present bounds on the convergence rate. We
demonstrate applications to digital advertising auctions and large language
model (LLM)-based mechanisms, where user engagement naturally reveals relevant
information.",2024
http://arxiv.org/abs/2412.16127v2,Revisiting Global Income Convergence in the 21st Century,2024-12-20 18:19:06+00:00,['Bipul Verma'],econ.GN,"Recent research has documented a reversal from divergence to convergence in
income levels between rich and poor countries after 2000. This paper employs a
growth accounting framework to investigate the proximate sources of convergence
over the period 1980-2019. I find that while output levels began to converge
only after 2000, capital (physical and human) had already been converging
during 1980-2000. The divergence in total factor productivity (TFP) during this
earlier period offset the gains from capital convergence, resulting in little
overall income convergence. After 2000, capital maintained its pattern of
convergence, and, unlike in the earlier period, TFP also began to converge.
Quantitatively, more than half of the convergence between 2000 and 2019-and
nearly all the convergence between 1980 and 2019 outside Sub-Saharan Africa-can
be attributed to the convergence in capital rather than to improvements in TFP.
These findings highlight that factor accumulation has played a much larger role
in driving long-run convergence dynamics than previously documented in the
literature.",2024
http://arxiv.org/abs/2412.16126v1,A Scenario-Based Assessment of the Long-Term Funding Adequacy of the German Nuclear Waste Fund KENFO,2024-12-20 18:18:38+00:00,"['Mahdi Awawda', 'Alexander Wimmers']",econ.GN,"The German site selection process for high-level nuclear waste was initially
planned to conclude in 2031, with the deep geological repository sealed around
the year 2080. However, in 2022, substantial delays were announced by the
responsible federal agency, pushing the site selection to 2046 or even 2068.
With this delay come uncertainties regarding the duration, consequential
knowledge management, and funding. German nuclear waste management activities
are funded by the external segregated fund KENFO, which is designed to ensure
sufficient funding via generating returns on investments (ROI) in the coming
decades. Given recent developments, we assess the adequacy of the fund volume
based on seven scenarios depicting potential process delays. We find that the
target ROI of 3.7% will not suffice in any case, even if the site selection
concludes in 2031, and that cash injections of up to EUR31.07 billion are
necessary today to ensure that the fund volume will suffice. We conclude that
cost estimations must be updated, KENFO must increase its target ROIs,
potential capital injections must be openly discussed by policymakers, and a
procedural acceleration should be implemented to ensure that financial
liabilities for nuclear waste management are minimized for future taxpayers.",2024
http://arxiv.org/abs/2412.16122v1,Multi-scale reconstruction of large supply networks,2024-12-20 18:11:14+00:00,"['Leonardo Niccolò Ialongo', 'Sylvain Bangma', 'Fabian Jansen', 'Diego Garlaschelli']",physics.soc-ph,"The structure of the supply chain network has important implications for
modelling economic systems, from growth trajectories to responses to shocks or
natural disasters. However, reconstructing firm-to-firm networks from available
information poses several practical and theoretical challenges: the lack of
publicly available data, the complexity of meso-scale structures, and the high
level of heterogeneity of firms. With this work we contribute to the literature
on economic network reconstruction by proposing a novel methodology based on a
recently developed multi-scale model. This approach has three main advantages
over other methods: its parameters are defined to maintain statistical
consistency at different scales of node aggregation, it can be applied in a
multi-scale setting, and it is computationally more tractable for very large
graphs. The consistency at different scales of aggregation, inherent to the
model definition, is preserved for any hierarchy of coarse-grainings. The
arbitrariness of the aggregation allows us to work across different scales,
making it possible to estimate model parameters even when node information is
inconsistent, such as when some nodes are firms while others are countries or
regions. Finally, the model can be fitted at an aggregate scale with lower
computational requirements, since the parameters are invariant to the grouping
of nodes. We assess the advantages and limitations of this approach by testing
it on two complementary datasets of Dutch firms constructed from inter-client
transactions on the bank accounts of two major Dutch banking institutions. We
show that the model reliably predicts important topological properties of the
observed network in several scenarios of practical interest and is therefore a
suitable candidate for reconstructing firm-to-firm networks at scale.",2024
http://arxiv.org/abs/2412.16269v1,(Mis)information diffusion and the financial market,2024-12-20 13:54:25+00:00,"['Tommaso Di Francesco', 'Daniel Torren Peraire']",econ.GN,"This paper investigates the interplay between information diffusion in social
networks and its impact on financial markets with an Agent-Based Model (ABM).
Agents receive and exchange information about an observable stochastic
component of the dividend process of a risky asset \`a la Grossman and
Stiglitz. A small proportion of the network has access to a private signal
about the component, which can be clean (information) or distorted
(misinformation). Other agents are uninformed and can receive information only
from their peers. All agents are Bayesian, adjusting their beliefs according to
the confidence they have in the source of information. We examine, by means of
simulations, how information diffuses in the network and provide a framework to
account for delayed absorption of shocks, that are not immediately priced as
predicted by classical financial models. We investigate the effect of the
network topology on the resulting asset price and evaluate under which
condition misinformation diffusion can make the market more inefficient.",2024
http://arxiv.org/abs/2412.18628v1,Streaming problems as (multi-issue) claims problems,2024-12-20 07:44:21+00:00,"['Gustavo Bergantiños', 'Juan D. Moreno-Ternero']",econ.TH,"We study the problem of allocating the revenues raised via paid subscriptions
to music streaming platforms among participating artists. We show that the main
methods to solve streaming problems (pro-rata, user-centric and families
generalizing them) can be seen as specific (well-known) rules to solve
(multi-issue) claims problems. Our results permit to provide strong links
between the well-established literature on claims problems and the emerging
literature on streaming problems.",2024
http://arxiv.org/abs/2501.14747v1,Enhancing Green Economy with Artificial Intelligence: Role of Energy Use and FDI in the United States,2024-12-20 03:03:21+00:00,"['Abdullah Al Abrar Chowdhury', 'Azizul Hakim Rafi', 'Adita Sultana', 'Abdulla All Noman']",econ.GN,"The escalating challenge of climate change necessitates an urgent exploration
of factors influencing carbon emissions. This study contributes to the
discourse by examining the interplay of technological, economic, and
demographic factors on environmental sustainability. This study investigates
the impact of artificial intelligence (AI) innovation, economic growth, foreign
direct investment (FDI), energy consumption, and urbanization on CO2 emissions
in the United States from 1990 to 2022. Employing the ARDL framework integrated
with the STIRPAT model, the findings reveal a dual narrative: while AI
innovation mitigates environmental stress, economic growth, energy use, FDI,
and urbanization exacerbate environmental degradation. Unit root tests (ADF,
PP, and DF-GLS) confirm mixed integration levels among variables, and the ARDL
bounds test establishes long-term co-integration. The analysis highlights that
AI innovation positively correlates with CO2 reduction when environmental
safeguards are in place, whereas GDP growth, energy consumption, FDI, and
urbanization intensify CO2 emissions. Robustness checks using FMOLS, DOLS, and
CCR validate the ARDL findings. Additionally, Pairwise Granger causality tests
reveal significant one-way causal links between CO2 emissions and economic
growth, AI innovation, energy use, FDI, and urbanization. These relationships
emphasize the critical role of AI-driven technological advancements,
sustainable investments, and green energy in fostering ecological
sustainability. The study suggests policy measures such as encouraging green
FDI, advancing AI technologies, adopting sustainable energy practices, and
implementing eco-friendly urban development to promote sustainable growth in
the USA.",2024
http://arxiv.org/abs/2412.15472v1,On the Fairness of Additive Welfarist Rules,2024-12-20 01:01:09+00:00,"['Karen Frilya Celine', 'Warut Suksompong', 'Sheung Man Yuen']",cs.GT,"Allocating indivisible goods is a ubiquitous task in fair division. We study
additive welfarist rules, an important class of rules which choose an
allocation that maximizes the sum of some function of the agents' utilities.
Prior work has shown that the maximum Nash welfare (MNW) rule is the unique
additive welfarist rule that guarantees envy-freeness up to one good (EF1). We
strengthen this result by showing that MNW remains the only additive welfarist
rule that ensures EF1 for identical-good instances, two-value instances, as
well as normalized instances with three or more agents. On the other hand, if
the agents' utilities are integers, we demonstrate that several other rules
offer the EF1 guarantee, and provide characterizations of these rules for
various classes of instances.",2024
http://arxiv.org/abs/2412.15433v1,Quantifying detection rates for dangerous capabilities: a theoretical model of dangerous capability evaluations,2024-12-19 22:31:34+00:00,"['Paolo Bova', 'Alessandro Di Stefano', 'The Anh Han']",cs.AI,"We present a quantitative model for tracking dangerous AI capabilities over
time. Our goal is to help the policy and research community visualise how
dangerous capability testing can give us an early warning about approaching AI
risks. We first use the model to provide a novel introduction to dangerous
capability testing and how this testing can directly inform policy. Decision
makers in AI labs and government often set policy that is sensitive to the
estimated danger of AI systems, and may wish to set policies that condition on
the crossing of a set threshold for danger. The model helps us to reason about
these policy choices. We then run simulations to illustrate how we might fail
to test for dangerous capabilities. To summarise, failures in dangerous
capability testing may manifest in two ways: higher bias in our estimates of AI
danger, or larger lags in threshold monitoring. We highlight two drivers of
these failure modes: uncertainty around dynamics in AI capabilities and
competition between frontier AI labs. Effective AI policy demands that we
address these failure modes and their drivers. Even if the optimal targeting of
resources is challenging, we show how delays in testing can harm AI policy. We
offer preliminary recommendations for building an effective testing ecosystem
for dangerous capabilities and advise on a research agenda.",2024
http://arxiv.org/abs/2412.15376v1,Countries across the world use more land for golf courses than wind or solar energy,2024-12-19 20:11:41+00:00,"['Jann Weinand', 'Tristan Pelser', 'Max Kleinebrahm', 'Detlef Stolten']",econ.GN,"Land use is a critical factor in the siting of renewable energy facilities
and is often scrutinized due to perceived conflicts with other land demands.
Meanwhile, substantial areas are devoted to activities such as golf, which are
accessible to only a select few and have a significant land and environmental
footprint. Our study shows that in countries such as the United States and the
United Kingdom, far more land is allocated to golf courses than to renewable
energy facilities. Areas equivalent to those currently used for golf could
support the installation of up to 842 GW of solar and 659 GW of wind capacity
in the top ten countries with the most golf courses. In many of these
countries, this potential exceeds both current installed capacity and
medium-term projections. These findings underscore the untapped potential of
rethinking land use priorities to accelerate the transition to renewable
energy.",2024
http://arxiv.org/abs/2412.15350v2,Prudence and higher-order risk attitudes in the rank-dependent utility model,2024-12-19 19:32:15+00:00,"['Ruodu Wang', 'Qinyu Wu']",econ.TH,"We obtain a full characterization of consistency with respect to higher-order
stochastic dominance within the rank-dependent utility model. Different from
the results in the literature, we do not assume any condition on the utility
functions and the probability weighting functions, such as differentiability or
continuity. It turns out that the level of generality that we offer leads to
models that do not have a continuous probability weighting function and yet
they satisfy prudence. In particular, the corresponding probability weighting
function can only have a jump at 1, and must be linear on [0,1).",2024
http://arxiv.org/abs/2412.15083v1,Assessing the viability of non-light water reactor concepts for electricity and heat generation in decarbonized energy systems,2024-12-19 17:29:14+00:00,"['Alexander Wimmers', 'Fanny Böse', 'Leonard Göke']",econ.GN,"Recent pledges to triple global nuclear capacity by 2050 suggest a ""nuclear
renaissance,"" bolstered by reactor concepts such as sodium-cooled fast
reactors, high-temperature reactors, and molten salt reactors. These
technologies claim to address the challenges of today's high-capacity
light-water reactors, i.e., cost overruns, delays, and social acceptance, while
also offering additional non-electrical applications. However, this analysis
reveals that none of these concepts currently meet the prerequisites of
affordability, competitiveness, or commercial availability. We omit social
acceptability. The cost analysis reveals optimistic FOAK cost assumptions of
5,623 to 9,511 USD per kW, and NOAK cost projections as low as 1,476 USD per
kW. At FOAK cost, the applied energy system model includes no nuclear power
capacity, and thus indicates that significant cost reductions would be required
for these technologies to contribute to energy system decarbonization. In
low-cost scenarios, reactors capable of producing high temperature heat become
competitive with other low-carbon technologies. We conclude that, for reactor
capacties to increase significantly, a focus on certain technology lines ist
necessary. However, until a concept becomes viable and commercially available,
policymakers should prioritize existing technologies to decarbonize energy
systems.",2024
http://arxiv.org/abs/2412.14996v2,Hydrodynamics of Cooperation and Self-Interest in a Two-Population Occupation Model,2024-12-19 16:07:27+00:00,"['Jerome Garnier-Brun', 'Ruben Zakine', 'Michael Benzaquen']",cond-mat.stat-mech,"We study the hydrodynamics of a system of agents who optimize either their
individual utility (self-interest) or the collective welfare (cooperation).
When agents act selfishly, their interactions are non-reciprocal, driving the
system out of equilibrium; by contrast, purely altruistic dynamics restore
reciprocity and yield an equilibrium-like description. We investigate how
mixtures of these two behaviors shape the macroscopic properties of the liquid
of agents. For highly rational agents, we find that introducing a small
fraction of altruists can suppress the sub-optimal clustering induced by
selfish dynamics. This phenomenon can be attributed to altruists localizing at
interfaces and acting as effective surfactants, shedding a new light on earlier
findings in fixed neighborhood-based models [Phys. Rev. Lett. \textbf{120},
208301 (2018)]. When agents are boundedly rational, we introduce a well-mixed
approximation that reduces the two-population model to a single effective
scalar field theory. This allows us to leverage state-of-the-art tools from
active matter to analytically characterize how altruism modifies surface
tension and nucleation dynamics.",2024
http://arxiv.org/abs/2412.14778v2,Testing linearity of spatial interaction functions à la Ramsey,2024-12-19 12:03:49+00:00,"['Abhimanyu Gupta', 'Jungyoon Lee', 'Francesca Rossi']",econ.EM,"We propose a computationally straightforward test for the linearity of a
spatial interaction function. Such functions arise commonly, either as
practitioner imposed specifications or due to optimizing behaviour by agents.
Our conditional heteroskedasticity robust test is nonparametric, but based on
the Lagrange Multiplier principle and reminiscent of the Ramsey RESET approach.
This entails estimation only under the null hypothesis, which yields an easy to
estimate linear spatial autoregressive model. Monte Carlo simulations show
excellent size control and power. An empirical study with Finnish data
illustrates the test's practical usefulness, shedding light on debates on the
presence of tax competition among neighbouring municipalities.",2024
http://arxiv.org/abs/2412.14624v2,The Diffusive Nature of Housing Prices,2024-12-19 08:21:51+00:00,"['Antoine-Cyrus Becharat', 'Michael Benzaquen', 'Jean-Philippe Bouchaud']",cond-mat.stat-mech,"We analyze the French housing market prices in the period 1970-2022, with
high-resolution data from 2018 to 2022. The spatial correlation of the observed
price field exhibits logarithmic decay characteristic of the two-dimensional
random diffusion equation -- local interactions may create long-range
correlations. We introduce a stylized model, used in the past to model spatial
regularities in voting patterns, that accounts for both spatial and temporal
correlations with reasonable values of parameters. Our analysis reveals that
price shocks are persistent in time and their amplitude is strongly
heterogeneous in space. Our study confirms and quantifies the diffusive nature
of housing prices that was anticipated long ago (Clapp et al. 1994, Pollakowski
et al. 1997), albeit on much restricted, local data sets.",2024
http://arxiv.org/abs/2412.14523v2,Provincial allocation of China's commercial building operational carbon towards carbon neutrality,2024-12-19 04:41:05+00:00,"['Yanqiao Deng', 'Minda Ma', 'Nan Zhou', 'Chenchen Zou', 'Zhili Ma', 'Ran Yan', 'Xin Ma']",econ.GN,"National carbon peak track and optimized provincial carbon allocations are
crucial for mitigating regional inequality within the commercial building
sector during China's transition to carbon neutrality. This study proposes a
top-down model to evaluate carbon trajectories in operational commercial
buildings up to 2060. Through Monte Carlo simulation, scenario analysis is
conducted to assess carbon peak values and the corresponding peaking year,
thereby optimizing carbon allocation schemes both nationwide and provincially.
The results reveal that (1) the nationwide carbon peak for commercial building
operations is projected to reach 890 (+- 50) megatons of carbon dioxide (MtCO2)
by 2028 (+- 3.7 years) in the case of the business-as-usual scenario, with a
7.87% probability of achieving the carbon peak under the decarbonization
scenario. (2) Significant disparities will exist among provinces, with
Shandong's carbon peak projected at 69.6 (+- 4.0) MtCO2 by 2029, approximately
11 times higher than Ningxia's peak of 6.0 (+- 0.3) MtCO2 by 2027. (3) Guided
by the principle of maximizing the emission reduction potential, the optimal
provincial allocation scheme reveals the top three provinces requiring the most
significant reductions in the commercial sector: Xinjiang (5.6 MtCO2), Shandong
(4.8 MtCO2), and Henan (4.7 MtCO2). Overall, this study offers optimized
provincial carbon allocation strategies within the commercial building sector
in China via dynamic scenario simulations, with the goal of hitting the carbon
peak target and progressing toward a low-carbon future for the building sector.",2024
http://arxiv.org/abs/2412.14447v2,Good Controls Gone Bad: Difference-in-Differences with Covariates,2024-12-19 01:54:35+00:00,"['Sunny Karim', 'Matthew D. Webb']",econ.EM,"This paper introduces the two-way common causal covariates (CCC) assumption,
which is necessary to get an unbiased estimate of the ATT when using
time-varying covariates in existing Difference-in-Differences methods. The
two-way CCC assumption implies that the effect of the covariates remain the
same between groups and across time periods. This assumption has been implied
in previous literature, but has not been explicitly addressed. Through
theoretical proofs and a Monte Carlo simulation study, we show that the
standard TWFE and the CS-DID estimators are biased when the two-way CCC
assumption is violated. We propose a new estimator called the Intersection
Difference-in-differences (DID-INT) which can provide an unbiased estimate of
the ATT under two-way CCC violations. DID-INT can also identify the ATT under
heterogeneous treatment effects and with staggered treatment rollout. The
estimator relies on parallel trends of the residuals of the outcome variable,
after appropriately adjusting for covariates. This covariate residualization
can recover parallel trends that are hidden with conventional estimators.",2024
http://arxiv.org/abs/2412.14400v2,On Monotone Persuasion,2024-12-18 23:15:23+00:00,"['Anton Kolotilin', 'Hongyi Li', 'Andriy Zapechelnyuk']",econ.TH,"We study monotone persuasion in the linear case, where posterior
distributions over states are summarized by their mean. We solve the two
leading cases where optimal unrestricted signals can be nonmonotone. First, if
the objective is s-shaped and the state is discrete, then optimal monotone
signals are upper censorship, whereas optimal unrestricted signals may require
randomization. Second, if the objective is m-shaped and the state is
continuous, then optimal monotone signals are interval disclosure, whereas
optimal unrestricted signals may require nonmonotone pooling. We illustrate our
results with an application to media censorship.",2024
http://arxiv.org/abs/2412.14370v3,The Role of Patents: Incentivizing Innovation or Hindering Progress?,2024-12-18 22:08:29+00:00,['Gaetan de Rassenfosse'],econ.GN,"This article examines the complex trade-offs inherent in the patent system,
exploring whether patents truly incentivize innovation or inadvertently hinder
progress. It traces the historical evolution of patent rights from their
origins in Renaissance Venice to the modern framework enshrined in
constitutional and international law. By balancing the exclusivity granted to
inventors with the need for public access to knowledge, the article highlights
how patents stimulate R&D investments while sometimes limiting follow-on
innovation due to prolonged monopolies. It discusses the economic rationales
for patent protection, such as increasing private returns to encourage
invention, against criticisms that patents restrict the free flow of ideas
essential for collective progress. Ultimately, the review argues that although
patents are designed to reward creativity and secure economic benefits, they
remain a contentious instrument whose benefits and limitations must be
carefully weighed in policy debates.",2024
http://arxiv.org/abs/2412.14307v1,Race Discrimination in Internet Advertising: Evidence From a Field Experiment,2024-12-18 20:19:57+00:00,"['Neil K. R. Sehgal', 'Dan Svirsky']",cs.CY,"We present the results of an experiment documenting racial bias on Meta's
Advertising Platform in Brazil and the United States. We find that darker skin
complexions are penalized, leading to real economic consequences. For every
\$1,000 an advertiser spends on ads with models with light-skin complexions,
that advertiser would have to spend \$1,159 to achieve the same level of
engagement using photos of darker skin complexion models. Meta's budget
optimization tool reinforces these viewer biases. When pictures of models with
light and dark complexions are allocated a shared budget, Meta funnels roughly
64\% of the budget towards photos featuring lighter skin complexions.",2024
http://arxiv.org/abs/2412.13689v1,A bibliometric analysis and scoping study to identify English-language perspectives on slums,2024-12-18 10:26:30+00:00,"['Katharina Henn', 'Michaela Lestakova', 'Kevin Logan', 'Jakob Hartig', 'Stefanos Georganos', 'John Friesen']",econ.GN,"Slums, informal settlements, and deprived areas are urban regions
characterized by poverty. According to the United Nations, over one billion
people reside in these areas, and this number is projected to increase.
Additionally, these settlements are integral components of urban systems. We
conducted a bibliometrical analysis and scoping study using the Web of Science
Database to explore various perspectives on urban poverty, searching for
scientific publications on the topic and providing details on the countries
where the studies were conducted. Based on 3947 publications, we identify the
extent to which domestic research organizations participate in studying urban
poverty and which categories of science they investigate, including life
sciences \& biomedicine, social sciences, technology, physical sciences, and
arts & humanities. Thereby, we find that research on slums is often limited to
specific countries, e.g. India, South Africa, Kenya, or Brazil. This focus is
not necessarily correlated with the number of people living in slums. The
scientific discourse is up to now predominantly shaped by medical and social
sciences with few studies addressing technological questions. Finally, our
analysis identifies several possible future directions for research on slums.",2024
http://arxiv.org/abs/2412.13556v2,An Analysis of the Relationship Between the Characteristics of Innovative Consumers and the Degree of Serious Leisure in User Innovation,2024-12-18 07:13:17+00:00,"['Taichi Abe', 'Yasunobu Morita']",econ.EM,"This study examines the relationship between the concept of serious leisure
and user innovation. We adopted the characteristics of innovative consumers
identified by Luthje (2004)-product use experience, information exchange, and
new product adoption speed-to analyze their correlation with serious leisure
engagement. The analysis utilized consumer behavior survey data from the
""Marketing Analysis Contest 2023"" sponsored by Nomura Research Institute,
examining the relationship between innovative consumer characteristics and the
degree of serious leisure (Serious Leisure Inventory and Measure: SLIM). Since
the contest data did not directly measure innovative consumer characteristics
or serious leisure engagement, we established alternative variables for
quantitative analysis. The results showed that the SLIM alternative variable
had positive correlations with diverse product experiences and early adoption
of new products. However, no clear relationship was found with information
exchange among consumers. These findings suggest that serious leisure practice
may serve as a potential antecedent to user innovation. The leisure career
perspective of the serious leisure concept may capture the motivations of user
innovators that Okada and Nishikawa (2019) identified.",2024
http://arxiv.org/abs/2412.13413v1,System and sub-system energy resilience during public safety power shutoffs (PSPS) in California -- An evidence-based argument,2024-12-18 01:10:49+00:00,"['Daniel Thompson', 'Gianluca Pescaroli', 'Maham Furqan']",econ.GN,"This study examines historical relationships between Public Safety Power
Shutoffs (PSPS) events enacted by California's investor-owned utilities (IOUs),
at the system and sub-system levels, along with other disruptions to macro
electricity systems. This study contributes to understanding the balance
between system-wide resilience goals, such as wildfire hazard mitigation, and
sub-system-level priorities, such as minimizing the frequency and duration of
localized disruptions. Focusing on circuit-level data from 2018 to 2023 as a
proxy for sub-systems, we evaluate differences in outage frequency, duration,
and customer impact across three major IOUs in California. Results highlight a
differentiation between 'higher impact' de-energization events, which have
occurred less frequently, and circuits impacted frequently but with lower
customer or duration impacts. Study outcomes suggest that resilience, from the
perspective of PSPS events, may be more temporal, which in this case is driven
by infrastructure and planning investments by IOUs. Future work aims to
incorporate socio-demographic factors, including urban-rural divide, to
identify further opportunities to enhance resilience at the circuit and
sub-circuit levels.",2024
http://arxiv.org/abs/2412.13362v1,Modeling coskewness with zero correlation and correlation with zero coskewness,2024-12-17 22:31:14+00:00,"['Carole Bernard', 'Jinghui Chen', 'Steven Vanduffel']",math.PR,"This paper shows that one needs to be careful when making statements on
potential links between correlation and coskewness. Specifically, we first show
that, on the one hand, it is possible to observe any possible values of
coskewness among symmetric random variables but zero pairwise correlations of
these variables. On the other hand, it is also possible to have zero coskewness
and any level of correlation. Second, we generalize this result to the case of
arbitrary marginal distributions showing the absence of a general link between
rank correlation and standardized rank coskewness.",2024
http://arxiv.org/abs/2412.13172v1,Expressions of Market-Based Correlations Between Prices and Returns of Two Assets,2024-12-17 18:52:19+00:00,['Victor Olkhov'],econ.GN,"This paper derives the expressions of correlations between prices of two
assets, returns of two assets, and price-return correlations of two assets that
depend on statistical moments and correlations of the current values, past
values, and volumes of their market trades. The usual frequency-based
expressions of correlations of time series of prices and returns describe a
partial case of our model when all trade volumes and past trade values are
constant. Such an assumptions are rather far from market reality, and its use
results in excess losses and wrong forecasts. Traders, banks, and funds that
perform multi-million market transactions or manage billion-valued portfolios
should consider the impact of large trade volumes on market prices and returns.
The use of the market-based correlations of prices and returns of two assets is
mandatory for them. The development of macroeconomic models and market
forecasts like those being created by BlackRock's Aladdin, JP Morgan, and the
U.S. Fed., is impossible without the use of market-based correlations of prices
and returns of two assets.",2024
http://arxiv.org/abs/2412.13076v1,Dual Interpretation of Machine Learning Forecasts,2024-12-17 16:44:39+00:00,"['Philippe Goulet Coulombe', 'Maximilian Goebel', 'Karin Klieber']",econ.EM,"Machine learning predictions are typically interpreted as the sum of
contributions of predictors. Yet, each out-of-sample prediction can also be
expressed as a linear combination of in-sample values of the predicted
variable, with weights corresponding to pairwise proximity scores between
current and past economic events. While this dual route leads nowhere in some
contexts (e.g., large cross-sectional datasets), it provides sparser
interpretations in settings with many regressors and little training data-like
macroeconomic forecasting. In this case, the sequence of contributions can be
visualized as a time series, allowing analysts to explain predictions as
quantifiable combinations of historical analogies. Moreover, the weights can be
viewed as those of a data portfolio, inspiring new diagnostic measures such as
forecast concentration, short position, and turnover. We show how weights can
be retrieved seamlessly for (kernel) ridge regression, random forest, boosted
trees, and neural networks. Then, we apply these tools to analyze post-pandemic
forecasts of inflation, GDP growth, and recession probabilities. In all cases,
the approach opens the black box from a new angle and demonstrates how machine
learning models leverage history partly repeating itself.",2024
http://arxiv.org/abs/2412.13013v4,The Emergence of Strategic Reasoning of Large Language Models,2024-12-17 15:34:00+00:00,"['Gavin Kader', 'Dongwoo Lee']",econ.GN,"As large language models (LLMs) have demonstrated strong reasoning abilities
in structured tasks (e.g., coding and mathematics), we explore whether these
abilities extend to strategic multi-agent environments. We investigate
strategic reasoning capabilities -- the process of choosing an optimal course
of action by predicting and adapting to others' actions -- of LLMs by analyzing
their performance in three classical games from behavioral economics. Using
hierarchical models of bounded rationality, we evaluate three standard LLMs
(ChatGPT-4, Claude-3.5-Sonnet, Gemini 1.5) and three reasoning LLMs (OpenAI-o1,
Claude-4-Sonnet-Thinking, Gemini Flash Thinking 2.0). Our results show that
reasoning LLMs exhibit superior strategic reasoning compared to standard LLMs
(which do not demonstrate substantial capabilities) and often match or exceed
human performance; this represents the first and thus most fundamental
transition in strategic reasoning capabilities documented in LLMs. Since
strategic reasoning is fundamental to future AI systems (including Agentic AI),
our findings demonstrate the importance of dedicated reasoning capabilities in
achieving effective strategic reasoning.",2024
http://arxiv.org/abs/2412.12784v1,Digital Transformation in Switzerland: The Current State and Expectations,2024-12-17 10:44:07+00:00,"['Johannes Lehmann', 'Michael Beckmann']",econ.GN,"This paper provides a comprehensive, descriptive overview of the current
state of digital transformation in the Swiss economy and delineates areas that
businesses should keep an eye on. Key findings illustrate that even established
technologies are not universally adopted, that companies tend to overestimate
their technological status compared to their competitors, and that it is
important to have the appropriate technological know-how when introducing new
technologies. In addition, companies expect changes in their work processes and
employment conditions in connection with the digital transformation.
Specifically, work tasks are expected to become more complex, diverse and
varied. Employees' knowledge acquisition will gain in importance, especially in
the form of formal further training and self-learning. Employees will also be
more autonomous in making decisions about their jobs and working hours.",2024
http://arxiv.org/abs/2412.12780v1,Digital technologies and performance incentives: Evidence from businesses in the Swiss economy,2024-12-17 10:40:31+00:00,"['Johannes Lehmann', 'Michael Beckmann']",econ.GN,"Using novel survey data from Swiss firms, this paper empirically examines the
relationship between the use of digital technologies and the prevalence of
performance incentives. We argue that digital technologies tend to reduce the
cost of organizational monitoring through improved measurement of employee
behavior and performance, as well as through employee substitution in
conjunction with a reduced agency problem. While we expect the former mechanism
to increase the prevalence of performance incentives, the latter is likely to
decrease it. Our doubly robust ATE estimates show that companies using business
software and certain key technologies of Industry 4.0 increasingly resort to
performance incentives, suggesting that the improved measurement effect
dominates the employee substitution effect. In addition, we find that companies
emerging as technology-friendly use performance incentives more frequently than
their technology-averse counterparts. Both findings hold for managerial and
non-managerial employees. Our estimation results are robust to a variety of
sensitivity checks and suggest that Swiss businesses leverage digital
technologies to enhance control over production or service processes, allowing
them to intensify their management of employees through performance incentives.",2024
http://arxiv.org/abs/2412.12752v1,Organizational culture and the usage of Industry 4.0 technologies: evidence from Swiss businesses,2024-12-17 10:16:16+00:00,"['Simon Alexander Wiese', 'Johannes Lehmann', 'Michael Beckmann']",econ.GN,"Using novel establishment-level observational data from Switzerland, we
empirically examine whether the usage of key technologies of Industry 4.0
distinguishes across firms with different types of organizational culture.
Based on the Technology-Organization-Environment and the Competing Values
framework, we hypothesize that the developmental culture has the greatest
potential to promote the usage of Industry 4.0 technologies. We also
hypothesize that companies with a hierarchical or rational culture are
especially likely to make use of automation technologies, such as AI and
robotics. By means of descriptive statistics and multiple regression analysis,
we find empirical support for our first hypothesis, while we cannot con-firm
our second hypothesis. Our empirical results provide important implications for
managerial decision-makers. Specifically, the link between organizational
culture and the implementation of Industry 4.0 technologies is relevant for
managers, as this knowledge helps them to cope with digital transformation in
turbulent times and keep their businesses competitive.",2024
http://arxiv.org/abs/2412.12721v1,"Information, entropy and the paradox of choice: A theoretical framework for understanding choice satisfaction",2024-12-17 09:38:24+00:00,"['Mojtaba Madadi Asl', 'Kamal Hajian', 'Rouzbeh Torabi', 'Mehdi Sadeghi']",physics.soc-ph,"Choice overload occurs when individuals feel overwhelmed by an excessive
number of options. Experimental evidence suggests that a larger selection can
complicate the decision-making process. Consequently, choice satisfaction may
diminish when the costs of making a choice outweigh its benefits, indicating
that satisfaction follows an inverted U-shaped relationship with the size of
the choice set. However, the theoretical underpinnings of this phenomenon
remain underexplored. Here, we present a theoretical framework based on
relative entropy and effective information to elucidate the inverted U-shaped
relationship between satisfaction and choice set size. We begin by positing
that individuals assign a probability distribution to a choice set based on
their preferences, characterized by an observed Shannon entropy. We then define
a maximum entropy that corresponds to a worst-case scenario where individuals
are indifferent among options, leading to equal probabilities for all
alternatives. We hypothesized that satisfaction is related to the probability
of identifying an ideal choice within the set. By comparing observed entropy to
maximum entropy, we derive the effective information of choice probabilities,
demonstrating that this metric reflects satisfaction with the options
available. For smaller choice sets, individuals can more easily identify their
best option, resulting in a sharper probability distribution around the
preferred choice and, consequently, minimum entropy, which signifies maximum
information and satisfaction. Conversely, in larger choice sets, individuals
struggle to compare and evaluate all alternatives, leading to missed
opportunities and increased entropy. This smooth probability distribution
ultimately reduces choice satisfaction, thereby producing the observed inverted
U-shaped trend.",2024
http://arxiv.org/abs/2412.12676v1,Raising Bidders' Awareness in Second-Price Auctions,2024-12-17 08:46:22+00:00,"['Ying Xue Li', 'Burkhard C. Schipper']",econ.TH,"When bidders bid on complex objects, they might be unaware of characteristics
effecting their valuations. We assume that each buyer's valuation is a sum of
independent random variables, one for each characteristic. When a bidder is
unaware of a characteristic, he omits the random variable from the sum. We
study the seller's decision to raise bidders' awareness of characteristics
before a second-price auction with entry fees. Optimal entry fees capture an
additional unawareness rent due to unaware bidders misperceiving their
probability of winning and the price to be paid upon winning. When raising a
bidder's individual awareness of a characteristic with positive expected value,
the seller faces a trade-off between positive effects on the expected first
order statistic and unawareness rents of remaining unaware bidders on one hand
and the loss of the unawareness rent from the newly aware bidder on the other.
We present characterization results on raising public awareness together with
no versus full information. We discuss the winner's curse due to unawareness of
characteristics.",2024
http://arxiv.org/abs/2412.12610v2,Gender Bias and Property Taxes,2024-12-17 07:14:23+00:00,"['Gordon Burtch', 'Alejandro Zentner']",econ.GN,"Gender bias distorts the economic behavior and outcomes of women and
households. We investigate gender biases in property taxes. We analyze records
of more than 100,000 property tax appeal hearings and more than 2.7 years of
associated audio recordings, considering how panelist and appellant genders
associate with hearing outcomes. We first observe that female appellants fare
systematically worse than male appellants in their hearings. Second, we show
that, whereas male appellants' hearing outcomes do not vary meaningfully with
the gender composition of the panel they face, those of female appellants' do,
such that female appellants obtain systematically lesser (greater) reductions
to their home values when facing female (male) panelists. Employing a
multi-modal large language model (M-LLM), we next construct measures of
participant behavior and tone from hearing audio recordings. We observe
markedly different behaviors between male and female appellants and, in the
case of male appellants, we find that these differences also depend on the
gender of the panelists they face (e.g., male appellants appear to behave
systematically more aggressively towards female panelists). In contrast, the
behavior of female appellants remains relatively constant, regardless of their
panel's gender. Finally, we show that female appellants continue to fare worse
in front of female panels, even when we condition upon the appelant's
in-hearing behavior and tone. Our results are thus consistent with the idea
that gender biases are driven, at least in part, by unvoiced beliefs and
perceptions on the part of ARB panelists. Our study documents the presence of
gender biases in property appraisal appeal hearings and highlights the
potential value of generative AI for analyzing large-scale, unstructured
administrative data.",2024
http://arxiv.org/abs/2412.12495v1,"Obvious manipulations, consistency, and the uniform rule",2024-12-17 02:53:45+00:00,"['R. Pablo Arribillaga', 'Agustin G. Bonifacio']",econ.TH,"In the problem of fully allocating an infinitely divisible commodity among
agents whose preferences are single-peaked, we show that the uniform rule is
the only allocation rule that satisfies efficiency, the equal division
guarantee, consistency, and non-obvious manipulability.",2024
http://arxiv.org/abs/2412.12393v1,Emergence of Power-Law and Other Wealth Distributions in Crowd of Heterogeneous Agents,2024-12-16 23:01:39+00:00,['Jake J. Xia'],econ.GN,"This study investigates the emergence of power-law and other concentrated
distributions through a feedback loop model in crowd interactions. Agents act
by their response functions to observations and external forces, while
observations change by the aggregated actions of all agents, weighted by their
respective influence, i.e. power or wealth. Agents wealth dynamically adjust
based on the alignment between an agents actions and observation outcomes:
agents gain wealth when their actions align with observed trends and lose
wealth otherwise. A reward function, that describes the change of agents wealth
at each time step, manifests the differences of response functions of agents to
observations. When all agents responses are set to zero and feedback loop is
broken, agents wealth follow a normal or lognormal distribution. Otherwise,
this response-reward iterative feedback mechanism results in concentrated
wealth distributions, characterized by a small number of dominant agents and
the marginalization of the majority. Contrasted to past studies, such
concentration is not limited only to asymptotic behavior at the upper tail for
large variables, nor does it require the reward function to be linear to agents
previous wealth as formulated in random growth model and network preferential
attachment. Probability density functions for various distributions are more
visually distinguishable for small values at the lower tail. In application of
this model, key differences in income and wealth distributions in the US vs
Japan are attributed to different response functions of agents in the two
countries. The model applicability extends beyond social systems to other
many-body systems with analogous feedback mechanisms, where power-law
distributions represent a rare subset of general concentrated outcomes.",2024
http://arxiv.org/abs/2412.11984v1,Quantifying Inefficiency,2024-12-16 17:04:57+00:00,"['Yannai A. Gonczarowski', 'Ella Segev']",econ.TH,"We axiomatically define a cardinal social inefficiency function, which, given
a set of alternatives and individuals' vNM preferences over the alternatives,
assigns a unique number -- the social inefficiency -- to each alternative.
These numbers -- and not only their order -- are uniquely defined by our axioms
despite no exogenously given interpersonal comparison, outside option, or
disagreement point. We interpret these numbers as per capita losses in
endogenously normalized utility. We apply our social inefficiency function to a
setting in which interpersonal comparison is notoriously hard to justify --
object allocation without money -- leveraging techniques from computer science
to prove an approximate-efficiency result for the Random Serial Dictatorship
mechanism.",2024
http://arxiv.org/abs/2412.11977v1,Weak Strategyproofness in Randomized Social Choice,2024-12-16 16:59:14+00:00,"['Felix Brandt', 'Patrick Lederer']",cs.GT,"An important -- but very demanding -- property in collective decision-making
is strategyproofness, which requires that voters cannot benefit from submitting
insincere preferences. Gibbard (1977) has shown that only rather unattractive
rules are strategyproof, even when allowing for randomization. However,
Gibbard's theorem is based on a rather strong interpretation of
strategyproofness, which deems a manipulation successful if it increases the
voter's expected utility for at least one utility function consistent with his
ordinal preferences. In this paper, we study weak strategyproofness, which
deems a manipulation successful if it increases the voter's expected utility
for all utility functions consistent with his ordinal preferences. We show how
to systematically design attractive, weakly strategyproof social decision
schemes (SDSs) and explore their limitations for both strict and weak
preferences. In particular, for strict preferences, we show that there are
weakly strategyproof SDSs that are either ex post efficient or
Condorcet-consistent, while neither even-chance SDSs nor pairwise SDSs satisfy
both properties and weak strategyproofness at the same time. By contrast, for
the case of weak preferences, we discuss two sweeping impossibility results
that preclude the existence of appealing weakly strategyproof SDSs.",2024
http://arxiv.org/abs/2412.11957v2,Multiplexing in Networks and Diffusion,2024-12-16 16:41:03+00:00,"['Arun G. Chandrasekhar', 'Vasu Chaudhary', 'Benjamin Golub', 'Matthew O. Jackson']",econ.GN,"Social and economic networks are often multiplexed, meaning that people are
connected by different types of relationships -- such as borrowing goods and
giving advice. We make two contributions to the study of multiplexing and the
understanding of simple versus complex contagion. On the theoretical side, we
introduce a model and theoretical results about diffusion in multiplex
networks. We show that multiplexing impedes the spread of simple contagions,
such as diseases or basic information that only require one interaction to
transmit an infection. We show, however that multiplexing enhances the spread
of a complex contagion when infection rates are low, but then impedes complex
contagion if infection rates become high. On the empirical side, we document
empirical multiplexing patterns in Indian village data. We show that
relationships such as socializing, advising, helping, and lending are
correlated but distinct, while commonly used proxies for networks based on
ethnicity and geography are nearly uncorrelated with actual relationships. We
also show that these layers and their overlap affect information diffusion in a
field experiment. The advice network is the best predictor of diffusion, but
combining layers improves predictions further. Villages with greater overlap
between layers -- more multiplexing -- experience less overall diffusion.
Finally, we identify differences in multiplexing by gender and connectedness.
These have implications for inequality in diffusion-mediated outcomes such as
access to information and adherence to norms.",2024
http://arxiv.org/abs/2412.11597v1,Transition dynamics of electricity asset-owning firms,2024-12-16 09:36:15+00:00,['Anton Pichler'],econ.GN,"Despite dramatic growth and cost improvements in renewables, existing energy
companies exhibit significant inertia in adapting to the evolving technological
landscape. This study examines technology transition patterns by analyzing over
140,000 investments in power assets over more than two decades, focusing on how
firms expand existing technology holdings and adopt new technologies. Building
on our comprehensive micro-level dataset, we provide a number of quantitative
metrics on global investment dynamism and the evolution of technology
portfolios. We find that only about 10\% of firms experience capacity changes
in a given year, and that technology portfolios of firms are highly
concentrated and persistent in time. We also identify a small subset of
frequently investing firms that tend to be large and are key drivers of global
technology-specific capacity expansion. Technology transitions within companies
are extremely rare. Less than 3% of the more than 8,400 fossil fuel-dominated
firms have substantially transformed their portfolios to a renewable focus and
firms fully transitioning to renewables are, up-to-date, virtually
non-existent. Notably, firms divesting into renewables do not exhibit very
characteristic technology-transition patterns but rather follow idiosyncratic
transition pathways. Our results quantify the complex technology diffusion
dynamics and the diverse corporate responses to a changing technology
landscape, highlighting the challenge of designing general policies aimed at
fostering technological transitions at the level of firms.",2024
http://arxiv.org/abs/2412.11285v1,Moderating the Mediation Bootstrap for Causal Inference,2024-12-15 19:15:29+00:00,"['Kees Jan van Garderen', 'Noud van Giersbergen']",econ.EM,"Mediation analysis is a form of causal inference that investigates indirect
effects and causal mechanisms. Confidence intervals for indirect effects play a
central role in conducting inference. The problem is non-standard leading to
coverage rates that deviate considerably from their nominal level. The default
inference method in the mediation model is the paired bootstrap, which
resamples directly from the observed data. However, a residual bootstrap that
explicitly exploits the assumed causal structure (X->M->Y) could also be
applied. There is also a debate whether the bias-corrected (BC) bootstrap
method is superior to the percentile method, with the former showing liberal
behavior (actual coverage too low) in certain circumstances. Moreover,
bootstrap methods tend to be very conservative (coverage higher than required)
when mediation effects are small. Finally, iterated bootstrap methods like the
double bootstrap have not been considered due to their high computational
demands. We investigate the issues mentioned in the simple mediation model by a
large-scale simulation. Results are explained using graphical methods and the
newly derived finite-sample distribution. The main findings are: (i)
conservative behavior of the bootstrap is caused by extreme dependence of the
bootstrap distribution's shape on the estimated coefficients (ii) this
dependence leads to counterproductive correction of the the double bootstrap.
The added randomness of the BC method inflates the coverage in the absence of
mediation, but still leads to (invalid) liberal inference when the mediation
effect is small.",2024
http://arxiv.org/abs/2412.11278v2,VAR models with an index structure: A survey with new results,2024-12-15 18:55:09+00:00,['Gianluca Cubadda'],econ.EM,"The main aim of this paper is to review recent advances in the multivariate
autoregressive index model [MAI], originally proposed by Reinsel (1983), and
their applications to economic and financial time series. MAI has recently
gained momentum because it can be seen as a link between two popular but
distinct multivariate time series approaches: vector autoregressive modeling
[VAR] and the dynamic factor model [DFM]. Indeed, on the one hand, the MAI is a
VAR model with a peculiar reduced-rank structure; on the other hand, it allows
for identification of common components and common shocks in a similar way as
the DFM. The focus is on recent developments of the MAI, which include
extending the original model with individual autoregressive structures,
stochastic volatility, time-varying parameters, high-dimensionality, and
cointegration. In addition, new insights on previous contributions and a novel
model are also provided.",2024
http://arxiv.org/abs/2412.11259v1,Navigating through Economic Complexity: Phase Diagrams & Parameter Sloppiness,2024-12-15 17:44:35+00:00,['Jean-Philippe Bouchaud'],econ.TH,"We argue that establishing the phase diagram of Agent Based Models (ABM) is a
crucial first step, together with a qualitative understanding of how collective
phenomena come about, before any calibration or more quantitative predictions
are attempted. Computer-aided *gedanken* experiments are by themselves of
genuine value: if we are not able to make sense of emergent phenomena in a
world in which we set all the rules, how can we expect to be successful in the
real world? ABMs indeed often reveal the existence of Black Swans/Dark Corners
i.e. discontinuity lines beyond which runaway instabilities appear, whereas
most classical economic/finance models are blind to such scenarii. Testing for
the overall robustness of the phase diagram against changes in heuristic rules
is a way to ascertain the plausibility of such scenarii. Furthermore, exploring
the phase diagrams of ABM in high dimensions should benefit enormously from the
identification of ``stiff'' and ``sloppy'' directions in parameter space.",2024
http://arxiv.org/abs/2412.11179v1,Treatment Evaluation at the Intensive and Extensive Margins,2024-12-15 13:07:22+00:00,"['Phillip Heiler', 'Asbjørn Kaufmann', 'Bezirgen Veliyev']",econ.EM,"This paper provides a solution to the evaluation of treatment effects in
selective samples when neither instruments nor parametric assumptions are
available. We provide sharp bounds for average treatment effects under a
conditional monotonicity assumption for all principal strata, i.e. units
characterizing the complete intensive and extensive margins. Most importantly,
we allow for a large share of units whose selection is indifferent to
treatment, e.g. due to non-compliance. The existence of such a population is
crucially tied to the regularity of sharp population bounds and thus
conventional asymptotic inference for methods such as Lee bounds can be
misleading. It can be solved using smoothed outer identification regions for
inference. We provide semiparametrically efficient debiased machine learning
estimators for both regular and smooth bounds that can accommodate
high-dimensional covariates and flexible functional forms. Our study of active
labor market policy reveals the empirical prevalence of the aforementioned
indifference population and supports results from previous impact analysis
under much weaker assumptions.",2024
http://arxiv.org/abs/2412.11122v2,Paid with Models: Optimal Contract Design for Collaborative Machine Learning,2024-12-15 08:55:16+00:00,"['Bingchen Wang', 'Zhaoxuan Wu', 'Fusheng Liu', 'Bryan Kian Hsiang Low']",cs.LG,"Collaborative machine learning (CML) provides a promising paradigm for
democratizing advanced technologies by enabling cost-sharing among
participants. However, the potential for rent-seeking behaviors among parties
can undermine such collaborations. Contract theory presents a viable solution
by rewarding participants with models of varying accuracy based on their
contributions. However, unlike monetary compensation, using models as rewards
introduces unique challenges, particularly due to the stochastic nature of
these rewards when contribution costs are privately held information. This
paper formalizes the optimal contracting problem within CML and proposes a
transformation that simplifies the non-convex optimization problem into one
that can be solved through convex optimization algorithms. We conduct a
detailed analysis of the properties that an optimal contract must satisfy when
models serve as the rewards, and we explore the potential benefits and welfare
implications of these contract-driven CML schemes through numerical
experiments.",2024
http://arxiv.org/abs/2412.11113v1,Optimal Strategy-proof Mechanisms on Single-crossing Domains,2024-12-15 08:23:15+00:00,['Mridu Prabal Goswami'],cs.GT,"We consider an economic environment with one buyer and one seller. For a
bundle $(t,q)\in [0,\infty[\times [0,1]=\mathbb{Z}$, $q$ refers to the winning
probability of an object, and $t$ denotes the payment that the buyer makes. We
consider continuous and monotone preferences on $\mathbb{Z}$ as the primitives
of the buyer. These preferences can incorporate both quasilinear and
non-quasilinear preferences, and multidimensional pay-off relevant parameters.
We define rich single-crossing subsets of this class and characterize
strategy-proof mechanisms by using monotonicity of the mechanisms and
continuity of the indirect preference correspondences. We also provide a
computationally tractable optimization program to compute the optimal mechanism
for mechanisms with finite range. We do not use revenue equivalence and virtual
valuations as tools in our proofs. Our proof techniques bring out the geometric
interaction between the single-crossing property and the positions of bundles
$(t,q)$s in the space $\mathbb{Z}$. We also provide an extension of our
analysis to an $n-$buyer environment, and to the situation where $q$ is a
qualitative variable.",2024
http://arxiv.org/abs/2510.27636v1,Delegate Pricing Decisions to an Algorithm? Experimental Evidence,2025-10-31 17:07:29+00:00,"['Hans-Theo Normann', 'Nina Rulié', 'Olaf Stypa', 'Tobias Werner']",econ.GN,"We analyze the delegation of pricing by participants, representing firms, to
a collusive, self-learning algorithm in a repeated Bertrand experiment. In the
baseline treatment, participants set prices themselves. In the other
treatments, participants can either delegate pricing to the algorithm at the
beginning of each supergame or receive algorithmic recommendations that they
can override. Participants delegate more when they can override the algorithm's
decisions. In both algorithmic treatments, prices are lower than in the
baseline. Our results indicate that while self-learning pricing algorithms can
be collusive, they can foster competition rather than collusion with
humans-in-the-loop.",2025
http://arxiv.org/abs/2510.27625v1,Hiring Intrinsically Motivated Agents: A Principal's Dilemma,2025-10-31 16:53:42+00:00,['Andrew Leal'],econ.GN,"Employers are concerned not only with a prospective worker's ability, but
also their propensity to avoid shirking. This paper proposes a new experimental
framework to study how Principals trade-off measures of ability and prosocial
behavior when ranking Agents for independent jobs. Subjects participate in a
simulated, incentivized job market. In an initial session, subjects are Workers
and generate a database of signals and job results. Managers in subsequent
sessions observe the signals of Worker behavior and ability and job details
before a rank-and-value task, ranking and reporting a value for each Worker for
two distinct jobs. Results highlight Managers' preference for ability over
prosocial behavior on average, especially for Managers in STEM fields. There is
evidence of homophily: the relative value of prosocial behavior is higher for
highly prosocial Managers, compensating for ability or even surpassing it in
value.",2025
http://arxiv.org/abs/2510.27112v1,Targeted Advertising Platforms: Data Sharing and Customer Poaching,2025-10-31 02:25:36+00:00,['Klajdi Hoxha'],econ.TH,"E-commerce platforms are rolling out ambitious targeted advertising
initiatives that rely on merchants sharing customer data with each other via
the platform. Yet current platform designs fail to address participating
merchants' concerns about customer poaching. This paper proposes a model of
designing targeted advertising platforms that incentivizes merchants to
voluntarily share customer data despite poaching concerns. I characterize the
optimal mechanism that maximizes a weighted sum of platform's revenues,
customer engagement and merchants' surplus. In sufficiently large platforms,
the optimal mechanism can be implemented through the design of three markets:
$i)$ selling market, where merchants can sell all their data at a posted price
$p$, $ii)$ exchange market, where merchants share all their data in exchange
for high click-through rate (CTR) ads, and $iii)$ buying market, where
high-value merchants buy high CTR ads at the full price. The model is broad in
scope with applications in other market design settings like the greenhouse gas
credit markets and reallocating public resources, and points toward new
directions in combinatorial market exchange designs.",2025
http://arxiv.org/abs/2510.27008v1,Algorithmic Predation: Equilibrium Analysis in Dynamic Oligopolies with Smooth Market Sharing,2025-10-30 21:14:24+00:00,"['Fabian Raoul Pieroth', 'Ole Petersen', 'Martin Bichler']",cs.GT,"Predatory pricing -- where a firm strategically lowers prices to undermine
competitors -- is a contentious topic in dynamic oligopoly theory, with
scholars debating practical relevance and the existence of predatory
equilibria. Although finite-horizon dynamic models have long been proposed to
capture the strategic intertemporal incentives of oligopolists, the existence
and form of equilibrium strategies in settings that allow for firm exit
(drop-outs following loss-making periods) have remained an open question. We
focus on the seminal dynamic oligopoly model by Selten (1965) that introduces
the subgame perfect equilibrium and analyzes smooth market sharing. Equilibrium
can be derived analytically in models that do not allow for dropouts, but not
in models that can lead to predatory pricing. In this paper, we leverage recent
advances in deep reinforcement learning to compute and verify equilibria in
finite-horizon dynamic oligopoly games. Our experiments reveal two key
findings: first, state-of-the-art deep reinforcement learning algorithms
reliably converge to equilibrium in both perfect- and imperfect-information
oligopoly models; second, when firms face asymmetric cost structures, the
resulting equilibria exhibit predatory pricing behavior. These results
demonstrate that predatory pricing can emerge as a rational equilibrium
strategy across a broad variety of model settings. By providing equilibrium
analysis of finite-horizon dynamic oligopoly models with drop-outs, our study
answers a decade-old question and offers new insights for competition
authorities and regulators.",2025
http://arxiv.org/abs/2510.26957v1,Predicting Household Water Consumption Using Satellite and Street View Images in Two Indian Cities,2025-10-30 19:32:34+00:00,"['Qiao Wang', 'Joseph George']",cs.LG,"Monitoring household water use in rapidly urbanizing regions is hampered by
costly, time-intensive enumeration methods and surveys. We investigate whether
publicly available imagery-satellite tiles, Google Street View (GSV)
segmentation-and simple geospatial covariates (nightlight intensity, population
density) can be utilized to predict household water consumption in
Hubballi-Dharwad, India. We compare four approaches: survey features
(benchmark), CNN embeddings (satellite, GSV, combined), and GSV semantic maps
with auxiliary data. Under an ordinal classification framework, GSV
segmentation plus remote-sensing covariates achieves 0.55 accuracy for water
use, approaching survey-based models (0.59 accuracy). Error analysis shows high
precision at extremes of the household water consumption distribution, but
confusion among middle classes is due to overlapping visual proxies. We also
compare and contrast our estimates for household water consumption to that of
household subjective income. Our findings demonstrate that open-access imagery,
coupled with minimal geospatial data, offers a promising alternative to
obtaining reliable household water consumption estimates using surveys in urban
analytics.",2025
http://arxiv.org/abs/2510.26783v1,A Unified Theory for Causal Inference: Direct Debiased Machine Learning via Bregman-Riesz Regression,2025-10-30 17:56:47+00:00,['Masahiro Kato'],stat.ML,"This note introduces a unified theory for causal inference that integrates
Riesz regression, covariate balancing, density-ratio estimation (DRE), targeted
maximum likelihood estimation (TMLE), and the matching estimator in average
treatment effect (ATE) estimation. In ATE estimation, the balancing weights and
the regression functions of the outcome play important roles, where the
balancing weights are referred to as the Riesz representer, bias-correction
term, and clever covariates, depending on the context. Riesz regression,
covariate balancing, DRE, and the matching estimator are methods for estimating
the balancing weights, where Riesz regression is essentially equivalent to DRE
in the ATE context, the matching estimator is a special case of DRE, and DRE is
in a dual relationship with covariate balancing. TMLE is a method for
constructing regression function estimators such that the leading bias term
becomes zero. Nearest Neighbor Matching is equivalent to Least Squares Density
Ratio Estimation and Riesz Regression.",2025
http://arxiv.org/abs/2510.26727v1,Neither Consent nor Property: A Policy Lab for Data Law,2025-10-30 17:27:03+00:00,"['Haoyi Zhang', 'Tianyi Zhu']",econ.GN,"This paper makes the opaque data market in the AI economy empirically legible
for the first time, constructing a computational testbed to address a core
epistemic failure: regulators governing a market defined by structural opacity,
fragile price discovery, and brittle technical safeguards that have paralyzed
traditional empirics and fragmented policy. The pipeline begins with multi-year
fieldwork to extract the market's hidden logic, and then embeds these grounded
behaviors into a high-fidelity ABM, parameterized via a novel LLM-based
discrete-choice experiment that captures the preferences of unsurveyable
populations. The pipeline is validated against reality, reproducing observed
trade patterns. This policy laboratory delivers clear, counter-intuitive
results. First, property-style relief is a false promise: ''anonymous-data''
carve-outs expand trade but ignore risk, causing aggregate welfare to collapse
once external harms are priced in. Second, social welfare peaks when the
downstream buyer internalizes the full substantive risk. This least-cost
avoider approach induces efficient safeguards, simultaneously raising welfare
and sustaining trade, and provides a robust empirical foundation for the legal
drift toward two-sided reachability. The contribution is a reproducible
pipeline designed to end the reliance on intuition. It converts qualitative
insight into testable, comparative policy experiments, obsoleting armchair
conjecture by replacing it with controlled evidence on how legal rules actually
shift risk and surplus. This is the forward-looking engine that moves the field
from competing intuitions to direct, computational analysis.",2025
http://arxiv.org/abs/2510.26723v1,Bridging the Gap between Empirical Welfare Maximization and Conditional Average Treatment Effect Estimation in Policy Learning,2025-10-30 17:23:40+00:00,['Masahiro Kato'],stat.ML,"The goal of policy learning is to train a policy function that recommends a
treatment given covariates to maximize population welfare. There are two major
approaches in policy learning: the empirical welfare maximization (EWM)
approach and the plug-in approach. The EWM approach is analogous to a
classification problem, where one first builds an estimator of the population
welfare, which is a functional of policy functions, and then trains a policy by
maximizing the estimated welfare. In contrast, the plug-in approach is based on
regression, where one first estimates the conditional average treatment effect
(CATE) and then recommends the treatment with the highest estimated outcome.
This study bridges the gap between the two approaches by showing that both are
based on essentially the same optimization problem. In particular, we prove an
exact equivalence between EWM and least squares over a reparameterization of
the policy class. As a consequence, the two approaches are interchangeable in
several respects and share the same theoretical guarantees under common
conditions. Leveraging this equivalence, we propose a novel regularization
method for policy learning. Our findings yield a convex and computationally
efficient training procedure that avoids the NP-hard combinatorial step
typically required in EWM.",2025
http://arxiv.org/abs/2510.26636v1,Putting a Price on Immobility: Food Deliveries and Pricing Approaches,2025-10-30 16:05:21+00:00,"['Runyu Wang', 'Haotian Zhong']",econ.GN,"Urban food delivery services have become an integral part of daily life, yet
their mobility and environmental externalities remain poorly addressed by
planners. Most studies neglect whether consumers pay enough to internalize the
broader social costs of these services. This study quantifies the value of
access to and use of food delivery services in Beijing, China, through two
discrete choice experiments. The first measures willingness to accept
compensation for giving up access, with a median value of CNY588 (approximately
USD80). The second captures willingness to pay for reduced waiting time and
improved reliability, showing valuations far exceeding typical delivery fees
(e.g., CNY96.6/hour and CNY4.83/min at work). These results suggest a
substantial consumer surplus and a clear underpricing problem. These findings
highlight the need for urban planning to integrate digital service economies
into pricing and mobility frameworks. We propose a quantity-based pricing model
that targets delivery speed rather than order volume, addressing the primary
source of externalities while maintaining net welfare gains. This approach
offers a pragmatic, equity-conscious strategy to curb delivery-related
congestion, emissions, and safety risks, especially in dense urban cores.",2025
http://arxiv.org/abs/2510.26857v1,Political Power and Mortality: Heterogeneous Effects of the U.S. Voting Rights Act,2025-10-30 16:00:02+00:00,"['Atheendar Venkataramani', ""Rourke O'Brien"", 'Elizabeth Bair', 'Christopher Lowenstein']",econ.GN,"We study the health consequences of redistributing political power through
the 1975 extension of the Voting Rights Act, which eliminated barriers to
voting for previously disenfranchised nonwhite populations. The intervention
led to broad declines in under-five mortality but sharply contrasting effects
in other age groups: mortality fell among non-white children, younger adults,
and older women, yet rose among whites and older non-white men. These
differences cannot be reconciled by changes in population composition or
material conditions. Instead, we present evidence suggesting psychosocial
stress and retaliatory responses arising from perceived status threat as key
mechanisms.",2025
http://arxiv.org/abs/2510.26613v1,Tests of exogeneity in duration models with censored data,2025-10-30 15:39:36+00:00,"['Gilles Crommen', 'Ingrid Van Keilegom', 'Jean-Pierre Florens']",econ.EM,"Consider the setting in which a researcher is interested in the causal effect
of a treatment $Z$ on a duration time $T$, which is subject to right censoring.
We assume that $T=\varphi(X,Z,U)$, where $X$ is a vector of baseline
covariates, $\varphi(X,Z,U)$ is strictly increasing in the error term $U$ for
each $(X,Z)$ and $U\sim \mathcal{U}[0,1]$. Therefore, the model is
nonparametric and nonseparable. We propose nonparametric tests for the
hypothesis that $Z$ is exogenous, meaning that $Z$ is independent of $U$ given
$X$. The test statistics rely on an instrumental variable $W$ that is
independent of $U$ given $X$. We assume that $X,W$ and $Z$ are all categorical.
Test statistics are constructed for the hypothesis that the conditional rank
$V_T= F_{T \mid X,Z}(T \mid X,Z)$ is independent of $(X,W)$ jointly. Under an
identifiability condition on $\varphi$, this hypothesis is equivalent to $Z$
being exogenous. However, note that $V_T$ is censored by $V_C =F_{T \mid X,Z}(C
\mid X,Z)$, which complicates the construction of the test statistics
significantly. We derive the limiting distributions of the proposed tests and
prove that our estimator of the distribution of $V_T$ converges to the uniform
distribution at a rate faster than the usual parametric $n^{-1/2}$-rate. We
demonstrate that the test statistics and bootstrap approximations for the
critical values have a good finite sample performance in various Monte Carlo
settings. Finally, we illustrate the tests with an empirical application to the
National Job Training Partnership Act (JTPA) Study.",2025
http://arxiv.org/abs/2510.26503v1,The sustainability of contribution norms with income dynamics,2025-10-30 13:55:56+00:00,"['Pau Juan-Bartroli', 'Esteban Muñoz-Sobrado']",econ.GN,"The sustainability of cooperation is crucial for understanding the progress
of societies. We study a repeated game in which individuals decide the share of
their income to transfer to other group members. A central feature of our model
is that individuals may, with some probability, switch incomes across periods,
our measure of income mobility, while the overall income distribution remains
constant over time. We analyze how income mobility and income inequality affect
the sustainability of contribution norms, informal agreements about how much
each member should transfer to the group. We find that greater income mobility
facilitates cooperation. In contrast, the effect of inequality is ambiguous and
depends on the progressivity of the contribution norm and the degree of
mobility. We apply our framework to an optimal taxation problem to examine the
interaction between public and private redistribution.",2025
http://arxiv.org/abs/2510.26470v1,In Defense of the Pre-Test: Valid Inference when Testing Violations of Parallel Trends for Difference-in-Differences,2025-10-30 13:21:23+00:00,"['Jonas M. Mikhaeil', 'Christopher Harshaw']",stat.ME,"The difference-in-differences (DID) research design is a key identification
strategy which allows researchers to estimate causal effects under the parallel
trends assumption. While the parallel trends assumption is counterfactual and
cannot be tested directly, researchers often examine pre-treatment periods to
check whether the time trends are parallel before treatment is administered.
Recently, researchers have been cautioned against using preliminary tests which
aim to detect violations of parallel trends in the pre-treatment period. In
this paper, we argue that preliminary testing can -- and should -- play an
important role within the DID research design. We propose a new and more
substantively appropriate conditional extrapolation assumption, which requires
an analyst to conduct a preliminary test to determine whether the severity of
pre-treatment parallel trend violations falls below an acceptable level before
extrapolation to the post-treatment period is justified. This stands in
contrast to prior work which can be interpreted as either setting the
acceptable level to be exactly zero (in which case preliminary tests lack
power) or assuming that extrapolation is always justified (in which case
preliminary tests are not required). Under mild assumptions on how close the
actual violation is to the acceptable level, we provide a consistent
preliminary test as well confidence intervals which are valid when conditioned
on the result of the test. The conditional coverage of these intervals
overcomes a common critique made against the use of preliminary testing within
the DID research design. We use real data as well as numerical simulations to
illustrate the performance of the proposed methods.",2025
http://arxiv.org/abs/2510.26387v1,Robust Welfare under Imperfect Competition,2025-10-30 11:30:30+00:00,"['Konstantin von Beringe', 'Mark Whitmeyer']",econ.TH,"We study welfare analysis for policy changes when supply behavior is only
partially known. We augment the robust-demand approach of Kang and Vasserman
(2025) with two supply primitives--intervals of feasible pass-through and
conduct (market-power) parameters--applied to two equilibrium snapshots. A
simple accounting identity distills the supply-side contribution to welfare to
a simple integral expression. From there, we deduce that the bounds are
produced by a single-threshold ""bang-bang"" inverse pass-through function. This,
plus a modification of Kang and Vasserman's (2025) demand-side
characterization, delivers simple bounds for consumer surplus, producer
surplus, tax revenue, total surplus, and deadweight loss. We also study an ad
valorem extension.",2025
http://arxiv.org/abs/2510.26263v2,The Effect of Using Popular Mathematical Puzzles on The Mathematical Thinking of Syrian Schoolchildren,2025-10-30 08:44:12+00:00,"['Duaa Abdullah', 'Jasem Hamoud']",econ.TH,"In this paper we provide a good overview of the problems and the background
of mathematics education in Syrian schools. We aimed to study the effect of
using popular mathematical puzzles on the mathematical thinking of
schoolchildren, by conducting a paired experimental study (pre-test and
post-test control group design) of the data we obtained through a sample taken
from students of sixth-grade primary school students in Syria the Lady Mary
School in Syria, in order to evaluate the extent of the impact of popular
mathematical puzzles on students' ability to solve problems and mathematical
skills, and then the skills were measured and the results were analyzed using a
t-test as a tool for statistical analysis.",2025
http://arxiv.org/abs/2510.26106v1,Causal Inference with Groupwise Matching,2025-10-30 03:31:49+00:00,"['Ratzanyel Rincón', 'Kyungchul Song']",econ.EM,"This paper examines methods of causal inference based on groupwise matching
when we observe multiple large groups of individuals over several periods. We
formulate causal inference validity through a generalized matching condition,
generalizing the parallel trend assumption in difference-in-differences
designs. We show that difference-in-differences, synthetic control, and
synthetic difference-in-differences designs are distinguished by the specific
matching conditions that they invoke. Through regret analysis, we demonstrate
that difference-in-differences and synthetic control with differencing are
complementary; the former dominates the latter if and only if the latter's
extrapolation error exceeds the former's matching error up to a term vanishing
at the parametric rate. The analysis also reveals that synthetic control with
differencing is equivalent to difference-in-differences when the parallel trend
assumption holds for both the pre-treatment and post-treatment periods. We
develop a statistical inference procedure based on synthetic control with
differencing and present an empirical application demonstrating its usefulness.",2025
http://arxiv.org/abs/2510.26091v2,TEE-BFT: Pricing the Security of Data Center Execution Assurance,2025-10-30 03:02:39+00:00,"['Alex Shamis', 'Matt Stephenson', 'Linfeng Zhou']",econ.TH,"Blockchains face inherent limitations when communicating outside their own
ecosystem, largely due to the Byzantine Fault Tolerant (BFT) 3f+1 security
model. Trusted Execution Environments (TEEs) are a promising mitigation because
they allow a single trusted broker to interface securely with external systems.
  This paper develops a cost-of-collusion principal-agent model for
compromising a TEE in a Data Center Execution Assurance design. The model
isolates the main drivers of attack profitability: a K-of-n coordination
threshold, independent detection risk q, heterogeneous per-member sanctions
F_i, and a short-window flow prize (omega) proportional to the value secured
(beta times V).
  We derive closed-form deterrence thresholds and a conservative design bound
(V_safe) that make collusion unprofitable under transparent parameter choices.
Calibrations based on time-advantaged arbitrage indicate that plausible TEE
parameters can protect on the order of one trillion dollars in value. The
analysis informs the design of TEE-BFT, a blockchain architecture that combines
BFT consensus with near-stateless TEEs, distributed key generation, and
on-chain attestation to maintain security when interacting with external
systems.",2025
http://arxiv.org/abs/2510.26065v1,Price Levels in Heterogeneous-Agent Models,2025-10-30 01:38:08+00:00,['Felix Höfer'],econ.TH,"We study a model of the Fiscal Theory of the Price Level (FTPL) in a
Bewley-Huggett-Aiyagari framework with heterogeneous agents. The model is set
in continuous time, and ex post heterogeneity arises due to idiosyncratic,
uninsurable income shocks. Such models have a natural interpretation as
mean-field games, introduced by Huang, Caines, and Malham\'e and by Lasry and
Lions. We highlight this connection and discuss the existence and multiplicity
of stationary equilibria in models with and without capital. Our focus is on
the mathematical analysis, and we prove the existence of two equilibria in
which the government runs constant primary deficits, which in turn implies the
existence of multiple price levels.",2025
http://arxiv.org/abs/2510.26051v1,Estimation and Inference in Boundary Discontinuity Designs: Distance-Based Methods,2025-10-30 01:03:57+00:00,"['Matias D. Cattaneo', 'Rocio Titiunik', 'Ruiqi', 'Yu']",econ.EM,"We study the statistical properties of nonparametric distance-based
(isotropic) local polynomial regression estimators of the boundary average
treatment effect curve, a key causal functional parameter capturing
heterogeneous treatment effects in boundary discontinuity designs. We present
necessary and/or sufficient conditions for identification, estimation, and
inference in large samples, both pointwise and uniformly along the boundary.
Our theoretical results highlight the crucial role played by the ``regularity''
of the boundary (a one-dimensional manifold) over which identification,
estimation, and inference are conducted. Our methods are illustrated with
simulated data. Companion general-purpose software is provided.",2025
http://arxiv.org/abs/2510.26035v1,Budget Forecasting and Integrated Strategic Planning for Leaders,2025-10-30 00:22:50+00:00,['Matt Salehi'],q-fin.GN,"This study explored how advanced budgeting techniques and economic indicators
influence funding levels and strategic alignment in California Community
Colleges (CCCs). Despite widespread implementation of budgeting reforms, many
CCCs continue to face challenges aligning financial planning with institutional
missions, particularly in supporting diversity, equity, and inclusion (DEI)
initiatives. The study used a quantitative correlational design, analyzing 30
years of publicly available economic data, including unemployment rates, GDP
growth, and CPI, in relation to CCC funding trends. Results revealed a strong
positive correlation between GDP growth and CCC funding levels, as well as
between CPI and funding levels, underscoring the predictive value of
macroeconomic indicators in budget planning. These findings emphasize the need
for educational leaders to integrate economic forecasting into budget planning
processes to safeguard institutional effectiveness and sustain programs serving
underrepresented student populations.",2025
http://arxiv.org/abs/2510.26030v1,World personal income distribution evolution measured by purchasing power parity exchange rates,2025-10-30 00:10:35+00:00,"['J. D. A. Islas-García', 'M. del Castillo-Mussot', 'Marcelo B. Ribeiro']",econ.GN,"The evolution of global income distribution from 1988 to 2018 is analyzed
using purchasing power parity exchange rates and well-established statistical
distributions. This research proposes the use of two separate distributions to
more accurately represent the overall data, rather than relying on a single
distribution. The global income distribution was fitted to log-normal and gamma
functions, which are standard tools in econophysics. Despite limitations in
data completeness during the early years, the available information covered the
vast majority of the world's population. Probability density function (PDF)
curves enabled the identification of key peaks in the distribution, while
complementary cumulative distribution function (CCDF) curves highlighted
general trends in inequality. Initially, the global income distribution
exhibited a bimodal pattern; however, the growth of middle classes in highly
populated countries such as China and India has driven the transition to a
unimodal distribution in recent years. While single-function fits with gamma or
log-normal distributions provided reasonable accuracy, the bimodal approach
constructed as a sum of log-normal distributions yielded near-perfect fits.",2025
http://arxiv.org/abs/2510.25743v1,Agentic Economic Modeling,2025-10-29 17:46:07+00:00,"['Bohan Zhang', 'Jiaxuan Li', 'Ali Hortaçsu', 'Xiaoyang Ye', 'Victor Chernozhukov', 'Angelo Ni', 'Edward Huang']",econ.EM,"We introduce Agentic Economic Modeling (AEM), a framework that aligns
synthetic LLM choices with small-sample human evidence for reliable econometric
inference. AEM first generates task-conditioned synthetic choices via LLMs,
then learns a bias-correction mapping from task features and raw LLM choices to
human-aligned choices, upon which standard econometric estimators perform
inference to recover demand elasticities and treatment effects.We validate AEM
in two experiments. In a large scale conjoint study with millions of
observations, using only 10% of the original data to fit the correction model
lowers the error of the demand-parameter estimates, while uncorrected LLM
choices even increase the errors. In a regional field experiment, a mixture
model calibrated on 10% of geographic regions estimates an out-of-domain
treatment effect of -65\pm10 bps, closely matching the full human experiment
(-60\pm8 bps).Under time-wise extrapolation, training with only day-one human
data yields -24 bps (95% CI: [-26, -22], p<1e-5),improving over the human-only
day-one baseline (-17 bps, 95% CI: [-43, +9], p=0.2049).These results
demonstrate AEM's potential to improve RCT efficiency and establish a
foundation method for LLM-based counterfactual generation.",2025
http://arxiv.org/abs/2510.25738v1,Walrasian equilibria are almost always finite in number,2025-10-29 17:41:59+00:00,"['Sofia B. S. D. Castro', 'Peter B. Gothen']",econ.TH,"We show that in the context of exchange economies defined by aggregate excess
demand functions on the full open price simplex, the generic economy has a
finite number of equilibria. Genericicity is proved also for critical economies
and, in both cases, in the strong sense that it holds for an open dense subset
of economies in the Whitney topology. We use the concept of finite singularity
type from singularity theory. This concept ensures that the number of
equilibria of a map appear only in finite number. We then show that maps of
finite singularity type make up an open and dense subset of all smooth maps and
translate the result to the set of aggregate excess demand functions of an
exchange economy.
  Along the way, we extend the classical results of Sonnenschein-Mantel-Debreu
to aggregate excess demand functions defined on the full open price simplex,
rather than just compact subsets of the simplex.",2025
http://arxiv.org/abs/2510.25607v1,Inference on Welfare and Value Functionals under Optimal Treatment Assignment,2025-10-29 15:16:11+00:00,"['Xiaohong Chen', 'Zhenxiao Chen', 'Wayne Yuan Gao']",econ.EM,"We provide theoretical results for the estimation and inference of a class of
welfare and value functionals of the nonparametric conditional average
treatment effect (CATE) function under optimal treatment assignment, i.e.,
treatment is assigned to an observed type if and only if its CATE is
nonnegative. For the optimal welfare functional defined as the average value of
CATE on the subpopulation with nonnegative CATE, we establish the $\sqrt{n}$
asymptotic normality of the semiparametric plug-in estimators and provide an
analytical asymptotic variance formula. For more general value functionals, we
show that the plug-in estimators are typically asymptotically normal at the
1-dimensional nonparametric estimation rate, and we provide a consistent
variance estimator based on the sieve Riesz representer, as well as a proposed
computational procedure for numerical integration on submanifolds. The key
reason underlying the different convergence rates for the welfare functional
versus the general value functional lies in that, on the boundary subpopulation
for whom CATE is zero, the integrand vanishes for the welfare functional but
does not for general value functionals. We demonstrate in Monte Carlo
simulations the good finite-sample performance of our estimation and inference
procedures, and conduct an empirical application of our methods on the
effectiveness of job training programs on earnings using the JTPA data set.",2025
http://arxiv.org/abs/2510.25487v1,The Latin Monetary Union and Trade: A Closer Look,2025-10-29 13:08:06+00:00,['Jacopo Timini'],econ.GN,"This paper reexamines the effects of the Latin Monetary Union (LMU) - a 19th
century agreement among several European countries to standardize their
currencies through a bimetallic system based on fixed gold and silver content -
on trade. Unlike previous studies, this paper adopts the latest advances in
gravity modeling and a more rigorous approach to defining the control group by
accounting for the diversity of currency regimes during the early years of the
LMU. My findings suggest that the LMU had a positive effect on trade between
its members until the early 1870s, when bimetallism was still considered a
viable monetary system. These effects then faded, converging to zero. Results
are robust to the inclusion of additional potential confounders, the use of
various samples spanning different countries and trade data sources, and
alternative methodological choices.",2025
http://arxiv.org/abs/2510.25275v1,New methods to compensate artists in music streaming platforms,2025-10-29 08:36:25+00:00,"['Gustavo Bergantiños', 'Juan D. Moreno-Ternero']",econ.TH,"We study the problem of measuring the popularity of artists in music
streaming platforms and the ensuing methods to compensate them (from the
revenues platforms raise by charging users). We uncover the space of popularity
indices upon exploring the implications of several axioms capturing principles
with normative appeal. As a result, we characterize several families of
indices. Some of them are intimately connected to the Shapley value, the
central tool in cooperative game theory. Our characterizations might help to
address the rising concern in the music industry to explore new methods that
reward artists more appropriately. We actually connect our families to the new
royalties models, recently launched by Spotify and Deezer.",2025
http://arxiv.org/abs/2510.25066v1,Frequentist Persuasion,2025-10-29 01:09:43+00:00,"['Arnav Sood', 'James Best']",econ.TH,"A sender persuades a strategically naive decisionmaker (DM) by committing
privately to an experiment. Sender's choice of experiment is unknown to the DM,
who must form her posterior beliefs nonparametrically by applying some learning
rule to an IID sample of (state, message) realizations.
  We show that, given mild regularity conditions, the empirical payoff
functions hypo-converge to the full-information counterpart. This is sufficient
to ensure that payoffs and optimal signals converge to the Bayesian benchmark.
  For finite sample sizes, the force of this ""sampling friction"" is
nonmonotonic: it can induce more informative experiments than the Bayesian
benchmark in settings like the classic Prosecutor-Judge game, and less
revelation even in situations with perfectly aligned preferences. For many
problems with state-independent preferences, we show that there is an optimal
finite sample size for the DM. Although the DM would always prefer a larger
sample for a fixed experiment, this result holds because the sample size
affects sender's choice of experiment.
  Our results are robust to imperfectly informative feedback and the choice of
learning rule.",2025
http://arxiv.org/abs/2510.24990v1,The Economics of AI Training Data: A Research Agenda,2025-10-28 21:37:35+00:00,"['Hamidah Oderinwale', 'Anna Kazlauskas']",cs.CY,"Despite data's central role in AI production, it remains the least understood
input. As AI labs exhaust public data and turn to proprietary sources, with
deals reaching hundreds of millions of dollars, research across computer
science, economics, law, and policy has fragmented. We establish data economics
as a coherent field through three contributions. First, we characterize data's
distinctive properties -- nonrivalry, context dependence, and emergent rivalry
through contamination -- and trace historical precedents for market formation
in commodities such as oil and grain. Second, we present systematic
documentation of AI training data deals from 2020 to 2025, revealing persistent
market fragmentation, five distinct pricing mechanisms (from per-unit licensing
to commissioning), and that most deals exclude original creators from
compensation. Third, we propose a formal hierarchy of exchangeable data units
(token, record, dataset, corpus, stream) and argue for data's explicit
representation in production functions. Building on these foundations, we
outline four open research problems foundational to data economics: measuring
context-dependent value, balancing governance with privacy, estimating data's
contribution to production, and designing mechanisms for heterogeneous,
compositional goods.",2025
http://arxiv.org/abs/2510.24923v1,Automation Experiments and Inequality,2025-10-28 19:52:42+00:00,"['Seth Benzell', 'Kyle Myers']",econ.GN,"An increasingly large number of experiments study the labor productivity
effects of automation technologies such as generative algorithms. A popular
question in these experiments relates to inequality: does the technology
increase output more for high- or low-skill workers? The answer is often used
to anticipate the distributional effects of the technology as it continues to
improve. In this paper, we formalize the theoretical content of this empirical
test, focusing on automation experiments as commonly designed. Worker-level
output depends on a task-level production function, and workers are
heterogeneous in their task-level skills. Workers perform a task themselves, or
they delegate it to the automation technology. The inequality effect of
improved automation depends on the interaction of two factors: ($i$) the
correlation in task-level skills across workers, and ($ii$) workers' skills
relative to the technology's capability. Importantly, the sign of the
inequality effect is often non-monotonic -- as technologies improve, inequality
may decrease then increase, or vice versa. Finally, we use data and theory to
highlight cases when skills are likely to be positively or negatively
correlated. The model generally suggests that the diversity of automation
technologies will play an important role in the evolution of inequality.",2025
http://arxiv.org/abs/2510.24916v1,Productivity Beliefs and Efficiency in Science,2025-10-28 19:36:59+00:00,"['Fabio Bertolotti', 'Kyle Myers', 'Wei Yang Tham']",econ.GN,"We develop a method to estimate producers' productivity beliefs when output
quantities and input prices are unobservable, and we use it to evaluate the
market for science. Our model of researchers' labor supply shows how their
willingness to pay for inputs reveals their productivity beliefs. We estimate
the model's parameters using data from a nationally representative survey of
researchers and find the distribution of productivity to be very skewed. Our
counterfactuals indicate that a more efficient allocation of the current budget
could be worth billions of dollars. There are substantial gains from developing
new ways of identifying talented scientists.",2025
http://arxiv.org/abs/2510.24899v1,Estimating Nationwide High-Dosage Tutoring Expenditures: A Predictive Model Approach,2025-10-28 19:02:07+00:00,"['Jason Godfrey', 'Trisha Banerjee']",econ.GN,"This study applies an optimized XGBoost regression model to estimate
district-level expenditures on high-dosage tutoring from incomplete
administrative data. The COVID-19 pandemic caused unprecedented learning loss,
with K-12 students losing up to half a grade level in certain subjects. To
address this, the federal government allocated \$190 billion in relief. We know
from previous research that small-group tutoring, summer and after school
programs, and increased support staff were all common expenditures for
districts. We don't know how much was spent in each category. Using a custom
scraped dataset of over 7,000 ESSER (Elementary and Secondary School Emergency
Relief) plans, we model tutoring allocations as a function of district
characteristics such as enrollment, total ESSER funding, urbanicity, and school
count. Extending the trained model to districts that mention tutoring but omit
cost information yields an estimated aggregate allocation of approximately
\$2.2 billion. The model achieved an out-of-sample $R^2$=0.358, demonstrating
moderate predictive accuracy given substantial reporting heterogeneity.
Methodologically, this work illustrates how gradient-boosted decision trees can
reconstruct large-scale fiscal patterns where structured data are sparse or
missing. The framework generalizes to other domains where policy evaluation
depends on recovering latent financial or behavioral variables from
semi-structured text and sparse administrative sources.",2025
http://arxiv.org/abs/2510.24714v1,Machine-Learning-Assisted Comparison of Regression Functions,2025-10-28 17:59:15+00:00,"['Jian Yan', 'Zhuoxi Li', 'Yang Ning', 'Yong Chen']",stat.ME,"We revisit the classical problem of comparing regression functions, a
fundamental question in statistical inference with broad relevance to modern
applications such as data integration, transfer learning, and causal inference.
Existing approaches typically rely on smoothing techniques and are thus
hindered by the curse of dimensionality. We propose a generalized notion of
kernel-based conditional mean dependence that provides a new characterization
of the null hypothesis of equal regression functions. Building on this
reformulation, we develop two novel tests that leverage modern machine learning
methods for flexible estimation. We establish the asymptotic properties of the
test statistics, which hold under both fixed- and high-dimensional regimes.
Unlike existing methods that often require restrictive distributional
assumptions, our framework only imposes mild moment conditions. The efficacy of
the proposed tests is demonstrated through extensive numerical studies.",2025
http://arxiv.org/abs/2510.24496v1,Panel data models with randomly generated groups,2025-10-28 15:12:15+00:00,"['Jean-Pierre Florens', 'Anna Simoni']",econ.EM,"We develop a structural framework for modeling and inferring unobserved
heterogeneity in dynamic panel-data models. Unlike methods treating clustering
as a descriptive device, we model heterogeneity as arising from a latent
clustering mechanism, where the number of clusters is unknown and estimated.
Building on the mixture of finite mixtures (MFM) approach, our method avoids
the clustering inconsistency issues of Dirichlet process mixtures and provides
an interpretable representation of the population clustering structure. We
extend the Telescoping Sampler of Fruhwirth-Schnatter et al. (2021) to dynamic
panels with covariates, yielding an efficient MCMC algorithm that delivers full
Bayesian inference and credible sets. We show that asymptotically the posterior
distribution of the mixing measure contracts around the truth at parametric
rates in Wasserstein distance, ensuring recovery of clustering and structural
parameters. Simulations demonstrate strong finite-sample performance. Finally,
an application to the income-democracy relationship reveals latent
heterogeneity only when controlling for additional covariates.",2025
http://arxiv.org/abs/2510.24433v1,Nearest Neighbor Matching as Least Squares Density Ratio Estimation and Riesz Regression,2025-10-28 14:01:51+00:00,['Masahiro Kato'],econ.EM,"This study proves that Nearest Neighbor (NN) matching can be interpreted as
an instance of Riesz regression for automatic debiased machine learning. Lin et
al. (2023) shows that NN matching is an instance of density-ratio estimation
with their new density-ratio estimator. Chernozhukov et al. (2024) develops
Riesz regression for automatic debiased machine learning, which directly
estimates the Riesz representer (or equivalently, the bias-correction term) by
minimizing the mean squared error. In this study, we first prove that the
density-ratio estimation method proposed in Lin et al. (2023) is essentially
equivalent to Least-Squares Importance Fitting (LSIF) proposed in Kanamori et
al. (2009) for direct density-ratio estimation. Furthermore, we derive Riesz
regression using the LSIF framework. Based on these results, we derive NN
matching from Riesz regression. This study is based on our work Kato (2025a)
and Kato (2025b).",2025
http://arxiv.org/abs/2510.24388v1,A Characterization of Egalitarian and Proportional Sharing Principles: An Efficient Extension Operator Approach,2025-10-28 13:03:44+00:00,"['Yukihiko Funaki', 'Yukio Koriyama', 'Satoshi Nakada']",econ.TH,"Some well-known solutions for cooperative games with transferable utility
(TU-games), such as the Banzhaf value, the Myerson value, and the Aumann-Dreze
value, fail to satisfy efficiency, although they possess other desirable
properties. This paper proposes a new approach to restore efficiency by
extending any underlying solution to an efficient one, through what we call an
efficient extension operator. We consider novel axioms for an efficient
extension operator and characterize the egalitarian surplus sharing method and
the proportional sharing method in a unified manner. These results can be
considered as new justifications for the f-ESS values and the f-PS values
introduced by Funaki and Koriyama (2025), which are generalizations of the
equal surplus sharing value and the proportional sharing value. Our results
offer an additional rationale for the values with an arbitrary underlying
solution. As applications, we develop an efficient-fair extension of the
solutions for the TU-games with communication networks and its variant for
TU-games with coalition structures.",2025
http://arxiv.org/abs/2510.24362v1,Implicit quantile preferences of the Fed and the Taylor rule,2025-10-28 12:35:33+00:00,"['Gabriel Montes-Rojas', 'Fernando Toledo', 'Nicolás Bertholet', 'Kevin Corfield']",econ.GN,"We study optimal monetary policy when a central bank maximizes a quantile
utility objective rather than expected utility. In our framework, the central
bank's risk attitude is indexed by the quantile index level, providing a
transparent mapping between hawkish/dovish stances and attention to adverse
macroeconomic realizations. We formulate the infinite-horizon problem using a
Bellman equation with the quantile operator. Implementing an Euler-equation
approach, we derive Taylor-rule-type reaction functions. Using an indirect
inference approach, we derive a central bank risk aversion implicit quantile
index. An empirical implementation for the US is outlined based on reduced-form
laws of motion with conditional heteroskedasticity, enabling estimation of the
new monetary policy rule and its dependence on the Fed risk attitudes. The
results reveal that the Fed has mostly a dovish-type behavior but with some
periods of hawkish attitudes.",2025
http://arxiv.org/abs/2510.24344v1,Are They Willing to Participate? A Review on Behavioral Economics Approach to Voters Turnout,2025-10-28 12:10:25+00:00,['Mostafa Raeisi Sarkandiz'],econ.GN,"This article investigates the fundamental factors influencing the rate and
manner of Electoral participation with an economic model-based approach. In
this study, the structural parameters affecting people's decision making are
divided into two categories. The first category includes general topics such as
economic and livelihood status, cultural factors and, also, psychological
variables. In this section, given that voters are analyzed within the context
of consumer behavior theory, inflation and unemployment are considered as the
most important economic factors. The second group of factors focuses more on
the type of voting, with emphasis on government performance. Since the
incumbent government and its supportive voters are in a game with two Nash
equilibrium, and also because the voters in most cases are retrospect, the
government seeks to keep its position by a deliberate change in economic
factors, especially inflation and unemployment rates. Finally, to better
understand the issue, a hypothetical example is presented and analyzed in a
developing country in the form of a state-owned populist employment plan.",2025
http://arxiv.org/abs/2510.24266v1,The Role of Mathematical Folk Puzzles in Developing mathematical Thinking and Problem-Solving Skills,2025-10-28 10:25:16+00:00,"['Duaa Abdullah', 'Jasem Hamoud']",econ.TH,"This paper covers a variety of mathematical folk puzzles, including geometric
(Tangrams, dissection puzzles), logic, algebraic, probability (Monty Hall
Problem, Birthday Paradox), and combinatorial challenges (Eight Queens Puzzle,
Tower of Hanoi). It also explores modern modifications, such as digital and
gamified approaches, to improve student involvement and comprehension.
Furthermore, a novel concept, the ""Minimal Dissection Path Problem for
Polyominoes,"" is introduced and proven, demonstrating that the minimum number
of straight-line cuts required to dissect a polyomino of N squares into its
constituent units is $\mathrm{N}-1$. This problem, along with other puzzles,
offers practical classroom applications that reinforce core mathematical
concepts like area, spatial reasoning, and optimization, making learning both
enjoyable and effective.",2025
http://arxiv.org/abs/2510.24225v1,The Effects of Immigration on Places and People -- Identification and Interpretation,2025-10-28 09:36:47+00:00,"['Christian Dustmann', 'Sebastian Otten', 'Uta Schönberg', 'Jan Stuhler']",econ.GN,"Most studies on the labor market effects of immigration use repeated
cross-sectional data to estimate the effects of immigration on regions. This
paper shows that such regional effects are composites of effects that address
fundamental questions in the immigration debate but remain unidentified with
repeated cross-sectional data. We provide a unifying empirical framework that
decomposes the regional effects of immigration into their underlying components
and show how these are identifiable from data that track workers over time. Our
empirical application illustrates that such analysis yields a far more
informative picture of immigration's effects on wages, employment, and
occupational upgrading.",2025
http://arxiv.org/abs/2510.24174v1,Moment connectedness and driving factors in the energy-food nexus: A time-frequency perspective,2025-10-28 08:29:11+00:00,"['Yun-Shi Dai', 'Peng-Fei Dai', 'Stéphane Goutte', 'Duc Khuong Nguyen', 'Wei-Xing Zhou']",econ.GN,"With escalating macroeconomic uncertainty, the risk interlinkages between
energy and food markets have become increasingly complex, posing serious
challenges to global energy and food security. This paper proposes an
integrated framework combining the GJRSK model, the time-frequency
connectedness analysis, and the random forest method to systematically
investigate the moment connectedness within the energy-food nexus and explore
the key drivers of various spillover effects. The results reveal significant
multidimensional risk spillovers with pronounced time variation, heterogeneity,
and crisis sensitivity. Return and skewness connectedness are primarily driven
by short-term spillovers, kurtosis connectedness is more prominent over the
medium term, while volatility connectedness is dominated by long-term dynamics.
Notably, crude oil consistently serves as a central transmitter in diverse
connectedness networks. Furthermore, the spillover effects are influenced by
multiple factors, including macro-financial conditions, oil supply-demand
fundamentals, policy uncertainties, and climate-related shocks, with the core
drivers of connectedness varying considerably across different moments and
timescales. These findings provide valuable insights for the coordinated
governance of energy and food markets, the improvement of multilayered risk
early-warning systems, and the optimization of investment strategies.",2025
http://arxiv.org/abs/2510.25782v1,Short-Run Multi-Outcome Effects of Nightlife Regulation in San Juan,2025-10-28 04:57:47+00:00,['Jorge A. Arroyo'],econ.GN,"I evaluate San Juan, Puerto Rico's late-night alcohol sales ordinance using a
multi-outcome synthetic control that pools economic and public-safety series. I
show that a common-weight estimator clarifies mechanisms under low-rank outcome
structure. I find economically meaningful reallocations in targeted sectors --
restaurants and bars, gasoline and convenience, and hospitality employment --
while late-night public disorder arrests and violent crime show no clear
departures from pre-policy trends. The short post-policy window and small donor
pool limit statistical power; joint conformal and permutation tests do not
reject the null at conventional thresholds. I therefore emphasize effect
magnitudes, temporal persistence, and pre-trend fit over formal significance.
Code and diagnostics are available for replication.",2025
http://arxiv.org/abs/2510.24002v1,How Does Environmental Information Disclosure Affect Corporate Environmental Performance? Evidence from Chinese A-Share Listed Companies,2025-10-28 02:16:13+00:00,['Zehao Lin'],econ.GN,"Global climate warming and air pollution pose severe threats to economic
development and public safety, presenting significant challenges to sustainable
development worldwide. Corporations, as key players in resource utilization and
emissions, have drawn increasing attention from policymakers, researchers, and
the public regarding their environmental strategies and practices. This study
employs a two-way fixed effects panel model to examine the impact of
environmental information disclosure on corporate environmental performance,
its regional heterogeneity, and the underlying mechanisms. The results
demonstrate that environmental information disclosure significantly improves
corporate environmental performance, with the effect being more pronounced in
areas of high population density and limited green space. These findings
provide empirical evidence supporting the role of environmental information
disclosure as a critical tool for improving corporate environmental practices.
The study highlights the importance of targeted, region-specific policies to
maximize the effectiveness of disclosure, offering valuable insights for
promoting sustainable development through enhanced corporate transparency.",2025
http://arxiv.org/abs/2510.23951v1,Strategic Learning with Asymmetric Rationality,2025-10-28 00:09:51+00:00,"['Qingmin Liu', 'Yuyang Miao']",econ.TH,"This paper analyzes the dynamic interaction between a fully rational,
privately informed sender and a boundedly rational, uninformed receiver with
memory constraints. The sender controls the flow of information, while the
receiver designs a decision-making protocol, modeled as a finite-state machine,
that governs how information is interpreted, how internal memory states evolve,
and when and what decisions are made. The receiver must use the limited set of
states optimally, both to learn and to create incentives for the sender to
provide information. We show that behavior patterns such as information
avoidance, opinion polarization, and indecision arise as equilibrium responses
to asymmetric rationality. The model offers an expressive framework for
strategic learning and decision-making in environments with cognitive and
informational asymmetries, with applications to regulatory review and media
distrust.",2025
http://arxiv.org/abs/2510.23762v1,Control VAR: a counterfactual based approach to inference in macroeconomics,2025-10-27 18:45:40+00:00,['Raimondo Pala'],econ.EM,"This paper addresses the challenges of giving a causal interpretation to
vector autoregressions (VARs). I show that under independence assumptions VARs
can identify average treatment effects, average causal responses, or a mix of
the two, depending on the distribution of the policy. But what about situations
in which the economist cannot rely on independence assumptions? I propose an
alternative method, defined as control-VAR, which uses control variables to
estimate causal effects. Control-VAR can estimate average treatment effects on
the treated for dummy policies or average causal responses over time for
continuous policies.
  The advantages of control-based approaches are demonstrated by examining the
impact of natural disasters on the US economy, using Germany as a control.
Contrary to previous literature, the results indicate that natural disasters
have a negative economic impact without any cyclical positive effect. These
findings suggest that control-VARs provide a viable alternative to strict
independence assumptions, offering more credible causal estimates and
significant implications for policy design in response to natural disasters.",2025
http://arxiv.org/abs/2510.23540v1,The causal interpretation of panel vector autoregressions,2025-10-27 17:14:20+00:00,['Raimondo Pala'],econ.EM,"This paper discusses the different contemporaneous causal interpretations of
Panel Vector Autoregressions (PVAR). I show that the interpretation of PVARs
depends on the distribution of the causing variable, and can range from average
treatment effects, to average causal responses, to a combination of the two. If
the researcher is willing to postulate a no residual autocorrelation
assumption, and some units can be thought of as controls, PVAR can identify
average treatment effects on the treated. This method complements the toolkits
already present in the literature, such as staggered-DiD, or LP-DiD, as it
formulates assumptions in the residuals, and not in the outcome variables. Such
a method features a notable advantage: it allows units to be ``sparsely''
treated, capturing the impact of interventions on the innovation component of
the outcome variables. I provide an example related to the evaluation of the
effects of natural disasters economic activity at the weekly frequency in the
US.I conclude by discussing solutions to potential violations of the SUTVA
assumption arising from interference.",2025
http://arxiv.org/abs/2510.23534v2,Direct Debiased Machine Learning via Bregman Divergence Minimization,2025-10-27 17:10:43+00:00,['Masahiro Kato'],econ.EM,"We develop a direct debiased machine learning framework comprising Neyman
targeted estimation and generalized Riesz regression. Our framework unifies
Riesz regression for automatic debiased machine learning, covariate balancing,
targeted maximum likelihood estimation (TMLE), and density-ratio estimation. In
many problems involving causal effects or structural models, the parameters of
interest depend on regression functions. Plugging regression functions
estimated by machine learning methods into the identifying equations can yield
poor performance because of first-stage bias. To reduce such bias, debiased
machine learning employs Neyman orthogonal estimating equations. Debiased
machine learning typically requires estimation of the Riesz representer and the
regression function. For this problem, we develop a direct debiased machine
learning framework with an end-to-end algorithm. We formulate estimation of the
nuisance parameters, the regression function and the Riesz representer, as
minimizing the discrepancy between Neyman orthogonal scores computed with known
and unknown nuisance parameters, which we refer to as Neyman targeted
estimation. Neyman targeted estimation includes Riesz representer estimation,
and we measure discrepancies using the Bregman divergence. The Bregman
divergence encompasses various loss functions as special cases, where the
squared loss yields Riesz regression and the Kullback-Leibler divergence yields
entropy balancing. We refer to this Riesz representer estimation as generalized
Riesz regression. Neyman targeted estimation also yields TMLE as a special case
for regression function estimation. Furthermore, for specific pairs of models
and Riesz representer estimation methods, we can automatically obtain the
covariate balancing property without explicitly solving the covariate balancing
objective.",2025
http://arxiv.org/abs/2510.23434v1,Choosing What to Learn: Experimental Design when Combining Experimental with Observational Evidence,2025-10-27 15:37:23+00:00,"['Aristotelis Epanomeritakis', 'Davide Viviano']",econ.EM,"Experiments deliver credible but often localized effects, tied to specific
sites, populations, or mechanisms. When such estimates are insufficient to
extrapolate effects for broader policy questions, such as external validity and
general-equilibrium (GE) effects, researchers combine trials with external
evidence from reduced-form or structural observational estimates, or prior
experiments. We develop a unified framework for designing experiments in this
setting: the researcher selects which parameters to identify experimentally
from a feasible set (which treatment arms and/or individuals to include in the
experiment), allocates sample size, and specifies how to weight experimental
and observational estimators. Because observational inputs may be biased in
ways unknown ex ante, we develop a minimax proportional regret objective that
evaluates any candidate design relative to an oracle that knows the bias and
jointly chooses the design and estimator. This yields a transparent
bias-variance trade-off that requires no prespecified bias bound and depends
only on information about the precision of the estimators and the estimand's
sensitivity to the underlying parameters. We illustrate the framework by (i)
designing small-scale cash transfer experiments aimed at estimating GE effects
and (ii) optimizing site selection for microfinance interventions.",2025
http://arxiv.org/abs/2510.23421v1,Exploring Vulnerability in AI Industry,2025-10-27 15:26:40+00:00,"['Claudio Pirrone', 'Stefano Fricano', 'Gioacchino Fazio']",econ.GN,"The rapid ascent of Foundation Models (FMs), enabled by the Transformer
architecture, drives the current AI ecosystem. Characterized by large-scale
training and downstream adaptability, FMs (as GPT family) have achieved massive
public adoption, fueling a turbulent market shaped by platform economics and
intense investment. Assessing the vulnerability of this fast-evolving industry
is critical yet challenging due to data limitations. This paper proposes a
synthetic AI Vulnerability Index (AIVI) focusing on the upstream value chain
for FM production, prioritizing publicly available data. We model FM output as
a function of five inputs: Compute, Data, Talent, Capital, and Energy,
hypothesizing that supply vulnerability in any input threatens the industry.
Key vulnerabilities include compute concentration, data scarcity and legal
risks, talent bottlenecks, capital intensity and strategic dependencies, as
well as escalating energy demands. Acknowledging imperfect input
substitutability, we propose a weighted geometrical average of aggregate
subindexes, normalized using theoretical or empirical benchmarks. Despite
limitations and room for improvement, this preliminary index aims to quantify
systemic risks in AI's core production engine, and implicitly shed a light on
the risks for downstream value chain.",2025
http://arxiv.org/abs/2510.23347v1,Macroeconomic Forecasting for the G7 countries under Uncertainty Shocks,2025-10-27 14:01:41+00:00,"['Shovon Sengupta', 'Sunny Kumar Singh', 'Tanujit Chakraborty']",econ.EM,"Accurate macroeconomic forecasting has become harder amid geopolitical
disruptions, policy reversals, and volatile financial markets. Conventional
vector autoregressions (VARs) overfit in high dimensional settings, while
threshold VARs struggle with time varying interdependencies and complex
parameter structures. We address these limitations by extending the Sims Zha
Bayesian VAR with exogenous variables (SZBVARx) to incorporate domain-informed
shrinkage and four newspaper based uncertainty shocks such as economic policy
uncertainty, geopolitical risk, US equity market volatility, and US monetary
policy uncertainty. The framework improves structural interpretability,
mitigates dimensionality, and imposes empirically guided regularization. Using
G7 data, we study spillovers from uncertainty shocks to five core variables
(unemployment, real broad effective exchange rates, short term rates, oil
prices, and CPI inflation), combining wavelet coherence (time frequency
dynamics) with nonlinear local projections (state dependent impulse responses).
Out-of-sample results at 12 and 24 month horizons show that SZBVARx outperforms
14 benchmarks, including classical VARs and leading machine learning models, as
confirmed by Murphy difference diagrams, multivariate Diebold Mariano tests,
and Giacomini White predictability tests. Credible Bayesian prediction
intervals deliver robust uncertainty quantification for scenario analysis and
risk management. The proposed SZBVARx offers G7 policymakers a transparent,
well calibrated tool for modern macroeconomic forecasting under pervasive
uncertainty.",2025
http://arxiv.org/abs/2510.23178v1,Feedback in Dynamic Contests: Theory and Experiment,2025-10-27 10:14:33+00:00,"['Sumit Goel', 'Yiqing Yan', 'Jeffrey Zeidel']",econ.TH,"We study the effect of interim feedback policies in a dynamic all-pay auction
where two players bid over two stages to win a common-value prize. We show that
sequential equilibrium outcomes are characterized by Cheapest Signal
Equilibria, wherein stage 1 bids are such that one player bids zero while the
other chooses a cheapest bid consistent with some signal. Equilibrium payoffs
for both players are always zero, and the sum of expected total bids equals the
value of the prize. We conduct an experiment with four natural feedback policy
treatments -- full, rank, and two cutoff policies -- and while the bidding
behavior deviates from equilibrium, we fail to reject the hypothesis of no
treatment effect on total bids. Further, stage 1 bids induce sunk costs and
head starts, and we test for the resulting sunk cost and discouragement effects
in stage 2 bidding.",2025
http://arxiv.org/abs/2510.22908v1,Bridging Stratification and Regression Adjustment: Batch-Adaptive Stratification with Post-Design Adjustment in Randomized Experiments,2025-10-27 01:25:34+00:00,['Zikai Li'],stat.ME,"To increase statistical efficiency in a randomized experiment, researchers
often use stratification (i.e., blocking) in the design stage. However,
conventional practices of stratification fail to exploit valuable information
about the predictive relationship between covariates and potential outcomes. In
this paper, I introduce an adaptive stratification procedure for increasing
statistical efficiency when some information is available about the
relationship between covariates and potential outcomes. I show that, in a
paired design, researchers can rematch observations across different batches.
For inference, I propose a stratified estimator that allows for nonparametric
covariate adjustment. I then discuss the conditions under which researchers
should expect gains in efficiency from stratification. I show that
stratification complements rather than substitutes for regression adjustment,
insuring against adjustment error even when researchers plan to use covariate
adjustment. To evaluate the performance of the method relative to common
alternatives, I conduct simulations using both synthetic data and more
realistic data derived from a political science experiment. Results demonstrate
that the gains in precision and efficiency can be substantial.",2025
http://arxiv.org/abs/2510.22884v1,"Identification, Estimation, and Inference in Two-Sided Interaction Models",2025-10-27 00:21:43+00:00,['Federico Crippa'],econ.EM,"This paper studies a class of models for two-sided interactions, where
outcomes depend on latent characteristics of two distinct agent types. Models
in this class have two core elements: the matching network, which records which
agent pairs interact, and the interaction function, which maps latent
characteristics of these agents to outcomes and determines the role of
complementarities. I introduce the Tukey model, which captures
complementarities with a single interaction parameter, along with two
extensions that allow richer complementarity patterns. First, I establish an
identification trade-off between the flexibility of the interaction function
and the density of the matching network: the Tukey model is identified under
mild conditions, whereas the more flexible extensions require dense networks
that are rarely observed in applications. Second, I propose a cycle-based
estimator for the Tukey interaction parameter and show that it is consistent
and asymptotically normal even when the network is sparse. Third, I use its
asymptotic distribution to construct a formal test of no complementarities.
Finally, an empirical illustration shows that the Tukey model recovers
economically meaningful complementarities.",2025
http://arxiv.org/abs/2510.22864v1,Unifying regression-based and design-based causal inference in time-series experiments,2025-10-26 23:12:17+00:00,"['Zhexiao Lin', 'Peng Ding']",stat.ME,"Time-series experiments, also called switchback experiments or N-of-1 trials,
play increasingly important roles in modern applications in medical and
industrial areas. Under the potential outcomes framework, recent research has
studied time-series experiments from the design-based perspective, relying
solely on the randomness in the design to drive the statistical inference.
Focusing on simpler statistical methods, we examine the design-based properties
of regression-based methods for estimating treatment effects in time-series
experiments. We demonstrate that the treatment effects of interest can be
consistently estimated using ordinary least squares with an appropriately
specified working model and transformed regressors. Our analysis allows for
estimating a diverging number of treatment effects simultaneously, and
establishes the consistency and asymptotic normality of the regression-based
estimators. Additionally, we show that asymptotically, the heteroskedasticity
and autocorrelation consistent variance estimators provide conservative
estimates of the true, design-based variances. Importantly, although our
approach relies on regression, our design-based framework allows for
misspecification of the regression model.",2025
http://arxiv.org/abs/2510.22841v1,Testing for Grouped Patterns in Panel Data Models,2025-10-26 21:20:02+00:00,"['Antonio Raiola', 'Nazarii Salish']",econ.EM,"While the literature on grouped patterns in panel data analysis has received
significant attention, little to no results are available on testing for their
presence. We propose using existing tools for testing slope homogeneity in
panels for this purpose. We highlight the key advantages and limitations of the
available testing frameworks under a sequence of doubly local alternatives,
where slopes are divided into dominant and remainder groups, with the size of
the remainder groups and the slopes differences shrinking at a certain rate as
the sample size increases. A Monte Carlo study corroborate our theoretical
findings.",2025
http://arxiv.org/abs/2510.22828v1,Efficiently Learning Synthetic Control Models for High-dimensional Disaggregated Data,2025-10-26 20:43:52+00:00,"['Ye Shen', 'Rui Song', 'Alberto Abadie']",stat.ME,"The Synthetic Control method (SC) has become a valuable tool for estimating
causal effects. Originally designed for single-treated unit scenarios, it has
recently found applications in high-dimensional disaggregated settings with
multiple treated units. However, challenges in practical implementation and
computational efficiency arise in such scenarios. To tackle these challenges,
we propose a novel approach that integrates the Multivariate Square-root Lasso
method into the synthetic control framework. We rigorously establish the
estimation error bounds for fitting the Synthetic Control weights using
Multivariate Square-root Lasso, accommodating high-dimensionality and time
series dependencies. Additionally, we quantify the estimation error for the
Average Treatment Effect on the Treated (ATT). Through simulation studies, we
demonstrate that our method offers superior computational efficiency without
compromising estimation accuracy. We apply our method to assess the causal
impact of COVID-19 Stay-at-Home Orders on the monthly unemployment rate in the
United States at the county level.",2025
http://arxiv.org/abs/2510.22817v1,Wildfire and house prices: A synthetic control case study of Altadena (Jan 2025),2025-10-26 20:16:28+00:00,['Yibo Sun'],econ.GN,"This study uses the Synthetic Control Method (SCM) to estimate the causal
impact of a January 2025 wildfire on housing prices in Altadena, California. We
construct a 'synthetic' Altadena from a weighted average of peer cities to
serve as a counterfactual; this approach assumes no spillover effects on the
donor pool. The results reveal a substantial negative price effect that
intensifies over time. Over the six months following the event, we estimate an
average monthly loss of $32,125. The statistical evidence for this effect is
nuanced. Based on the robust post-to-pre-treatment RMSPE ratio, the result is
statistically significant at the 10% level (p = 0.0508). In contrast, the
effect is not statistically significant when measured by the average
post-treatment gap (p = 0.3220). This analysis highlights the significant
financial risks faced by communities in fire-prone regions and demonstrates
SCM's effectiveness in evaluating disaster-related economic damages.",2025
http://arxiv.org/abs/2510.23669v2,What Work is AI Actually Doing? Uncovering the Drivers of Generative AI Adoption,2025-10-26 19:13:37+00:00,"['Peeyush Agarwal', 'Harsh Agarwal', 'Akshat Rana']",econ.GN,"Purpose: The rapid integration of artificial intelligence (AI) systems like
ChatGPT, Claude AI, etc., has a deep impact on how work is done. Predicting how
AI will reshape work requires understanding not just its capabilities, but how
it is actually being adopted. This study investigates which intrinsic task
characteristics drive users' decisions to delegate work to AI systems.
Methodology: This study utilizes the Anthropic Economic Index dataset of four
million Claude AI interactions mapped to O*NET tasks. We systematically scored
each task across seven key dimensions: Routine, Cognitive, Social Intelligence,
Creativity, Domain Knowledge, Complexity, and Decision Making using 35
parameters. We then employed multivariate techniques to identify latent task
archetypes and analyzed their relationship with AI usage. Findings: Tasks
requiring high creativity, complexity, and cognitive demand, but low
routineness, attracted the most AI engagement. Furthermore, we identified three
task archetypes: Dynamic Problem Solving, Procedural & Analytical Work, and
Standardized Operational Tasks, demonstrating that AI applicability is best
predicted by a combination of task characteristics, over individual factors.
Our analysis revealed highly concentrated AI usage patterns, with just 5% of
tasks accounting for 59% of all interactions. Originality: This research
provides the first systematic evidence linking real-world generative AI usage
to a comprehensive, multi-dimensional framework of intrinsic task
characteristics. It introduces a data-driven classification of work archetypes
that offers a new framework for analyzing the emerging human-AI division of
labor.",2025
http://arxiv.org/abs/2510.22750v1,Information-Credible Stability in Matching with Incomplete Information,2025-10-26 16:53:18+00:00,['Kaibalyapati Mishra'],econ.TH,"In this paper, I develop a refinement of stability for matching markets with
incomplete information. I introduce Information-Credible Pairwise Stability
(ICPS), a solution concept in which deviating pairs can use credible, costly
tests to reveal match-relevant information before deciding whether to block. By
leveraging the option value of information, ICPS strictly refines Bayesian
stability, rules out fear-driven matchings, and connects belief-based and
information-based notions of stability. ICPS collapses to Bayesian stability
when testing is uninformative or infeasible and coincides with
complete-information stability when testing is perfect and free. I show that
any ICPS-blocking deviation strictly increases total expected surplus, ensuring
welfare improvement. I also prove that ICPS-stable allocations always exist,
promote positive assortative matching, and are unique when the test power is
sufficiently strong. The framework extends to settings with non-transferable
utility, correlated types, and endogenous or sequential testing.",2025
http://arxiv.org/abs/2510.22714v1,Pairwise Difference Representations of Moments: Gini and Generalized Lagrange identities,2025-10-26 15:16:54+00:00,"['Jean-Marie Dufour', 'Abderrahim Taamouti', 'Meilin Tong']",stat.ME,"We provide pairwise-difference (Gini-type) representations of higher-order
central moments for both general random variables and empirical moments. Such
representations do not require a measure of location. For third and fourth
moments, this yields pairwise-difference representations of skewness and
kurtosis coefficients. We show that all central moments possess such
representations, so no reference to the mean is needed for moments of any
order. This is done by considering i.i.d. replications of the random variables
considered, by observing that central moments can be interpreted as covariances
between a random variable and powers of the same variable, and by giving
recursions which link the pairwise-difference representation of any moment to
lower order ones. Numerical summation identities are deduced. Through a similar
approach, we give analogues of the Lagrange and Binet-Cauchy identities for
general random variables, along with a simple derivation of the classic
Cauchy-Schwarz inequality for covariances. Finally, an application to unbiased
estimation of centered moments is discussed.",2025
http://arxiv.org/abs/2510.22411v1,"Politics, Inequality, and the Robustness of Shared Infrastructure Systems",2025-10-25 19:12:18+00:00,"['Adam Wiechman', 'John M. Anderies', 'Margaret Garcia']",econ.TH,"Our infrastructure systems enable our well-being by allowing us to move,
store, and transform materials and information given considerable social and
environmental variation. Critically, this ability is shaped by the degree to
which society invests in infrastructure, a fundamentally political question in
large public systems. There, infrastructure providers are distinguished from
users through political processes, such as elections, and there is considerable
heterogeneity among users. Previous political economic models have not taken
into account (i) dynamic infrastructures, (ii) dynamic user preferences, and
(iii) alternatives to rational actor theory. Meanwhile, engineering often
neglects politics. We address these gaps with a general dynamic model of shared
infrastructure systems that incorporates theories from political economy,
social-ecological systems, and political psychology. We use the model to
develop propositions on how multiple characteristics of the political process
impact the robustness of shared infrastructure systems to capacity shocks and
unequal opportunity for private infrastructure investment. Under user fees,
inequality decreases robustness, but taxing private infrastructure use can
increase robustness if non-elites have equal political influence. Election
cycle periods have a nonlinear effect where increasing them increases
robustness up to a point but decreases robustness beyond that point. Further,
there is a negative relationship between the ideological sensitivity of
candidates and robustness. Overall, the biases of voters and candidates
(whether they favor tax increases or decreases) mediate these
political-economic effects on robustness because biases may or may not match
the reality of system needs (whether system recovery requires tax increases).",2025
http://arxiv.org/abs/2510.22399v1,Estimating unrestricted spatial interdependence in panel spatial autoregressive models with latent common factors,2025-10-25 18:51:21+00:00,"['Deborah Gefang', 'Stephen G Hall', 'George S. Tavlas']",econ.EM,"We develop a new Bayesian approach to estimating panel spatial autoregressive
models with a known number of latent common factors, where N, the number of
cross-sectional units, is much larger than T, the number of time periods.
Without imposing any a priori structures on the spatial linkages between
variables, we let the data speak for themselves. Extensive Monte Carlo studies
show that our method is super-fast and our estimated spatial weights matrices
and common factors strongly resemble their true counterparts. As an
illustration, we examine the spatial interdependence of regional gross value
added (GVA) growth rates across the European Union (EU). In addition to
revealing the clear presence of predominant country-level clusters, our results
indicate that only a small portion of the variation in the data is explained by
the latent shocks that are uncorrelated with the explanatory variables.",2025
http://arxiv.org/abs/2510.24781v1,Dual-Channel Technology Diffusion: Spatial Decay and Network Contagion in Supply Chain Networks,2025-10-25 16:46:34+00:00,['Tatsuru Kikuchi'],econ.EM,"This paper develops a dual-channel framework for analyzing technology
diffusion that integrates spatial decay mechanisms from continuous functional
analysis with network contagion dynamics from spectral graph theory. Building
on our previous studies, which establish Navier-Stokes-based approaches to
spatial treatment effects and financial network fragility, we demonstrate that
technology adoption spreads simultaneously through both geographic proximity
and supply chain connections. Using comprehensive data on six technologies
adopted by 500 firms over 2010-2023, we document three key findings. First,
technology adoption exhibits strong exponential geographic decay with spatial
decay rate $\kappa \approx 0.043$ per kilometer, implying a spatial boundary of
$d^* \approx 69$ kilometers beyond which spillovers are negligible (R-squared =
0.99). Second, supply chain connections create technology-specific networks
whose algebraic connectivity ($\lambda_2$) increases 300-380 percent as
adoption spreads, with correlation between $\lambda_2$ and adoption exceeding
0.95 across all technologies. Third, traditional difference-in-differences
methods that ignore spatial and network structure exhibit 61 percent bias in
estimated treatment effects. An event study around COVID-19 reveals that
network fragility increased 24.5 percent post-shock, amplifying treatment
effects through supply chain spillovers in a manner analogous to financial
contagion documented in our recent study. Our framework provides
micro-foundations for technology policy: interventions have spatial reach of 69
kilometers and network amplification factor of 10.8, requiring coordinated
geographic and supply chain targeting for optimal effectiveness.",2025
http://arxiv.org/abs/2510.22347v1,Distributionally Robust Dynamic Structural Estimation: Serial Dependence and Sensitivity Analysis,2025-10-25 16:08:33+00:00,['Ertian Chen'],econ.EM,"Distributional assumptions that discipline serially correlated latent
variables play a central role in dynamic structural models. We propose a
framework to quantify the sensitivity of scalar parameters of interest (e.g.,
welfare, elasticity) to such distributional assumptions. We derive bounds on
the scalar parameter by perturbing a reference distribution, while imposing a
stationarity condition for time-homogeneous models or a Markovian condition for
time-inhomogeneous models. The bounds are the solutions to optimization
problems, for which we derive a computationally tractable dual formulation. We
establish consistency, convergence rate, and asymptotic distribution for the
estimator of the bounds. We demonstrate the approach with two applications: an
infinite-horizon dynamic demand model for new cars in the United Kingdom,
Germany, and France, and a finite-horizon dynamic labor supply model for taxi
drivers in New York City. In the car application, perturbed price elasticities
deviate by at most 15.24% from the reference elasticities, while perturbed
estimates of consumer surplus from an additional $3,000 electric vehicle
subsidy vary by up to 102.75%. In the labor supply application, the perturbed
Frisch labor supply elasticity deviates by at most 76.83% for weekday drivers
and 42.84% for weekend drivers.",2025
http://arxiv.org/abs/2510.22294v1,There's Nothing in the Air,2025-10-25 13:38:06+00:00,"['Jacob Adenbaum', 'Fil Babalievsky', 'William Jungerman']",econ.GN,"Why do wages grow faster in bigger cities? We use French administrative data
to decompose the urban wage growth premium and find that the answer has
surprisingly little to do with cities themselves. While we document
substantially faster wage growth in larger cities, 80% of the premium
disappears after controlling for the composition of firms and coworkers. We
also document significantly higher job-to-job transition rates in larger
cities, suggesting workers climb the job ladder faster. Most strikingly, when
we focus on workers who remain in the same job -- eliminating the job ladder
mechanism -- the urban wage growth premium falls by 94.1% after accounting for
firms and coworkers. The residual effect is statistically indistinguishable
from zero. These results challenge the view that cities generate human capital
spillovers ``in the air,'' suggesting instead that urban wage dynamics reflect
the sorting of firms and workers and the pace of job mobility.",2025
http://arxiv.org/abs/2510.22232v1,Rational Adversaries and the Maintenance of Fragility: A Game-Theoretic Theory of Rational Stagnation,2025-10-25 09:28:15+00:00,['Daisuke Hirota'],cs.GT,"Cooperative systems often remain in persistently suboptimal yet stable
states. This paper explains such ""rational stagnation"" as an equilibrium
sustained by a rational adversary whose utility follows the principle of
potential loss, $u_{D} = U_{ideal} - U_{actual}$. Starting from the Prisoner's
Dilemma, we show that the transformation $u_{i}' = a\,u_{i} + b\,u_{j}$ and the
ratio of mutual recognition $w = b/a$ generate a fragile cooperation band
$[w_{\min},\,w_{\max}]$ where both (C,C) and (D,D) are equilibria. Extending to
a dynamic model with stochastic cooperative payoffs $R_{t}$ and intervention
costs $(C_{c},\,C_{m})$, a Bellman-style analysis yields three strategic
regimes: immediate destruction, rational stagnation, and intervention
abandonment. The appendix further generalizes the utility to a
reference-dependent nonlinear form and proves its stability under reference
shifts, ensuring robustness of the framework. Applications to social-media
algorithms and political trust illustrate how adversarial rationality can
deliberately preserve fragility.",2025
http://arxiv.org/abs/2510.24775v1,Dynamic Spatial Treatment Effects and Network Fragility: Theory and Evidence from European Banking,2025-10-25 06:59:36+00:00,['Tatsuru Kikuchi'],econ.EM,"This paper develops and empirically implements a continuous functional
framework for analyzing systemic risk in financial networks, building on the
dynamic spatial treatment effect methodology established in our previous
studies. We extend the Navier-Stokes-based approach from our previous studies
to characterize contagion dynamics in the European banking system through the
spectral properties of network evolution operators. Using high-quality
bilateral exposure data from the European Banking Authority Transparency
Exercise (2014-2023), we estimate the causal impact of the COVID-19 pandemic on
network fragility using spatial difference-in-differences methods adapted from
our previous studies. Our empirical analysis reveals that COVID-19 elevated
network fragility, measured by the algebraic connectivity $\lambda_2$ of the
system Laplacian, by 26.9% above pre-pandemic levels (95% CI: [7.4%, 46.5%],
p<0.05), with effects persisting through 2023. Paradoxically, this occurred
despite a 46% reduction in the number of banks, demonstrating that
consolidation increased systemic vulnerability by intensifying
interconnectedness-consistent with theoretical predictions from continuous
spatial dynamics. Our findings validate the key predictions from
\citet{kikuchi2024dynamical}: treatment effects amplify over time through
spatial spillovers, consolidation increases fragility when coupling strength
rises, and systems exhibit structural hysteresis preventing automatic reversion
to pre-shock equilibria. The results demonstrate the empirical relevance of
continuous functional methods for financial stability analysis and provide new
insights for macroprudential policy design. We propose network-based capital
requirements targeting spectral centrality and stress testing frameworks
incorporating diffusion dynamics to address the coupling externalities
identified in our analysis.",2025
http://arxiv.org/abs/2510.22086v1,Social preferences or moral concerns: What drives rejections in the Ultimatum game?,2025-10-24 23:54:06+00:00,"['Pau Juan-Bartroli', 'José Ignacio Rivero-Wildemauwe']",econ.TH,"Rejections of positive offers in the Ultimatum Game have been attributed to
different motivations. We show that a model combining social preferences and
moral concerns provides a unifying explanation for these rejections while
accounting for additional evidence. Under the preferences considered, a
positive degree of spite is a necessary and sufficient condition for rejecting
positive offers. This indicates that social preferences, rather than moral
concerns, drive rejection behavior. This does not imply that moral concerns do
not matter. We show that rejection thresholds increase with individuals' moral
concerns, suggesting that morality acts as an amplifier of social preferences.
Using data from van Leeuwen and Alger (2024), we estimate individuals' social
preferences and moral concerns using a finite mixture approach. Consistent with
previous evidence, we identify two types of individuals who reject positive
offers in the Ultimatum Game, but that differ in their Dictator Game behavior.",2025
http://arxiv.org/abs/2510.21959v1,Beliefs about Bots: How Employers Plan for AI in White-Collar Work,2025-10-24 18:36:26+00:00,"['Eduard Brüll', 'Samuel Mäurer', 'Davud Rostam-Afschar']",econ.GN,"We provide experimental evidence on how employers adjust expectations to
automation risk in high-skill, white-collar work. Using a randomized
information intervention among tax advisors in Germany, we show that firms
systematically underestimate automatability. Information provision raises risk
perceptions, especially for routine-intensive roles. Yet, it leaves short-run
hiring plans unchanged. Instead, updated beliefs increase productivity and
financial expectations with minor wage adjustments, implying within-firm
inequality like limited rent-sharing. Employers also anticipate new tasks in
legal tech, compliance, and AI interaction, and report higher training and
adoption intentions.",2025
http://arxiv.org/abs/2510.21943v1,MacroEnergy.jl: A large-scale multi-sector energy system framework,2025-10-24 18:16:48+00:00,"['Ruaridh Macdonald', 'Filippo Pecci', 'Luca Bonaldo', 'Jun Wen Law', 'Yu Weng', 'Dharik Mallapragada', 'Jesse Jenkins']",physics.soc-ph,"MacroEnergy.jl (aka Macro) is an open-source framework for multi-sector
capacity expansion modeling and analysis of macro-energy systems. It is written
in Julia and uses the JuMP package to interface with a wide range of
mathematical solvers. It enables researchers and practitioners to design and
analyze energy and industrial systems that span electricity, fuels, bioenergy,
steel, chemicals, and other sectors. The framework is organized around a small
set of sector-agnostic components that can be combined into flexible graph
structures, making it straightforward to extend to new technologies, policies,
and commodities. Its companion packages support decomposition methods and other
advanced techniques, allowing users to scale models across fine temporal and
spatial resolutions. MacroEnergy.jl provides a versatile platform for studying
energy transitions at the detail and scale demanded by modern research and
policy.",2025
http://arxiv.org/abs/2510.21397v1,Optimal policies for environmental assets under spatial heterogeneity and global awareness,2025-10-24 12:38:50+00:00,"['Emmanuelle Augeraud-Véron', 'Daria Ghilli', 'Fausto Gozzi', 'Marta Leocata']",math.OC,"The aim of this paper is to formulate and study a stochastic model for the
management of environmental assets in a geographical context where in each
place the local authorities take their policy decisions maximizing their own
welfare, hence not cooperating each other. A key feature of our model is that
the welfare depends not only on the local environmental asset, but also on the
global one, making the problem much more interesting but technically much more
complex to study, since strategic interaction among players arise.
  We study the problem first from the $N$-players game perspective and find
open and closed loop Nash equilibria in explicit form. We also study the
convergence of the $N$-players game (when $n\to +\infty$) to a suitable Mean
Field Game whose unique equilibrium is exactly the limit of both the open and
closed loop Nash equilibria found above, hence supporting their meaning for the
game. Then we solve explicitly the problem from the cooperative perspective of
the social planner and compare its solution to the equilibria of the
$N$-players game. Moreover we find the Pigouvian tax which aligns the
decentralized closed loop equilibrium to the social optimum.",2025
http://arxiv.org/abs/2510.21231v1,Scale-robust Auctions,2025-10-24 08:07:05+00:00,"['Jason Hartline', 'Aleck Johnsen', 'Yingkai Li']",cs.GT,"We study auctions that are robust at any scale, i.e., they can be applied to
sell both expensive and cheap items and achieve the best multiplicative
approximations of the optimal revenue in the worst case. We show that the
optimal mechanism is scale invariant, which randomizes between selling at the
second-price and a 2.45 multiple of the second-price.",2025
http://arxiv.org/abs/2510.21178v1,Instance-Adaptive Hypothesis Tests with Heterogeneous Agents,2025-10-24 06:00:44+00:00,"['Flora C. Shi', 'Martin J. Wainwright', 'Stephen Bates']",cs.GT,"We study hypothesis testing over a heterogeneous population of strategic
agents with private information. Any single test applied uniformly across the
population yields statistical error that is sub-optimal relative to the
performance of an oracle given access to the private information. We show how
it is possible to design menus of statistical contracts that pair type-optimal
tests with payoff structures, inducing agents to self-select according to their
private information. This separating menu elicits agent types and enables the
principal to match the oracle performance even without a priori knowledge of
the agent type. Our main result fully characterizes the collection of all
separating menus that are instance-adaptive, matching oracle performance for an
arbitrary population of heterogeneous agents. We identify designs where
information elicitation is essentially costless, requiring negligible
additional expense relative to a single-test benchmark, while improving
statistical performance. Our work establishes a connection between proper
scoring rules and menu design, showing how the structure of the hypothesis test
constrains the elicitable information. Numerical examples illustrate the
geometry of separating menus and the improvements they deliver in error
trade-offs. Overall, our results connect statistical decision theory with
mechanism design, demonstrating how heterogeneity and strategic participation
can be harnessed to improve efficiency in hypothesis testing.",2025
http://arxiv.org/abs/2510.21071v2,"Central Bank Digital Currency, Flight-to-Quality, and Bank-Runs in an Agent-Based Model",2025-10-24 01:00:52+00:00,"['Emilio Barucci', 'Andrea Gurgone', 'Giulia Iori', 'Michele Azzone']",econ.GN,"We analyse financial stability and welfare impacts associated with the
introduction of a Central Bank Digital Currency (CBDC) in a macroeconomic
agent-based model. The model considers firms, banks, and households interacting
on labour, goods, credit, and interbank markets. Households move their
liquidity from deposits to CBDC based on the perceived riskiness of their
banks. We find that the introduction of CBDC exacerbates bank-runs and may lead
to financial instability phenomena. The effect can be changed by introducing a
limit on CBDC holdings. The adoption of CBDC has little effect on macroeconomic
variables but the interest rate on loans to firms goes up and credit goes down
in a limited way. CBDC leads to a redistribution of wealth from firms and banks
to households with a higher bank default rate. CBDC may have negative welfare
effects, but a bound on holding enables a welfare improvement.",2025
http://arxiv.org/abs/2510.23628v1,Matchings Under Biased and Correlated Evaluations,2025-10-24 00:33:52+00:00,"['Amit Kumar', 'Nisheeth K. Vishnoi']",physics.soc-ph,"We study a two-institution stable matching model in which candidates from two
distinct groups are evaluated using partially correlated signals that are
group-biased. This extends prior work (which assumes institutions evaluate
candidates in an identical manner) to a more realistic setting in which
institutions rely on overlapping, but independently processed, criteria. These
evaluations could consist of a variety of informative tools such as
standardized tests, shared recommendation systems, or AI-based assessments with
local noise. Two key parameters govern evaluations: the bias parameter $\beta
\in (0,1]$, which models systematic disadvantage faced by one group, and the
correlation parameter $\gamma \in [0,1]$, which captures the alignment between
institutional rankings. We study the representation ratio, i.e., the ratio of
disadvantaged to advantaged candidates selected by the matching process in this
setting. Focusing on a regime in which all candidates prefer the same
institution, we characterize the large-market equilibrium and derive a
closed-form expression for the resulting representation ratio. Prior work shows
that when $\gamma = 1$, this ratio scales linearly with $\beta$. In contrast,
we show that the representation ratio increases nonlinearly with $\gamma$ and
even modest losses in correlation can cause sharp drops in the representation
ratio. Our analysis identifies critical $\gamma$-thresholds where institutional
selection behavior undergoes discrete transitions, and reveals structural
conditions under which evaluator alignment or bias mitigation are most
effective. Finally, we show how this framework and results enable interventions
for fairness-aware design in decentralized selection systems.",2025
http://arxiv.org/abs/2510.20996v2,SLIM: Stochastic Learning and Inference in Overidentified Models,2025-10-23 20:50:35+00:00,"['Xiaohong Chen', 'Min Seong Kim', 'Sokbae Lee', 'Myung Hwan Seo', 'Myunghyun Song']",econ.EM,"We propose SLIM (Stochastic Learning and Inference in overidentified Models),
a scalable stochastic approximation framework for nonlinear GMM. SLIM forms
iterative updates from independent mini-batches of moments and their
derivatives, producing unbiased directions that ensure almost-sure convergence.
It requires neither a consistent initial estimator nor global convexity and
accommodates both fixed-sample and random-sampling asymptotics. We further
develop an optional second-order refinement achieving full-sample GMM
efficiency and inference procedures based on random scaling and plug-in
methods, including plug-in, debiased plug-in, and online versions of the
Sargan--Hansen $J$-test tailored to stochastic learning. In Monte Carlo
experiments based on a nonlinear demand system with 576 moment conditions, 380
parameters, and $n = 10^5$, SLIM solves the model in under 1.4 hours, whereas
full-sample GMM in Stata on a powerful laptop converges only after 18 hours.
The debiased plug-in $J$-test delivers satisfactory finite-sample inference,
and SLIM scales smoothly to $n = 10^6$.",2025
http://arxiv.org/abs/2510.20992v1,Urban Planning in 3D with a Two-tier LUTI model,2025-10-23 20:42:31+00:00,"['Flora Roumpani', 'Joel Dearden', 'Alan Wilson']",physics.soc-ph,"The two-tier Lowry model brings dynamic simulations of population and
employment directly into the planning process. By linking regional modelling
with neighbourhood design, the framework enables planners to explore how
alternative planning scenarios may evolve over time. The upper tier captures
regional flows of people, jobs, and services, while the lower tier allocates
these to fine-grain zones such as neighbourhoods or parcels. Implemented in
CityEngine, the approach allows interactive visualisation and evaluation of
multi-scale scenarios. A case study in South Yorkshire (UK) illustrates how
regional forecasts can be translated into local design responses, connecting
quantitative modelling with 3D spatial planning.",2025
http://arxiv.org/abs/2510.20986v1,Constrained Mediation: Bayesian Implementability of Joint Posteriors,2025-10-23 20:26:28+00:00,"['David Lagziel', 'Ehud Lehrer']",econ.TH,"We examine information structures in settings with privately informed agents
and an informationally constrained mediator who supplies additional public
signals. Our focus is on characterizing the set of posteriors that the mediator
can induce. To this end, we employ a graph-theoretic framework: states are
represented as vertices, information sets correspond to edges, and a likelihood
ratio function on edges encodes the posterior beliefs. Within this framework,
we derive necessary and sufficient conditions, internal and external
consistency, for the rationalization of posteriors. Finally, we identify
conditions under which a single mediator can implement multiple posteriors,
effectively serving as a generator of Blackwell experiments.",2025
http://arxiv.org/abs/2510.20921v1,Discrete Screening,2025-10-23 18:19:46+00:00,"['Alejandro Francetich', 'Burkhard C. Schipper']",econ.TH,"We consider a principal who wishes to screen an agent with \emph{discrete}
types by offering a menu of \emph{discrete} quantities and \emph{discrete}
transfers. We assume that the principal's valuation is discrete strictly
concave and use a discrete first-order approach. We model the agent's cost
types as non-integer, with integer types as a limit case. Our modeling of cost
types allows us to replicate the typical constraint-simplification results and
thus to emulate the well-treaded steps of screening under a continuum of
contracts.
  We show that the solutions to the discrete F.O.C.s need not be unique
\textit{even under discrete strict concavity}, but we also show that there
cannot be more than two optimal contract quantities for each type, and that --
if there are two -- they must be adjacent. Moreover, we can only ensure weak
monotonicity of the quantities \textit{even if virtual costs are strictly
monotone}, unless we limit the ``degree of concavity'' of the principal's
utility. Our discrete screening approach facilitates the use of
rationalizability to solve the screening problem. We introduce a
rationalizability notion featuring robustness with respect to an open set of
beliefs over types called \textit{$\Delta$-O Rationalizability}, and show that
the set of $\Delta$-O rationalizable menus coincides with the set of usual
optimal contracts -- possibly augmented to include irrelevant contracts.",2025
http://arxiv.org/abs/2510.20918v1,Rationalizable Screening and Disclosure under Unawareness,2025-10-23 18:15:38+00:00,"['Alejandro Francetich', 'Burkhard C. Schipper']",econ.TH,"We analyze a principal-agent procurement problem in which the principal (she)
is unaware some of the marginal cost types of the agent (he). Communication
arises naturally as some types of the agent may have an incentive to raise the
principal's awareness (totally or partially) before a contract menu is offered.
The resulting menu must not only reflect the principal's change in awareness,
but also her learning about types from the agent's decision to raise her
awareness in the first place. We capture this reasoning in a discrete concave
model via a rationalizability procedure in which marginal beliefs over types
are restricted to log-concavity, ``reverse'' Bayesianism, and mild assumptions
of caution.
  We show that if the principal is ex ante only unaware of high-cost types, all
of these types have an incentive raise her awareness of them -- otherwise, they
would not be served. With three types, the two lower-cost types that the
principal is initially aware of also want to raise her awareness of the
high-cost type: Their quantities suffer no additional distortions and they both
earn an extra information rent. Intuitively, the presence of an even higher
cost type makes the original two look better. With more than three types, we
show that this intuition may break down for types of whom the principal is
initially aware of so that raising the principal's awareness could cease to be
profitable for those types. When the principal is ex ante only unaware of more
efficient (low-cost) types, then \textit{no type} raises her awareness, leaving
her none the wiser.",2025
http://arxiv.org/abs/2510.20907v1,The Economics of Convex Function Intervals,2025-10-23 18:02:21+00:00,"['Victor Augias', 'Lina Uhe']",econ.TH,"We introduce convex function intervals (CFIs): families of convex functions
satisfying given level and slope constraints. CFIs naturally arise as
constraint sets in economic design, including problems with type-dependent
participation constraints and two-sided (weak) majorization constraints. Our
main results include: (i) a geometric characterization of the extreme points of
CFIs; (ii) sufficient optimality conditions for linear programs over CFIs; and
(iii) methods for nested optimization on their lower level boundary that can be
applied, e.g., to the optimal design of outside options. We apply these results
to four settings: screening and delegation problems with type-dependent outside
options, contest design with limited disposal, and mean-based persuasion with
informativeness constraints. We draw several novel economic implications using
our tools. For instance, we show that better outside options lead to larger
delegation sets, and that posted price mechanisms can be suboptimal in the
canonical monopolistic screening problem with nontrivial, type-dependent
participation constraints.",2025
http://arxiv.org/abs/2510.20748v1,Reinforcement Learning and Consumption-Savings Behavior,2025-10-23 17:14:49+00:00,['Brandon Kaplowitz'],econ.GN,"This paper demonstrates how reinforcement learning can explain two puzzling
empirical patterns in household consumption behavior during economic downturns.
I develop a model where agents use Q-learning with neural network approximation
to make consumption-savings decisions under income uncertainty, departing from
standard rational expectations assumptions. The model replicates two key
findings from recent literature: (1) unemployed households with previously low
liquid assets exhibit substantially higher marginal propensities to consume
(MPCs) out of stimulus transfers compared to high-asset households (0.50 vs
0.34), even when neither group faces borrowing constraints, consistent with
Ganong et al. (2024); and (2) households with more past unemployment
experiences maintain persistently lower consumption levels after controlling
for current economic conditions, a ""scarring"" effect documented by Malmendier
and Shen (2024). Unlike existing explanations based on belief updating about
income risk or ex-ante heterogeneity, the reinforcement learning mechanism
generates both higher MPCs and lower consumption levels simultaneously through
value function approximation errors that evolve with experience. Simulation
results closely match the empirical estimates, suggesting that adaptive
learning through reinforcement learning provides a unifying framework for
understanding how past experiences shape current consumption behavior beyond
what current economic conditions would predict.",2025
http://arxiv.org/abs/2510.20631v1,Bilevel Programming Problems: A view through Set-valued Optimization,2025-10-23 15:04:19+00:00,"['Kuntal Som', 'Thirumulanathan D', 'Joydeep Dutta']",math.OC,"Bilevel programming is one of the very active areas of research with many
real-life applications in economics and engineering. Bilevel problems are
hierarchical problems consisting of lower-level and upper-level problems,
respectively. The leader or the decision-maker for the upper-level problem
decides first, and then the follower or the lower-level decision-maker chooses
his/her strategy. In the case of multiple lower-level solutions, the bilevel
problems are not well defined, and there are many ways to handle such a
situation. One standard way is to put restrictions on the lower level problems
(like strict convexity) so that nonuniqueness does not arise. However, those
restrictions are not viable in many situations. Therefore, there are two
standard formulations, called pessimistic formulations and optimistic
formulations of the upper-level problem. A set-valued formulation has been
proposed and has been studied in the literature. However, the study is limited
to the continuous set-up with the assumption of value attainment, and the
general case has not been considered. In this paper, we focus on the general
case and study the connection among various notions of solution. Our main
findings suggest that the set-valued formulation may not hold any bigger
advantage than the existing optimistic and pessimistic formulation.",2025
http://arxiv.org/abs/2510.20612v1,Black Box Absorption: LLMs Undermining Innovative Ideas,2025-10-23 14:43:09+00:00,['Wenjun Cao'],cs.CY,"Large Language Models are increasingly adopted as critical tools for
accelerating innovation. This paper identifies and formalizes a systemic risk
inherent in this paradigm: \textbf{Black Box Absorption}. We define this as the
process by which the opaque internal architectures of LLM platforms, often
operated by large-scale service providers, can internalize, generalize, and
repurpose novel concepts contributed by users during interaction. This
mechanism threatens to undermine the foundational principles of innovation
economics by creating severe informational and structural asymmetries between
individual creators and platform operators, thereby jeopardizing the long-term
sustainability of the innovation ecosystem. To analyze this challenge, we
introduce two core concepts: the idea unit, representing the transportable
functional logic of an innovation, and idea safety, a multidimensional standard
for its protection. This paper analyzes the mechanisms of absorption and
proposes a concrete governance and engineering agenda to mitigate these risks,
ensuring that creator contributions remain traceable, controllable, and
equitable.",2025
http://arxiv.org/abs/2510.20606v1,Strategic Costs of Perceived Bias in Fair Selection,2025-10-23 14:38:05+00:00,"['L. Elisa Celis', 'Lingxiao Huang', 'Milind Sohoni', 'Nisheeth K. Vishnoi']",cs.GT,"Meritocratic systems, from admissions to hiring, aim to impartially reward
skill and effort. Yet persistent disparities across race, gender, and class
challenge this ideal. Some attribute these gaps to structural inequality;
others to individual choice. We develop a game-theoretic model in which
candidates from different socioeconomic groups differ in their perceived
post-selection value--shaped by social context and, increasingly, by AI-powered
tools offering personalized career or salary guidance. Each candidate
strategically chooses effort, balancing its cost against expected reward;
effort translates into observable merit, and selection is based solely on
merit. We characterize the unique Nash equilibrium in the large-agent limit and
derive explicit formulas showing how valuation disparities and institutional
selectivity jointly determine effort, representation, social welfare, and
utility. We further propose a cost-sensitive optimization framework that
quantifies how modifying selectivity or perceived value can reduce disparities
without compromising institutional goals. Our analysis reveals a
perception-driven bias: when perceptions of post-selection value differ across
groups, these differences translate into rational differences in effort,
propagating disparities backward through otherwise ""fair"" selection processes.
While the model is static, it captures one stage of a broader feedback cycle
linking perceptions, incentives, and outcome--bridging rational-choice and
structural explanations of inequality by showing how techno-social environments
shape individual incentives in meritocratic systems.",2025
http://arxiv.org/abs/2510.26810v1,Emergent Dynamical Spatial Boundaries in Emergency Medical Services: A Navier-Stokes Framework from First Principles,2025-10-23 11:04:32+00:00,['Tatsuru Kikuchi'],stat.AP,"Emergency medical services (EMS) response times are critical determinants of
patient survival, yet existing approaches to spatial coverage analysis rely on
discrete distance buffers or ad-hoc geographic information system (GIS)
isochrones without theoretical foundation. This paper derives continuous
spatial boundaries for emergency response from first principles using fluid
dynamics (Navier-Stokes equations), demonstrating that response effectiveness
decays exponentially with time: $\tau(t) = \tau_0 \exp(-\kappa t)$, where
$\tau_0$ is baseline effectiveness and $\kappa$ is the temporal decay rate.
Using 10,000 simulated emergency incidents from the National Emergency Medical
Services Information System (NEMSIS), I estimate decay parameters and calculate
critical boundaries $d^*$ where response effectiveness falls below
policy-relevant thresholds. The framework reveals substantial demographic
heterogeneity: elderly populations (85+) experience 8.40-minute average
response times versus 7.83 minutes for younger adults (18-44), with 33.6\% of
poor-access incidents affecting elderly populations despite representing 5.2\%
of the sample. Non-parametric kernel regression validation confirms exponential
decay is appropriate (mean squared error 8-12 times smaller than parametric),
while traditional difference-in-differences analysis validates treatment effect
existence (DiD coefficient = -1.35 minutes, $p < 0.001$). The analysis
identifies vulnerable populations--elderly, rural, and low-income
communities--facing systematically longer response times, informing optimal EMS
station placement and resource allocation to reduce health disparities.",2025
http://arxiv.org/abs/2510.20404v1,Identification and Debiased Learning of Causal Effects with General Instrumental Variables,2025-10-23 10:10:11+00:00,"['Shuyuan Chen', 'Peng Zhang', 'Yifan Cui']",stat.ME,"Instrumental variable methods are fundamental to causal inference when
treatment assignment is confounded by unobserved variables. In this article, we
develop a general nonparametric framework for identification and learning with
multi-categorical or continuous instrumental variables. Specifically, we
propose an additive instrumental variable framework to identify mean potential
outcomes and the average treatment effect with a weighting function. Leveraging
semiparametric theory, we derive efficient influence functions and construct
consistent, asymptotically normal estimators via debiased machine learning.
Extensions to longitudinal data, dynamic treatment regimes, and multiplicative
instrumental variables are further developed. We demonstrate the proposed
method by employing simulation studies and analyzing real data from the Job
Training Partnership Act program.",2025
http://arxiv.org/abs/2510.20372v2,Testing Most Influential Sets,2025-10-23 09:12:29+00:00,"['Lucas Darius Konrad', 'Nikolas Kuschnig']",stat.ML,"Small subsets of data with disproportionate influence on model outcomes can
have dramatic impacts on conclusions, with a few data points sometimes
overturning key findings. While recent work has developed methods to identify
these most influential sets, no formal theory exists to determine when their
influence reflects genuine problems rather than natural sampling variation. We
address this gap by developing a principled framework for assessing the
statistical significance of most influential sets. Our theoretical results
characterize the extreme value distributions of maximal influence and enable
rigorous hypothesis tests for excessive influence, replacing current ad-hoc
sensitivity checks. We demonstrate the practical value of our approach through
applications across economics, biology, and machine learning benchmarks.",2025
http://arxiv.org/abs/2510.20863v1,"State capacity, innovation, and endogenous development in Chile",2025-10-22 22:40:39+00:00,['Rodrigo Barra Novoa'],econ.GN,"The study explores the evolution of Chile's industrial policy from 1990 to
2022 through the lens of state capacity, innovation and endogenous development.
In a global context where governments are reasserting their role as active
agents of innovation, Chile presents a paradox. It is a stable and open economy
that has expanded investment in science and technology but still struggles to
transform this effort into sustainable capabilities. Drawing on the works of
Mazzucato, Aghion, Howitt, Mokyr, Samuelson and Sampedro, the study integrates
evolutionary economics, public policy and humanist ethics. Using a longitudinal
case study approach and official data, it finds that Chile has improved its
innovation institutions but continues to experience weak coordination, regional
inequality and a fragile culture of knowledge. The research concludes that
achieving inclusive innovation requires adaptive governance and an ethical
vision of innovation as a public good.",2025
http://arxiv.org/abs/2510.20066v1,A Multi-Layer Machine Learning and Econometric Pipeline for Forecasting Market Risk: Evidence from Cryptoasset Liquidity Spillovers,2025-10-22 22:36:34+00:00,"['Yimeng Qiu', 'Feihuang Fang']",cs.LG,"We study whether liquidity and volatility proxies of a core set of
cryptoassets generate spillovers that forecast market-wide risk. Our empirical
framework integrates three statistical layers: (A) interactions between core
liquidity and returns, (B) principal-component relations linking liquidity and
returns, and (C) volatility-factor projections that capture cross-sectional
volatility crowding. The analysis is complemented by vector autoregression
impulse responses and forecast error variance decompositions (see Granger 1969;
Sims 1980), heterogeneous autoregressive models with exogenous regressors
(HAR-X, Corsi 2009), and a leakage-safe machine learning protocol using
temporal splits, early stopping, validation-only thresholding, and SHAP-based
interpretation. Using daily data from 2021 to 2025 (1462 observations across 74
assets), we document statistically significant Granger-causal relationships
across layers and moderate out-of-sample predictive accuracy. We report the
most informative figures, including the pipeline overview, Layer A heatmap,
Layer C robustness analysis, vector autoregression variance decompositions, and
the test-set precision-recall curve. Full data and figure outputs are provided
in the artifact repository.",2025
http://arxiv.org/abs/2510.20032v1,Evaluating Local Policies in Centralized Markets,2025-10-22 21:22:04+00:00,"['Dmitry Arkhangelsky', 'Wisse Rutgers']",econ.GN,"We study a policy evaluation problem in centralized markets. We show that the
aggregate impact of any marginal reform, the Marginal Policy Effect (MPE), is
nonparametrically identified using data from a baseline equilibrium, without
additional variation in the policy rule. We achieve this by constructing the
equilibrium-adjusted outcome: a policy-invariant structural object that
augments an agent's outcome with the full equilibrium externality their
participation imposes on others. We show that these externalities can be
constructed using estimands that are already common in empirical work. The MPE
is identified as the covariance between our structural outcome and the reform's
direction, providing a flexible tool for optimal policy targeting and a novel
bridge to the Marginal Treatment Effects literature.",2025
http://arxiv.org/abs/2510.19799v1,"Integrating Transparent Models, LLMs, and Practitioner-in-the-Loop: A Case of Nonprofit Program Evaluation",2025-10-22 17:35:13+00:00,"['Ji Ma', 'Albert Casella']",cs.CY,"Public and nonprofit organizations often hesitate to adopt AI tools because
most models are opaque even though standard approaches typically analyze
aggregate patterns rather than offering actionable, case-level guidance. This
study tests a practitioner-in-the-loop workflow that pairs transparent
decision-tree models with large language models (LLMs) to improve predictive
accuracy, interpretability, and the generation of practical insights. Using
data from an ongoing college-success program, we build interpretable decision
trees to surface key predictors. We then provide each tree's structure to an
LLM, enabling it to reproduce case-level predictions grounded in the
transparent models. Practitioners participate throughout feature engineering,
model design, explanation review, and usability assessment, ensuring that field
expertise informs the analysis at every stage. Results show that integrating
transparent models, LLMs, and practitioner input yields accurate, trustworthy,
and actionable case-level evaluations, offering a viable pathway for
responsible AI adoption in the public and nonprofit sectors.",2025
http://arxiv.org/abs/2510.19672v1,Policy Learning with Abstention,2025-10-22 15:18:29+00:00,"['Ayush Sawarni', 'Jikai Jin', 'Justin Whitehouse', 'Vasilis Syrgkanis']",cs.LG,"Policy learning algorithms are widely used in areas such as personalized
medicine and advertising to develop individualized treatment regimes. However,
most methods force a decision even when predictions are uncertain, which is
risky in high-stakes settings. We study policy learning with abstention, where
a policy may defer to a safe default or an expert. When a policy abstains, it
receives a small additive reward on top of the value of a random guess. We
propose a two-stage learner that first identifies a set of near-optimal
policies and then constructs an abstention rule from their disagreements. We
establish fast O(1/n)-type regret guarantees when propensities are known, and
extend these guarantees to the unknown-propensity case via a doubly robust (DR)
objective. We further show that abstention is a versatile tool with direct
applications to other core problems in policy learning: it yields improved
guarantees under margin conditions without the common realizability assumption,
connects to distributionally robust policy learning by hedging against small
data shifts, and supports safe policy improvement by ensuring improvement over
a baseline policy with high probability.",2025
http://arxiv.org/abs/2510.21843v1,A quality of mercy is not trained: the imagined vs. the practiced in healthcare process-specialized AI development,2025-10-22 14:48:35+00:00,"['Anand Bhardwaj', 'Samer Faraj']",cs.CY,"In high stakes organizational contexts like healthcare, artificial
intelligence (AI) systems are increasingly being designed to augment complex
coordination tasks. This paper investigates how the ethical stakes of such
systems are shaped by their epistemic framings: what aspects of work they
represent, and what they exclude. Drawing on an embedded study of AI
development for operating room (OR) scheduling at a Canadian hospital, we
compare scheduling-as-imagined in the AI design process: rule-bound,
predictable, and surgeon-centric, with scheduling-as-practiced as a fluid,
patient-facing coordination process involving ethical discretion. We show how
early representational decisions narrowed what the AI could support, resulting
in epistemic foreclosure: the premature exclusion of key ethical dimensions
from system design. Our findings surface the moral consequences of abstraction
and call for a more situated approach to designing healthcare
process-specialized artificial intelligence systems.",2025
http://arxiv.org/abs/2510.19630v2,Network Contagion Dynamics in European Banking: A Navier-Stokes Framework for Systemic Risk Assessment,2025-10-22 14:27:12+00:00,['Tatsuru Kikuchi'],econ.EM,"This paper develops a continuous functional framework for analyzing contagion
dynamics in financial networks, extending the Navier-Stokes-based approach to
network-structured spatial processes. We model financial distress propagation
as a diffusion process on weighted networks, deriving a network diffusion
equation from first principles that predicts contagion decay depends on the
network's algebraic connectivity through the relation $\kappa =
\sqrt{\lambda_2/D}$, where $\lambda_2$ is the second-smallest eigenvalue of the
graph Laplacian and $D$ is the diffusion coefficient. Applying this framework
to European banking data from the EBA stress tests (2018, 2021, 2023), we
estimate interbank exposure networks using maximum entropy methods and track
the evolution of systemic risk through the COVID-19 crisis. Our key finding is
that network connectivity declined by 45\% from 2018 to 2023, implying a 26\%
reduction in the contagion decay parameter. Difference-in-differences analysis
reveals this structural change was driven by regulatory-induced deleveraging of
systemically important banks, which experienced differential asset reductions
of 17\% relative to smaller institutions. The networks exhibit lognormal rather
than scale-free degree distributions, suggesting greater resilience than
previously assumed in the literature. Extensive robustness checks across
parametric and non-parametric estimation methods confirm declining systemic
risk, with cross-method correlations exceeding 0.95. These findings demonstrate
that post-COVID-19 regulatory reforms effectively reduced network
interconnectedness and systemic vulnerability in the European banking system.",2025
http://arxiv.org/abs/2510.19511v2,Compensation-based risk-sharing,2025-10-22 12:05:05+00:00,"['Jan Dhaene', 'Atibhav Chaudhry', 'Ka Chun Cheung', 'Austin Riis-Due']",q-fin.RM,"This paper studies the mathematical problem of allocating payouts
(compensations) in an endowment contingency fund using a risk-sharing rule that
satisfies full allocation. Besides the participants, an administrator manages
the fund by collecting ex-ante contributions to establish the fund and
distributing ex-post payouts to members. Two types of administrators are
considered. An 'active' administrator both invests in the fund and receives the
payout of the fund when no participant receives a payout. A 'passive'
administrator performs only administrative tasks and neither invests in nor
receives a payout from the fund. We analyze the actuarial fairness of both
compensation-based risk-sharing schemes and provide general conditions under
which fairness is achieved. The results extend earlier work by Denuit and
Robert (2025) and Dhaene and Milevsky (2024), who focused on payouts based on
Bernoulli distributions, by allowing for general non-negative loss
distributions.",2025
http://arxiv.org/abs/2510.19450v1,Towards a feminist understanding of digital platform work,2025-10-22 10:27:45+00:00,['Clara Punzi'],cs.CY,"The rapid growth of the digital platform economy is transforming labor
markets, offering new employment opportunities with promises of flexibility and
accessibility. However, these benefits often come at the expense of increased
economic exploitation, occupational segregation, and deteriorating working
conditions. Research highlights that algorithmic management disproportionately
impacts marginalized groups, reinforcing gendered and racial inequalities while
deepening power imbalances within capitalist systems. This study seeks to
elucidate the complex nature of digital platform work by drawing on feminist
theories that have historically scrutinized and contested the structures of
power within society, especially in the workplace. It presents a framework
focused on four key dimensions to lay a foundation for future research: (i)
precarity and exploitation, (ii) surveillance and control, (iii) blurring
employment boundaries, and (iv) colonial legacies. It advocates for
participatory research, transparency in platform governance, and structural
changes to promote more equitable conditions for digital platform workers.",2025
http://arxiv.org/abs/2510.19426v1,"Using did_multiplegt_dyn to Estimate Event-Study Effects in Complex Designs: Overview, and Four Examples Based on Real Datasets",2025-10-22 09:52:27+00:00,"['Clément de Chaisemartin', 'Diego Ciccia', 'Felix Knau', 'Mélitine Malézieux', 'Doulo Sow', 'David Arboleda', 'Romain Angotti', ""Xavier D'Haultfoeuille"", 'Bingxue Li', 'Henri Fabre', 'Anzony Quispe']",econ.EM,"The command did_multiplegt_dyn can be used to estimate event-study effects in
complex designs with a potentially non-binary and/or non-absorbing treatment.
This paper starts by providing an overview of the estimators computed by the
command. Then, simulations based on three real datasets are used to demonstrate
the estimators' properties. Finally, the command is used on four real datasets
to estimate event-study effects in complex designs. The first example has a
binary treatment that can turn on an off. The second example has a continuous
absorbing treatment. The third example has a discrete multivalued treatment
that can increase or decrease multiple times over time. The fourth example has
two, binary and absorbing treatments, where the second treatment always happens
after the first.",2025
http://arxiv.org/abs/2510.19377v1,Government Transparency Affects Innovation: Evidence from Wireless Products,2025-10-22 08:51:35+00:00,['Šimon Trlifaj'],econ.GN,"Does government transparency affect innovation? I evaluate the launch of a
government database with detailed technical information on the universe of
wireless-enabled products on the U.S. market (N 347 thousand). The results show
the launch approximately doubled the use of new technologies in the following
ten years, an indicator of follow-on innovation. The increase affected both
products in the same and new product classes, suggesting novelty; waned over
several years, potentially due to an increase in secrecy and patenting; and
boosted foreign more than U.S. domestic competitors. These results highlight
the importance of information for private sector innovation.",2025
http://arxiv.org/abs/2510.20854v1,Edgeworth's exact and naturally weighted evolutionary utilitarianism and the happiness of Mr. Pongo,2025-10-22 07:41:10+00:00,['Alberto Baccini'],econ.GN,"This article challenges the conventional reading of Francis Ysidro Edgeworth
by reconstructing his intellectual project of unifying the moral sciences
through mathematics. The contribution he made in the first phase of his
writing, culminating in \textit{Mathematical Psychics}, aimed to reconfigure
utilitarianism as an exact science, grounding it in psychophysics and
evolutionary biology. In order to solve the utilitarian problem of maximizing
pleasure for a given set of sentient beings, he modeled individuals as
``quasi-Fechnerian'' functions, which incorporated their capacity for pleasure
as determined by their place in the evolutionary order. The problem of
maximization is solved by distributing means according to the individuals'
capacity for pleasure. His radical anti-egalitarian conclusions did not stem
from an abstract principle of justice, but from the necessity to maximize
welfare among naturally unequal beings. This logic was applied not only to
sentients of different evolutionary orders, such as Mr. Pongo, a famous
gorilla, and humans, but also to human races, sexes, and classes. The system,
in essence, uses the apparent neutrality of science to naturalize and justify
pre-existing social hierarchies. This analysis reveals that the subsequent
surgical removal of his utilitarianism by economists, starting with Schumpeter,
while making his tools palatable, eviscerates his overarching philosophical
system.",2025
http://arxiv.org/abs/2510.19306v1,Topology of Currencies: Persistent Homology for FX Co-movements: A Comparative Clustering Study,2025-10-22 07:10:54+00:00,"['Pattravadee de Favereau de Jeneret', 'Ioannis Diamantis']",stat.ML,"This study investigates whether Topological Data Analysis (TDA) can provide
additional insights beyond traditional statistical methods in clustering
currency behaviours. We focus on the foreign exchange (FX) market, which is a
complex system often exhibiting non-linear and high-dimensional dynamics that
classical techniques may not fully capture. We compare clustering results based
on TDA-derived features versus classical statistical features using monthly
logarithmic returns of 13 major currency exchange rates (all against the euro).
Two widely-used clustering algorithms, \(k\)-means and Hierarchical clustering,
are applied on both types of features, and cluster quality is evaluated via the
Silhouette score and the Calinski-Harabasz index. Our findings show that
TDA-based feature clustering produces more compact and well-separated clusters
than clustering on traditional statistical features, particularly achieving
substantially higher Calinski-Harabasz scores. However, all clustering
approaches yield modest Silhouette scores, underscoring the inherent difficulty
of grouping FX time series. The differing cluster compositions under TDA vs.
classical features suggest that TDA captures structural patterns in currency
co-movements that conventional methods might overlook. These results highlight
TDA as a valuable complementary tool for analysing financial time series, with
potential applications in risk management where understanding structural
co-movements is crucial.",2025
